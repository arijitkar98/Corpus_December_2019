1.6 Performance 
281.7 The Power Wall 
401.8 The Sea Change: The Switch from Uniprocessors to 
Multiprocessors 
431.9 Real Stuff: Benchmarking the Intel Core i7 
461.10 Fallacies and Pitfalls 
491.11 Concluding Remarks 
521.12 Historical Perspective and Further Reading 
541.13 Exercises 54 1.1 IntroductionWelcome to this book! We’re delighted to have this opportunity to convey the 
excitement of the world of computer system
 is is not a dry and dreary
 eld, where progress is glacial and where new ideas atrophy from neglect. No! Computers 

are the product of the incredibly vibrant information technology industry, all 

aspects of which are responsible for almost 10% of the gross national product of 

the United States, and whose economy has become dependent in part on the rapid 

improvements in information technology promised by Moore’s Law
 is unusual 
industry embraces innovation at a breath-taking rate. In the last 30 years, there have 

been a number of new computers whose introduction appeared to revolutionize 

the computing industry; these revolutions were cut short only because someone 

else built an even better computer.
 is race to innovate has led to unprecedented progress since the inception 
of electronic computing in the late 1940s. Had the transportation industry kept 

pace with the computer industry, for example, today we could travel from New 

York to London in a second for a penny. Take just a moment to contemplate how 

such an improvement would change society—living in Tahiti while working in San 

Francisco, going to Moscow for an evening at the Bolshoi Ballet—and you can 

appreciate the implications of such a change.

4 Chapter 1 Computer Abstractions and Technology
Computers have led to a third revolution for civilization, with the information 
revolution taking its place alongside the agricultural and the industrial revolutions. 
 e resulting multiplication of humankind’s intellectual strength and reach 
naturally has a
 ected our everyday lives profoundly and changed the ways in which 
the search for new knowledge is carried ou
 ere is now a new vein of scien
 c investigation, with computational scientists joining theoretical and experimental 

scientists in the exploration of new frontiers in astronomy, biology, chemistry, and 

physics, among others.
 e computer revolution continues. Each time the cost of computing improves 
by another factor of 10, the opportunities for computers multiply. Applications that 

were economically infeasible suddenly beco
me practical. In the recent past, the 
following applications were “computer s
 ction.”
 Computers in automobiles:
 Until microprocessors improved dramatically 
in price and performance in the early 1980s, computer control of cars was 

ludicrous. Today, computers reduce pollution, improve f
  ciency via 
engine controls, and increase safety through blind spot warnings, lane 

departure warnings, moving object detection, and a
 ation to protect 
occupants in a crash.
 Cell phones:
 Who would have dreamed that advances in computer 
systems would lead to more than half of the planet having mobile phones, 

allowing person-to-person communication to almost anyone anywhere in 

the world?
 Human genome project:
 e cost of computer equipment to map and analyze 
human DNA sequences was hundreds of mil
lions of dollars. It’s unlikely that 
anyone would have considered this project had the computer costs been 10 

to 100 times higher, as they would have been 15 to 25 years earlier. Moreover, 

costs continue to drop; you will soon be able to acquire your own genome, 

allowing medical care to be tailored to you.
 World Wide Web:
 Not in existence at the time of th
 rst edition of this book, 
the web has transformed our society. For many, the web has replaced libraries 

and newspapers.
 Search engines:
 As the content of the web grew in size and in value,
 nding 
relevant information became increasingly important. Today, many people 

rely on search engines for such a large part of their lives that it would be a 

hardship to go without them.
Clearly, advances in this technology now a
 ect almost every aspect of our 
society. Hardware advances have allowed programmers to create wonderfully 

useful so
 ware, which explains why computers are omnipresent. Today’s science 
 ction suggests tomorrow’s killer applications: already on their way are glasses that 
augment reality, the cashless society, and cars that can drive themselves.

 1.1 Introduction 5Classes of Computing Applications and Their CharacteristicsAlthough a common set of hardware technologies (see Sections 1.4 and 1.5) is used 
in computers ranging from smart home appliances to cell phones to the largest 

supercomputers, thes
 erent applications hav
 erent design requirements 
and employ the core hardware techno
 erent ways. Broadly speaking, 
computers are used in thre
 erent classes of applications.
Personal computers (PCs)
 are possibly the best known form of computing, 
which readers of this book have likely used extensively. Personal computers 
emphasize delivery of good performance to single users at low cost and usually 

execute third-party so
 ware. 
 is class of computing drove the evolution of many 
computing technologies, which is only about 35 years old!
Servers
 are the modern form of what were once much larger computers, and 
are usually accessed only via a network. Servers are oriented to carrying large 
workloads, which may consist of either single complex applications—usually a 

scien
 c or engineering application—or handling many small jobs, such as would 
occur in building a large web server
 ese applications are usually based on 
so
 ware from another source (such as a database or simulation system), but are 
 en mo
 ed or customized for a particular function. Servers are built from the 
same basic technology as desktop computers, but provide for greater computing, 

storage, and input/output capacity. In general, servers also place a greater emphasis 

on dependability, since a crash is usually mo
re costly than it would be on a single-
user PC.
Servers span the widest range in cost and capability. At the low end, a server 
may be little more than a desktop computer without a screen or keyboard and 

cost a thousand dollar
 ese low-end servers are typically used fo
 le storage, 
small business applications, or simple web serving (see Section 6.10). At the other 

extreme are 
supercomputers
, which at the present consist of tens of thousands of 
processors and many 
terabytes
 of memory, and cost tens to hundreds of millions 
of dollars. Supercomputers are usually used for high-end scien
 c and engineering 
calculations, such as weather forecasting, oil exploration, protein structure 

determination, and other large-scale problems. Although such supercomputers 

represent the peak of computing capability, they represent a relatively small fraction 

of the servers and a relatively small fraction of the overall computer market in 

terms of total revenue.
Embedded computers
 are the largest class of computers and span the widest 
range of applications and performance. Embedded computers include the 
microprocessors found in your car, the computers in a television set, and the 

networks of processors that control a modern airplane or cargo ship. Embedded 

computing systems are designed to run one application or one set of related 

applications that are normally integrated with the hardware and delivered as a 

single system; thus, despite the large number of embedded computers, most users 

never really see that they are using a computer!
personal computer 
(PC) A computer 
designed for use by 
an individual, usually 

incorporating a graphics 

display, a keyboard, and a 

mouse.
server
 A computer 
used for running 

larger programs for 

multiple users, o
 en simultaneously, and 

typically accessed only via 

a network.
supercomputer
 A class 
of computers with the 

highest performance and 

cost; they are co
 gured 
as servers and typically 

cost tens to hundreds of 

millions of dollars.
terabyte (TB)
 Originally 
1,099,511,627,776 
(240) bytes, although 
communications and 

secondary storage 

systems developers 

started using the term to 

mean 1,000,000,000,000 

(1012) bytes. To reduce 
confusion, we now use the 

term 
tebibyte (TiB)
 for 
240 byt
 ning 
terabyte
 (TB) to mean 10
12 bytes. 
Figure 1.1
 shows the full 

range of decimal and 

binary values and names.
embedded computer
 A computer inside another 

device used for running 

one predetermined 

application or collection of 

so
 ware.

6 Chapter 1 Computer Abstractions and Technology
Embedded applications o
 en have unique application requirements that 
combine a minimum performance with stringent limitations on cost or power. For 
example, consider a music player: the processor need only be as fast as necessary 

to handle its limited function, and beyond that, minimizing cost and power are the 

most important objectives. Despite their low cost, embedded computers o
 en have 
lower tolerance for failure, since the results can vary from upsetting (when your 

new television crashes) to devastating (such as might occur when the computer in a 

plane or cargo ship crashes). In consumer-oriented embedded applications, such as 

a digital home appliance, dependability is achieved primarily through simplicity—

the emphasis is on doing one function as perfectly as possible. In large embedded 

systems, techniques of redundancy from the server world are o
 en employed. 
Although this book focuses on general-purpose computers, most concepts apply 

directly, or with slight mo
 cations, to embedded computers.
Elaboration: Elaborations are short sections used throughout the text to provide more 
detail on a particular subject that may be of interest. Disinterested readers may skip 
over an elaboration, since the subsequent material will never depend on the contents 

of the elaboration.Many embedded processors are designed using 
processor cores
, a version of a 
processor written in a hardware description language, such as Verilog or VHDL (see 

 c hardware 

with the processor core for fabrication on a single chip.Welcome to the PostPC Era
 e continuing march of technology brings about generational changes in 
computer hardware that shake up the entire information technology industry. 
Since the last edition of the book we have undergone such a change
 cant 
in the past as the switch starting 30 years ago to personal computers. Replacing the 
FIGURE 1.1 The 2X vs. 10Y bytes ambiguity was resolved by adding a binary notation for 
all the common size terms.
 In the last column we note how much larger the binary term is than its 
corresponding decimal term, which is compounded as we head down the char
 ese pr
 xes work for bits 
as well as bytes, so 
gigabit
 (Gb) is 109 bits while 
gibibits
 (Gib) is 2
30 bits.
Decimal term
Abbreviation
Value
Binary 
term
Abbreviation
Value
% Largerkilobyte
KB103kibibyte
KiB2102%megabyteMB10
6mebibyteMiB
2205%gigabyte
GB109gibibyte
GiB2307%terabyte
TB1012tebibyte
TiB24010%petabytePB10
15pebibytePiB
25013%exabyteEB10
18exbibyteEiB
26015%zettabyteZB10
21zebibyteZiB
27018%yottabyteYB10
24yobibyteYiB
28021%
 1.1 Introduction 70200400
60080010001200
1400200720082009201020112012
Tablet
Smart phone sales
MillionsPC (not including tablet)Cell phone (not 
including smart phone)FIGURE 1.2 The number manufactured per year of tablets and smart phones, which 
reﬂ ect the PostPC era, versus personal computers and traditional cell phones.
 Smart phones 
represent the recent growth in the cell phone industry, and they passed PCs in 2011. Tablets are the fastest 
growing category, nearly doubling between 2011 and 2012. Recent PCs and traditional cell phone categories 

are relatively
 at or declining.  
PC is the 
personal mobile device (PMD)
. PMDs are battery operated with wireless 
connectivity to the Internet and typically cost hundreds of dollars, and, like PCs, 
users can download so
 ware (“apps”) to run on them. Unlike PCs, they no longer 
have a keyboard and mouse, and are more likely to rely on a touch-sensitive screen 
or even speech input. Today’s PMD is a smart phone or a tablet computer, but 

tomorrow it may include electronic glasses. 
Figure 1.2
 shows the rapid growth time 

of tablets and smart phones versus that of PCs and traditional cell phones.
Taking over from the traditional server is 
Cloud Computing
, which relies upon 
giant datacenters that are now known as 
Warehouse Scale Computers
 (WSCs). 
Companies like Amazon and Google build these WSCs containing 100,000 servers 

and then let companies rent portions of them so that they can provide so
 ware 
services to PMDs without having to build WSCs of their own. Indeed, 
So
 ware as 
a Service (SaaS)
 deployed via the cloud is revolutionizing the so
 ware industry just 
as PMDs and WSCs are revolutionizing the hardware industry. Today’s so
 ware 
developers will o
 en have a portion of their application that runs on the PMD and 
a portion that runs in the Cloud.
What You Can Learn in This Book
Successful programmers have always been concerned about the performance of 

their programs, because getting results to the user quickly is critical in creating 

successful so
 ware. In the 1960s and 1970s, a primary constraint on computer 
performance was the size of the computer’s memory
 us, programmers o
 en followed a simple credo: minimize memory space to make programs fast. In the 
Personal mobile 
devices (PMDs)
 are 
small wireless devices to 
connect to the Internet; 

they rely on batteries for 

power, and so
 ware is 
installed by downloading 

apps. Conventional 

examples are smart 

phones and tablets. 
Cloud Computing
  refers 
to large collections of 

servers that provide services 

over the Internet; some 

providers rent dynamically 

varying numbers of servers 

as a utility.
So
 ware as a Service 
(SaaS)
 delivers so
 ware 
and data as a service over 

the Internet, usually via 

a thin program such as a 

browser that runs on local 

client devices, instead of 

binary code that must be 

installed, and runs wholly 

on that device. Examples 

include web search and 

social networking.

8 Chapter 1 Computer Abstractions and Technology
last decade, advances in computer design and memory technology have greatly 
reduced the importance of small memory size in most applications other than 

those in embedded computing systems.
Programmers interested in performance now need to understand the issues 
that have replaced the simple memory model of the 1960s: the parallel nature 

of processors and the hierarchical nature of memories. Moreover, as we explain 

in Section 1.7, today’s programmers need to worry about energ
  ciency of 
their programs running either on the PMD or in the Cloud, which also requires 

understanding what is below your code. Programmers who seek to build 

competitive versions of so ware will therefore need to increase their knowledge of 

computer organization.
We are honored to have the opportunity to explain what’s inside this revolutionary 
machine, unraveling the so
 ware below your program and the hardware under the 
covers of your computer. By the time you complete this book, we believe you will 

be able to answer the following questions:
 How are programs written in a high-level language, such as C or Java, 
translated into the language of the hardware, and how does the hardware 

execute the resulting program? Comprehending these concepts forms the 

basis of understanding the aspects of both the hardware and so
 ware that 
 ect program performance.
 What is the interface between the so
 ware and the hardware, and how does 
so
 ware instruct the hardware to perform needed function
 ese concepts 
are vital to understanding how to write many kinds of so
 ware.
 What determines the performance of a program, and how can a programmer 
improve the performance? As we will see, this depends on the original 

program, the so
 ware translation of that program into the computer’s 
language, and th
 ectiveness of the hardware in executing the program.
 What techniques can be used by hardware designers to improve performance? 
 is book will introduce the basic concepts of modern comput
 e interested re
 nd much more material on this topic in our advanced 
book, 
Computer Architecture: A Quantitative Approach
. What techniques can be used by hardware designers to improve energy 
  ciency? What can the programmer do to help or hinder energ
  ciency?
 What are the reasons for and the consequences of the recent switch from 
sequential processing to parallel processin
 is book gives the motivation, 
describes the current hardware mechanisms to support parallelism, and 

surveys the new generation of  
“multicore” microprocessors
 (see Chapter 6).
 Since th
 rst commercial computer in 1951, what great ideas did computer 
architects come up with that lay the foundation of modern computing?
multicore 
microprocessor
 A microprocessor 
containing multiple 

processors (“cores”) in a 

single integrated circuit.

 1.1 Introduction 9Without understanding the answers to these questions, improving the 
performance of your program on a modern computer or evaluating what features 
might make one computer better than another for a particular application will be 

a complex process of trial and error, rather than a scien
 c procedure driven by 
insight and analysis.
 is 
 rst chapter lays the foundation for the rest of the book. It introduces the 
basic ideas an
 nitions, places the major components of so
 ware and hardware 
in perspective, shows how to evaluate performance and energy, introduces 

integrated circuits (the technology that fuels the computer revolution), and explains 

th
  to multicores.
In this chapter and later ones, you will likely see many new words, or words 
that you may have heard but are not sure what they mean. Don’t panic! Yes, there 

is a lot of special terminology used in describing modern computers, but the 

terminology actually helps, since it enables us to describe precisely a function or 

capability. In addition, computer designers (including your authors) 
love
 using 
acronyms
, which are 
easy to understand once you know what the letters stand for! 
To help you remember and locate terms, we have included a 
highlighted
 nition 
of every term in the margins th
 rst time it appears in the t
 er a short 
time of working with the terminology, you will be
 uent, and your friends will 
be impressed as you correctly use acronyms such as BIOS, CPU, DIMM, DRAM, 

PCIe, SATA, and many others.
To reinforce how the so
 ware and hardware systems used to run a program will 
 ect performance, we use a special section, 
Understanding Program Performance
, throughout the book to summarize important insights into program performance. 

 e 
 rst one appears below.
 e performance of a program depends on a combination of th
 ectiveness of the 
algorithms used in the program, the so
 ware systems used to create and translate 
the program into machine instructions, and th
 ectiveness of the computer in 
executing those instructions, which may include input/output (I/O) operations. 

 is table summarizes how the hardware and so
 ware a
 ect performance.
Hardware or software 
componentHow this component affects performance
Where is this 
topic covered?
AlgorithmDetermines both the number of source-level 
statements and the number of I/O operations executedOther books!Programming language, 

compiler, and architecture
Determines the number of computer instructions 

for each source-level statement
Chapters 2 and 3
Processor and memory 

systemDetermines how fast instructions can be executedChapters 4, 5, and 6
I/O system (hardware and 

operating system)Determines how fast I/O operations may be 

executedChapters 4, 5, and 6
acronym
 A word 
constructed by taking the 
initial letters of a string 

of words. For example: 
RAM
 is an acronym for 
Random Access Memory, 

and 
CPU is an acronym 
for Central Processing 

Unit.
Understanding 
Program 

Performance

10 Chapter 1 Computer Abstractions and Technology
To demonstrate the impact of the ideas in this book, we improve the performance 
of a C program that multiplies a matrix times a vector in a sequence of 

chapters. Each step leverages understanding how the underlying hardware 

really works in a modern microprocessor to improve performance by a factor 

of 200!
 In the category of 
data level parallelism
, in Chapter 3 we use 
subword 
parallelism via C intrinsics
 to increase performance by a factor of 3.8.
 In the category of 
instruction level parallelism
, in Chapter 4 we use 
loop unrolling to exploit multiple instruction issue and out-of-order execution 

hardware
 to increase performance by another factor of 2.3.
 In the category of 
memory hierarchy optimization
, in Chapter 5 we use 
cache blocking
 to increase performance on large matrices by another factor 
of 2.5.
 In the category of 
thread level parallelism
, in Chapter 6 we use 
parallel for 
loops in OpenMP to exploit multicore hardware
 to increase performance by 
another factor of 14.
Check Yourself
 sections are designed to help readers assess whether they 
comprehend the major concepts introduced in a chapter and understand the 

implications of those concepts. Some 
Check Yourself
 questions have simple answers; 
others are for discussion among a group. Answers to the sp
 c questions can 
be found at the end of the chapter. 
Check Yourself
 questions appear only at the 
end of a section, making it easy to skip them if you are sure you understand the 

material.
 e number of embedded processors sold every year greatly outnumbers 
the number of PC and even PostPC processors. Can you co
 rm or deny 
this insight based on your own experience? Try to count the number of 

embedded processors in your home. How does it compare with the number 

of conventional computers in your home?
2. As mentioned earlier, both the so
 ware and hardware a
 ect the performance 
of a program. Can you think of examples where each of the following is the 

right place to look for a performance bottleneck?
 e algorithm chosen
 e programming language or compiler
 e operating system
 e processor
 e I/O system and devices
Check Yourself

 1.2 Eight Great Ideas in Computer Architecture 
11 1.2  Eight Great Ideas in Computer ArchitectureWe now introduce eight great ideas that computer architects have been invented in 
the last 60 years of comput
 ese ideas are so powerful they have lasted 
long a er th
 rst computer that used them, with newer architects demonstrating 
their admiration by imitating their predecessor
 ese great ideas are themes that 
we will weave through this and subsequent chapters as examples arise. To point 

out th
 uence, in this section we introduce icons and highlighted terms that 
represent the great ideas and we use them to identify the nearly 100 sections of the 

book that feature use of the great ideas.
Design for Moore’s Law
 e one constant for computer designers is rapid change, which is driven largely by 
Moore’s Law
. It states that integrated circuit resources double every 18–24 months. 

Moore’s Law resulted from a 1965 prediction of such growth in IC capacity made 

by Gordon Moore, one of the founders of Intel. As computer designs can take years, 

the resources available per chip can easily double or quadruple between the start 

an
 nish of the project. Like a skeet shooter, computer architects must anticipate 
where the technology will be when th
 nishes rather than design for where 
it starts. We use an “up and to the right” Moore’s Law graph to represent designing 

for rapid change.
Use Abstraction to Simplify DesignBoth computer architects and programmers had to invent techniques to make 

themselves more productive, for otherwise design time would lengthen as 

dramatically as resources grew by Moore’s Law. A major productivity technique for 

hardware and so
 ware is to use 
abstractions
 to represent the design a
 erent 
levels of representation; lower-level details are hidden to o
 er a simpler model at 
higher levels. We’ll use the abstract painting icon to represent this second great 

idea.
Make the Common Case Fast
Making the 
common case fast
 will tend to enhance performance better than 
optimizing the rare case. Ironically, the common case is o
 en simpler than the 
rare case and hence is o
 en easier to enhance
 is common sense advice implies 
that you know what the common case is, which is only possible with careful 

experimentation and measurement (see Section 1.6). We use a sports car as the 

icon for making the common case fast, as the most common trip has one or two 

passengers, and it’s surely easier to make a fast sports car than a fast minivan!

12 Chapter 1 Computer Abstractions and Technology
Performance via Parallelism
Since the dawn of computing, computer architects have o
 ered designs that get 
more performance by performing operations in parallel. We’ll see many examples 
of parallelism in this book. We use multiple jet engines of a plane as our icon for 

parallel performance
.Performance via Pipelining
A particular pattern of parallelism is so prevalent in computer architecture that 

it merits its own name: 
pipelining
. For example, befor
 re engines, a “bucket 
brigade” would respond t
 re, which many cowboy movies show in response to 
a dastardly act by the villa
 e townsfolk form a human chain to carry a water 
source t
 re, as they could much more quickly move buckets up the chain instead 
of individuals running back and forth. Our pipeline icon is a sequence of pipes, 

with each section representing one stage of the pipeline.
Performance via Prediction
Following the saying that it can be better to ask for forgiveness than to ask for 

permission, th
 nal great idea is 
prediction
. In some cases it can be faster on 
average to guess and start working rather than wait until you know for sure, 

assuming that the mechanism to recover from a misprediction is not too expensive 

and your prediction is relatively accurate. We use the fortune-teller’s crystal ball as 

our prediction icon.
Hierarchy of Memories
Programmers want memory to be fast, large, and cheap, as memory speed o
 en 
shapes performance, capacity limits the size of problems that can be solved, and the 

cost of memory today is o
 en the majority of computer cost. Architects have found 
that they can address these co
 icting demands with a 
hierarchy of memories
, with 
the fastest, smallest, and most expensive memory per bit at the top of the hierarchy 

and the slowest, largest, and cheapest per bit at the bottom. As we shall see in 

Chapter 5, caches give the programmer the illusion that main memory is nearly 

as fast as the top of the hierarchy and nearly as big and cheap as the bottom of 

the hierarchy. We use a layered triangle icon to represent the memory hierarchy. 

 e shape indicates speed, cost, and size: the closer to the top, the faster and more 
expensive per bit the memory; the wider the base of the layer, the bigger the memory.
Dependability via RedundancyComputers not only need to be fast; they 
need to be dependable. Since any physical 
device can fail, we make systems 
dependable
 by including redundant components that 
can take over when a failure occurs 
and
 to help detect failures. We use the tractor-trailer 
as our icon, since the dual tires on each side of its rear axels allow the truck to continue 

driving even when one tire fails. (Presumably, the truck driver heads immediately to a 

repair facility so th
 at tire can be
 xed, thereby restoring redundancy!)

 1.3 Below Your Program 
13 1.3 Below Your Program
A typical application, such as a word processor or a large database system, may 
consist of millions of lines of code and rely on sophisticated so
 ware libraries that 
implement complex functions in support of the application. As we will see, the 

hardware in a computer can only execute extremely simple low-level instructions. 

To go from a complex application to the simple instructions involves several layers 

of s
 ware that interpret or translate high-level operations into simple computer 
instructions, an example of the great idea of 
abstraction
.Figure 1.3
 shows that these layers of so
 ware are organized primarily in a 
hierarchical fashion, with applications being the outermost ring and a variety of 
systems so
 ware sitting between the hardware and applications so
 ware.
 ere are many types of systems so
 ware, but two types of systems so
 ware 
are central to every computer system today: an operating system and a compiler. 

An operating system
 interfaces between a user’s program and the hardware 
and provides a variety of services and supervisory functions. Among the most 
important functions are:
 Handling basic input and output operations
 Allocating storage and memory
 Providing for protected sharing of the computer among multiple applications 
using it simultaneously.
Examples of operating systems in use today are Linux, iOS, and Windows.
In Paris they simply 

stared when I spoke to 

them in French; I never 

did succeed in making 

those idiots understand 

their own language.
Mark Twain, 
 e Innocents Abroad
, 1869systems so
 ware
 So
 ware that provides 
services that are 
commonly useful, 

including operating 

systems, compilers, 

loaders, and assemblers.
operating system
 Supervising program that 

manages the resources of 

a computer for the b
 t of the programs that run 

on that computer.
Applications software Systems software Hardware
FIGURE 1.3 A simpliﬁ ed view of hardware and software as hierarchical layers, shown as 
concentric circles with hardware in the center and applications software outermost.
 In 
complex applications, there are o
 en multiple layers of application so ware as well. For example, a database 
system may run on top of the systems so
 ware hosting an application, which in turn runs on top of the 
database.

14 Chapter 1 Computer Abstractions and Technology
Compilers
 perform another vital function: the translation of a program written 
in a high-level language, such as C, C
, Java, or Visual Basic into instructions 
that the hardware can execute. Given the sophistication of modern programming 
languages and the simplicity of the instructions executed by the hardware, the 

translation from a high-level language program to hardware instructions is 

complex. We give a brief overview of the process here and then go into more depth 

in Chapter 2 and in Appendix A.
From a High-Level Language to the Language of Hardware
To actually speak to electronic hardware, you need to send electr
 e easiest signals for computers to understand are 
on and 
 , and so the computer 
alphabet is just two letters. Just as the 26 letters of the English alphabet do not limit 

how much can be written, the two letters of the computer alphabet do not limit 

what computers can do
 e two symbols for these two letters are the numbers 0 
and 1, and we commonly think of the computer language as numbers in base 2, or 

binary numbers
. We refer to each “letter” as a 
binary digit
 or 
bit. Computers are 
slaves to our commands, which are called 
instructions
. Instructions, which are just 
collections of bits that the computer understands and obeys, can be thought of as 
numbers. For example, the bits
1000110010100000tell one computer to add two numbers. Chapter 2 explains why we use numbers 

for instructions 
and
 data; we don’t want to steal that chapter’s thunder, but using 
numbers for both instructions and data is a foundation of computing.
 e 
 rst programmers communicated to computers in binary numbers, but this 
was so tedious that they quickly invented new notations that were closer to the way 

humans think. A
 rst, these notations were translated to binary by hand, but this 
process was still tiresome. Using the computer to help program the computer, the 

pioneers invented programs to translate from symbolic notation to binary
 e 
 rst of 
these programs was named an 
assembler
 is program translates a symbolic version 
of an instruction into the binary version. For example, the programmer would write
add A,Band the assembler would translate this notation into
1000110010100000 is instruction tells the computer to add the two numbers 
A and 
B e name coined 
for this symbolic language, still used today, is 
assembly language
. In contrast, the 
binary language that the machine understands is the 
machine language
.Although a tremendous improvement, assembly language is still far from the 
notations a scientist might like to use to simulat
 ow or that an accountant 
might use to balance the books. Assembly language requires the programmer 

to write one line for every instruction that the computer will follow, forcing the 

programmer to think like the computer.
compiler
 A program 
that translates high-level 
language statements 

into assembly language 

statements.
binary digit 
Also called 
a bit. One of the two 
numbers in base 2 (0 or 1) 

that are the components 

of information.
instruction 
A command 
that computer hardware 

understands and obeys.
assembler
 A program 
that translates a symbolic 

version of instructions 

into the binary version.
assembly language
 A symbolic representation 

of machine instructions.
machine language
 A binary representation of 

machine instructions.

 e recognition that a program could be written to translate a more powerful 
language into computer instructions was one of the great breakthroughs in the 
early days of computing. Programmers today owe their productivity—and their 

sanity—to the creation of 
high-level programming languages
 and compilers 
that translate programs in such languages into instructions. 
Figure 1.4
 shows the 
relationships among these programs and languages, which are more examples of 

the power of 
abstraction
.high-level 
programming 

language
 A portable 
language such as C, C
, Java, or Visual Basic that 
is composed of words 

and algebraic notation 

that can be translated by 

a compiler into assembly 

language.
FIGURE 1.4 C program compiled into assembly language and then assembled into binary 
machine language. Although the translation from high-level language to binary machine language is 
shown in two steps, some compilers cut out the middleman and produce binary machine language directly. 

 ese languages and this program are examined in more detail in Chapter 2.
 1.3 Below Your Program 
15swap(int v[], int k){int temp;
   temp = v[k];
   v[k] = v[k+1];
   v[k+1] = temp;
}swap:
      multi $2, $5,4
      add   $2, $4,$2
      lw    $15, 0($2)
      lw    $16, 4($2)
      sw    $16, 0($2)
      sw    $15, 4($2)
      jr    $3100000000101000100000000100011000
00000000100000100001000000100001
10001101111000100000000000000000
10001110000100100000000000000100
10101110000100100000000000000000
10101101111000100000000000000100
00000011111000000000000000001000Assembler
CompilerBinary machine

language
program

(for MIPS)
Assembly
language
program

(for MIPS)
High-level
language
program

(in C)
16 Chapter 1 Computer Abstractions and Technology
A compiler enables a programmer to write this high-level language expression:
A + B e compiler would compile it into this assembly language statement:
add A,BAs shown above, the assembler would translate this statement into the binary 
instructions that tell the computer to add the two numbers 
A and 
B.High-level programming languages o
 er several important b
 ts. First, they 
allow the programmer to think in a more natural language, using English words 

and algebraic notation, resulting in programs that look much more like text than 

like tables of cryptic symbols (see 
Figure 1.4
). Moreover, they allow languages to be 

designed according to their intended use. Hence, Fortran was designed for scien
 c computation, Cobol for business data processing, Lisp for symbol manipulation, 

and so o
 ere are also domain-spe
 c languages for even narrower groups of 
users, such as those interested in simulation o
 uids, for example.
 e second advantage of programming languages is improved programmer 
productivity. One of the few areas of widespread agreement in so
 ware development 
is that it takes less time to develop programs when they are written in languages 

that require fewer lines to express an idea. Conciseness is a clear advantage of high-

level languages over assembly language.
 e 
 nal advantage is that programming languages allow programs to be 
independent of the computer on which they were developed, since compilers and 

assemblers can translate high-level language programs to the binary instructions of 

any computer ese three advantages are so strong that today little programming 

is done in assembly language.
 1.4 Under the Covers
Now that we have looked below your program to uncover the underlying so
 ware, 
let’s open the covers of your computer to learn about the underlying hardware
 e underlying hardware in any computer performs the same basic functions: inputting 

data, outputting data, processing data, and storing data. How these functions are 

performed is the primary topic of this book, and subsequent chapters deal with 

 erent parts of these four tasks.
When we come to an important point in this book, a point so important that 
we hope you will remember it forever, we emphasize it by identifying it as a 
Big 
Picture
 item. We have about a dozen Big Pictures in this book, th
 rst being the 
 ve components of a computer that perform the tasks of inputting, outputting, 
processing, and storing data.
Two key components of computers are 
input devices
, such as the microphone, 
and output devices
, such as the speaker. As the names suggest, input feeds the 
input device
 A mechanism through 
which the computer is 

fed information, such as a 

keyboard.
output device
 A mechanism that 

conveys the result of a 

computation to a user, 

such as a display, or to 

another computer.

 1.4 Under the Covers 
17FIGURE 1.5 The organization of a computer, showing the ﬁ
 ve classic components.
 e processor gets instructions and data from memory. Input writes data to memory, and output reads data from 
memory. Control sends the signals that determine the operations of the datapath, memory, input, and output.
 e 
 ve classic components of a computer are input, output, memory, 
datapath, and control, with the last two sometimes combined and called 
the processor. 
Figure 1.5
 shows the standard organization of a computer. 

 is organization is independent of hardware technology: you can place 
every piece of every computer, past and present, into one of thes
 ve 
categories. To help you keep all this in perspective, th ve components of 

a computer are shown on the front page of each of the following chapters, 

with the portion of interest to that chapter highlighted.
The BIGPicturecomputer, and output is the result of computation sent to the user. Some devices, 

such as wireless networks, provide both input and output to the computer.
Chapters 5 and 6 describe input/output (I/O) devices in more detail, but let’s 
take an introductory tour through the computer hardware, starting with the 

external I/O devices.

18 Chapter 1 Computer Abstractions and Technology
Through the Looking Glass e most fascinating I/O device is probably the graphics display. Most personal 
mobile devices use 
liquid crystal displays (LCDs)
 to get a thin, low-power display. 
 e LCD is not the source of light; instead, it controls the transmission of light. 
A typical LCD includes rod-shaped molecules in a liquid that form a twisting 
helix that bends light entering the display, from either a light source behind the 

display or less o
 en from r
 ected ligh
 e rods straighten out when a current is 
applied and no longer bend the light. Since the liquid crystal material is between 

two screens polarized at 90 degrees, the light cannot pass through unless it is bent. 

Today, most LCD displays use an 
active matrix
 that has a tiny transistor switch at 
each pixel to precisely control current and make sharper images. A red-green-blue 
mask associated with each dot on the display determines the intensity of the three-

color components in th
 nal image; in a color active matrix LCD, there are three 
transistor switches at each point.
 e image is composed of a matrix of picture elements, or 
pixels
, which can 
be represented as a matrix of bits, called a 
bit map
. Depending on the size of the 
screen and the resolution, the display matrix in a typical tablet ranges in size from 

1024  768 to 2048 
 1536. A color display might use 8 bits for each of the three 
colors (red, blue, and green), for 24 bits per pixel, permitting millions of
 erent 
colors to be displayed.
 e computer hardware support for graphics consists mainly of a 
raster refresh 
 er, or 
frame bu
 er, to store the bit map.
 e image to be represented onscreen 
is stored in the frame bu
 er, and the bit pattern per pixel is read out to the graphics 
display at the refresh rate. 
Figure 1.6
 shows a frame b
 er with a simp
 ed design 
of just 4 bits per pixel.
 e goal of the bit map is to faithfully represent what is on the scre
 e challenges in graphics systems arise because the human eye is very good at detecting 

even subtle changes on the screen.
liquid crystal display
 A display technology 
using a thin layer of liquid 

polymers that can be used 

to transmit or block light 

according to whether a 

charge is applied.
pixel
 e smallest 
individual picture 

element. Screens are 

composed of hundreds 

of thousands to millions 

of pixels, organized in a 

matrix.
X0X1Y0Frame buffer
Raster scan CRT display
00111101Y1X0X1Y0Y1FIGURE 1.6 Each coordinate in the frame buffer on the left determines the shade of the 
corresponding coordinate for the raster scan CRT display on the right.
 Pixel (X
0, Y0) contains 
the bit pattern 0011, which is a lighter shade on the screen than the bit pattern 1101 in pixel (X
1, Y1).active matrix display
 A liquid crystal display 

using a transistor to 

control the transmission 

of light at each individual 

pixel.
 rough computer 
displays I have landed 
an airplane on the 

deck of a moving 

carrier, observed a 

nuclear particle hit a 

potential we
 own 
in a rocket at nearly 

the speed of light and 

watched a computer 

reveal its innermost 

workings.
Ivan Sutherland, the 
“father” of computer 

graphics, 
Scienti
 c American
, 1984
 1.4 Under the Covers 
19Touchscreen
While PCs also use LCD displays, the tablets and smartphones of the PostPC era 
have replaced the keyboard and mouse with touch sensitive displays, which has 

the wonderful user interface advantage of users pointing directly what they are 

interested in rather than indirectly with a mouse.
While there are a variety of ways to implement a touch screen, many tablets 
today use capacitive sensing. Since people are electrical conductors, if an insulator 

like glass is covered with a transparent conductor, touching distorts the electrostatic 

 eld of the screen, which results in a change in capacitance
 is technology can 
allow multiple touches simultaneously, which allows gestures that can lead to 

attractive user interfaces.
Opening the BoxFigure 1.7
 shows the contents of the Apple iPad 2 tablet computer. Unsurprisingly, 

of th
 ve classic components of the computer, I/O dominates this reading device. 
 e list of I/O devices includes a capacitive multitouch LCD display, front facing 
camera, rear facing camera, microphone, headphone jack, speakers, accelerometer, 

gyroscope, Wi-Fi network, and Bluetooth networ
 e datapath, control, and 
memory are a tiny portion of the components.
 e small rectangles in 
Figure 1.8
 contain the devices that drive our advancing 
technology, called 
integrated circuits
 and nicknamed 
chips e A5 package seen 
in the middle of in 
Figure 1.8
 contains two ARM processors that operate with a 

clock rate of 1 GH
 e processor
 is the active part of the computer, following the 
instructions of a program to the letter.
 It adds numbers, tests numbers, signals I/O 
devices to activate, and so on. Occasionally, people call the processor the 
CPU, for 
the more bureaucratic-sounding 
central processor unit
.Descending even lower into the hardware, 
Figure 1.9
 reveals details of a 
microprocessor.
 e processor logically comprises two main components: datapath 
and control, the respective brawn and brain of the processor
 e datapath
 performs 
the arithmetic operations, and 
control
 tells the datapath, memory, and I/O devices 
what to do according to the wishes of the instructions of the program. Chapter 4 
explains the datapath and control for a higher-performance design.
 e A5 package in 
Figure 1.8
 also includes two memory chips, each with 
2 gibibits of capacity, thereby supplying 512 MiB
 e memory
 is where the 
programs are kept when they are running; it also contains the data needed by the 
running program
 e memory is built from DRAM chips. 
DRAM
 stands for 
dynamic random access memory
. Multiple DRAMs are used together to contain 
the instructions and data of a program. In contrast to sequential access memories, 
such as magnetic tapes, the 
RAM
 portion of the term DRAM means that memory 
accesses take basically the same amount of time no matter what portion of the 

memory is read.
Descending into the depths of any component of the hardware reveals insights 
into the computer. Inside the processor is another type of memory—cache memory. 
integrated circuit
 Also 
called a chip. A device 
combining dozens to 
millions of transistors.
central processor unit 
(CPU) Also called 
processor.
 e active part 
of the computer, which 
contains the datapath and 

control and which adds 

numbers, tests numbers, 

signals I/O devices to 

activate, and so on.
datapath
 e component of the 

processor that performs 

arithmetic operations
control
 e component 
of the processor that 

commands the datapath, 

memory, and I/O 

devices according to 

the instructions of the 

program.
memory
 e storage 
area in which programs 

are kept when they are 

running and that contains 

the data needed by the 

running programs.
dynamic random access 
memory (DRAM)
 Memory built as an 
integrated circuit; it 

provides random access to 

any location. Access times 

are 50 nanoseconds and 

cost per gigabyte in 2012 

was $5 to $10.

20 Chapter 1 Computer Abstractions and Technology
FIGURE 1.7 Components of the Apple iPad 2 A1395.
 e metal back of the iPad (with the reversed 
Apple logo in the middle) is in the center. At the top is the capacitive multitouch screen and LCD display. To 
the far right is the 3.8 V, 25 watt-hour, polymer battery, which consists of three Li-ion cell cases and o
 ers 
10 hours of battery life. To the fa
  is the metal frame that attaches the LCD to the back of the iPad
 e small components surrounding the metal back in the center are what we think of as the computer; they 

are
 en L-shaped t
 t compactly inside the case next to the battery. 
Figure 1.8
 shows a close-up of the 
L-shaped board to the low  of the metal case, which is the logic printed circuit board that contains the 

processor and the memory
 e tiny rectangle below the logic board contains a chip that provides wireless 
communication: Wi-Fi, Bluetooth, and FM tuner. I
 ts into a small slot in the low
  corner of the logic 
board. Near the upp
  corner of the case is another L-shaped component, which is a front-facing camera 
assembly that includes the camera, headphone jack, and microphone. Near the right upper corner of the case 

is the board containing the volume control and silent/screen rotation lock button along with a gyroscope and 

accelerometer
 ese last two chips combine to allow the iPad to recognize 6-axis motio
 e tiny rectangle 
next to it is the rear-facing camera. Near the bottom right of the case is the L-shaped speaker assembly
 e cable at the bottom is the connector between the logic board and the camera/volume control board
 e board between the cable and the speaker assembly is the controller for the capacitive touchscreen. (Courtesy 

iFixit, 
www
 xit.com
)FIGURE 1.8 e logic board of Apple iPad 2 in 
Figure 1.7
 e photo highligh
 ve integrated circuits. 
 e large integrated circuit in the middle is the Apple A5 chip, which contains a dual ARM processor cores 
that run at 1 GHz as well as 512 MB of main memory inside the package. 
Figure 1.9
 shows a photograph of 

the processor chip inside the A5 package
 e similar sized chip to th
  is the 32 
 ash memory chip 
for non-volatile storage.
 ere is an empty space between the two chips where a seco
 ash chip can be 
installed to double storage capacity of the iPad
 e chips to the right of the A5 include power controller and 
I/O controller chips. (Courtesy iFixit, 
www
 xit.com
)
 1.4 Under the Covers 
21FIGURE 1.9 e processor integrated circuit inside the A5 package
 e size of chip is 12.1 by 10.1 mm, and 
it was manufactured originally in a 45-nm process (see Section 1.5). It has two identical ARM processors or 
cores in th
  of the chip and a PowerVR graphical processor unit (GPU) with four datapaths in the 
upp  quadrant. To th
  and bottom side of the ARM cores are interfaces to main memory (DRAM). 
(Courtesy Chipworks, 
www.chipworks.com
)Cache memory
 consists of a small, fast memory that acts as a bu
 er for the DRAM 
memory
 e nontec
 nition of 
cache is a safe place for hiding things.) 
Cache is built usin
 erent memory technology, 
static random access memory 
(SRAM)
. SRAM is faster but less dense, and hence more expensive, than DRAM 
(see Chapter 5). SRAM and DRAM are two layers of the 
memory hierarchy
.cache memory
 A small, 
fast memory that acts as a 
bu
 er for a slower, larger 
memory.
static random access 
memory (SRAM)
 Also 
memory built as an 
integrated circuit, but 

faster and less dense than 

DRAM.

22 Chapter 1 Computer Abstractions and Technology
As mentioned above, one of the great ideas to improve design is abstraction. 
One of the most important 
abstractions
 is the interface between the hardware 
and the lowest-level so
 ware. Because of its importance, it is given a special 
name: the 
instruction set architecture
, or simply 
architecture
, of a computer. 
 e instruction set architecture includes anything programmers need to know to 
make a binary machine language program work correctly, including instructions, 
I/O devices, and so on. Typically, the operating system will encapsulate the 

details of doing I/O, allocating memory, and other low-level system functions 

so that application programmers do not need to worry about such detai
 e combination of the basic instruction set and the operating system interface 

provided for application programmers is called the 
application binary interface 
(ABI).An instruction set architecture allows computer designers to talk about 
functions independently from the hardware that performs them. For example, 

we can talk about the functions of a digital clock (keeping time, displaying the 

time, setting the alarm) independently from the clock hardware (quartz crystal, 

LED displays, plastic buttons). Computer designers distinguish architecture from 

an 
implementation
 of an architecture along the same lines: an implementation is 
hardware that obeys the architecture abstractio
 ese ideas bring us to another 
Big Picture.
instruction set 
architecture
 Also 
called architecture
. An abstract interface between 
the hardware and the 

lowest-level so
 ware 
that encompasses all the 
information necessary to 

write a machine language 

program that will run 

correctly, including 

instructions, registers, 

memory access, I/O, and 

so on.
application binary 
interface (ABI)
 e user 
portion of the instruction 
set plus the operating 

system interfaces used by 

application programmers. 

I
 nes a standard for 
binary portability across 

computers.
implementation
 Hardware that obeys the 

architecture abstraction.
Both hardware and so
 ware consist of hierarchical layers using abstraction, 
with each lower layer hiding details from the level above. One key interface 
between the levels of abstraction is the 
instruction set architecture
—the 
interface between the hardware and low-level so
 ware. 
 is abstract 
interface enables many 
implementations
 of varying cost and performance 
to run identical so
 ware.
The BIGPictureA Safe Place for Data us far, we have seen how to input data, compute using the data, and display 
data. If we were to lose power to the computer, however, everything would be lost 

because the memory inside the computer is 
volatile
—that is, when it loses power, 
it forgets. In contrast, a DVD disk doesn’t forget the movie when you turn o
  the 
power to the DVD player, and is thus a 
nonvolatile memory
 technology.
volatile memory
 Storage, such as DRAM, 
that retains data only if it 

is receiving power.
nonvolatile memory
 A form of memory that 

retains data even in the 

absence of a power source 

and that is used to store 

programs between runs. 

A DVD disk is nonvolatile.

 1.4 Under the Covers 
23To distinguish between the volatile memory used to hold data and programs 
while they are running and this nonvolatile memory used to store data and 
programs between runs, the term 
main memory
 or 
primary memory
 is used for 
the former, and 
secondary memory
 for the latter. Secondary memory forms the 
next lower layer of the 
memory hierarchy
. DRAMs have dominated main memory 
since 1975, but 
magnetic disks
 dominated secondary memory starting even earlier. 
Because of their size and form factor, personal Mobile Devices use 
 ash memory
, a nonvolatile semiconductor memory, instead of disks. 
Figure 1.8
 shows the chip 
containing th
 ash memory of the iPad 2. While slower than DRAM, it is much 
cheaper than DRAM in addition to being nonvolatile. Although costing more per 

bit than disks, it is smaller, it comes in much smaller capacities, it is more rugged, 

and it is more pow
  cient than disks. Hence,
 ash memory is the standard 
secondary memory for PMDs. Alas, unlike disks and DR
 ash memory bits 
wear out
 er 100,000 to 1,000,000 writ
 us, 
 le systems must keep track of 
the number of writes and have a strategy to avoid wearing out storage, such as by 

moving popular data. Chapter 5 describes disks an
 ash memory in more detail.
Communicating with Other Computers
We’ve explained how we can input, compute, display, and save data, but there is 

still one missing item found in today’s computers: computer networks. Just as the 

processor shown in 
Figure 1.5
 is connected to memory and I/O devices, networks 

interconnect whole computers, allowing computer users to extend the power of 

computing by including communication. Networks have become so popular that 

they are the backbone of current computer systems; a new personal mobile device 

or server without a network interface would be ridiculed. Networked computers 

have several major advantages:
 Communication
: Information is exchanged between computers at high 
speeds.
 Resource sharing
: Rather than each computer having its own I/O devices, 
computers on the network can share I/O devices.
 Nonlocal access
: By connecting computers over long distances, users need not 
be near the computer they are using.
Networks vary in length and performance, with the cost of communication 

increasing according to both the speed of communication and the distance that 

information travels. Perhaps the most popular type of network is 
Ethernet
. It can 
be up to a kilometer long and transfer at up to 40 gigabits per second. Its length and 

speed make Ethernet useful to connect computers on the sa
 oor of a building; 
main memory
 Also 
called primary memory
. Memory used to hold 
programs while they are 

running; typically consists 

of DRAM in today’s 

computers.
secondary memory
  Nonvolatile memory 

used to store programs 

and data between runs; 

typically consists of
 ash memory in PMDs and 

magnetic disks in servers.
magnetic disk
 Also 
called hard disk
. A form 
of nonvolatile secondary 

memory composed of 

rotating platters coated 

with a magnetic recording 

material. Because they 

are rotating mechanical 

devices, access times are 

about 5 to 20 milliseconds 

and cost per gigabyte in 

2012 was $0.05 to $0.10.
 ash memory
 A nonvolatile semi-

conductor memory. It 

is cheaper and slower 

than DRAM but more 

expensive per bit and 

faster than magnetic disks. 

Access times are about 5 

to 50 microseconds and 

cost per gigabyte in 2012 

was $0.75 to $1.00.

24 Chapter 1 Computer Abstractions and Technology
hence, it is an example of what is generically called a 
local area network
. Local area 
networks are interconnected with switches that can also provide routing services 
and security. 
Wide area networks
 cross continents and are the backbone of the 
Internet, which supports the web.
 ey are typically based on opt
 bers and are 
leased from telecommunication companies.
Networks have changed the face of computing in the last 30 years, both by 
becoming much more ubiquitous and by making dramatic increases in performance. 
In the 1970s, very few individuals had access to electronic mail, the Internet and 

web did not exist, and physically mailing ma
gnetic tapes was the primary way to 
transfer large amounts of data between two locations. Local area networks were 

almost nonexistent, and the few existing wide area networks had limited capacity 

and restricted access.
As networking technology improved, it became much cheaper and had a much 
higher capacity. For example, th
 rst standardized local area network technology, 
developed about 30 years ago, was a version of Ethernet that had a maximum capacity 

(also called bandwidth) of 10 million bits per second, typically shared by tens of, if 

not a hundred, computers. Today, local area network technology o
 ers a capacity 
of from 1 to 40 gigabits per second, usually shared by at most a few computers. 

Optical communications technology has allowed similar growth in the capacity of 

wide area networks, from hundreds of kilobits to gigabits and from hundreds of 

computers connected to a worldwide network to millions of computers connected. 

 is combination of dramatic rise in deployment of networking combined with 
increases in capacity have made network technology central to the information 

revolution of the last 30 years.
For the last decade another innovation in networking is reshaping the way 
computers communicate. Wireless technology is widespread, which enabled 

the P
 e ability to make a radio in the same low-cost semiconductor 
technology (CMOS) used for memory and microprocessors enab
 cant 
improvement in price, leading to an explosion in deployment. Currently available 

wireless technologies, called by the IEEE standard name 802.11, allow for transmission 

rates from 1 to nearly 100 million bits per second. Wireless technology is quite a bit 

 erent from wire-based networks, since all users in an immediate area share the 
airwaves.
 Semiconductor DRAM memory
 ash memory, and disk storag
 er  cantly. For each technology, list its volatility, approximate relative 
access time, and approximate relative cost compared to DRAM.
 1.5  Technologies for Building Processors 
y
Processors and memory have improved at an incredible rate, because computer 
designers have long embraced the latest in electronic technology to try to win the 

race to design a better computer. 
Figure 1.10
 shows the technologies that have 
local area network 
(LAN)
 A network 
designed to carry data 
within a geographically 

co
 ned area, typically 
within a single building.
wide area network 
(WAN)
 A network 
extended over hundreds 
of kilometers that can 

span a continent.
Check Yourself

FIGURE 1.10 Relative performance per unit cost of technologies used in computers over 
time. Source: Computer Museum, Boston, with 2013 extrapolated by the authors. See 
 Section 1.12
.  1,000,000  10,000,000197619781980198219841986Year of introduction
1988199019921994199619982000
200220042006200820102012
Kibibit capacity16K64K256K1M4M16M64M128M256M512M1G2G4G100,00010,000100010010FIGURE 1.11 Growth of capacity per DRAM chip over time.
 e y-axis is measured in kibibits (2
10 bi
 e DRAM industry 
quadrupled capacity almost every three years, a 60% increase per year, for 20 years. In recent years, the rate has slowed down 
and is somewhat 
closer to doubling every two years to three years.
 1.5 Technologies for Building Processory 
25been used over time, with an estimate of the relative performance per unit cost for 
each technology. Since this technology shapes what computers will be able to do 

and how quickly they will evolve, we believe all computer professionals should be 

familiar with the basics of integrated circuits.
A transistor
 is simply an on/o
  switch controlled by electricity
 e integrated 
circuit
 (IC) combined dozens to hundreds of transistors into a single chip. When 

Gordon Moore predicted the continuous doubling of resources, he was predicting 

the growth rate of the number of transistors per chip. To describe the tremendous 

increase in the number of transistors from hundreds to millions, the adjective 
very 
large scale
 is added to the term, creating the abbreviation 
VLSI
, for 
very large-scale 
integrated circuit
. is rate of increasing integration has been remarkably stable. 
Figure 1.11
 shows 
the growth in DRAM capacity since 1977. For decades, the industry has consistently 

quadrupled capacity every 3 years, resulting in an increase in excess of 16,000 times!
To understand how manufacture integrated circuits, we start at the beginning. 
 e manufacture of a chip begins with 
silicon
, a substance found in sand. Because 
silicon does not conduct electricity well, it is called a 
semiconductor
. With a special 
chemical process, it is possible to add materials to silicon that allow tiny areas to 
transform into one of three devices:
 Excellent conductors of electricity (using either microscopic copper or 
aluminum wire)
transistor
 An on/o
  switch controlled by an 
electric signal.
very large-scale 
integrated (VLSI) 

circuit
 A device 
containing hundreds of 
thousands to millions of 

transistors.
silicon
  A natural 
element that is a 

semiconductor.
semiconductor
 A substance that does not 

conduct electricity well.
YearTechnology used in computersRelative performance/unit cost
1951Vacuum tube
11965351975Integrated circuitVery large-scale integrated circuit

Ultra large-scale integrated circuitTransistor
90019952,400,0002013250,000,000,000
26 Chapter 1 Computer Abstractions and Technology
 Excellent insulators from electricity (like plastic sheathing or glass)
 Areas that can conduct or insulate under special conditions (as a switch)
Transistors fall in the last category. A VLSI circuit, then, is just billions of 
combinations of conductors, insulators, and switches manufactured in a single 
small package.
 e manufacturing process for integrated circuits is critical to the cost of the 
chips and hence important to computer designers. 
Figure 1.12
 shows that process. 

 e process starts with a 
silicon crystal ingot
, which looks like a giant sausage. 
Today, ingots are 8–12 inches in diameter and about 12–24 inches long. An ingot 
 nely sliced into 
wafers
 no more than 0.1 inches thic
 ese wafers then go 
through a series of processing steps, during which patterns of chemicals are placed 

on each wafer, creating the transistors, conductors, and insulators discussed earlier. 

Today’s integrated circuits contain only one layer of transistors but may have from 

two to eight levels of metal conductor, separated by layers of insulators.
silicon crystal ingot
 A rod composed of a 
silicon crystal that is 

between 8 and 12 inches 

in diameter and about 12 

to 24 inches long.
wafer
 A slice from a 
silicon ingot no more than 

0.1 inches thick, used to 

create chips.
SlicerDicer20 to 40processing stepsBond die topackageSilicon ingotWafertesterParttesterShip tocustomersTested diesTestedwaferBlankwafersPackaged diesPatterned wafersTested packaged diesFIGURE 1.12 The chip manufacturing process. er being sliced from the silicon ingot, blank 
wafers are put through 20 to 40 steps to create patterned wafers (see 
Figure 1.13
 ese patterned wafers are 
then tested with a wafer tester, and a map of the good parts is made
 en, the wafers are diced into dies (see 
Figure 1.9
). In t
 gure, one wafer produced 20 dies, of which 17 passed testing. (X means the die is bad.) 
 e yield of good dies in this case was 17/20, o
 ese good dies are then bonded into packages and 
tested one more time before shipping the packaged parts to customers. One bad packaged part was found 
in t
 nal test.
A single microscop
 aw in the wafer itself or in one of the dozens of patterning 
steps can result in that area of the wafer failin
 ese 
defects
, as they are called, 
make it virtually impossible to manufacture a perfect wafer
 e simplest way to 
cope with imperfection is to place many independent components on a single 
wafer.
 e patterned wafer is then chopped up, or 
diced,
 into these components, 
defect
 A microscopic 
 aw in a wafer or in 
patterning steps that can 

result in the failure of the 

die containing that defect.

FIGURE 1.13 A 12-inch (300 mm) wafer of Intel Core i7 (Courtesy Intel).
 e number of 
dies on this 300 mm (12 inch) wafer at 100% yield is 280, each 20.7 by 10.5 
 e several dozen partially 
rounded chips at the boundaries of the wafer are useless; they are included because it’s easier to create the 
masks used to pattern the silico
 is die uses a 32-nanometer technology, which means that the smallest 
features are approximately 32 nm in size, although they are typically somewhat smaller than the actual feature 

size, which refers to the size of the transistors as “drawn” versus th
 nal manufactured size.
 1.6 Performance 
27called dies
 and more informally known as 
chips. Figure 1.13
 shows a photograph 
of a wafer containing microprocessors before they have been diced; earlier, 
Figure
 1.9 shows an individual microprocessor die.
Dicing enables you to discard only those dies that were unlucky enough to 
contain th aws, rather than the whole wafer
 is concept is quan
 ed by the 
yield
 of a process, whic
 ned as the percentage of good dies from the total 
number of dies on the wafer.
 e cost of an integrated circuit rises quickly as the die size increases, due both 
to the lower yield and the smaller number of dies tha
 t on a wafer. To reduce the 
cost, using the next generation process sh
rinks a large die as it uses smaller sizes for 
both transistors and wir
 is improves the yield and the die count per wafer. A 
32-nanometer (nm) process was typical in 2012, which means essentially that the 
smallest feature size on the die is 32 nm.
die
 e individual 
rectangular sections that 
are cut from a wafer, more 

informally known as 
chips.yield
 e percentage of 
good dies from the total 

number of dies on the 

wafer.

28 Chapter 1 Computer Abstractions and Technology
Once you’ve found good dies, they are connected to the input/output pins of a 
package, using a process called 
bonding
 ese packaged parts are test
 nal time, 
since mistakes can occur in packaging, and then they are shipped to customers.
Elaboration: The cost of an integrated circuit can be expressed in three simple 
equations:Cost per die
Cost per wafer
Dies per waferyield
Dies per waf
fer
Wafer area
Die areaYield
Defects per areaDie are
11((a
a/2))
2 rst equation is straightforward to derive. The second is an approximation, 
since it does not subtract the area near the border of the round wafer that cannot 
accommodate the rectangular dies (see Figure 1.13
 nal equation is based on 

empirical observations of yields at integrated circuit factories, with the exponent related 

to the number of critical processing steps.Hence, depending on the defect rate and the size of the die and wafer, costs are 
generally not linear in the die area.A key factor in determining the cost of an integrated circuit is volume. Which of 
the following are reasons why a chip made in high volume should cost less?
1. With high volumes, the manufacturing process can be tuned to a particular 
design, increasing the yield.
2. It is less work to design a high-volume part than a low-volume part.
 e masks used to make the chip are expensive, so the cost per chip is lower 
for higher volumes.
4. Engineering development costs are high and largely independent of volume; 
thus, the development cost per die is lower with high-volume parts.
5. High-volume parts usually have smaller die sizes than low-volume parts and 
therefore have higher yield per wafer.
 1.6 Performance
Assessing the performance of computers can be quite challenging
 e scale and 
intricacy of modern so
 ware systems, together with the wide range of performance 
improvement techniques employed by hardware designers, have made performance 
assessment much more
  cult.
When trying to choose amon
 erent computers, performance is an important 
attribute. Accurately measuring and comparin
 erent computers is critical to 
Check Yourself

AirplanePassenger capacityCruising range 
(miles)Cruising speed 
(m.p.h.)Passenger throughput 
 m.p.h.)Boeing 777375
46300610228,750Boeing 74747013214641500610286,700BAC/Sud Concorde40001350178,200Douglas DC-8-5087200544 79,424(passengers ×  m.p.h.)FIGURE 1.14 The capacity, range, and speed for a number of commercial airplanes.
 e last 
column shows the rate at which the airplane transports passengers, which is the capacity times the cruising 
speed (ignoring range and takeo
  and landing times).
 1.6 Performance 
29purchasers and therefore to designer
 e people selling computers know this as 
well
 en, salespeople would like you to see their computer in the best possible 
light, whether or not this light accurately re
 ects the needs of the purchaser’s 
application. Hence, understanding how best to measure performance and the 
limitations of performance measurements is important in selecting a computer.
 e rest of this section describ
 erent ways in which performance can be 
determined; then, we describe the metrics for measuring performance from the 

viewpoint of both a computer user and a designer. We also look at how these metrics 

are related and present the classical processor performance equation, which we will 

use throughout the text.
Deﬁ ning Performance
When we say one computer has better performance than another, what do we 

mean? Although this question might seem simple, an analogy with passenger 

airplanes shows how subtle the question of performance can be. 
Figure 1.14
 
lists some typical passenger airplanes, together with their cruising speed, range, 

and capacity. If we wanted to know which of the planes in this table had the best 

performance, we would
 rst need to
 ne performance. For example, considering 
 erent measures of performance, we see that the plane with the highest cruising 
speed was the Concorde (retired from service in 2003), the plane with the longest 

range is the DC-8, and the plane with the largest capacity is the 747.
Let’s suppose we
 ne performance in terms of speed
 is still leaves two 
possib
 nitions. You co
 ne the fastest plane as the one with the highest 
cruising speed, taking a single passenger from one point to another in the least time. 

If you were interested in transporting 450 passengers from one point to another, 

however, the 747 would clearly be the fastest, as the last column of th
 gure shows. 
Similarly, we ca
 ne computer performance in sev
 erent ways.
If you were running a program on tw
 erent desktop computers, you’d say 
that the faster one is the desktop computer that gets the job do
 rst. If you were 
running a datacenter that had several servers running jobs submitted by many 

users, you’d say that the faster computer was the one that completed the most 

jobs during a day. As an individual computer user, you are interested in reducing 
response time
—the time between the start and completion of a task—also referred 
response time
 Also 
called execution time
.  e total time required 
for the computer to 
complete a task, including 

disk accesses, memory 

accesses, I/O activities, 

operating system 

overhead, CPU execution 

time, and so on.

30 Chapter 1 Computer Abstractions and Technology
to as 
execution time
. Datacenter managers are o
 en interested in increasing 
throughput
 or 
bandwidth
—the total amount of work done in a given time. Hence, 
in most cases, w
 erent performance metrics as we
 erent sets 
of applications to benchmark personal mobile devices, which are more focused on 
response time, versus servers, which are more focused on throughput.
Throughput and Response Time
Do the following changes to a computer system increase throughput, decrease 

response time, or both?
1. Replacing the processor in a computer with a faster version
2. Adding additional processors to a system that uses multiple processors 
for separate tasks—for example, searching the web
Decreasing response time almost always improves throughput. Hence, in case 
1, both response time and throughput are improved. In case 2, no one task gets 

work done faster, so only throughput increases.
If, however, the demand for processing in the second case was almost 
as large as the throughput, the system might force requests to queue up. In 

this case, increasing the throughput could also improve response time, since 

it would reduce the waiting time in the queue
 us, in many real computer 
systems, changing either execution time or throughput o
 en a
 ects the other.
In discussing the performance of computers, we will be primarily concerned with 

response time for th
 rst few chapters. To maximize performance, we want to 
minimize response time or execution time for some ta
 us, we can relate 
performance and execution time for a computer X:
Performance
Execution time
XX1 is means that for two computers X and Y, if the performance of X is greater than 
the performance of Y, we have
PerformancePerformance
Execution timeExecution time
XYXY11EExecution timeExecution time
YX at is, the execution time on Y is longer than that on X, if X is faster than Y.
throughput
 Also called 
bandwidth
. Another 
measure of performance, 
it is the number of tasks 

completed per unit time.
EXAMPLEANSWER
In discussing a computer design, we o
 en want to relate the performance of two 
 erent computers quantitatively. We will use the phrase “X is 
n times faster than 
Y”—or equivalently “X is 
n times as fast as Y”—to mean
Performance
Performance
XYnIf X is 
n times as fast as Y, then the execution time on Y is 
n times as long as it is 
on X:
Performance
Performance
Execution time
Execution time
XYYXnRelative Performance
If computer A runs a program in 10 seconds and computer B runs the same 
program in 15 seconds, how much faster is A than B?
We know that A is 
n times as fast as B ifPerformance
Performance
Execution time
Execution time
ABBAn us the performance ratio is
151015.and A is therefore 1.5 times as fast as B.
In the above example, we could also say that computer B is 1.5 times 
slower than
 computer A, since
Performance
Performance
AB15.means that
Performance
Performance
AB15.EXAMPLEANSWER 1.6 Performance 
31
32 Chapter 1 Computer Abstractions and Technology
For simplicity, we will normally use the terminology 
as fast as
 when we try to 
compare computers quantitatively. Because performance and execution time are 
reciprocals, increasing performance requires decreasing execution time. To avoid 

the potential confusion between the terms 
increasing
 and 
decreasing
, we usually 
say “improve performance” or “improve execution time” when we mean “increase 

performance” and “decrease execution time.”
Measuring Performance
Time is the measure of computer performance: the computer that performs the 

same amount of work in the least time is the fastest. Program 
execution time
 is measured in seconds per program. However, time can b
 
 erent ways, 
depending on what we coun
 e most straightforwar
 nition of time is called 
wall clock time
, response time
, or 
elapsed time
 ese terms mean the total time 
to complete a task, including disk accesses, memory accesses, 
input/output
 (I/O) activities, operating system overhead—everything.
Computers are o
 en shared, however, and a processor may work on several 
programs simultaneously. In such cases, the system may try to optimize throughput 

rather than attempt to minimize the elapsed time for one program. Hence, we 

 en want to distinguish between the elapsed time and the time over which the 
processor is working on our behalf. 
CPU execution time
 or simply 
CPU time, which recognizes this distinction, is the time the CPU spends computing for this 
task and does not include time spent waiting for I/O or running other programs. 

(Remember, though, that the response time experienced by the user will be the 

elapsed time of the program, not the CPU time.) CPU time can be further divided 

into the CPU time spent in the program, called 
user CPU time
, and the CPU time 
spent in the operating system performing tasks on behalf of the program, called 
system CPU time
 erentiating between system and us
  cult to 
do accurately, because it is o
 en hard to assign responsibility for operating system 
activities to one user program rather than another and because of the functionality 

 erences among operating systems.
For consistency, we maintain a distinction between performance based on 
elapsed time and that based on CPU execution time. We will use the term 
system 
performance
 to refer to elapsed time on an unloaded system and 
CPU performance
 to refer to user CPU time. We will focus on CPU performance in this chapter, 

although our discussions of how to summarize performance can be applied to 

either elapsed time or CPU time measurements.
 erent applications are sensitive t
 erent aspects of the performance of a 
computer system. Many applications, especially those running on servers, depend 
as much on I/O performance, which, in turn, relies on both hardware and so
 ware. 
Total elapsed time measured by a wall clock is the measurement of interest. In 
CPU execution 
time Also called 
CPU time e actual time the 
CPU spends computing 
for a sp
 c task.
user CPU time
 e CPU time spent in a 

program itself.
system CPU time
 e CPU time spent in 

the operating system 

performing tasks on 

behalf of the program.
Understanding 
Program 
Performance

some application environments, the user may care about throughput, response 
time, or a complex combination of the two (e.g., maximum throughput with a 

worst-case response time). To improve the performance of a program, one must 

have a clea
 nition of what performance metric matters and then proceed to 
look for performance bottlenecks by measuring program execution and looking 

for the likely bottlenecks. In the following chapters, we will describe how to search 

for bottlenecks and improve performance in various parts of the system.
Although as computer users we care about time, when we examine the details 
of a computer it’s convenient to think about performance in other metrics. In 

particular, computer designers may want to think about a computer by using a 

measure that relates to how fast the hardware can perform basic functions. Almost 

all computers are constructed using a clock that determines when events take 

place in the hardware
 ese discrete time intervals are called 
clock cycles
 (or 
ticks
, clock ticks
, clock periods
, clocks
, cycles
). Designers refer to the length of a 
clock period
 both as the time for a complete 
clock cycle
 (e.g., 250 picoseconds, or 
250 ps) and as the 
clock rate
 (e.g., 4 gigahertz, or 4 GHz), which is the inverse of the 
clock period. In the next subsection, we will formalize the relationship between the 

clock cycles of the hardware designer and the seconds of the computer user.
1. Suppose we know that an application that uses both personal mobile 
devices and the Cloud is limited by network performance. For the following 

changes, state whether only the throughput improves, both response time 

and throughput improve, or neither improves.
a. An extra network channel is added between the PMD and the Cloud, 
increasing the total network throughput and reducing the delay to obtain 
network access (since there are now two channels).
b.
 e networking so
 ware is improved, thereby reducing the network 
communication delay, but not increasing throughput.
c. More memory is added to the computer.
2. Computer C’s performance is 4 times as fast as the performance of computer 
B, which runs a given application in 28 seconds. How long will computer C 

take to run that application?
CPU Performance and Its Factors
Users and designers o
 en examine performance usin
 erent metrics. If we could 
relate thes
 erent metrics, we could determine th
 ect of a design change 
on the performance as experienced by the user. Since we are co
 ning ourselves 
to CPU performance at this point, the bottom-line performance measure is CPU 
clock cycle
 Also called 
tick
, clock tick
, clock 
period
, clock
, or 
cycle
.  e time for one clock 
period, usually of the 
processor clock, which 

runs at a constant rate.
clock period
 e length 
of each clock cycle.
Check 
Yourself
 1.6 Performance 
33
34 Chapter 1 Computer Abstractions and Technology
execution time. A simple formula relates the most basic metrics (clock cycles and 
clock cycle time) to CPU time:
CPU execution time
for a programCPU clock cycles
for a progrram
Clock cycle time
Alternatively, because clock rate and clock cycle time are inverses,
CPU execution time
for a programCPU clock cycles for a pro
g
gramClock rate is formula makes it clear that the hardware designer can improve performance 
by reducing the number of clock cycles required for a program or the length of 
the clock cycle. As we will see in later chapters, the designer o
 en faces a trade-o
  between the number of clock cycles needed for a program and the length of each 

cycle. Many techniques that decrease the number of clock cycles may also increase 

the clock cycle time.
Improving Performance
Our favorite program runs in 10 seconds on computer A, which has a 2 GHz 

clock. We are trying to help a computer designer build a computer, B, which will 

run this program in 6 seco
 e designer has determined that a substantial 
increase in the clock rate is possible, but this increase will a
 ect the rest of the 
CPU design, causing computer B to require 1.2 times as many clock cycles as 

computer A for this program. What clock rate should we tell the designer to 

target?
Let’
 rst 
 nd the number of clock cycles required for the program on A:
CPU time
CPU clock cycles
Clock rate seconds
CPU clock
AAA10  cycles
cyclessecond
CPU clock cycles seconds
AA210
1021
9002010
99cyclessecond
 cyclesEXAMPLEANSWER
CPU time for B can be found using this equation:
CPU time
CPU clock cyclesClock rate seconds
BAB1261220
..10122010
699 cyclesClock rateClock rate cycles secoBB.n
nds cyclessecond
 cyclessecond
 GHz022010410
499.To run the program in 6 seconds, B must have twice the clock rate of A.
Instruction Performance
 e performance equations above did not include any reference to the number of 
instructions needed for the program. However, since the compiler clearly generated 
instructions to execute, and the computer had to execute the instructions to run 

the program, the execution time must depend on the number of instructions in a 

program. One way to think about execution time is that it equals the number of 

instructions executed multiplied by the average time per instructio
 erefore, the 
number of clock cycles required for a program can be written as
CPU clock cyclesInstructions for a program
Average clock ccyclesper instruction
 e term 
clock cycles per instruction
, which is the average number of clock 
cycles each instruction takes to execute, is o
 en abbreviated as 
CPI. S
 erent 
instructions may tak
 erent amounts of time depending on what they do, CPI is 
an average of all the instructions executed in the program. CPI provides one way of 
comparing tw
 erent implementations of the same instruction set architecture, 
since the number of instructions executed for a program will, of course, be the 

same.
Using the Performance Equation
Suppose we have two implementations of the same instruction set architecture. 

Computer A has a clock cycle time of 250 ps and a CPI of 2.0 for some program, 

and computer B has a clock cycle time of 500 ps and a CPI of 1.2 for the same 

program. Which computer is faster for this program and by how much?
clock cycles 
per instruction 

(CPI) Average number 
of clock cycles per 
instruction for a program 

or program fragment.
EXAMPLE 1.6 Performance 
35
36 Chapter 1 Computer Abstractions and Technology
We know that each computer executes the same number of instructions for 
the program; let’s call this number 
I. Fir nd the number of processor clock 
cycles for each computer:
CPU clock cycles
CPU clock cycles
ABII××2012..Now we can compute the CPU time for each computer:
CPU timeCPU clock cyclesClock cycle time
 psAAI20250
.5500I psLikewise, for B:
CPU time
 ps ps
BII12500600
.Clearly, computer A is faster
 e amount faster is given by the ratio of the 
execution times:
CPU performanceCPU performanceExecution time
Execution 
ABBttime
pspsA600
50012I
I.We can conclude that computer A is 1.2 times as fast as computer B for this 
program.
The Classic CPU Performance Equation
We can now write this basic performance equation in terms of 
instruction count
 (the number of instructions executed by the program), CPI, and clock cycle time:
CPU timeInstruction countCPIClock cycle time
or, since the clock rate is the inverse of clock cycle time:
CPU time
Instruction countCPI
Clock rate ese formulas are particularly useful because they separate the three key factors 
that a
 ect performance. We can use these formulas to compare tw
 erent 
implementations or to evaluate a design alternative if we know its impact on these 

three parameters.
ANSWERinstruction count
 e number of instructions 
executed by the program.

Comparing Code SegmentsA compiler designer is trying to decide between two code sequences for a 
particular computer
 e hardware designers have supplied the following facts:
CPI for each instruction class
 ABCCPI123For a particular high-level language statement, the compiler writer is 

considering two code sequences that require the following instruction counts:
Instruction counts for each instruction class
Code sequenceABC12122411Which code sequence executes the most instructions? Which will be faster? 
What is the CPI for each sequence?
Sequence 1 executes 2 
 1  2  5 instructions. Sequence 2 executes 4 
 1  1  6 instruction
 erefore, sequence 1 executes fewer instructions.
We can use the equation for CPU clock cycles based on instruction count 
and CPI to
 nd the total number of clock cycles for each sequence:
CPU clock cyclesCPIC
()iiin1 is yields
CPU clock cycles cycles121122322610
()()()
CPU clock cycles cycles24112134239
()()()
So code sequence 2 is faster, even though it executes one extra instruction. Since 
code sequence 2 takes fewer overall clock cycles but has more instructions, it 

must have a low
 e CPI values can be computed by
CPI
CPU clock cycles
Instruction count
CPI
CPU clock cycles
111
12210520Instruction count
CPI
CPU clock cycles
Instruct
.iion count
29615.EXAMPLEANSWER 1.6 Performance 
37
38 Chapter 1 Computer Abstractions and Technology
Figure 1.15
 shows the basic measurements at
 erent levels in the 
computer and what is being measured in each case. We can see how these 
factors are combined to yield execution time measured in seconds per 

program:
TimeSeconds/Program
Instructions
ProgramClock cycles
Instr
uuctionSeconds
Clock cycle
Always bear in mind that the only complete and reliable measure of 
computer performance is time. For example, changing the instruction set 

to lower the instruction count may lead to an organization with a slower 

clock cycle time or higher CPI that o
 sets the improvement in instruction 
count. Similarly, because CPI depends on type of instructions executed, 

the code that executes the fewest number of instructions may not be the 

fastest.
The BIGPictureComponents of performance
Units of measure
CPU execution time for a programSeconds for the programInstruction count
Instructions executed for the program
Clock cycles per instruction (CPI)
Average number of clock cycles per instruction
Clock cycle time Seconds per clock cycleFIGURE 1.15 The basic components of performance and how each is measured.
How can we determine the value of these factors in the performance equation? 

We can measure the CPU execution time by running the program, and the clock 

cycle time is usually published as part of the documentation for a computer
 e instruction count and CPI can be mor
  cult to obtain. Of course, if we know 
the clock rate and CPU execution time, we need only one of the instruction count 

or the CPI to determine the other.
We can measure the instruction count by using so
 ware tools that pro
 le the 
execution or by using a simulator of the architecture. Alternatively, we can use 

hardware counters, which are included in most processors, to record a variety of 

measurements, including the number of inst
ructions executed, the average CPI, 
and
 en, the sources of performance loss. Since the instruction count depends 
on the architecture, but not on the exact implementation, we can measure the 

instruction count without knowing all the details of the implementatio
 e CPI, 
however, depends on a wide variety of design details in the computer, including 

both the memory system and the processor structure (as we will see in Chapter 4 

and Chapter 5), as well as on the mix of instruction types executed in an application. 

 us, CPI varies by application, as well as among implementations with the same 
instruction set.

 e above example shows the danger of using only one factor (instruction count) 
to assess performance. When comparing two computers, you must look at all three 
components, which combine to form execution time. If some of the factors are 

identical, like the clock rate in the above example, performance can be determined 

by comparing all the nonidentical factors. Since CPI varies by 
instruction mix
, both instruction count and CPI must be compared, even if clock rates are identical. 
Several exercises at the end of this chapter ask you to evaluate a series of computer 

and compiler enhancements that a
 ect clock rate, CPI, and instruction count. In 
 Section 1.10
, we’ll examine a common performance measurement that does not 
incorporate all the terms and can thus be misleading.
 e performance of a program depends on the algorithm, the language, the 
compiler, the architecture, and the actual hardware
 e following table summarizes 
how these components a
 ect the factors in the CPU performance equation.
Hardware 
or software 
componentAffects what?How?
AlgorithmInstruction count, 
possibly CPIThe algorithm determines the number of source program 
instructions executed and hence the number of processor 

instructions executed. The algorithm may also affect the CPI, 

by favoring slower or faster instructions. For example, if the 

algorithm uses more divides, it will tend to have a higher CPI.
Programming 
languageInstruction count, 

CPIThe programming language certainly affects the instruction 

count, since statements in the language are translated to 

processor instructions, which determine instruction count. The 

language may also affect the CPI because of its features; for 

example, a language with heavy support for data abstraction 

(e.g., Java) will require indirect calls, which will use higher CPI 

instructions.
CompilerInstruction count, 
CPI ciency of the compiler affects both the instruction 

count and average cycles per instruction, since the compiler 

determines the translation of the source language instructions 

into computer instructions. The compiler’s role can be very 

complex and affect the CPI in complex ways.
Instruction set 

architectureInstruction count, 

clock rate, CPI
The instruction set architecture affects all three aspects of 

CPU performance, since it affects the instructions needed for a 

function, the cost in cycles of each instruction, and the overall 

clock rate of the processor.
Elaboration: Although you might expect that the minimum CPI is 1.0, as we’ll see in 
Chapter 4, some processors fetch and execute multiple instructions per clock cycle. To 
 ect that approach, some designers invert CPI to talk about 
IPC, or 
instructions per clock cycle. If a processor executes on average 2 instructions per clock cycle, then it has 

an IPC of 2 and hence a CPI of 0.5.instruction mix
 A measure of the dynamic 
frequency of instructions 

across one or many 

programs.
Understanding 
Program 

Performance
 1.7 The Power Wall 
39
40 Chapter 1 Computer Abstractions and Technology
26673300340012.51620002006625360075.395877729.110.14.94.13.3103110100100010,00080286(1982)80386(1985)80486(1989)Pentium(1993)PentiumPro (1997)Pentium 4Willamette(2001)Pentium 4Prescott(2004)Core 2Kentsfield(2007)Clock Rate (MHz)02040
60
80100
120Power (watts)Clock RatePowerCore i5Clarkdale (2010)Core i5Ivy Bridge(2012)FIGURE 1.16 Clock rate and Power for Intel x86 microprocessors over eight generations 
and 25 years.
 e Pentium 4 made a dramatic jump in clock rate and power but less so in performance
 e Prescott thermal problems led to the abandonment of the Pentium 4 line
 e Core 2 line reverts to a simpler 
pipeline with lower clock rates and multiple processors per chip
 e Core i5 pipelines follow in its footsteps.
Elaboration:  xed, to save energy 
or temporarily boost performance, today’s processors can vary their clock rates, so we 
would need to use the average
 clock rate for a program. For example, the Intel Core i7 

will temporarily increase clock rate by about 10% until the chip gets too warm. Intel calls 

this Turbo mode
.A given application written in Java runs 15 seconds on a desktop processor. A new 
Java compiler is released that requires only 0.6 as many instructions as the old 

compiler. Unfortunately, it increases the CPI by 1.1. How fast can we expect the 

application to run using this new compiler? Pick the right answer from the three 

choices below:
a. 1506
1182... sec
b. 15 
 0.6  1.1  9.9 sec
c. 1511
06275
... sec
 1.7 The Power Wall
Figure 1.16
 shows the increase in clock rate and power of eight generations of Intel 

microprocessors over 30 years. Both clock rate and power increased rapidly for 

decades, and th
 attened o
  recently
 e reason they grew together is that they 
are correlated, and the reason for their recent slowing is that we have run into the 

practical power limit for cooling commodity microprocessors.
Check Yourself

Although power provides a limit to what we can cool, in the PostPC Era the 
really critical resource is energy. Battery life can trump performance in the personal 
mobile device, and the architects of warehouse scale computers try to reduce the 

costs of powering and cooling 100,000 servers as the costs are high at this scale. Just 

as measuring time in seconds is a safer measure of program performance than a 

rate like MIPS (see Section 1.10), the energy metric joules is a better measure than 

a power rate like watts, which is just joules/second.
 e dominant technology for integrated circuits is called CMOS (complementary 
metal oxide semiconductor). For CMOS, the primary source of energy consumption 

is so-called dynamic energy—that is, energy that is consumed when transistors 

switch states from 0 to 1 and vice vers
 e dynamic energy depends on the 
capacitive loading of each transistor and the voltage applied:
EnergyCapacitiveloadVoltage
 2 is equation is the energy of a pulse during the logic transition of 0 
 1  0 or 
1  0  e energy of a single transition is then
EnergyCapacitiveloadVoltage
122/ e power required per transistor is just the product of energy of a transition and 
the frequency of transitions:
PowerCapacitiveloadVoltageFrequencyswitche
d122/ 
Frequency switched is a function of the clock rate
 e capacitive load per transistor 
is a function of both the number of transistors connected to an output (called the 

fanout
) and the technology, which determines the capacitance of both wires and 
transistors.
With regard to 
Figure 1.16
, how could clock rates grow by a factor of 1000 
while power grew by only a factor of 30? Energy and thus power can be reduced by 

lowering the voltage, which occurred with each new generation of technology, and 

power is a function of the voltage squared. Typically, the voltage was reduced about 

15% per generation. In 20 years, voltages have gone from 5 V to 1 V, which is why 

the increase in power is only 30 times.
Relative Power
Suppose we developed a new, simpler processor that has 85% of the capacitive 

load of the more complex older processor. Further, assume that it has adjustable 

voltage so that it can reduce voltage 15% compared to processor B, which 

results in a 15% shrink in frequency. What is the impact on dynamic power?
EXAMPLE 1.7 The Power Wall 
41
42 Chapter 1 Computer Abstractions and Technology
PowerPowerCapacitive loadVoltage
Fnewold

085
085
2..r
requency switched
Capacitive loadVoltageFrequency
085
2.  switched
 us the power ratio is
085052
4..Hence, the new processor uses about half the power of the old processor.
 e problem today is that further lowering of the voltage appears to make the 
transistors too leaky, like water faucets that cannot be completely shut o
 . Even 
today about 40% of the power consumption in server chips is due to leakage. If 
transistors started leaking more, the whole process could become unwieldy.
To try to address the power problem, designers have already attached large 
devices to increase cooling, and they turn o  parts of the chip that are not used in 

a given clock cycle. Although there are many more expensive ways to cool chips 

and thereby raise their power to, say, 300 watts, these techniques are generally 

too expensive for personal computers and even servers, not to mention personal 

mobile devices.
Since computer designers slammed into a power wall, they needed a new way 
forward.
 ey chos
 erent path from the way they designed microprocessors 
for th rst 30 years.
Elaboration: Although dynamic energy is the primary source of energy consumption 
in CMOS, static energy consumption occurs because of leakage cur ows even 
when a transistor is off. In servers, leakage is typically responsible for 40% of the energy 

consumption. Thus, increasing the number of transistors increases power dissipation, 

even if the transistors are always off. A variety of design techniques and technology 

innovations are being deployed to control leakage, but it’s hard to lower voltage further.
Elaboration: Power is a challenge for integrated circuits for two reasons. First, power 
must be brought in and distributed around the chip; modern microprocessors use 

hundreds of pins just for power and ground! Similarly, multiple levels of chip interconnect 

are used solely for power and ground distribution to portions of the chip. Second, power 

is dissipated as heat and must be removed. Server chips can burn more than 100 watts, 

and cooling the chip and the surrounding system is a major expense in Warehouse Scale 

Computers (see Chapter 6).
ANSWER
 1.8 The Sea Change: The Switch from Uniprocessors to Multiprocessors 
43 1.8  The Sea Change: The Switch from Uniprocessors to Multiprocessors
 e power limit has forced a dramatic change in the design of microprocessors. 
Figure 1.17
 shows the improvement in response time of programs for desktop 
microprocessors over time. Since 2002, the rate has slowed from a factor of 1.5 per 

year to a factor of 1.2 per year.
Rather than continuing to decrease the response time of a single program 
running on the single processor, as of 2006 all desktop and server companies are 

shipping microprocessors with multiple processors per chip, where the b
 t is 
 en more on throughput than on response time. To reduce confusion between the 
words processor and microprocessor, companies refer to processors as “cores,” and 

such microprocessors are generically called multicore microprocessors. Hence, a 

“quadcore” microprocessor is a chip that contains four processors or four cores.
In the past, programmers could rely on innovations in hardware, architecture, 
and compilers to double performance of their programs every 18 months without 

having to change a line of code. Today, for programmers to g
 cant 
improvement in response time, they need to rewrite their programs to take 

advantage of multiple processors. Moreover, to get the historic be
 t of running 
faster on new microprocessors, programmers will have to continue to improve 

performance of their code as the number of cores increases.
To reinforce how the so
 ware and hardware systems work hand in hand, we use 
a special section, 
Hardware/So
 ware Interface
, throughout the book, with th
 rst 
one appearing below
 ese elements summarize important insights at this critical 
interface.
Parallelism
 has always been critical to performance in computing, but it was 
 en hidden. Chapter 4 will explain 
pipelining
, an elegant technique that runs 
programs faster by overlapping the execution of instruction
 is is one example of 
instruction-level parallelism
, where the parallel nature of the hardware is abstracted 
away so the programmer and compiler can think of the hardware as executing 
instructions sequentially.
Forcing programmers to be aware of the parallel hardware and to explicitly 
rewrite their programs to be parallel had been the “third rail” of computer 

architecture, for companies in the past that depended on such a change in behavior 

failed (see 
 Section 6.15
). From this historical perspective, it’s startling that the 
whole IT industry has bet its future that programmer
 nally successfully 
switch to explicitly parallel programming.
Up to now, most 

so
 ware has been like 
music written for a 

solo performer; with 

the current generation 

of chips we’re getting a 

little experience with 

duets and quartets and 

other small ensembles; 

but scoring a work for 

large orchestra and 

chorus is a di
 erent 
kind of challenge.
Brian Hayes, 
Computing 
in a Parallel Universe,
 2007.Hardware/ 
Software 

Interface
44 Chapter 1 Computer Abstractions and Technology
15913182451801171832804816499931,2671,7793,0164,1956,0436,6817,10811,86514,38719,48421,87124,129110100100010,000100,00019781980198219841986198819901992199419961998200020022004200620082010
20142012Performance (vs. VAX-11/780)25%/year
52%/year
22%/year
 IBM POWERstation 100, 150 MHz
Digital Alphastation 4/266, 266 MHzDigital Alphastation 5/300, 300 MHzDigital Alphastation 5/500, 500 MHz AlphaServer 4000 5/600, 600 MHz 21164
Digital AlphaServer 8400 6/575, 575 MHz 21264
Professional Workstation XP1000, 667 MHz 21264A
Intel VC820 motherboard, 1.0 GHz Pentium III processor
 IBM Power4, 1.3 GHz
 Intel Xeon EE 3.2 GHz AMD Athlon, 2.6 GHz Intel Core 2 Extreme 2 cores, 2.9 GHz 
 Intel Core Duo Extreme 2 cores, 3.0 GHz
 Intel Core i7 Extreme 4 cores 3.2 GHz (boost to 3.5 GHz) Intel Xeon 4 cores, 3.3 GHz (boost to 3.6 GHz)
 Intel Xeon 6 cores, 3.3 GHz (boost to 3.6 GHz)
Intel D850EMVR motherboard (3.06 GHz, Pentium 4 processor with Hyper-threading Technology)
1.5, VAX-11/785
 AMD Athlon 64, 2.8 GHzDigital 3000 AXP/500, 150 MHzHP 9000/750, 66 MHzIBM RS6000/540, 30 MHzMIPS M2000, 25 MHz MIPS M/120, 16.7 MHzSun-4/260, 16.7 MHzVAX 8700, 22 MHz
AX-11/780, 5 MHz Intel Core i7 4 cores 3.4 GHz (boost to 3.8 GHz)31,999Intel Xeon 4 cores 3.6 GHz (Boost to 4.0) 34,967FIGURE 1.17 Growth in processor performance since the mid-1980s.
 is chart plots performance relative to the VAX 11/780 
as measured by the SPECint benchmarks (see Section 1.10). Prior to the mid-1980s, processor performance growth was largely tech
nology-
driven and averaged about 25% per year
 e increase in growth to about 52% since then is attributable to more advanced architectural and 
organizational ide
 e higher annual performance improvement of 52% since the mid-1980s meant performance was about a factor of seven 
higher in 2002 than it would have been had it stayed at 25%. Since 2002, the limits of power, available instruction-level paral
lelism, and long 
memory latency have slowed uniprocessor performance recently, to about 22% per year.
Why has it been so hard for programmers to write explicitly parallel programs? 
 e 
 rst reason is that parallel programming is b
 nition performance 
programming, which increases th
  culty of programming. Not only does the 
program need to be correct, solve an important problem, and provide a useful 
interface to the people or other programs that invoke it, the program must also be 

fast. Otherwise, if you don’t need performance, just write a sequential program.
 e second reason is that fast for parallel hardware means that the programmer 
must divide an application so that each processor has roughly the same amount to 

do at the same time, and that the overhead of scheduling and coordination doesn’t 

fritter away the potential performance b
 ts of parallelism.
As an analogy, suppose the task was to write a newspaper story. Eight reporters 
working on the same story could potentially write a story eight times faster. To achieve 

this increased speed, one would need to break up the task so that each reporter had 

something to do at the same time
 us, we must 
schedule
 the sub-tasks. If anything 
went wrong and just one reporter took longer than the seven others did, then the 

bene
 ts of having eight writers would be diminished
 us, we must 
balance the 

 1.8 The Sea Change: The Switch from Uniprocessors to Multiprocessors 
45load evenly to get the desired speedup. Another danger would be if reporters had to 
spend a lot of time talking to each other to write their sections. You would also fall 
short if one part of the story, such as the conclusion, couldn’t be written until all of 

the other parts were completed
 us, care must be taken to 
reduce communication 
and synchronization overhead
. For both this analogy and parallel programming, the 
challenges include scheduling, load balancing, time for synchronization, and overhead 

for communication between the parties. As you might guess, the challenge
 er with 
more reporters for a newspaper story and more processors for parallel programming.
To re
 ect this sea change in the industry, th
 ve chapters in this edition of the 
book each have a section on the implications of the parallel revolution to that chapter:
 Chapter 2, Section 2.11: Parallelism and Instructions: Synchronization
. Usually 
independent parallel tasks need to coor
dinate at times, such as to say when 
they have completed their wo
 is chapter explains the instructions used 
by multicore processors to synchronize tasks.
 Chapter 3, Section 3.6: Parallelism and Computer Arithmetic: Subword 

Parallelism
. Perhaps the simplest form of parallelism to build involves 
computing on elements in parallel, such as when multiplying two vectors. 

Subword parallelism takes advantage of the resources supplied by 
Moore’s 
Law
 to provider wider arithmetic units that can operate on many operands 
simultaneously.
 Chapter 4, Section 4.10: Parallelism via Instructions
. Given th
  culty of 
explicitly parallel programming, tremendo
 ort was invested in the 1990s 
in having the hardware and the compiler uncover implicit parallelism, initially 

via pipelining
 is chapter describes some of these aggressive techniques, 
including fetching and executing multiple instructions simultaneously and 

guessing on the outcomes of decisions, and executing instructions speculatively 

using 
prediction
. Chapter 5, Section 5.10: Parallelism and 
Memory Hierarchies
: Cache 
Coherence
. One way to lower the cost of communication is to have all 
processors use the same address space, so that any processor can read or 

write any data. Given that all processors today use caches to keep a temporary 

copy of the data in faster memory near the processor, it’s easy to imagine that 

parallel programming would be even more
  cult if the caches associated 
with each processor had inconsistent values of the shared dat
 is chapter 
describes the mechanisms that keep the data in all caches consistent.
 Chapter 5,  Section 5.11: Parallelism and Memory Hierarchy: Redundant 

Arrays of Inexpensive Disks
 is section describes how using many disks 
in conjunction can o
 er much higher throughput, which was the original 
inspiration of 
Redundant Arrays of Inexpensive Disks
 e real 
popularity of RAID proved to be to the much greater dependability o
 ered 
by including a modest number of redundan
 e section explains the 
 erences in performance, cost, and dependability between th
 erent 
RAID levels.

46 Chapter 1 Computer Abstractions and Technology
In addition to these sections, there is a full chapter on parallel processing. Chapter 6 
goes into more detail on the challenges of parallel programming; presents the 

two contrasting approaches to communication of shared addressing and explicit 

message passing; describes a restricted model of parallelism that is easier to 

program; discusses th
  culty of benchmarking parallel processors; introduces 
a new simple performance model for multicore microprocessors; and,
 nally, 
describes and evaluates four examples of multicore microprocessors using this 

model.
As mentioned above, Chapters 3 to 6 use matrix vector multiply as a running 
example to show how each type of parallelism ca
 cantly increase performance. 
 Appendix C
 describes an increasingly popular hardware component that 
is included with desktop computers, the 
graphics processing unit
 (GPU). Invented 
to accelerate graphics, GPUs are becoming programming platforms in their 

own right. As you might expect, given these times, GPUs rely on 
parallelism.
 Appendix C
 describes the NVIDIA GPU and highlights parts of its parallel 
programming environment. 
 1.9  Real Stuff: Benchmarking the 
Intel Core i7Each chapter has a section entitled “Real St ” that ties the concepts in the book 
with a computer you may use every day
 ese sections cover the technology 
underlying modern computers. For th
 rst “Real St ” section, we look at 
how integrated circuits are manufactured and how performance and power are 

measured, with the Intel Core i7 as the example.
SPEC CPU Benchmark
A computer user who runs the same programs day in and day out would be the 

perfect candidate to evaluate a new computer
 e set of programs run would form 
a workload
. To evaluate two computer systems, a user would simply compare 
the execution time of the workload on the two computers. Most users, however, 
are not in this situation. Instead, they must rely on other methods that measure 

the performance of a candidate computer, hoping that the methods will r
 ect how well the computer will perform with the user’s workload
 is alternative is 
usually followed by evaluating the computer using a set of 
benchmarks
—programs 
sp cally chosen to measure performance
 e benchmarks form a workload that 
the user hopes will predict the performance of the actual workload. As we noted 

above, to make the 
common case fast
, yo
 rst need to know accurately which case 
is common, so benchmarks play a critical role in computer architecture.
SPEC (
System Performance Evaluation Cooperative
) is a
 ort funded and 
supported by a number of computer vendors to create standard sets of benchmarks 

for modern computer systems. In 1989, SPEC originally created a benchmark 
I thought [computers] 

would be a universally 

applicable idea, like 

a book is. But I didn’t 

think it would develop 

as fast as it did, because 

I didn’t envision we’d 

be able to get as many 

parts on a chip as 

 nally go
 e transistor came along 

unexpectedly. It all 

happened much faster 

than we expected.
J. Presper Eckert, 
coinventor of ENIAC, 

speaking in 1991
workload
 A set of 
programs run on a 
computer that is either 

the actual collection of 

applications run by a user 

or constructed from real 

programs to approximate 

such a mix. A typical 

workload sp
 es both 
the programs and the 

relative frequencies.
benchmark
 A program 
selected for use in 

comparing computer 

performance.

set focusing on processor performance (now called SPEC89), which has evolved 
throug
 ve generation
 e latest is SPEC CPU2006, which consists of a set of 12 
integer benchmarks (CINT2006) an
 oating-point benchmarks (CFP2006). 
 e integer benchmarks vary from part of a C compiler to a chess program to a 
quantum computer simulatio
 e 
 oating-point benchmarks include structured 
grid codes fo
 nite element modeling, particle method codes for molecular 
dynamics, and sparse linear algebra codes fo
 uid dynamics.
Figure 1.18
 describes the SPEC integer benchmarks and their execution time 
on the Intel Core i7 and shows the factors that explain execution time: instruction 

count, CPI, and clock cycle time. Note that CPI varies by more than a factor of 5.
To simplify the marketing of computers, SPEC decided to report a single number 
to summarize all 12 integer benchmarks. Dividing the execution time of a reference 

processor by the execution time of the measured computer normalizes the execution 

time measurements; this normalization yields a measure, called the 
SPECratio,
 which 
has the advantage that bigger numeric results indicate faster performance.
 at is, 
the SPECratio is the inverse of execution time. A CINT2006 or CFP2006 summary 

measurement is obtained by taking the geometric mean of the SPECratios.
Elaboration: When comparing two computers using SPECratios, use the geometric 
mean so that it gives the same relative answer no matter what computer is used to 
normalize the results. If we averaged the normalized execution time values with an 

arithmetic mean, the results would vary depending on the computer we choose as the 

reference. 1.9 Real Stuff: Benchmarking the Intel Core i7 
47FIGURE 1.18 SPECINTC2006 benchmarks running on a 2.66 GHz Intel Core i7 920.
 As the equation on page 35 explains, 
execution time is the product of the three factors in this table: instruction count in billions, clocks per instruction (CPI), and clock cycle time in 
nanoseconds. SPECratio is simply the reference time, which is 
supplied by SPEC, divided by the measured execution time
 e single number 
quoted as SPECINTC2006 is the geometric mean of the SPECratios.
DescriptionName
InstructionCount x 109CPIClock cycle time(seconds x 10Œ9)ExecutionTime  
(seconds)Reference
Time  
(seconds)SPECratio
Interpreted string processing perl 
 2252    0.60    0.376  
508  9770   19.2 
Block-sorting bzip2  2390   0.70   0.376  629  9650   15.4 
compressionGNU C compiler gcc  794   1.20   0.376  358  8050   22.5 

Combinatorial optimization mcf  221   2.66   0.376  221  9120   41.2 

Go game (AI) go  1274   1.10   0.376  527  10490   19.9 

Search gene sequence hmmer  2616   0.60   0.376  590  9330   15.8 

Chess game (AI) sjeng  1948   0.80   0.376  586  12100   20.7 
Quantum computer libquantum               659   0.44   0.376  109  20720   190.0 

simulation
Video compression h264avc  3793   0.50   0.376  713  22130   31.0 

Discrete event  omnetpp  367   2.10   0.376  290  6250   21.5 
simulation libraryGames/path finding  astar  1250   1.00   0.376  470  7020   14.9 

XML parsing xalancbmk  1045   0.70   0.376  275  6900   25.1 

Geometric mean       Œ                          Œ                     Œ                     Œ 
  Œ    25.7 
Œ
48 Chapter 1 Computer Abstractions and Technology
The formula for the geometric mean is
Execution 
time ratio
iinn1where Execution time ratioi is the execution time, normalized to the reference computer, 
for the ith program of a total of n in the workload, and
aaaa
inin means the product 121–SPEC Power Benchmark
Given the increasing importance of energy and power, SPEC added a benchmark 
to measure power. It reports power consumption of servers at
 erent workload 
levels, divided into 10% increments, over a period of time. 
Figure 1.19
 shows the 

results for a server using Intel Nehalem processors similar to the above.
FIGURE 1.19 SPECpower_ssj2008 running on a dual socket 2.66 GHz Intel Xeon X5650 
with 16 GB of DRAM and one 100 GB SSD disk.
Target Load %Performance  (ssj_ops)Average Power  (watts) 100% 865,618 258 90% 786,688 242 80% 698,051 224 70% 607,826 204 60% 521,391 185 50% 436,757 170 40% 345,919 157 30% 262,071 146 20% 176,061 135 10% 86,784 121 0% 0 80  Overall Sum  
4,787,166 1922   ssj_ops / power =  2490SPECpower started with another SPEC benchmark for Java business applications 
(SPECJBB2005), which exercises the processors, caches, and main memory as well 

as the Java virtual machine, compiler, garbage collector, and pieces of the operating 

system. Performance is measured in throughput, and the units are business 

operations per second. Once again, to simplify the marketing of computers, SPEC 

 1.10 Fallacies and Pitfalls 
49boils these numbers down to a single number, called “overall ssj_ops per watt.
 e formula for this single summarizing metric is
overall ssj_ops
 per wattssj_ops
power
iii010
ii
010
where ssj_ops
i is performance at each 10% increment and power
i is power 
consumed at each performance level.
  Fallacies and Pitfalls
 e purpose of a section on fallacies and pitfalls, which will be found in every 
chapter, is to explain some commonly held misconceptions that you might 
encounter. We call them 
fallacies
. When discussing a fallacy, we try to give a 
counterexample. We also discuss 
pitfalls
, or easily made mistak
 en pitfalls are 
generalizations of principles that are only true in a limited cont
 e purpose 
of these sections is to help you avoid making these mistakes in the computers you 

may design or use. Cost/performance fallacies and pitfalls have ensnared many a 

computer architect, including us. Accordingly, this sectio
 ers no shortage of 
relevant examples. We start with a pitfall that traps many designers and reveals an 

important relationship in computer design.
Pitfall: Expecting the improvement of one aspect of a computer to increase overall 

performance by an amount proportional to the size of the improvement.
 e great idea of making the 
common case fast
 has a demoralizing corollary 
that has plagued designers of both hardware and so
 ware. It reminds us that the 
opportunity for improvement is a
 ected by how much time the event consumes.
A simple design problem illustrates it well. Suppose a program runs in 100 
seconds on a computer, with multiply operations responsible for 80 seconds of this 

time. How much do I have to improve the speed of multiplication if I want my 

program to r
 ve times faster?
 e execution time of the program a
 er making the improvement is given by 
the following simple equation known as 
Amdahl’s Law
:Execution time 
mprovement
Execution tiyy improvement
Amount of improvement
Execution time unaec
ttedFor this problem:
Execution time 
mprovement
 seconds
 secon
8010080
n(dds)
Science must begin 
with myths, and the 

criticism of myths.
Sir Karl Popper, 
 e Philosophy of Science,
 1957Amdahl’s Law
 A rule stating that 
the performance 

enhancement possible 

with a given improvement 

is limited by the amount 

that the improved feature 

is used. It is a quantitative 

version of the law of 

diminishing returns.
1.10
50 Chapter 1 Computer Abstractions and Technology
Since we want the performance to be
 ve times faster, the new execution time 
should be 20 seconds, giving
208020080 seconds
 seconds
 seconds
 seconds
nn at is, there is 
no amount
 by which we can enhance-multiply to achiev
 vefold 
increase in performance, if multiply accounts for only 80% of the workload
 e performance enhancement possible with a given improvement is limited by the amount 
that the improved feature is used. In everyday life this concept also yields what we call 

the law of diminishing returns.
We can use Amdahl’s Law to estimate performance improvements when we 
know the time consumed for some function and its potential speedup. Amdahl’s 

Law, together with the CPU performance equation, is a handy tool for evaluating 

potential enhancements. Amdahl’s Law is explored in more detail in the exercises.
Amdahl’s Law is also used to argue for practical limits to the number of parallel 
processors. We examine this argument in the Fallacies and Pitfalls section of 

Chapter 6.
Fallacy: Computers at low utilization use little power.
Pow  ciency matters at low utilizations because server workloads vary. 

Utilization of servers in Google’s warehouse scale computer, for example, is 

between 10% and 50% most of the time and at 100% less than 1% of the time. Even 

giv
 ve years to learn how to run the SPECpower benchmark well, the specially 
co
 gured computer with the best results in 2012 still uses 33% of the peak power 
at 10% of the load. Systems in th
 eld that are not co
 gured for the SPECpower 
benchmark are surely worse.
Since servers’ workloads vary but use a large fraction of peak power, Luiz 
Barroso and Urs Hölzle [2007] argue that we should redesign hardware to achieve 

“energy-proportional computing.” If future servers used, say, 10% of peak power at 

10% workload, we could reduce the electricity bill of datacenters and become good 

corporate citizens in an era of increasing concern about CO
2 emissions.
Fallacy: Designing for performance and designing for energy e
  ciency are 
unrelated goals.
Since energy is power over time, it is o
 en the case that hardware or so
 ware 
optimizations that take less time save energy overall even if the optimization takes 

a bit more energy when it is used. One reason is that all of the rest of the computer is 

consuming energy while the program is running, so even if the optimized portion 

uses a little more energy, the reduced time can save the energy of the whole system.
Pitfall: Using a subset of the performance equation as a performance metric.
We have already warned about the danger of predicting performance based on 

simply one of clock rate, instruction count, or CPI. Another common mistake 

 1.10 Fallacies and Pitfalls 
51is to use only two of the three factors to compare performance. Although using 
two of the three factors may be valid in a limited context, the concept is also 

easily misused. Indeed, nearly all proposed alternatives to the use of time as the 

performance metric have led eventually to misleading claims, distorted results, or 

incorrect interpretations.
One alternative to time is 
MIPS (million instructions per second)
. For a given 
program, MIPS is simply
MIPSInstruction count
Execution time
106Since MIPS is an instruction execution rate, MIPS sp
 es performance inversely 
to execution time; faster computers have a higher MIPS rating
 e good news 
about MIPS is that it is easy to understand, and faster computers mean bigger 

MIPS, which matches intuition.
 ere are three problems with using MIPS as a measure for comparing computers. 
First, MIPS sp
 es the instruction execution rate but does not take into account 
the capabilities of the instructions. We cannot compare computers with
 erent 
instruction sets using MIPS, since the instruction counts will certainl
 er. 
Second, MIPS varies between programs on the same computer; thus, a computer 

cannot have a single MIPS rating. For example, by substituting for execution time, 

we see the relationship between MIPS, clock rate, and CPI:
MIPSInstruction count
Instruction countCPI
Clock rate106CClock rateCPI
106 e CPI varied by a factor of 5 for SPEC CPU2006 on an Intel Core i7 computer 
in Figure 1.18
, so MIPS does as well. Finally, and most importantly, if a new 
program executes more instructions but each instruction is faster, MIPS can vary 

independently from performance!
Consider the following performance measurements for a program:
MeasurementComputer AComputer B
Instruction count10 billion
8 billionClock rate4 GHz
4 GHzCPI1.01.1a. Which computer has the higher MIPS rating?
b. Which computer is faster?
million instructions 
per second (MIPS)
 A measurement of 
program execution speed 

based on the number of 

millions of instructions. 

MIPS is computed as the 

instruction count divided 

by the product of the 

execution time and 10
6.Check 
Yourself

52 Chapter 1 Computer Abstractions and Technology
 1.11 Concluding Remarks
Although it is di
  cult to predict exactly what level of cost/performance computers 
will have in the future, it’s a safe bet that they will be much better than they are 
today. To participate in these advances, computer designers and programmers 

must understand a wider variety of issues.
Both hardware and so
 ware designers construct computer systems in hierarchical 
layers, with each lower layer hiding details from the level above
 is great idea 
of 
abstraction
 is fundamental to understanding today’s computer systems, but it 
does not mean that designers can limit themselves to knowing a single abstraction. 

Perhaps the most important example of abstraction is the interface between 

hardware and low-level so
 ware, called the 
instruction set architecture
. Maintaining 
the instruction set architecture as a constant enables many implementations of 

that architecture—presumably varying in cost and performance—to run identical 

so
 ware. On the downside, the architecture may preclude introducing innovations 
that require the interface to change.
 ere is a reliable method of determining and reporting performance by using 
the execution time of real programs as the metr
 is execution time is related to 
other important measurements we can make by the following equation:
Seconds
Program
Instructions
Program
Clock cycles
Instruction
Seconds
Clock cycle
We will use this equation and its constituent factors many times. Remember, 

though, that individually the factors do not determine performance: only the 

product, which equals execution time, is a reliable measure of performance.
Execution time is the only valid and unimpeachable measure of 

performance. Many other metrics have been proposed and found wanting. 

Sometimes these metrics ar
 awed from the start by not re
 ecting 
execution time; other times a metric that is valid in a limited context 

is extended and used beyond that context or without the additional 

clari
 cation needed to make it valid.
The BIGPictureWhere … the ENIAC 

is equipped with 

18,000 vacuum tubes 

and weighs 30 tons, 

computers in the 

future may have 1,000 

vacuum tubes and 

perhaps weigh just 1½ 

tons.
Popular Mechanics
, March 1949

 1.11 Concluding Remarks 
53 e key hardware technology for modern processors is silicon. Equal in 
importance to an understanding of integrated circuit technology is an understanding 
of the expected rates of technological change, as predicted by 
Moore’s Law.
 While 
silicon fuels the rapid advance of hardware, new ideas in the organization of 

computers have improved price/performance. Two of the key ideas are exploiting 

parallelism in the program, typically today via multiple processors, and exploiting 

locality of accesses to a 
memory hierarchy
, typically via caches.
Energy
  ciency has replaced die area as the most critical resource of 
microprocessor design. Conserving power while trying to increase performance 

has forced the hardware industry to switch to multicore microprocessors, thereby 

forcing the so
 ware industry to switch to programming parallel hardware. 
Parallelism
 is now required for performance.
Computer designs have always been measured by cost and performance, as well 
as other important factors such as energy, dependability, cost of ownership, and 

scalability. Although this chapter has focused on cost, performance, and energy, 

the best designs will strike the appropriate balance for a given market among all 

the factors.
Road Map for This BookAt the bottom of these abstractions are th
 ve classic components of a computer: 
datapath, control, memory, input, and output (refer to 
Figure 1.5
 ese 
 ve 
components also serve as the framework for the rest of the chapters in this book:
 Datapath:
 Chapter 3, Chapter 4, Chapter 6, and 
 Appendix C
 Control:
 Chapter 4, Chapter 6, and 
 Appendix C
 Memory:
 Chapter 5
 Input:
 Chapters 5 and 6
 Output:
 Chapters 5 and 6
As mentioned above, Chapter 4 describes how processors exploit implicit 

parallelism, Chapter 6 describes the explicitly parallel multicore microprocessors 

that are at the heart of the parallel revolution, and 
 Appendix C
 describes 
the highly parallel graphics processor chip. Chapter 5 describes how a memory 
hierarchy exploits locality. Chapter 2 describes instruction sets—the interface 

between compilers and the computer—and emphasizes the role of compilers and 

programming languages in using the features of the instruction set. Appendix A 

provides a reference for the instruction set of Chapter 2. Chapter 3 describes how 

computers handle arithmetic data. Appendix B introduces logic design. 

54 Chapter 1 Computer Abstractions and Technology
1.12  Historical Perspective and Further 
ReadingFor each chapter in the text, a section devoted to a historical perspective can be 
found online on a site that accompanies this book. We may trace the development 

of an idea through a series of computers or describe some important projects, and 

we provide references in case you are interested in probing further.
 e historical perspective for this chapter provides a background for some of the 
key ideas presented in this opening chapter. Its purpose is to give you the human 

story behind the technological advances and to place achievements in their historical 

context. By understanding the past, you may be better able to understand the forces 

that will shape computing in the future. Each Historical Perspective section online 

ends with suggestions for further reading, 
which are also collected separately online 
under the section “
Further Reading
.”
 e rest of 
 Section 1.12
 is found online.
 1.13 Exercises e relative time ratings of exercises are shown in square brackets a
 er each 
exercise number. On average, an exercise rated [10] will take you twice as long as 

one rated [5]. Sections of the text that should be read before attempting an exercise 

will be given in angled brackets; for example, <§1.4> means you should have read 

Section 1.4, Under the Covers, to help you solve this exercise.
1.1 [2] <§1.1> Aside from the smart cell phones used by a billion people, list and 
describe four other types of computers.

1.2  e eight great ideas in computer architecture are similar to ideas 
from ot
 elds.  Match the eight ideas from computer architecture, “Design for 
Moore’s Law”, “Use Abstraction to Simplify Design”, “Make the Common Case 
Fast”, “Performance via Parallelism”, “Performance via Pipelining”, “Performance 

via Prediction”, “Hierarchy of Memories”, and “Dependability via Redundancy” to 

the following ideas from ot
 elds:a. Assembly lines in automobile manufacturing
b. Suspension bridge cables
c. Aircra
  and marine navigation systems that incorporate wind information
d. Express elevators in buildings
An acti
 eld of 
science is like an 
immense anthill; the 

individual almost 

vanishes into the mass 

of minds tumbling over 

each other, carrying 

information from place 

to place, passing it 

around at the speed of 

light.
Le
 omas, “Natural 
Science,” in 
 e Lives of 
a Cell
, 1974
 1.13 Exercises 55e. Library reserve desk
f. Increasing the gate area on a CMOS transistor to decrease its switching time
g. Adding electromagnetic aircra
  catapults (which are electrically-powered 
as opposed to current steam-powered models), allowed by the increased power 
generation o
 ered by the new reactor technology
h. Building self-driving cars whose control systems partially rely on existing sensor 

systems already installed into the base vehicle, such as lane departure systems and 

smart cruise control systems
1.3 [2] <§1.3> Describe the steps that transform a program written in a high-level 
language such as C into a representation that is directly executed by a computer 
processor.
1.4 [2] <§1.4> Assume a color display using 8 bits for each of the primary colors 
(red, green, blue) per pixel and a frame size of 1280 
× 1024.a. What is the minimum size in bytes of the frame b
 er to store a frame?
b. How long would it take, at a minimum, for the frame to be sent over a 100 
Mbit/s network?
1.5 [4] <§1.6> Consider thre
 erent processors P1, P2, and P3 executing 
the same instruction set.  P1 has a 3 GHz clock rate and a CPI of 1.5.  P2 has a 
2.5 GHz clock rate and a CPI of 1.0.  P3 has a 4.0 GHz clock rate and has a CPI 

of 2.2.
a. Which processor has the highest performance expressed in instructions per second?

b. If the processors each execute a program in 10 seco
 nd the number of 
cycles and the number of instructions.

c. We are trying to reduce the execution time by 30% but this leads to an increase 
of 20% in the CPI. What clock rate should we have to get this time reduction?
1.6 [20] <§1.6> Consider tw
 erent implementations of the same instruction 
set architecture
 e instructions can be divided into four classes according to 
their CPI (class A, B, C, and D). P1 with a clock rate of 2.5 GHz and CPIs of 1, 2, 3, 
and 3, and P2 with a clock rate of 3 GHz and CPIs of 2, 2, 2, and 2.
Given a program with a dynamic instruct
ion count of 1.0E6 instructions divided 
into classes as follows: 10% class A, 20% 
class B, 50% class C, and 20% class D, 
which implementation is faster?

a. What is the global CPI for each implementation?

b. Find the clock cycles required in both cases.

56 Chapter 1 Computer Abstractions and Technology
1.7 [15] <§1.6> Compilers can have a profound impact on the performance 
of an application. Assume that for a program, compiler A results in a dynamic 
instruction count of 1.0E9 and has an execution time of 1.1 s, while compiler B 

results in a dynamic instruction count of 1.2E9 and an execution time of 1.5 s.
a. Find the average CPI for each program given that the processor has a clock cycle 
time of 1 ns.
b. Assume the compiled programs run on tw
 erent processors. If the execution 
times on the two processors are the same, how much faster is the clock of the 
processor running compiler A’s code versus the clock of the processor running 

compiler B’s code?
c. A new compiler is developed that uses only 6.0E8 instructions and has an 
average CPI of 1.1. What is the speedup of using this new compiler versus using 
compiler A or B on the original processor?
1.8  e Pentium 4 Prescott processor, released in 2004, had a clock rate of 3.6 
GHz and voltage of 1.25 V.  Assume that, on average, it consumed 10 W of static 
power and 90 W of dynamic power.
 e Core i5 Ivy Bridge, released in 2012, had a clock rate of 3.4 GHz and voltage 
of 0.9 V.  Assume that, on average, it consumed 30 W of static power and 40 W of 
dynamic power.
1.8.1 [5] <§1.7> For each processo
 nd the average capacitive loads.
1.8.2 [5] <§1.7> Find the percentage of the total dissipated power comprised by 
static power and the ratio of static power to dynamic power for each technology.
1.8.3 [15] <§1.7> If the total dissipated power is to be reduced by 10%, how much 
should the voltage be reduced to maintain the same leakage current?  Note:  power 

 ned as the product of voltage and current.
1.9 Assume for arithmetic, load/store, and branch instructions, a processor has 
CPIs of 1, 12, and 5, respectively.  Also assume that on a single processor a program 
requires the execution of 2.56E9 arithmetic instructions, 1.28E9 load/store 

instructions, and 256 million branch instructions.  Assume that each processor has 

a 2 GHz clock frequency.
Assume that, as the program is parallelized to run over multiple cores, the number 
of arithmetic and load/store instructions per processor is divided by 0.7 x 
p (where 
p is the number of processors) but the number of branch instructions per processor 

remains the same.
1.9.1 [5] <§1.7> Find the total execution time for this program on 1, 2, 4, and 8 
processors, and show the relative speedup of the 2, 4, and 8 processor result relative 

to the single processor result. 

 1.13 Exercises 571.9.2 [10] <§§1.6, 1.8> If the CPI of the arithmetic instructions was doubled, 
what would the impact be on the execution time of the program on 1, 2, 4, or 8 

processors?
1.9.3 [10] <§§1.6, 1.8> To what should the CPI of load/store instructions be 
reduced in order for a single processor to match the performance of four processors 

using the original CPI values?
1.10 Assume a 15 cm diameter wafer has a cost of 12, contains 84 dies, and has 
0.020 defects/cm
2. Assume a 20 cm diameter wafer has a cost of 15, contains 100 
dies, and has 0.031 defects/cm
2.1.10.1 [10] <§1.5> Find the yield for both wafers.
1.10.2 [5] <§1.5> Find the cost per die for both wafers.

1.10.3 [5] <§1.5> If the number of dies per wafer is increased by 10% and the 
defects per area unit increases b
 nd the die area and yield.
1.10.4 [5] <§1.5> Assume a fabrication process improves the yield from 0.92 to 

0.95.  Find the defects per area unit for each version of the technology given a die 

area of 200 mm
2.1.11  e results of the SPEC CPU2006 bzip2 benchmark running on an AMD 
Barcelona has an instruction count of 2.389E12, an execution time of 750 s, and a 

reference time of 9650 s.
1.11.1 [5] <§§1.6, 1.9> Find the CPI if the clock cycle time is 0.333 ns.

1.11.2 [5] <§1.9> Find the SPECratio.

1.11.3 [5] <§§1.6, 1.9> Find the increase in CPU time if the number of instructions 
of the benchmark is increased by 10% without a
 ecting the CPI.
1.11.4 [5] <§§1.6, 1.9> Find the increase in CPU time if the number of instructions 

of the benchmark is increased by 10% and the CPI is increased by 5%.
1.11.5 [5] <§§1.6, 1.9> Find the change in the SPECratio for this change.

1.11.6 [10] <§1.6> Suppose that we are developing a new version of the AMD 
Barcelona processor with a 4 GHz clock rate. We have added some additional 

instructions to the instruction set in such a way that the number of instructions 

has been reduced b
 e execution time is reduced to 700 s and the new 
SPECratio is 13.7.  Find the new CPI.
1.11.7 
 is CPI value is larger than obtained in 1.11.1 as the clock 
rate was increased from 3 GHz to 4 GHz. Determine whether the increase in the 
CPI is similar to that of the clock rate. If they are dissimilar, why?
1.11.8 [5] <§1.6> By how much has the CPU time been reduced?

58 Chapter 1 Computer Abstractions and Technology
1.11.9 [10] <§1.6> For a second benchmark, libquantum, assume an execution 
time of 960 ns, CPI of 1.61, and clock rate of 3 GHz.  If the execution time is 

reduced by an additional 10% without a
 ecting to the CPI and with a clock rate of 
4 GHz, determine the number of instructions.
1.11.10 [10] <§1.6> Determine the clock rate required to give a further 10% 
reduction in CPU time while maintaining the number of instructions and with the 

CPI unchanged.
1.11.11 [10] <§1.6> Determine the clock rate if the CPI is reduced by 15% and 
the CPU time by 20% while the number of instructions is unchanged.
1.12 Section 1.10 cites as a pitfall the utilization of a subset of the performance 
equation as a performance metric. To illustrate this, consider the following two 
processors. P1 has a clock rate of 4 GHz, average CPI of 0.9, and requires the 

execution of 5.0E9 instructions.  P2 has a clock rate of 3 GHz, an average CPI of 

0.75, and requires the execution of 1.0E9 instructions.
1.12.1 [5] <§§1.6, 1.10> One usual fallacy is to consider the computer with the 
largest clock rate as having the largest performance. Check if this is true for P1 and 

P2.1.12.2 [10] <§§1.6, 1.10> Another fallacy is to consider that the processor executing 
the largest number of instructions will need a larger CPU time. Considering that 

processor P1 is executing a sequence of 1.0E9 instructions and that the CPI of 

processors P1 and P2 do not change, determine the number of instructions that P2 

can execute in the same time that P1 needs to execute 1.0E9 instructions.
1.12.3 [10] <§§1.6, 1.10> A common fallacy is to use MIPS (millions of 
instructions per second) to compare the performance of tw
 erent processors, 
and consider that the processor with the largest MIPS has the largest performance. 

Check if this is true for P1 and P2.
1.12.4 [10] <§1.10> Another common performa
 gure is MFLOPS (millions 
of
 oating-point operations per seco
 ned as
MFLOPS = No. FP operations / (execution time 
× 1E6)but th gure has the same problems as MIPS. Assume that 40% of the instructions 
executed on both P1 and P2 ar
 oating-point instructions.  Find the MFLOPS 
 gures for the programs.
1.13 Another pitfall cited in Section 1.10 is expecting to improve the overall 
performance of a computer by improving only one aspect of the computer. Consider 

a computer running a program that requires 250 s, with 70 s spent executing FP 

instructions, 85 s executed L/S instructions, and 40 s spent executing branch 

instructions.
1.13.1 [5] <§1.10> By how much is the total time reduced if the time for FP 
operations is reduced by 20%?

 1.13 Exercises 591.13.2 [5] <§1.10> By how much is the time for INT operations reduced if the 
total time is reduced by 20%?
1.13.3 [5] <§1.10> Can the total time can be reduced by 20% by reducing only 
the time for branch instructions?
1.14 Assume a program requires the execution of 50 × 106 FP instructions, 
110 × 106 INT instructions, 80 × 106 L/S instructions, and 16 × 106 branch 
instruction
 e CPI for each type of instruction is 1, 1, 4, and 2, respectively.  
Assume that the processor has a 2 GHz clock rate.
1.14.1 [10] <§1.10> By how much must we improve the CPI of FP instructions if 
we want the program to run two times faster?
1.14.2 [10] <§1.10> By how much must we improve the CPI of L/S instructions 
if we want the program to run two times faster?
1.14.3 [5] <§1.10> By how much is the execution time of the program improved 
if the CPI of INT and FP instructions is reduced by 40% and the CPI of L/S and 

Branch is reduced by 30%?
1.15 [5] <§1.8> When a program is adapted to run on multiple processors in 
a multiprocessor system, the execution time on each processor is comprised of 
computing time and the overhead time required for locked critical sections and/or 

to send data from one processor to another.
Assume a program requires t = 100 s of execution time on one processor.  When run 
p processors, each processor requires t/p s, as well as an additional 4 s of overhead, 

irrespective of the number of processors.  Compute the per-processor execution 

time for 2, 4, 8, 16, 32, 64, and 128 processors.  For each case, list the corresponding 

speedup relative to a single processor and the ratio between actual speedup versus 

ideal speedup (speedup if there was no overhead). 
§1.1, page 10: Discussion questions: many answers are acceptable.
§1.4, page 24: DRAM memory: volatile, short access time of 50 to 70 nanoseconds, 

and cost per GB is $5 to $10. Disk memory: nonvolatile, access times are 100,000 

to 400,000 times slower than DRAM, and cost per GB is 100 times cheaper than 

DRAM. Flash memory: nonvolatile, access times are 100 to 1000 times slower than 

DRAM, and cost per GB is 7 to 10 times cheaper than DRAM.

§1.5, page 28: 1, 3, and 4 are valid reasons. Answer 5 can be generally true because 

high volume can make the extra investment to reduce die size by, say, 10% a good 

economic decision, but it doesn’t have to be true.

§1.6, page 33: 1. a: both, b: latency, c: neither. 7 seconds.

§1.6, page 40: b.

§1.10, page 51: a. Computer A has the higher MIPS rating. b. Computer B is faster.
Answers to 
Check Yourself

2I speak Spanish 
to God, Italian to 

women, French to 

men, and German to 

my horse.
Charles V, Holy Roman Emperor 
(1500–1558)Instructions: 
Language of the 
Computer2.1 Introduction 622.2 Operations of the Computer Hardware 
632.3 Operands of the Computer Hardware 
662.4 Signed and Unsigned Numbers 
732.5 Representing Instructions in the 
Computer 802.6 Logical Operations 
872.7 Instructions for Making Decisions 
90Computer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
2.8 Supporting Procedures in Computer Hardware 
962.9 Communicating with People 
1062.10 MIPS Addressing for 32-Bit Immediates and Addresses 
1112.11 Parallelism and Instructions: Synchronization 
1212.12 Translating and Starting a Program 
1232.13 A C Sort Example to Put It All Together 
1322.14 Arrays versus Pointers 
1412.15 Advanced Material: Compiling C and Interpreting Java 
1452.16 Real Stuff: ARMv7 (32-bit) Instructions 
1452.17 Real Stuff: x86 Instructions 
1492.18 Real Stuff: ARMv8 (64-bit) Instructions 
1582.19 Fallacies and Pitfalls 
1592.20 Concluding Remarks 
1612.21 Historical Perspective and Further Reading 
1632.22 Exercises 164The Five Classic Components of a Computer

62 Chapter 2 Instructions: Language of the Computer
 2.1 IntroductionTo command a computer’s hardware, you must speak its language
 e words of a 
computer’s language are called 
instructions
, and its vocabulary is called an 
instruction 
set. In this chapter, you will see the instruction set of a real computer, both in the form 
written by people and in the form read by the computer. We introduce instructions in 
a top-down fashion. Starting from a notation that looks like a restricted programming 
language, we re
 ne it step-by-step until you see the real language of a real computer. 
Chapter 3 continues our downward descent, unveiling the hardware for arithmetic 

and the representation o
 oating-point numbers.
You might think that the languages of computers would be as diverse as those of 
people, but in reality computer languages are quite similar, more like regional dialects 

than like independent languages. Hence, once you learn one, it is easy to pick up others.
 e chosen instruction set comes from MIPS Technologies, and is an elegant 
example of the instruction sets designe
d since the 1980s. To demonstrate how 
easy it is to pick up other instruction sets, we will take a quick look at three other 

popular instruction sets.
1. ARMv7 is similar to MIPS. More than 9 billion chips with ARM processors 
were manufactured in 2011, making it the most popular instruction set in 

the world.
 e second example is the Intel x86, which powers both the PC and the 
cloud of the PostPC Era.
 e third example is ARMv8, which extends the address size of the ARMv7 
from 32 bits to 64 bits. Ironically, as we shall see, this 2013 instruction set is 

closer to MIPS than it is to ARMv7.
 is similarity of instruction sets occurs because all computers are constructed 
from hardware technologies based on similar underlying principles and because 

there are a few basic operations that all computers must provide. Moreover, 

computer designers have a common goal: t
 nd a language that makes it easy 
to build the hardware and the compiler while maximizing performance and 

minimizing cost and energy
 is goal is time honored; the following quote 
was written before you could buy a computer, and it is as true today as it was in 1947:
It is easy to see by formal-logical methods that there exist certain [instruction 

sets] that are in abstract adequate to control and cause the execution of any 

sequence of operations 
 e really decisive considerations from the present 
point of view, in selecting an [instruction set], are more of a practical nature: 

simplicity of the equipment demanded by the [instruction set], and the clarity of 

its application to the actually important problems together with the speed of its 

handling of those probl
ems.Burks, Goldstine, and von Neumann, 1947
instruction set
 e vocabulary of commands 
understood by a given 

architecture.

 2.2 Operations of the Computer Hardware 
63 e “simplicity of the equipment” is as valuable a consideration for today’s 
computers as it was for those of th
 e goal of this chapter is to teach 
an instruction set that follows this advice, showing both how it is represented 
in hardware and the relationship between high-level programming languages 

and this more primitive one. Our examples are in the C programming language; 
 Section 2.15
 shows how these would change for an object-oriented language 
like Java.
By learning how to represent instructions, you will also discover the secret of 
computing: the 
stored-program concept
. Moreover, you will exercise your “foreign 
language” skills by writing programs in the language of the computer and running 
them on the simulator that comes with this book. You will also see the impact of 

programming languages and compiler optimization on performance. We conclude 

with a look at the historical evolution of instruction sets and an overview of other 

computer dialects.
We reveal o
 rst instruction set a piece at a time, giving the rationale along 
with the computer structur
 is top-down, step-by-step tutorial weaves the 
components with their explanations, making the computer’s language more 

palatable. 
Figure 2.1
 gives a sneak preview of the instruction set covered in this 

chapter.
 2.2 Operations of the Computer Hardware
Every computer must be able to perform arit
 e MIPS assembly language 
notation
add a, b, cinstructs a computer to add the two variables 
b and 
c and to put their sum in 
a. is notation is rigid in that each MIPS arithmetic instruction performs only 
one operation and must always have exactly three variables. For example, suppose 

we want to place the sum of four variables 
b, c, d, and 
e into variable 
a. (In this 
section we are being deliberately vague about what a “variable” is; in the next 

section we’ll explain in detail.)
 e following sequence of instructions adds the four variables:
add a, b, c    # The sum of b and c is placed in aadd a, a, d    # The sum of b, c, and d is now in a
add a, a, e    # The sum of b, c, d, and e is now in a us, it takes three instructions to sum the four variables.
 e words to the right of the sharp symbol (
#) on each line above are 
comments
 for the human reader, so the computer ignores them. Note that unlike other 
programming languages, each line of this language can contain at most one 
stored-program 
concept
 e idea that 
instructions and data of 
many types can be stored 

in memory as numbers, 

leading to the stored-

program computer.
 ere must certainly 
be instructions 

for performing 

the fundamental 

arithmetic operations.
Burks, Goldstine, and 
von Neumann, 1947

64 Chapter 2 Instructions: Language of the Computer
MIPS operandsNameExampleComments32 registers
$s0Œ$s7, $t0Œ$t9, $zero, $a0Œ$a3, $v0Œ$v1, $gp, $fp, 

$sp, $ra, $atFast locations for data. In MIPS, data must be in registers to perform arithmetic, 

register 
$zero always equals 0, and register 
$at is reserved by the assembler to 
handle large constants.
230 memory 
words
Memory[0], Memory[4], . . . , 

Memory[4294967292]
Accessed only by data transfer instructions. MIPS uses byte addresses, so 

sequential word addresses differ by 4. Memory holds data structures, arrays, and 

spilled registers. 
MIPS assembly languageCategory InstructionExample
MeaningCommentsArithmeticaddadd  $s1,$s2,$s3$s1
 = $s2 + $s3Three register operands
subtractsub  $s1,$s2,$s3$s1
 = $s2 Œ $s3Three register operands
add immediateaddi $s1,$s2,20$s1
=$s2 + 20Used to add constantsData 
transferload wordlw  $s1,20($s2)$s1
 = Memory[
$s2 + 20]Word from memory to register
store word
sw  $s1,20($s2)Memory[
$s2 + 20] = $s1Word from register to memory
load halflh  $s1,20($s2)$s1 = Memory[
$s2+ 20]Halfword memory to register
load half unsignedlhu  $s1,20($s2)$s1 = Memory[
$s2+ 20]Halfword memory to register
store half
sh  $s1,20($s2)Memory[
$s2+ 20] = $s1Halfword register to memory
load bytelb  $s1,20($s2)$s1 = Memory[
$s2+ 20]Byte from memory to register
load byte unsignedlbu  $s1,20($s2)$s1 = Memory[
$s2+ 20]Byte from memory to register
store byte
sb  $s1,20($s2)Memory[
$s2+ 20] = $s1Byte from register to memory
load linked word
ll  $s1,20($s2)$s1 = Memory[
$s2+ 20]Load word as 1st half of atomic swap 
store condition. word
sc  $s1,20($s2)Memory[
$s2+20]=$s1;$s1=0 or1Store word as 2nd half of atomic swap 
load upper immed.lui  $s1,20$s1 = 20 * 216Loads constant in upper 16 bitsLogicaland and  $s1,$s2,$s3$s1
 = $s2 & $s3Three reg. operands; bit-by-bit AND
oror   $s1,$s2,$s3$s1
 = $s2 | $s3Three reg. operands; bit-by-bit OR
nornor  $s1,$s2,$s3$s1
 = ~ ($s2 | $s3)Three reg. operands; bit-by-bit NOR
and immediateandi $s1,$s2,20$s1
 = $s2 & 20Bit-by-bit AND reg with constant
or immediateori  $s1,$s2,20$s1
 = $s2 | 20Bit-by-bit OR reg with constant
shift left logicalsll  $s1,$s2,10$s1
 = $s2 << 10Shift left by constantshift right logicalsrl  $s1,$s2,10$s1
 = $s2 >> 10Shift right by constantConditional 
branchbranch on equalbeq  $s1,$s2,25if ($s1 == $s2) go to PC + 4 + 100Equal test; PC-relative branch
branch on not equalbne  $s1,$s2,25if ($s1!=  $s2) go to PC + 4 + 100Not equal test; PC-relative 
set on less thanslt  $s1,$s2,$s3if ($s2 < $s3)  $s1= 1; else $s1 = 0Compare less than; for 
beq, bneset on less than 
unsignedsltu  $s1,$s2,$s3if ($s2 < $s3)  $s1= 1; else $s1 = 0Compare less than unsigned
set less than 
immediate slti $s1,$s2,20if ($s2 < 20) $s1= 1; else $s1= 0Compare less than constant
set less than 

immediate unsignedsltiu $s1,$s2,20if ($s2 < 20) $s1= 1; else $s1= 0Compare less than constant 

unsignedUnconditional 
jumpjumpj   2500
go to 10000Jump to target address
jump register
jr   $rago to $raFor switch, procedure return
jump and linkjal  2500$ra = PC + 4; go to 10000For procedure call
FIGURE 2.1 MIPS assembly language revealed in this chapter. 
 is information is also found in Column 1 of the MIPS Reference 
Data Card at the front of this book.

 2.2 Operations of the Computer Hardware 
65instruction. Anot
 erence from C is that comments always terminate at the 
end of a line.
 e natural number of operands for an operation like addition is three: the 
two numbers being added together and a place to put the sum. Requiring every 
instruction to have exactly three operands, no more and no less, conforms to the 

philosophy of keeping the hardware simple: hardware for a variable number of 

operands is more complicated than hardware fo
 xed number
 is situation 
illustrates th
 rst of three underlying principles of hardware design:
Design Principle 1: 
Simplicity favors regularity.
We can now show, in the two examples that follow, the relationship of programs 
written in higher-level programming languages to programs in this more primitive 

notation.
Compiling Two C Assignment Statements into MIPS
 is segment of a C program contains th
 ve variables 
a, b, c, d, and 
e. Since 
Java evolved from C, this example and the next few work for either high-level 

programming language:
a = b + c;d = a – e; e translation from C to MIPS assembly language instructions is performed 
by the 
compiler
. Show the MIPS code produced by a compiler.
A MIPS instruction operates on two source operands and places the result 
in one destination operand. Hence, the two simple statements above compile 

directly into these two MIPS assembly language instructions:
add a, b, csub d, a, eCompiling a Complex C Assignment into MIPSA somewhat complex statement contains th
 ve variables 
f, g, h, i, and 
j:f = (g + h) – (i + j);What might a C compiler produce?
EXAMPLEANSWEREXAMPLE
66 Chapter 2 Instructions: Language of the Computer
 e compiler must break this statement into several assembly instructions, 
since only one operation is performed per MIPS instructio
 e 
 rst MIPS 
instruction calculates the sum of 
g and 
h. We must place the result somewhere, 
so the compiler creates a temporary variable, called 
t0:add t0,g,h # temporary variable t0 contains g + hAlthough the next operation is subtract, we need to calculate the sum of 
i and 
j before we can subtrac us, the second instruction places the sum of 
i and 
j in another temporary variable created by the compiler, called 
t1:add t1,i,j # temporary variable t1 contains i + jFinally, the subtract instruction subtracts the second sum from th
 rst and 
places th
 erence in the variable 
f, completing the compiled code:
sub f,t0,t1 # f gets t0 – t1, which is (g + h) – (i + j)For a given function, which programming language likely takes the most lines of 
code? Put the three representations below in order.
1. Java
2. C
3. MIPS assembly language
Elaboration: To increase portability, Java was originally envisioned as relying on a 
software interpreter. The instruction set of this interpreter is called 
Java bytecodes 
(see  Section 2.15), which is quite different from the MIPS instruction set. To get 
performance close to the equivalent C program, Java systems today typically compile 
Java bytecodes into the native instruction sets like MIPS. Because this compilation is 

normally done much later than for C programs, such Java compilers are often called 
Just 
In Time (JIT) compilers. Section 2.12 shows how JITs are used later than C compilers 

in the start-up process, and Section 2.13 shows the performance consequences of 

compiling versus interpreting Java programs.
 2.3 Operands of the Computer Hardware
Unlike programs in high-level languages, the operands of arithmetic instructions 
are restricted; they must be from a limited number of special locations built directly 

in hardware called 
registers
. Registers are primitives used in hardware design that 
are also visible to the programmer when the computer is completed, so you can 

think of registers as the bricks of computer constructio
 e size of a register in 
the MIPS architecture is 32 bits; groups of 32 bits occur so frequently that they are 

given the name 
word
 in the MIPS architecture.
ANSWERCheck Yourself
word
 e natural unit 
of access in a computer, 
usually a group of 32 bits; 

corresponds to the size 

of a register in the MIPS 

architecture.

 2.3 Operands of the Computer Hardware 
67One majo
 erence between the variables of a programming language and 
registers is the limited number of registers, typically 32 on current computers, 
like MIPS. (See 
 Section 2.21
 for the history of the number of register
 us, continuing in our top-down, stepwise evolution of the symbolic representation of 
the MIPS language, in this section we have added the restriction that the three 

operands of MIPS arithmetic instructions must each be chosen from one of the 32 

32-bit registers.
 e reason for the limit of 32 registers may be found in the second of our three 
underlying design principles of hardware technology:
Design Principle 2: 
Smaller is faster
.A very large number of registers may increase the clock cycle time simply because 

it takes electronic signals longer when they must travel farther.
Guidelines such as “smaller is faster” are not absolutes; 31 registers may not be 
faster than 32. Yet, the truth behind such observations causes computer designers 

to take them seriously. In this case, the designer must balance the craving of 

programs for more registers with the designer’s desire to keep the clock cycle fast. 

Another reason for not using more than 32 is the number of bits it would take in 

the instruction format, as Section 2.5 demonstrates.
Chapter 4 shows the central role that registers play in hardware construction; 
as we shall see in this chapter
 ective use of registers is critical to program 
performance.
Although we could simply write instructions using numbers for registers, from 
0 to 31, the MIPS convention is to use two-character names following a dollar sign 

to represent a register. Section 2.8 will explain the reasons behind these names. For 

now, we will use 
$s0, $s1, . . . 
for registers that correspond to variables in C and 
Java programs and 
$t0, $t1, . . . 
for temporary registers needed to compile the 
program into MIPS instructions.
Compiling a C Assignment Using Registers
It is the compiler’s job to associate program variables with registers. Take, for 

instance, the assignment statement from our earlier example:
f = (g + h) – (i + j); e variables 
f, g, h, i, and 
j are assigned to the registers 
$s0, $s1, $s2, $s3, and 
$s4, respectively. What is the compiled MIPS code?
EXAMPLE
68 Chapter 2 Instructions: Language of the Computer
 e compiled program is very similar to the prior example, except we replace 
the variables with the register names mentioned above plus two temporary 
registers, 
$t0 and 
$t1, which correspond to the temporary variables above:
add $t0,$s1,$s2 # register $t0 contains g + hadd $t1,$s3,$s4 # register $t1 contains i + j
sub $s0,$t0,$t1 # f gets $t0 – $t1, which is (g + h)–(i + j)Memory Operands
Programming languages have simple variables that contain single data elements, 
as in these examples, but they also have more complex data structures—arrays and 

structur
 ese complex data structures can contain many more data elements 
than there are registers in a computer. How can a computer represent and access 

such large structures?
Recall th
 ve components of a computer introduced in Chapter 1 and repeated 
on pag
 e processor can keep only a small amount of data in registers, but 
computer memory contains billions of data elements. Hence, data structures 

(arrays and structures) are kept in memory.
As explained above, arithmetic operations occur only on registers in MIPS 
instructions; thus, MIPS must include instructions that transfer data between 

memory and registers. Such instructions are called 
data transfer instructions
. To access a word in memory, the instruction must supply the memory 
address
. Memory is just a large, single-dimensional array, with the address acting as the 
index to that array, starting at 0. For example, in 
Figure 2.2
, the address of the third 

data element is 2, and the value of Memory [2] is 10.
ANSWERdata transfer 
instruction
 A command 
that moves data between 
memory and registers.
address
 A value used to 
delineate the location of 

a sp
 c data element 
within a memory array.
Processor
Memory
AddressData
1101101000123FIGURE 2.2 Memory addresses and contents of memory at those locations.
 If these elements 
were words, these addresses would be incorrect, since MIPS actually uses byte addressing, with each word 
representing four bytes. 
Figure 2.3
 shows the memory addressing for sequential word addresses.
 e data transfer instruction that copies data from memory to a register is 
traditionally called 
load e format of the load instruction is the name of the 
operation followed by the register to be loaded, then a constant and register used to 
access memory
 e sum of the constant portion of the instruction and the contents 
of the second register forms the memory addr
 e actual MIPS name for this 
instruction is lw, standing for 
load word
.
Compiling an Assignment When an Operand Is in Memory
Let’s assume that 
A is an array of 100 words and that the compiler has 
associated the variables 
g and 
h with the registers 
$s1 and 
$s2 as before. 
Let’s also assume that the starting address, or 
base address,
 of the array is in 
$s3. Compile this C assignment statement:
g = h + A[8];Although there is a single operation in this assignment statement, one of 
the operands is in memory, so we mu
 rst transfer 
A[8] to a register
 e address of this array element is the sum of the base of the array 
A, found in 
register 
$s3, plus the number to select elemen
 e data sh
ould be pl
aced 
in a temporary register for use in the next instruction. Based on 
Figure 2.2
, the 

 rst compiled instruction is
lw    $t0,8($s3) # Temporary reg $t0 gets A[8](We’ll be making a slight adjustment to this instruction, but we’ll use this 

simp
 ed version for now
 e following instruction can operate on the value 
in $t0 (which equals 
A[8]) since it is in a register
 e instruction must add 
h (contained in 
$s2) to 
A[8] (contained in
 $t0) and put the sum in the 
register corresponding to 
g (associated with 
$s1):add   $s1,$s2,$t0 # g = h + A[8] e constant in a data transfer instruction (8) is called the 
 set,
 and the 
register added to form the address (
$s3) is called the 
base register
.In addition to associating variables with registers, the compiler allocates data 

structures like arrays and structures to locations in memory
 e compiler can then 
place the proper starting address into the data transfer instructions.
Since 8-bit 
bytes
 are useful in many programs, virtually all architectures today 
address individual byt
 erefore, the address of a word matches the address of 
one of the 4 bytes within the word, and addresses of sequential word
 er by 4. 
For example, 
Figure 2.3
 shows the actual MIPS addresses for the words in 
Figure
 
2.2; the byte address of the third word is 8.
In MIPS, words must start at addresses that are multiples of
 is requirement 
is called an 
alignment restriction
, and many architectures have it. (Chapter 4 
suggests why alignment leads to faster data transfers.)
EXAMPLEANSWERHardware/

Software 

Interfacealignment restriction
 A requirement that data 
be aligned in memory on 

natural boundaries.
 2.3 Operands of the Computer Hardware 
69
70 Chapter 2 Instructions: Language of the Computer
Computers divide into those that use the address of th
 most or “big end” byte 
as the word address versus those that use the rightmost or “little end” byte. MIPS is 
in the 
big-endian
 camp. Since the order matters only if you access the identical data 

both as a word and as four bytes, few need to be aware of the endianess. (Appendix 

A shows the two options to number bytes in a word.)
Byte addressing also a
 ects the array index. To get the proper byte address in the 
code above, 
the
 set to be added to the base register
 $s3 must be 4
  8, or 32,
 so 
that the load address will select 
A[8] and not 
A[8/4]. (See the related pitfall on 
page 160 of Section 2.19.)
 e instruction complementary to load is traditionally called 
store;
 it copies data 
from a register to memory
 e format of a store is similar to that of a load: the 
name of the operation, followed by the register to be stored, then o
 set to select 
the array element, an
 nally the base register. Once again, the MIPS address is 
sp
 ed in part by a constant and in part by the contents of a register
 e actual 
MIPS name is 
sw, standing for 
store word
.As the addresses in loads and stores are binary numbers, we can see why the 

DRAM for main memory comes in binary sizes rather tha
 at 
is, in gebibytes (2
30) or tebibytes (2
40), not in gigabytes (10
9) or terabytes (10
12); see 
Figure 1.1.
Hardware/
Software 
InterfaceProcessor
Memory
Byte AddressData
11011010004812FIGURE 2.3 Actual MIPS memory addresses and contents of memory for those words.
  e changed addresses are highlighted to contrast with Figure 2.2. Since MIPS addresses each byte, word 
addresses are multiples of 4: there are 4 bytes in a word.

Compiling Using Load and StoreAssume variable 
h is associated with register 
$s2 and the base address of 
the array 
A is in $s3. What is the MIPS assembly code for the C assignment 
statement below?
A[12] = h + A[8];Although there is a single operation in the C statement, now two of the 
operands are in memory, so we need even more MIPS instruction
 e 
 rst 
two instructions are the same as in the prior example, except this time we use 

the proper o
 set for byte addressing in the load word instruction to select 
A[8], and the add instruction places the sum in 
$t0:lw   $t0,32($s3)  # Temporary reg $t0 gets A[8]add  $t0,$s2,$t0  # Temporary reg $t0 gets h + A[8] e 
 nal instruction stores the sum into 
A[12], using 48 (4 
 12) as the o
 set 
and register 
$s3 as the base register.
sw  $t0,48($s3)  # Stores h + A[8] back into A[12]Load word and store word are the instructions that copy words between 
memory and registers in the MIPS architecture. Other brands of computers use 
other instructions along with load and store to transfer data. An architecture with 

such alternatives is the Intel x86, described in Section 2.17.
Many programs have more variables than computers have registers. Consequently, 
the compiler tries to keep the most frequently used variables in registers and places 

the rest in memory, using loads and stores to move variables between registers and 

memory
 e process of putting less commonly used variables (or those needed 
later) into memory is called 
spilling
 registers.
 e hardware principle relating size and speed suggests that memory must be 
slower than registers, since there are fewer register
 is is indeed the case; data 
accesses are faster if data is in registers instead of memory.
Moreover, data is more useful when in a register. A MIPS arithmetic instruction 
can read two registers, operate on them, and write the result. A MIPS data transfer 

instruction only reads one operand or writes one operand, without operating on it.
 us, registers take less time to access 
and
 have higher throughput than memory, 
making data in registers both faster to access and simpler to use. Accessing registers 

also uses less energy than accessing memory. To achieve highest performance and 

conserve energy, an instruction set architecture must hav
  cient number of 
registers, and compilers must use register
  ciently.
EXAMPLEANSWERHardware/

Software 

Interface 2.3 Operands of the Computer Hardware 
71
72 Chapter 2 Instructions: Language of the Computer
Constant or Immediate OperandsMany times a program will use a constant in an operation—for example, 
incrementing an index to point to the next 
element of an array. In fact, more than 
half of the MIPS arithmetic instructions have a constant as an operand when 

running the SPEC CPU2006 benchmarks.
Using only the instructions we have seen so far, we would have to load a constant 
from memory to use on
 e constants would have been placed in memory when 
the program was loaded.) For example, to add the constant 4 to register 
$s3, we 
could use the code
lw $t0, AddrConstant4($s1)   # $t0 = constant 4add $s3,$s3,$t0              # $s3 = $s3 + $t0 ($t0 == 4)assuming that 
$s1 + AddrConstant4 is the memory address of the constant 4.
An alternative that avoids the load instruction is to o
 er versions of the arithmetic 
instructions in which one operand is a constan
 is quick add instruction with 
one constant operand is called 
add immediate
 or 
addi. To add 4 to register 
$s3, we just write
addi    $s3,$s3,4            # $s3 = $s3 + 4Constant operands occur frequently, and by including constants inside 
arithmetic instructions, operations are much faster and use less energy than if 
constants were loaded from memory.
 e constant zero has another role, which is to simplify the instruction set 
by
 ering useful variations. For example, the move operation is just an add 
instruction where one operand is zero. Hence, MIPS dedicates a register 
$zero to be hard-wired to the value zero. (As you might expect, it is register number 0.) 

Using frequency to justify the inclusions of constants is another example of the 

great idea of making the
 common case fast
.Given the importance of registers, what is the rate of increase in the number of 
registers in a chip over time?
1. Very
 ey increase as fast as Moore’s law, which predicts doubling the 
number of transistors on a chip every 18 months.
2. Very slow: Since programs are usually distributed in the language of the 
computer, there is inertia in instruction set architecture, and so the number 

of registers increases only as fast as new instruction sets become viable.
Elaboration: Although the MIPS registers in this book are 32 bits wide, there is a 
64-bit version of the MIPS instruction set with 32 64-bit registers. To keep them straight, 
the cially called MIPS-32 and MIPS-64. In this chapter, we use a subset of 

MIPS-32.  Appendix E shows the differences between MIPS-32 and MIPS-64. Sections 
2.16 and 2.18 show the much more dramatic difference between the 32-bit address ARMv7 and its 64-bit successor, ARMv8.
Check Yourself

 2.4 Signed and Unsigned Numbers 
73Elaboration: The MIPS offset plus base register addressing is an excellent match to 
structures as well as arrays, since the register can point to the beginning of the structure 
and the offset can select the desired element. We’ll see such an example in Section 

2.13.Elaboration: The register in the data transfer instructions was originally invented to 
hold an index of an array with the offset used for the starting address of an array. Thus, 

the base register is also called the index register
. Today’s memories are much larger and 
the software model of data allocation is more sophisticated, so the base address of 

the array is normally passed in a register since it won’ t in the offset, as we shall see.
Elaboration: Since MIPS supports negative constants, there is no need for subtract 
immediate in MIPS. 2.4 Signed and Unsigned Numbers
First, let’s quickly review how a computer represents numbers. Humans are taught 
to think in base 10, but numbers may be represented in any base. For example, 123 

base 10 
 1111011 base 2.
Numbers are kept in computer hardware as a series of high and low electronic 
signals, and so they are considered base 2 numbers. (Just as base 10 numbers are 

called decimal
 numbers, base 2 numbers are called 
binary
 numbers.)
A single digit of a binary number is thus the “atom” of computing, since all 
information is composed of 
binary digits
 or 
bits
 is fundamental building block 
can be one of two values, which can be thought of as several alternatives: high or 

low, on or
 , true or false, or 1 or 0.
Generalizing the point, in any number base, the value of 
ith digit 
d isdiBase
where 
i starts at 0 and increases from right to
 . 
 is representation leads to an 
obvious way to number the bits in the word: simply use the power of the base for 

that bit. We subscript decimal numbers with 
ten
 and binary numbers with 
two. For 
example,
1011tworepresents
(1 x 23)   + (0 x 2
2) + (1 x 21)  + (1 x 2
0)ten= (1 x 8)   + (0 x 4)  + (1 x 2)  + (1 x 1)
ten=    8      +    0     +    2     +    1
ten= 11tenbinary digit
 Also 
called binary bit
. One of the two numbers 
in base 2, 0 or 1, that 

are the components of 

information.

74 Chapter 2 Instructions: Language of the Computer
We number the bits 0, 1, 2, 3, . . . from 
right to
  in a word
 e drawing below 
shows the numbering of bits within a MIPS word and the placement of the number 
1011two
:313029282726252423222120191817161514131211109876543210
00000000000000000000000000001011
(32 bits wide)Since words are drawn vertically as well as horizontally,
 most and rightmost 
may be unclear. Hence, the phrase 
least sig
 cant bit
 is used to refer to the right-
most bit (bit 0 above) and 
most sig
 cant bit
 to th
 most bit (bit 31).
 e MIPS word is 32 bits long, so we can represent 2
32 erent 32-bit patterns. 
It is natural to let these combinations represent the numbers from 0 to 2
32 1 (4,294,967,295ten
):0000 0000 0000 0000 0000 0000 0000 0000two = 0ten0000 0000 0000 0000 0000 0000 0000 0001two = 1ten0000 0000 0000 0000 0000 0000 0000 0010two = 2ten . . .                                        . . 
.1111 1111 1111 1111 1111 1111 1111 1101two = 4,294,967,293ten1111 1111 1111 1111 1111 1111 1111 1110two = 4,294,967,294ten1111 1111 1111 1111 1111 1111 1111 1111two = 4,294,967,295ten at is, 32-bit binary numbers can be represented in terms of the bit value times a 
power of 2 (here 
xi means the 
ith bit of 
x):()()()()()
xxxxx
3123022921202
31302910–For reasons we will shortly see, these positive numbers are called unsigned numbers.
Base 2 is not natural to human beings; we have
 ngers and so
 nd base 10 
natural. Why didn’t computers use decimal? In fact, th
 rst commercial computer 
did
 er decimal arit
 e problem was that the computer still used on 
and
  signals, so a decimal digit was simply
 represented by several binary digits. 
Decimal proved s  cient that subsequent computers reverted to all binary, 
converting to base 10 only for the relatively infrequent input/output events.
Keep in mind that the binary bit patterns above are simply 
representatives
 of 
numbers. Numbers really have an
 nite number of digits, with almost all being 
0 except for a few of the rightmost digits. We just don’t normally show leading 0s.
Hardware can be designed to add, subtract, multiply, and divide these binary 
bit patterns. If the number that is the proper result of such operations cannot be 
represented by these rightmost hardware bits, 
over
 ow is said to have occurred. 
least sig
 cant bit
 e rightmost bit in a MIPS 
word.most sig
 cant bit
 e  most bit in a MIPS 
word.Hardware/
Software 
Interface
It’s up to the programming language, the operating system, and the program to 
determine what to do if over
 ow occurs.
Computer programs calculate both positive and negative numbers, so we need a 
representation that distinguishes the positive from the negative
 e most obvious 
solution is to add a separate sign, which conveniently can be represented in a single 

bit; the name for this representation is 
sign and magnitude
.Alas, sign and magnitude representation has several shortcomings. First, it’s 
not obvious where to put the sign bit. To the right? To th
 ? Early computers 
tried both. Second, adders for sign and magnitude may need an extra step to set 

the sign because we can’t know in advance what the proper sign will be. Finally, a 

separate sign bit means that sign and magnitude has both a positive and a negative 

zero, which can lead to problems for inattentive programmers. As a result of these 

shortcomings, sign and magnitude representation was soon abandoned.
In the search for a more attractive alternative, the question arose as to what 
would be the result for unsigned numbers if we tried to subtract a large number 

from a small one.
 e answer is that it would try to borrow from a string of leading 
0s, so the result would have a string of leading 1s.
Given that there was no obvious better alternative, th
 nal solution was to pick 
the representation that made the hardware simple: leading 0s mean positive, and 

leading 1s mean negative
 is convention for representing signed binary numbers 
is called 
two’s complement
 representation:
0000 0000 0000 0000 0000 0000 0000 0000two = 0ten0000 0000 0000 0000 0000 0000 0000 0001two = 1ten0000 0000 0000 0000 0000 0000 0000 0010two = 2ten . . .                                        . . 
.0111 1111 1111 1111 1111 1111 1111 1101two = 2,147,483,645ten0111 1111 1111 1111 1111 1111 1111 1110two = 2,147,483,646ten0111 1111 1111 1111 1111 1111 1111 1111two = 2,147,483,647ten1000 0000 0000 0000 0000 0000 0000 0000two = –2,147,483,648ten1000 0000 0000 0000 0000 0000 0000 0001two = –2,147,483,647ten1000 0000 0000 0000 0000 0000 0000 0010two = –2,147,483,646ten. . .                                        . . 
.1111 1111 1111 1111 1111 1111 1111 1101two = –3ten1111 1111 1111 1111 1111 1111 1111 1110two = –2ten1111 1111 1111 1111 1111 1111 1111 1111two = –1ten e positive half of the numbers, from 0 to 2,147,483,647
ten
 (231 1), use the same 
representation as before
 e following bit pattern (1000 . . . 0000
two
) represents the most 
negative number 
2,147,483,648ten
 (231). It is followed by a declining set of negative 
numbers: 
2,147,483,647ten
 (1000 . . . 0001two
) down to 
1ten
 (1111 . . . 1111two
).Two’s complement does have one negative number, 
2,147,483,648ten
, that 
has no corresponding positive number. Such imbalance was also a worry to the 

inattentive programmer, but sign and magnitude had problems for both the 

programmer 
and
 the hardware designer. Consequently, every computer today uses 
two’s complement binary representations for signed numbers.
 2.4 Signed and Unsigned Numbers 
75
76 Chapter 2 Instructions: Language of the Computer
Two’s complement representation has the advantage that all negative numbers 
have a 1 in th
 cant bit. Consequently, hardware needs to test only 
this bit to see if a number is positive or negative (with the number 0 considered 
positiv
 is bit is
 en called the 
sign bit
. By recognizing the role of the sign bit, 
we can represent positive and negative 32-bit numbers in terms of the bit value 

times a power of 2:
()()()()()
xxxxx
3123022921202
31302910+– e sign bit is multiplied by 
231, and the rest of the bits are then multiplied by 
positive versions of their respective base values.
Binary to Decimal Conversion
What is the decimal value of this 32-bit two’s complement number?
1111  1111  1111  1111  1111  1111  1111  1100twoSubstituting the number’s bit values into the formula above:
()()()()()()
121212120202
222
3130291103130
–2292200
21474836482147483644
4–,,,,,,
tetnenten
We’ll see a shortcut to simplify conversion from negative to positive soon.
Just as an operation on unsigned numbers can over
 ow the capacity of hardware 
to represent the result, so can an operation on two’s complement numbers. Over
 ow 
occurs when th
 most retained bit of the binary bit pattern is not the same as the 
 nite number of digits to th
  (the sign bit is incorrect): a 0 on th
  of the bit 
pattern when the number is negative or a 1 when the number is positive.
Signed versus unsigned applies to loads as well as to arit
 e function
 of a 
signed load is to copy the sign repeatedly to
 ll the rest of the register—called 
sign 
extension
—but its 
purpose
 is to place a correct representation of the number within 
that register. Unsigned loads simpl
 ll with 0s to th
  of the data, since the 
number represented by the bit pattern is unsigned.
When loading a 32-bit word into a 32-bit register, the point is moot; signed and 
unsigned loads are identical. MIPS does o
 er tw
 avors of byte loads: 
load byte
 (lb) treats the byte as a signed number and thus sign-extends t
 ll th
 -most bits 
of the register, while 
load byte unsigned
 (lbu) works with unsigned integers. Since C 
programs almost always use bytes to represent characters rather than consider bytes 
as very short signed integers, 
lbu is used practically exclusively for byte loads.
EXAMPLEANSWERHardware/
Software 
Interface
Unlike the numbers discussed above, memory addresses naturally start at 0 
and continue to the largest address. Put another way, negative addresses make 

no sens
 us, programs want to deal sometimes with numbers that can be 
positive or negative and sometimes with numbers that can be only positive. 

Some programming languages r
 ect this distinction. C, for example, names the 
former 
integers
 (declared as 
int in the program) and the latter 
unsigned integers
 (unsigned int). Some C style guides even recommend declaring the former as 

signed int to keep the distinction clear.
Let’s examine two useful shortcuts when working with two’s complement 
number
 e 
 rst shortcut is a quick way to negate a two’s complement binary 
number. Simply invert every 0 to 1 and every 1 to 0, then add one to the result. 

 is shortcut is based on the observation that the sum of a number and its inverted 
representation must be 111 . . . 111
two
, which represents 
1. Since 
xx1, therefore 
xx10 or 
xx1. (We use the notation 
x to mean invert 
every bit in 
x from 0 to 1 and vice versa.)
Negation Shortcut
Negate 2
ten
, and then check the result by negating 
2ten
.2ten
  0000 0000 0000 0000 0000 0000 0000 0010two
Negating this number by inverting the bits and adding one,
  1111 
 1111  1111  1111  1111  1111  1111  1101
two +                                    1
two =  1111 
 1111  1111  1111  1111  1111  1111  1110
two =  –2
tenGoing the other direction,
1111 1111 1111 1111 1111 1111 1111 1110two rst inverted and then incremented:
 0000  0000  0000  0000  0000  0000  0000  0001two +                                     1
two = 0000 
 0000  0000  0000  0000  0000  0000  0010two = 2
tenHardware/

Software 

InterfaceEXAMPLEANSWER 2.4 Signed and Unsigned Numbers 
77
78 Chapter 2 Instructions: Language of the Computer
Our next shortcut tells us how to convert a binary number represented in 
n bits 
to a number represented with more than 
n bits. For example, the immediat
 eld in the load, store, branch, add, and set on less than instructions contains a two’s 
complement 16-bit number, representing 
32,768ten
 (215) to 32,767
ten
 (215  1). To add the immediate
 eld to a 32-bit register, the computer must convert that 16-
bit number to its 32-bit equivalen
 e shortcut is to take th
 cant bit 
from the smaller quantity—the sign bit—and replicate it to
 ll the new bits of the 
larger quantity
 e old nonsign bits are simply copied into the right portion of the 
new word
 is shortcut is commonly called 
sign extension
.Sign Extension Shortcut
Convert 16-bit binary versions of 2
ten
 and 
2ten
 to 32-bit binary numbers.
 e 16-bit binary version of the number 2 is
0000 0000 0000 0010two = 2tenIt is converted to a 32-bit number by making 16 copies of the value in the most 

 cant bit (0) and placing that in th
 -hand half of the word
 e right 
half gets the old value:
0000 0000 0000 0000 0000 0000 0000 0010two = 2tenLet’s negate the 16-bit version of 2 using the earlier shortcu
 us,0000 0000 0000 0010twobecomes
1111  1111  1111  1101
two+                
1two= 1111  1111  1111  1110
twoCreating a 32-bit version of the negative number means copying the sign bit 
16 times and placing it on th
 :1111 1111 1111 1111 1111 1111 1111 1110two = –2ten is trick works because positive two’s complement numbers really have an
 nite 
number of 0s on th
  and negative two’s complement numbers have a
 nite 
number o
 e binary bit pattern representing a number hides leading bits t
 t the width of the hardware; sign extension simply restores some of them.
EXAMPLEANSWER
Summary
 e main point of this section is that we need to represent both positive and 
negative integers within a computer word, and although there are pros and cons to 
any option, the unanimous choice since 1965 has been two’s complement.
Elaboration: For signed decimal numbers, we used “
” to represent negative because there are no limits to the size of a decimal number xed word size, 
binary and hexadecimal (see 
Figure 2.4
) bit strings can encode the sign; hence we do 

not normally use “
” or “” with binary or hexadecimal notation.
What is the decimal value of this 64-bit two’s complement number?
1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1111 1000two1) –4ten
2) –8ten
3) –16ten
4) 18,446,744,073,709,551,609ten
Elaboration: Two’s complement gets its name from the rule that the unsigned sum 
of an n-bit number and its n-bit negative is 2n; hence, the negation or complement of a 
number x is 2n  x, or its “two’s complement.”
A third alternative representation to two’s complement and sign and magnitude is 
called one’s complement
. The negative of a one’s complement is found by inverting 
each bit, from 0 to 1 and from 1 to 0, or 
x. This relation helps explain its name since the complement of x is 2n  x  1. It was also an attempt to be a better solution 
than sign and magnitude, and several ear c computers did use the notation. 

This representation is similar to two’s complement except that it also has two 0s: 

00 . . . 00
two is positive 0 and 11 . . . 11
two is negative 0. The most negative number, 
10 . . . 000
two, represents 
2,147,483,647ten, and so the positives and negatives are 
balanced. One’s complement adders did need an extra step to subtract a number, and 

hence two’s complement dominates today.
 nal notation, which we will look at when w oating point in Chapter 3, 
is to represent the most negative value by 00 . . . 000
two and the most positive value by 11 . . . 11
two, with 0 typically having the value 10 . . . 00
two. This is called a biased notation, since it biases the number such that the number plus the bias has a non-
negative representation.Check Yourself
one’s complement
 A notation that represents 

the most negative value 

by 10 . . . 000
two
 and the 
most positive value by 

01 . . . 11two
, leaving an 
equal number of negatives 

and positives but ending 

up with two zeros, one 

positive (00 . . . 00
two
) and 
one negative (11 . . . 11
two
).  e term is also used to 
mean the inversion of 

every bit in a pattern: 0 to 

1 and 1 to 0.
biased notation
 A notation that represents 

the most negative value 

by 00 . . . 000
two
 and the 
most positive value by 11 

. . . 11two
, with 0 typically 
having the value 10 . . . 

00two
, thereby biasing 
the number such that 

the number plus the 

bias has a non-negative 

representation.
 2.4 Signed and Unsigned Numbers 
79
80 Chapter 2 Instructions: Language of the Computer
 2.5  Representing Instructions in the Computer
We are now ready to explain th
 erence between the way humans instruct 
computers and the way computers see instructions.
Instructions are kept in the computer as a series of high and low electronic 
signals and may be represented as numbers. In fact, each piece of an instruction 
can be considered as an individual number, and placing these numbers side by side 

forms the instruction.
Since registers are referred to in instructions, there must be a convention to 
map register names into numbers. In MIPS assembly language, registers 
$s0 to 
$s7 map onto registers 16 to 23, and registers 
$t0 to 
$t7 map onto registers 8 
to 15. Hence, 
$s0 means register 16, 
$s1 means register 17, 
$s2 means register 
18, . . . , 
$t0 means register 8, 
$t1 means register 9, and so on. We’ll describe the 
convention for the rest of the 32 registers in the following sections.
Translating a MIPS Assembly Instruction into a Machine Instruction
Let’s do the next step in the re
 nement of the MIPS language as an example. 
We’ll show the real MIPS language version of the instruction represented 

symbolically as
add $t0,$s1,$s2 rst as a combination of decimal numbers and then of binary numbers.
 e decimal representation is
0171
8803
2Each of these segments of an instruction is called a 
 eld
 e 
 rst and 
last
 elds (containing 0 and 32 in this case) in combination tell the MIPS 
computer that this instruction performs additio
 e seco
 eld gives the 
number of the register that is th
 rst source operand of the addition operation 
(17  $s1), and the thir
 eld gives the other source operand for the addition 
(18  $s2 e fourt
 eld contains the number of the register that is to 
receive the sum (8 
 $t0 e 
 h 
 eld is unused in this instruction, so it is 
set t
 us, this instruction adds register 
$s1 to register 
$s2 and places the 
sum in register 
$t0. is instruction can also be represente
 elds of binary numbers as 
opposed to decimal:
00000010001100100100000000100000
6 bits5 bits5 bits5 bits5 bits6 bits
EXAMPLEANSWER
 2.5 Representing Instructions in the Computer 
81 is layout of the instruction is called the 
instruction format
. As you can see 
from counting the number of bits, this MIPS instruction takes exactly 32 bits—the 
same size as a data word. In keeping with our design principle that simplicity favors 
regularity, all MIPS instructions are 32 bits long.
To distinguish it from assembly language, we call the numeric version of 
instructions 
machine language
 and a sequence of such instructions 
machine code
.It would appear that you would now be reading and writing long, tedious strings 
of binary numbers. We avoid that tedium by using a higher base than binary that 

converts easily into binary. Since almost all computer data sizes are multiples of 

4, hexadecimal
 (base 16) numbers are popular. Since base 16 is a power of 2, 
we can trivially convert by replacing each group of four binary digits by a single 
hexadecimal digit, and vice versa. 
Figure 
2.4 converts between hexadecimal and 

binary.
instruction format
 A form of representation 
of an instruction 

composed of
 elds of 
binary numbers.
machine language
 Binary 
representation used for 
communication within a 

computer system.
hexadecimal
 Numbers 
in base 16.
Hexadecimal Binary Hexadecimal Binary Hexadecimal Binary Hexadecimal Binary 
0hex0000two4hex0100two8hex1000twochex1100two1hex0001two5hex0101two9hex1001twodhex1101two2hex0010two6hex0110twoahex1010twoehex1110two3hex0011two7hex0111twobhex1011twofhex1111twoFIGURE 2.4 The hexadecimal-binary conversion table.
 Just replace one hexadecimal digit by the corresponding four binary digits, 
and vice versa. If the length of the binary number is not a multiple of 4, go from right t
 .Because we frequently deal with
 erent number bases, to avoid confusion 
we will subscript decimal numbers with 
ten
, binary numbers with 
two, and 
hexadecimal numbers with 
hex
. (If there is no subscript, the default is base 10.) By 
the way, C and Java use the notation 0x
nnnn for hexadecimal numbers.
Binary to Hexadecimal and Back
Convert the following hexadecimal and binary numbers into the other base:
eca8  6420hex0001  0011 0101  0111 1001  1011  1101  1111twoEXAMPLE
82 Chapter 2 Instructions: Language of the Computer
Using 
Figure 2.4
, the answer is just a table lookup one way:
MIPS Fields
 elds are given names to make them easier to discuss:
oprsrtrdshamtfunct
6 bits5 bits5 bits5 bits5 bits
6 bitsHere is the meaning of each name of th
 elds in MIPS instructions:
 op:
 Basic operation of the instruction, traditionally called the 
opcode
. rs: e 
 rst register source operand.
 rt:
 e second register source operand.
 rd:
 e register destination operand. It gets the result of the operation.
 shamt:
 S
  amount. (Section 2.6 explain
  instructions and this term; it 
will not be used until then, and hence th
 eld contains zero in this section.)
 funct:
 Functio
 is 
 eld, o
 en called the 
function code,
 selects the sp
 c variant of the operation in the o
 eld.A problem occurs when an instruction needs long
 elds than those shown 
above. For example, the load word instruction must specify two registers and a 
constant. If the address were to use one of the 5-bi
 elds in the format above, the 
constant within the load word instruction would be limited to only 2
5 o
 is constant is used to select elements from arrays or data structures, and it o
 en needs 
to be much larger than
 is 5-bi
 eld is too small to be useful.
Hence, we have a co
 ict between the desire to keep all instructions the same 
length and the desire to have a single instruction forma
 is leads us to th
 nal 
hardware design principle:
ANSWERopcode
 e 
 eld that 
denotes the operation and 
format of an instruction.
eca8  6420
hex 1110  
 1100   1010   1000   0110  0100   0010   0000
twoAnd then the other direction: 
 0001  
 0011 0101    0111 1001  1011    1101   1111
two 1357 9bdfhex
Design Principle 3: 
Good design demands good compromises
. e compromise chosen by the MIPS designers is to keep all instructions the 
same length, thereby requirin
 erent kinds of instruction formats fo
 erent 
kinds of instructions. For example, the format above is called 
R-type
 (for register) 
or 
R-format
. A second type of instruction format is called 
I-type
 (for immediate) 
or 
I-format
 and is used by the immediate and data transfer instruction
 e 
 elds of I-format are
oprsrtconstant or address6 bits5 bits5 bits16 bits
 e 16-bit address means a load word instruction can load any word within 
a region of 
215 or 32,768 bytes (
213 or 8192 words) of the address in the base 
register rs. Similarly, add immediate is limited to constants no larger than 
215. We see that more than 32 registers would b
  cult in this format, as the rs and rt 
 elds would each need another bit, making it harder t t everything in one word.
Let’s look at the load word instruction from page 71:
lw   $t0,32($s3)   # Temporary reg $t0 gets A[8]Here, 19 (for 
$s3) is placed in the r
 eld, 8 (for 
$t0) is placed in the r
 eld, and 
32 is placed in the addr
 eld. Note that the meaning of the rt
 eld has changed 
for this instruction: in a load word instruction, the r
 eld sp
 es the 
destination
 register, which receives the result of the load.
Although multiple formats complicate the hardware, we can reduce the complexity 
by keeping the formats similar. For example, th
 rst thre
 elds of the R-type and 
I-type formats are the same size and have the same names; the length of the fourth 
 eld in I-type is equal to the sum of the lengths of the last thre
 elds of R-type.
In case you were wondering, the formats are distinguished by the values in the 
 rst 
 eld: each format is assigned a 
distinct set of values in th
 rst 
 eld (op) so that 
the hardware knows whether to treat the last half of the instruction as thre
 elds (R-type) or as a sing
 eld (I-type). 
Figure 2.5
 shows the numbers used in each 
 eld for the MIPS instructions covered so far.
 2.5 Representing Instructions in the Computer 
83InstructionFormatoprsrtrdshamtfunctaddress
addR0regregreg032
tenn.a.sub (subtract)R0regregreg034
tenn.a.addimmediate
I8tenregregn.a.n.a.n.a.constant
lw (load word)
I35
tenregregn.a.n.a.n.a.address
sw (store word) 
I43
tenregregn.a.n.a.n.a.address
FIGURE 2.5 MIPS instruction encoding.
 In the table above, “reg” means a register number between 0 
and 31, “address” means a 16-bit address, and “n.a.” (not applicable) means th
 eld does not appear in this 
format. Note that add and sub instructions have the same value in the o
 eld; the hardware uses the funct 
 eld to decide the variant of the operation: add (32) or subtract (34).

84 Chapter 2 Instructions: Language of the Computer
Translating MIPS Assembly Language into Machine Language
We can now take an example all the way from what the programmer writes 
to what the computer executes. If 
$t1 has the base of the array 
A and 
$s2 corresponds to 
h, the assignment statement
A[300] = h + A[300];is compiled into
lw   $t0,1200($t1) # Temporary reg $t0 gets A[300]add  $t0,$s2,$t0   # Temporary reg $t0 gets h + A[300]
sw   $t0,1200($t1) # Stores h + A[300] back into A[300]What is the MIPS machine language code for these three instructions?
For convenience, let’
 rst represent the machine language instructions using 
decimal numbers. From 
Figure 2.5
, we can determine the three machine 
language instructions:
Oprs
rt
rdaddress/shamtfunct
359812000188803
243981200 e lw instruction is iden
 ed by 35 (see 
Figure 2.5
) in th
 rst 
 eld (o
 e base register 9 (
$t1) is sp
 ed in the seco
 eld (rs), and the 
destination register 8 (
$t0) is sp
 ed in the thir
 eld (r
 e o
 set to 
select 
A[300] (1200  300  4) is found in th
 nal 
 eld (address).
 e add instruction that follows is sp
 ed with 0 in th
 rst 
 eld (op) and 
32 in th
 eld (func
 e three register operands (18, 8, and 8) are found 
in the second, third, and fourt
 elds and correspond to 
$s2, $t0, and 
$t0. e sw instruction is iden
 ed with 43 in th
 rst 
 eld. 
 e rest of this
 nal 
instruction is identical to the 
lw instruction.
Since 1200
ten
  0000 0100 1011 0000two
, the binary equivalent to the decimal 
form is:
EXAMPLEANSWER1000110100101000
0000 0100 1011 000000000010010010000100000000100000
1010110100101000
0000 0100 1011 0000
Note the similarity of the binary representations of th
 rst and last 
instruction e onl
 erence is in the third bit from th , which is 
highlighted here.
 e desire to keep all instructions the same size is in co
 ict with the desire to 
have as many registers as possible. Any increase in the number of registers uses 
up at least one more bit in every regist
 eld of the instruction format. Given 
these constraints and the design princple that smaller is faster, most instruction 

sets today have 16 or 32 general purpose registers.
Hardware/

Software 

InterfaceMIPS machine languageNameFormat
ExampleCommentsaddR0181917032
add$s1,$s2,$s3subR0181917034
sub$s1,$s2,$s3addiI81817
100addi$s1,$s2,100lwI351817
100lw$s1,100($s2)swI431817
100sw$s1,100($s2)Field size6 bits5 bits5 bits5 bits5 bits6 bitsAll MIPS instructions are 32 bits long
R-formatRoprsrtrdshamtfunctArithmetic instruction format
I-formatIoprsrt
address
Data transfer format
FIGURE 2.6 MIPS architecture revealed through Section 2.5.
 e two MIPS instruction formats so far are R an
 e 
 rst 16 bits 
are the same: both contain an 
op eld, giving the base operation; an 
rs eld, giving one of the sources; and the 
rt eld, which sp
 es the other 
source operand, except for load word, where it sp
 es the destination register. R-format divides the last 16 bits into an 
rd eld, specifying 
the destination register; the 
shamt
 eld, which Section 2.6 explains; and the 
funct
 eld, which sp
 es the sp
 c operation of R-format 
instructions. I-format combines the last 16 bits into a single 
address
 eld. 2.5 Representing Instructions in the Computer 
85Figure 2.6
 summarizes the portions of MIPS machine language described in this 
section. As we shall see in Chapter 4, the similarity of the binary representations 
of related instructions simp
 es hardwar
 ese similarities are another 
example of regularity in the MIPS architecture.

86 Chapter 2 Instructions: Language of the Computer
Today’s computers are built on two key principles:
1. Instructions are represented as numbers.
2. Programs are stored in memory to be read or written, just like 
data.
 ese principles lead to the 
stored-program
 concept; its invention let 
the computing genie out of its bottle. 
Figure 2.7
 shows the power of the 
concept; sp
 cally, memory can contain the source code for an editor 
program, the corresponding compiled machine code, the text that the 

compiled program is using, and even the compiler that generated the 

machine code.
One consequence of instructions as numbers is that programs are o
 en shippe
 les of binary number
 e commercial implication is that 
computers can inherit ready-made so
 ware provided they are compatible 
with an existing instruction set. Such “binary compatibility” o
 en leads 
industry to align around a small number of instruction set architectures.
The BIGPictureMemory
Accounting program
(machine code)Processor
Editor program
(machine code)C compiler(machine code)Payroll data
Book text
Source code in Cfor editor program
FIGURE 2.7 The stored-program concept. Stored programs allow a computer that performs 
accounting to become, in the blink of an eye, a computer that helps an author write a boo
 e switch 
happens simply by loading memory with programs and data and then telling the computer to begin executing 
at a given location in memory. Treating instructions in the same way as data greatly simp
 es both the 
memory hardware and the so
 ware of computer systems. Sp
 cally, the memory technology needed for 
data can also be used for programs, and programs like
 compilers, for instance, can translate code written in a 
notation far more convenient for humans into code that the computer can understand.

 2.6 Logical Operations 87What MIPS instruction does this represent? Choose from one of the four options 
below.
oprsrtrdshamtfunct
08910034
1. sub $t0, $t1, $t22. add $t2, $t0, $t13. sub $t2, $t1, $t04. sub $t2, $t0, $t1 2.6 Logical Operations
Although the
 rst computers operated on full words, it soon became clear that 
it was useful to operate o
 elds of bits within a word or even on individual bits. 
Examining characters within a word, each of which is stored as 8 bits, is one example 

of such an operation (see Section 2.9). It follows that operations were added to 

programming languages and instruction set architectures to simplify, among other 

things, the packing and unpacking of bits into word
 ese instructions are called 
logical operations. 
Figure 2.8
 shows logical operations in C, Java, and MIPS.
Check Yourself
“Contrariwise,” 
continued Tweedledee, 

“if it was so, it might 

be; and if it were so, 

it would be; but as it 

isn’t, it ain’t.
 at’s 
logic.”
Lewis Carroll, 
Alice’s Adventures in 

Wonderland
, 1865FIGURE 2.8 C and Java logical operators and their corresponding MIPS instructions.
 MIPS implements NOT using a NOR with one operand being zero.
 e 
 rst class of such operations is called 
 s ey move all the bits in a word 
to th  or righ
 lling the emptied bits with 0s. For example, if register 
$s0 contained
0000 0000 0000 0000 0000 0000 0000 1001two = 9tenand the instruction t
  
  by 4 was executed, the new value would be:
0000 0000 0000 0000 0000 0000 1001 0000two = 144tenLogical operationsC operatorsJava operatorsMIPS instructions
Shift left<<<< sll
Shift right>>>>> srl
Bit-by-bit AND&& and,andi
Bit-by-bit OR|| or,ori
Bit-by-bit NOT~~ nor

88 Chapter 2 Instructions: Language of the Computer
 e dual o
  
 
  righ
 e actual name of the tw
  instructions are called 
  
  logical
 (sll) and 
  right logical
 (srl e following instruction performs the operation above, assuming that the original 
value was in register 
$s0 and the result should go in register 
$t2:sll  $t2,$s0,4  # reg $t2 = reg $s0 << 4 bitsWe delayed explaining the 
shamt
 eld in the R-format. Us
  instructions, 
it stands for 
  amount
. Hence, the machine language version of the instruction 
above is
oprs
rt
rdshamtfunct00161040 e encoding of 
sll is 0 in both the op and func
 elds, rd contains 10 (register 
$t2), rt contains 16 (register 
$s0), and shamt contain
 e r
 eld is unused 
and thus is set to 0.
S
  
  logical provides a bonus be
 t. S
 ing 
  by 
i bits gives the same 
result as multiplying by 2
i, ju
 ing a decimal number by 
i digits is equivalent 
to multiplying by 10
i. For example, the above 
sll s by 4, which gives the same 
result as multiplying by 2
4 o
 e 
 rst bit pattern above represents 9, and 9 
16  144, the value of the second bit pattern.
Another useful operation that isolat
 elds is 
AND. (We capitalize the word to 
avoid confusion between the operation and the English conjunction.) AND is a bit-
by-bit operation that leaves a 1 in the result only if both bits of the operands are 1. 

For example, if register 
$t2 contains
0000 0000 0000 0000 0000 1101 1100 0000twoand register 
$t1 contains
0000 0000 0000 0000 0011 1100 0000 0000twothen, a
 er executing the MIPS instruction
and $t0,$t1,$t2    # reg $t0 = reg $t1 & reg $t2the value of register 
$t0 would be
0000 0000 0000 0000 0000 1100 0000 0000twoAs you can see, AND can apply a bit pattern to a set of bits to force 0s where there 

is a 0 in the bit pattern. Such a bit pattern in conjunction with AND is traditionally 

called a mask
, since the mask “conceals” some bits.
AND A logical bit-
by-bit operation with two 
operands that calculates 

a 1 only if there is a 1 in 

both
 operands.

To place a value into one of these seas of 0s, there is the dual to AND, called 
OR. It is a bit-by-bit operation that places a 1 in the result if 
either
 operand bit is 
a 1. To elaborate, if the registers 
$t1 and 
$t2 are unchanged from the preceding 
example, the result of the MIPS instruction
or $t0,$t1,$t2 # reg $t0 = reg $t1 | reg $t2is this value in register 
$t0:0000 0000 0000 0000 0011 1101 1100 0000two e 
 nal logical operation is a contrarian. 
NOT
 takes one operand and places a 1 
in the result if one operand bit is a 0, and vice versa. Using our prior notation, it 
calculates 
x.In keeping with the three-operand format, the designers of MIPS decided to 
include the instruction 
NOR (NOT OR) instead of NOT. If one operand is zero, 
then it is equivalent to NOT: A NOR 0 
 NOT (A OR 0) 
 NOT (A).
If the register 
$t1 is unchanged from the preceding example and register 
$t3 has the value 0, the result of the MIPS instruction
nor $t0,$t1,$t3 # reg $t0 = ~ (reg $t1 | reg $t3)is this value in register 
$t0:1111 1111 1111 1111 1100 0011 1111 1111twoFigure 2.8
 above shows the relationship between the C and Java operators and the 
MIPS instructions. Constants are useful in AND and OR logical operations as well 

as in arithmetic operations, so MIPS also provides the instructions 
and immediate
 (andi) and 
or immediate
 (ori). Constants are rare for NOR, since its main use is 
to invert the bits of a single operand; thus, the MIPS instruction set architecture has 

no immediate version of NOR.
Elaboration: The full MIPS instruction set also includes exclusive or (XOR), which 
sets the bit to 1 when two corresponding bits differ, and to 0 when they are the same. C 
allows bit ﬁ elds
 or ﬁ elds
 ned within words, both allowing objects to be packed 
within a word and to match an externally enforced interface such as an I/O device. All 

  t within a single word. Fields are unsigned integers that can be as short as 

1 bit. C compilers inser elds using logical instructions in MIPS: 
and, or, sll, and 
srl.Elaboration: Logical AND immediate and logical OR immediate put 0s into the upper 
16 bits to form a 32-bit constant, unlike add immediate, which does sign extension.
Which operations can isolat
 eld in a word?
1. AND2. A 
  
  followed by
  right
OR A logical bit-by-
bit operation with two 
operands that calculates 

a 1 if there is a 1 in 
either
 operand.
NOT
 A logical bit-by-
bit operation with one 

operand that inverts the 

bits; that is, it replaces 

every 1 with a 0, and 

every 0 with a 1.
NOR A logical bit-by-
bit operation with two 

operands that calculates 

the NOT of the OR of the 

two opera
 at is, it 
calculates a 1 only if there 

is a 0 in both
 operands.
Check 
Yourself
 2.6 Logical Operations 89
90 Chapter 2 Instructions: Language of the Computer
 2.7 Instructions for Making Decisions
What distinguishes a computer from a simple calculator is its ability to make 
decisions. Based on the input data and the values created during computation, 

 erent instructions execute. Decision making is commonly represented in 
programming languages using the 
if statement, sometimes combined with 
go to
 statements and labels. MIPS assembly 
language includes two decision-making 
instructions, similar to an 
if statement with a 
go to
 e 
 rst instruction is
beq register1, register2, L1 is instruction means go to the statement labeled 
L1 if the value in 
register1 equals the value in 
register2 e mnemonic beq stands for 
branch if equal
.  e second instruction is
bne register1, register2, L1It means go to the statement labeled 
L1 if the value in 
register1 does 
not equal 
the value in 
register2 e mnemonic 
bne stands for 
branch if not equal
 ese 
two instructions are traditionally called 
conditional branches
.Compiling if-then-else into Conditional BranchesIn the following code segment, 
f, g, h, i, and 
j are variables. If th
 ve 
variables 
f through 
j correspond to th
 ve registers 
$s0 through 
$s4, what 
is the compiled MIPS code for this C 
if statement?
if (i == j) f = g + h; else f = g – h;Figure 2.9
 sho
 owchart of what the MIPS code should do
 e 
 rst 
expression compares for equality, so it would seem that we would want the 

branch if registers are equal instruction (
beq). In general, the code will be 
more
  cient if we test for the opposite condition to branch over the code that 
performs the subsequent 
then
 part of the 
if (the label 
Else ned below) 
and so we use the branch if registers are 
not equal instruction (
bne): e utility of an 
automatic computer lies 

in the possibility of using 

a given sequence of 

instructions repeatedly, 

the number of times it is 

iterated being dependent 

upon the results of 

the computation . . . . 

 is choice can be 
made to depend upon 

the sign of a number 

(zero being reckoned 

as plus for machine 

purposes). Consequently, 

we introduce an 

[instruction] (the 

conditional transfer 

[instruction]) which 

will, depending on the 

sign of a given number, 

cause the proper one 

of two routines to be 

executed.
Burks, Goldstine, and 
von Neumann, 1947
EXAMPLEANSWER
 2.7 Instructions for Making Decisions 
91 e next assignment statement performs a single operation, and if all the 
operands are allocated to registers, it is just one instruction:
We now need to go to the end of the 
if statemen
 is example introduces 
another kind of branch, o
 en called an 
unconditional branch
 is instruction 
says that the processor always follows the branch. To distinguish between 
conditional and unconditional branches, the MIPS name for this type of 

instruction is 
jump, abbreviated as 
j (the label 
Exit ned below).
j Exit     # go to Exit e assignment statement in the 
else
 portion of the 
if statement can again be 
compiled into a single instruction. We just need to append the label 
Else to 
this instruction. We also show the label 
Exit that is a
 er this instruction, 
showing the end of the 
if-then-else
 compiled code:
Else:sub $s0,$s1,$s2  # f = g – h (skipped if i = j)Exit:Notice that the assembler relieves the compiler and the assembly language 
programmer from the tedium of calculating addresses for branches, just as it does 

for calculating data addresses for loads and stores (see Section 2.12).
f=g+hf=gŒhi=ji ji= =j?
Else:Exit:FIGURE 2.9 Illustration of the options in the if statement above.
 e 
  box corresponds to 
the 
then
 part of the 
if statement, and the right box corresponds to the 
else
 part.
conditional branch
 An instruction that requires 
the comparison of two 

values and that allows for 

a subsequent transfer of 

control to a new address 

in the program based 

on the outcome of the 

comparison.

92 Chapter 2 Instructions: Language of the Computer
Compilers frequently create branches and labels where they do not appear in 
the programming language. Avoiding the burden of writing explicit labels and 

branches is one be
 t of writing in high-level programming languages and is a 
reason coding is faster at that level.
LoopsDecisions are important both for choosing between two alternatives—found in 
if statements—and for iterating a computation—found in loo
 e same assembly 
instructions are the building blocks for both cases.
Compiling a while
 Loop in CHere is a traditional loop in C:
while (save[i] == k)i += 1;Assume that 
i and 
k correspond to registers 
$s3 and 
$s5 and the base of the 
array 
save is in $s6. What is the MIPS assembly code corresponding to this 
C segment?
 e 
 rst step is to load 
save[i] into a temporary register. Before we can load 
save[i] into a temporary register, we need to have its address. Before we 
can add 
i to the base of array 
save to form the address, we must multiply the 
index i by 4 due to the byte addressing problem. Fortunately, we can us
    logical
 ing 
  by 2 bits multiplies by 2
2 or 4 (see page 88 in the 
prior section). We need to add the label 
Loop to it so that we can branch back 
to that instruction at the end of the loop:
Loop: sll  $t1,$s3,2    # Temp reg $t1 = i * 4To get the address of 
save[i], we need to add 
$t1 and the base of save in 
$s6:add $t1,$t1,$s6     # $t1 = address of save[i]Now we can use that address to load 
save[i] into a temporary register:
lw $t0,0($t1)       # Temp reg $t0 = save[i] e next instruction performs the loop test, exiting if 
:Hardware/
Software 
InterfaceEXAMPLEANSWER
 e next instruction adds 1 to 
i:addi $s3,$s3,1      # i = i + 1 e end of the loop branches back to the 
while
 test at the top of the loop. We 
just add the 
Exit label a
 er it, and we’re done:
j     Loop          # go to LoopExit:(See the exercises for an optimization of this sequence.)
Such sequences of instructions that end in a branch are so fundamental to compiling 
that they are given their own buzzword: a 
basic block
 is a sequence of instructions 
without branches, except possibly at the end, and without branch targets or branch 
labels, except possibly at the beginning. One of th
 rst early phases of compilation 
is breaking the program into basic blocks.
 e test for equality or inequality is probably the most popular test, but sometimes 
it is useful to see if a variable is less than another variable. For example, a 
for loop 
may want to test to see if the index variable is less than 0. Such comparisons are 

accomplished in MIPS assembly language with an instruction that compares two 

registers and sets a third register to 1 if th
 rst is less than the second; otherwise, 
it is set t
 e MIPS instruction is called s
et on less than,
 or 
slt. For example,
slt    $t0, $s3, $s4   # $t0 = 1 if $s3 < $s4means that register 
$t0 is set to 1 if the value in register 
$s3 is less than the value 
in register 
$s4; otherwise, register 
$t0 is set to 0.
Constant operands are popular in comparisons, so there is an immediate version 
of the set on less than instruction. To test if register 
$s2 is less than the constant 
10, we can just write
slti    $t0,$s2,10     # $t0 = 1 if $s2 < 10MIPS compilers use the 
slt, slti, beq, bne, and th
 xed value of 0 (always 
available by reading register 
$zero) to create all relative conditions: equal, not 
equal, less than, less than or equal, greater than, greater than or equal.
Hardware/

Software 

Interfacebasic block
 A sequence 
of instructions without 
branches (except possibly 

at the end) and without 

branch targets or branch 

labels (except possibly at 

the beginning).
Hardware/
Software 

Interface 2.7 Instructions for Making Decisions 
93
94 Chapter 2 Instructions: Language of the Computer
Heeding von Neumann’s warning about the simplicity of the “equipment,” the 
MIPS architecture doesn’t include branch on less than because it is too complicated; 
either it would stretch the clock cycle time or it would take extra clock cycles per 

instruction. Two faster instructions are more useful.
Comparison instructions must deal with the dichotomy between signed and 
unsigned numbers. Sometimes a bit pattern with a 1 in th
 cant bit 
represents a negative number and, of course, is less than any positive number, 

which must have a 0 in th
 cant bit. With unsigned integers, on the 
other hand, a 1 in th
 cant bit represents a number that is 
larger
 than 
any that begins with a 0. (We’ll soon take advantage of this dual meaning of the 

 cant bit to reduce the cost of the array bounds checking.)
MIPS o
 ers two versions of the set on less than comparison to handle these 
alternatives. 
Set on less than
 (slt) and 
set on less than immediate
 (slti) work with 
signed integers. Unsigned integers are compared using 
set on less than unsigned
 (sltu) and 
set on less than immediate unsigned
 (sltiu).Signed versus Unsigned Comparison
Suppose register 
$s0 has the binary number
1111 1111 1111 1111 1111 1111 1111 1111twoand that register 
$s1 has the binary number
0000 0000 0000 0000 0000 0000 0000 0001twoWhat are the values of registers 
$t0 and 
$t1 er these two instructions?
slt      $t0, $s0, $s1 # signed comparisonsltu     $t1, $s0, $s1 # unsigned comparison e value in register 
$s0 represents 
1ten
 if it is an integer and 4,294,967,295
ten
 if it is an unsigned integer
 e value in register 
$s1 represents 1
ten
 in either 
case
 en register 
$t0 has the value 1, since 
1ten
 1ten
, and register 
$t1 has the value 0, since 4,294,967,295
ten
 1ten
.Hardware/
Software 
InterfaceEXAMPLEANSWER
Treating signed numbers as if they were unsigned gives us a low cost way of 
checking if 0 
 x  y, which matches the index out-of-bounds check for arra
 e key is that negative integers in two’s complement notation look like large numbers 
in unsigned notation; that is, th
 cant bit is a sign bit in the former 
notation but a large part of the number in the latter
 us, an unsigned comparison 
of 
x  y also checks if 
x is negative as well as if 
x is less than 
y.Bounds Check Shortcut
Use this shortcut to reduce an index-out-of-bounds check: jump to 

IndexOutOfBounds if  or if 
$s1 is negative.
 e checking code just uses 
u to do both checks:
sltu $t0,$s1,$t2 # $t0=0 if $s1>=length or $s1<0beq  $t0,$zero,IndexOutOfBounds #if bad, goto ErrorCase/Switch StatementMost programming languages have a 
case
 or 
switch
 statement that allows the 
programmer to select one of many alternatives depending on a single value
 e simplest way to implement 
switch
 is via a sequence of conditional tests, turning the 
switch
 statement into a chain of 
if-then-else
 statements.
Sometimes the alternatives may be mor
  ciently encoded as a table of 
addresses of alternative instruction sequences, called a 
jump address table
 or 
jump table
, and the program needs only to index into the table and then jump to 
the appropriate sequence
 e jump table is then just an array of words containing 
addresses that correspond to labels in the code
 e program loads the appropriate 
entry from the jump table into a register. It then needs to jump using the address 
in the register. To support such situations, computers like MIPS include a 
jump 
register
 instruction (
jr), meaning an unconditional jump to the address sp
 ed in a register
 en it jumps to the proper address using this instruction. We’ll see an 
even more popular use of 
jr in the next section.
EXAMPLEANSWERjump address 
table
 Also called 
jump 
table
. A table of addresses 
of alternative instruction 
sequences.
 2.7 Instructions for Making Decisions 
95
96 Chapter 2 Instructions: Language of the Computer
Although there are many statements for decisions and loops in programming 
languages like C and Java, the bedrock statement that implements them at the 

instruction set level is the conditional branch.
Elaboration: If you have heard about 
delayed branches
, covered in Chapter 4, don’t 
worry: the MIPS assembler makes them invisible to the assembly language programmer.
 I. C has many statements for decisions and loops, while MIPS has few. Which 
of the following do or do not explain this imbalance? Why?
1. More decision statements make code easier to read and understand.

2. Fewer decision statements simplify the task of the underlying layer that is 
responsible for execution.
3. More decision statements mean fewer lines of code, which generally 
reduces coding time.
4. More decision statements mean fewer lines of code, which generally 
results in the execution of fewer operations.
II. Why does C provide two sets of operators for AND (& and &&) and two sets 
of operators for OR (| and ||), while MIPS doesn’t?

1. Logical operations AND and OR implement & and |, while conditional 
branches implement && and ||.
 e previous statement has it backwards: && and || correspond to logical 
operations, while & and | map to conditional branches.
 ey are redundant and mean the same thing: && and || are simply 
inherited from the programming language B, the predecessor of C.
 2.8  Supporting Procedures in Computer 
Hardware
A procedure
 or function is one tool programmers use to structure programs, both 
to make them easier to understand and to allow code to be reused. Procedures 
allow the programmer to concentrate on just one portion of the task at a time; 
parameters act as an interface between the procedure and the rest of the program 

and data, since they can pass values and return results. We describe the equivalent 

to procedures in Java in 
 Section 2.15
, but Java needs everything from a computer 
that C needs. Procedures are one way to implement 
abstraction
 in so
 ware.
Hardware/
Software 
InterfaceCheck Yourself
procedure
 A stored 
subroutine that performs 
a sp
 c task based 
on the parameters with 

which it is provided.

 2.8 Supporting Procedures in Computer Hardware 
97You can think of a procedure like a spy who leaves with a secret plan, acquires 
resources, performs the task, covers his or her tracks, and then returns to the point 
of origin with the desired result. Nothing else should be perturbed once the mission 

is complete. Moreover, a spy operates on only a “need to know” basis, so the spy 

can’t make assumptions about his employer.
Similarly, in the execution of a procedure, the program must follow these six 
steps:
1. Put parameters in a place where the procedure can access them.
2. Transfer control to the procedure.

3. Acquire the storage resources needed for the procedure.

4. Perform the desired task.

5. Put the result value in a place wher
e the calling program can access it.
6. Return control to the point of origin, since a procedure can be called from 
several points in a program.
As mentioned above, registers are the fastest place to hold data in a computer, 
so we want to use them as much as possible. MIPS so
 ware follows the following 
convention for procedure calling in allocating its 32 registers:
 $a0–$a3: four argument registers in which to pass parameters
 $v0–$v1: two value registers in which to return values
 $ra: one return address register to return to the point of origin
In addition to allocating these registers, MIPS assembly language includes an 
instruction just for the procedures: it jumps to an address and simultaneously 

saves the address of the following instruction in register 
$ra e jump-and-link 
instruction
 (jal) is simply written
jal ProcedureAddress e link portion of the name means that an address or link is formed that points 
to the calling site to allow the procedure to return to the proper addr
 is “link,” 
stored in register
$ra (register 31), is called the 
return address
 e return address 
is needed because the same procedure could be called from several parts of the 

program.
To support such situations, computers like MIPS use 
jump register
 instruction 
(jr), introduced above to help with case statements, meaning an unconditional 

jump to the address sp
 ed in a register:
jr   $rajump-and-link 
instruction
 An instruction that jumps 
to an address and 

simultaneously saves the 

address of the following 

instruction in a register 

($ra in MIPS).return address
 A link to 
the calling site that allows 

a procedure to return 

to the proper address; 

in MIPS it is stored in 

register 
$ra.
98 Chapter 2 Instructions: Language of the Computer
 e jump register instruction jumps to the address stored in register 
$ra—which is just what we wan
 us, the calling program, or 
caller
, puts the parameter 
values in $a0–$a3 and uses 
jal X to jump to procedure 
X (sometimes named 
the 
callee
 e callee then performs the calculations, places the results in 
$v0 and 
$v1, and returns control to the caller using 
jr $ra.Implicit in the stored-program idea is the need to have a register to hold the 
address of the current instruction being executed. For historical reasons, this 
register is almost always called the 
program counter
, abbreviated 
PC in the MIPS 
architecture, although a more sensible name would have been 
instruction address 
register
 e jal instruction actually saves PC 
 4 in register 
$ra to link to the 
following instruction to set up the procedure return.
Using More Registers
Suppose a compiler needs more registers for a procedure than the four argument 

and two return value registers. Since we must cover our tracks a
 er our mission 
is complete, any registers needed by the caller must be restored to the values that 

they contained 
before
 the procedure was invoked.
 is situation is an example in 
which we need to spill registers to memory, as mentioned in the 
Hardware/So
 ware 
Interface
 section above.
 e ideal data structure for spilling registers is a 
stack rst-out 
queue. A stack needs a pointer to the most recently allocated address in the stack 

to show where the next procedure should place the registers to be spilled or where 

old register values are found
 e stack pointer
 is adjusted by one word for each 
register that is saved or restored. MIPS so
 ware reserves register 29 for the stack 
pointer, giving it the obvious name 
$sp. Stacks are so popular that they have their 
own buzzwords for transferring data to and from the stack: placing data onto the 

stack is called a 
push
, and removing data from the stack is called a 
pop
.By historical precedent, stacks “grow” from higher addresses to lower addresses. 
 is convention means that you push values onto the stack by subtracting from the 
stack pointer. Adding to the stack pointer shrinks the stack, thereby popping values 

  the stack.
Compiling a C Procedure That Doesn’t Call Another Procedure
Let’s turn the example on page 65 from Section 2.2 into a C procedure:
int leaf_example (int g, int h, int i, int j){
     int f;     f = (g + h) – (i + j);     return f;
}What is the compiled MIPS assembly code?
caller
 e program that 
instigates a procedure and 
provides the necessary 

parameter values.
callee
 A procedure that 
executes a series of stored 

instructions based on 

parameters provided by 

the caller and then returns 

control to the caller.
program counter 
(PC) e register 
containing the address 
of the instruction in the 

program being executed.
stack A data structure 
for spilling registers 

organized as a last-in-

 rst-out queue.
stack pointer
 A value 
denoting the most 

recently allocated address 

in a stack that shows 

where registers should 

be spilled or where old 

register values can be 

found. In MIPS, it is 

register 
$sp.push
 Add element to 
stack.
pop
 Remove element 
from stack.
EXAMPLE
 e parameter variables 
g, h, i, and 
j correspond to the argument registers 
$a0, $a1, $a2, and 
$a3, and 
f corresponds to 
$s0 e compiled program 
starts with the label of the procedure:
leaf_example: e next step is to save the registers used by the procedure e C assignment 
statement in the procedure body is identical to the example on page 68, which 
uses two temporary register us, we need to save three registers: 
$s0, $t0, and 
$t1. We “push” the old values onto the stack by creating space for three 
words (12 bytes) on the stack and then store them:
addi $sp, $sp, –12  # adjust stack to make room for 3 itemssw  $t1, 8($sp)     # save register $t1 for use afterwards
sw  $t0, 4($sp)     # save register $t0 for use afterwards
sw  $s0, 0($sp)     # save register $s0 for use afterwardsFigure 2.10
 shows the stack before, during, and a
 er the procedure call.
 e next three statements correspond to the body of the procedure, which 
follows the example on page 68:
add $t0,$a0,$a1 # register  $t0 contains g + h
add $t1,$a2,$a3 # register  $t1 contains i + j
sub $s0,$t0,$t1 # f = $t0 – $t1, which is (g + h)–(i + j)To return the value of f, we copy it into a return value register:
add $v0,$s0,$zero # returns f ($v0 = $s0 + 0)Before returning, we restore the three old values of the registers we saved by 
“popping” them from the stack:
lw $s0, 0($sp)  # restore register $s0 for callerlw $t0, 4($sp)  # restore register $t0 for caller
lw $t1, 8($sp)  # restore register $t1 for caller
addi $sp,$sp,12 # adjust stack to delete 3 items e procedure ends with a jump register using the return address:
jr   $ra    # jump back to calling routineIn the previous example, we used temporary registers and assumed their old 
values must be saved and restored. To avoid saving and restoring a register whose 
value is never used, which might happen with a temporary register, MIPS so
 ware 
separates 18 of the registers into two groups:
 $t0–$t9: temporary registers that are 
not preserved by the callee (called 
procedure) on a procedure call
 $s0–$s7: saved registers that must be preserved on a procedure call (if 

used, the callee saves and restores them)
ANSWER 2.8 Supporting Procedures in Computer Hardware 
99
100 Chapter 2 Instructions: Language of the Computer
 is simple convention reduces register spilling. In the example above, since the 
caller does not expect registers 
$t0 and 
$t1 to be preserved across a procedure 
call, we can drop two stores and two loads from the code. We still must save and 
restore 
$s0, since the callee must assume that the caller needs its value.
Nested ProceduresProcedures that do not call others are called 
leaf
 procedures. Life would be simple if 
all procedures were leaf procedures, but they aren’t. Just as a spy might employ other 

spies as part of a mission, who in turn might use even more spies, so do procedures 

invoke other procedures. Moreover, recursive procedures even invoke “clones” of 

themselves. Just as we need to be careful when using registers in procedures, more 

care must also be taken when invoking nonleaf procedures.
For example, suppose that the main program calls procedure A with an argument 
of 3, by placing the value 3 into register 
$a0 and then using 
jal en suppose 
that procedure A calls procedure B via 
jal B with an argument of 7, also placed 
in $a0. Since A hasn’
 nished its task yet, there is a co
 ict over the use of register 
$a0. Similarly, there is a co
 ict over the return address in register 
$ra, since it 
now has the return address for B. Unless we take steps to prevent the problem, this 

co
 ict will eliminate procedure A’s ability to return to its caller.
One solution is to push all the other registers that must be preserved onto 
the stack, just as we did with the saved register
 e caller pushes any argument 
registers (
$a0–$a3) or temporary registers (
$t0–$t9) that are needed a
 er the call
 e callee pushes the return address register 
$ra and any saved registers 
($s0–$s7) used by the callee
 e stack pointer 
$sp is adjusted to account for the 
number of registers placed on the stack. Upon the return, the registers are restored 

from memory and the stack pointer is readjusted.
High addressLow address
Contents of register $t1Contents of register $t0Contents of register $s0$sp$sp$sp(a)(b)(c)FIGURE 2.10 The values of the stack pointer and the stack (a) before, (b) during, and (c) 
after the procedure call. e stack pointer always points to the “top” of the stack, or the last word in the 
stack in this drawing.

Compiling a Recursive C Procedure, Showing Nested Procedure 
LinkingLet’s tackle a recursive procedure that calculates factorial:
int fact (int n){
    if (n < 1) return (1);
          else return (n * fact(n – 1));
}What is the MIPS assembly code?
 e parameter variable 
n corresponds to the argument register 
$a0 e compiled program starts with the label of the procedure and then saves two 
registers on the stack, the return address and 
$a0:fact:    addi  $sp, $sp, –8 # adjust stack for 2 items
    sw    $ra, 4($sp)  # save the return address
    sw    $a0, 0($sp)  # save the argument n e 
 rst time 
fact is called, 
sw saves an address in the program that called 
fact e next two instructions test whether 
n is less than 1, going to 
L1 if  1.slti  $t0,$a0,1     # test for n < 1
beq   $t0,$zero,L1  # if n >= 1, go to L1If 
n is less than 1, 
fact returns 1 by putting 1 into a value register: it adds 1 to 
0 and places that sum in 
$v0. It then pops the two saved values o
  the stack 
and jumps to the return address:
addi  $v0,$zero,1 # return 1
addi  $sp,$sp,8   # pop 2 items off stack
jr    $ra         # return to callerBefore popping two items o
  the stack, we could have loaded 
$a0 and 
$ra. Since 
$a0 and 
$ra don’t change when 
n is less than 1, we skip those 
instructions.
If 
n is not less than 1, the argument 
n is decremented and then 
fact is called again with the decremented value:
L1: addi $a0,$a0,–1  # n >= 1: argument gets (n – 1)
    jal fact         # call fact with (n –1)EXAMPLEANSWER 2.8 Supporting Procedures in Computer Hardware 
101
102 Chapter 2 Instructions: Language of the Computer
 e next instruction is where 
fact returns. Now the old return address and 
old argument are restored, along with the stack pointer:
lw   $a0, 0($sp)  # return from jal: restore argument nlw   $ra, 4($sp)  # restore the return address

addi $sp, $sp, 8  # adjust stack pointer to pop 2 itemsNext, the value register 
$v0 gets the product of old argument 
$a0 and 
the current value of the value register. We assume a multiply instruction is 
available, even though it is not covered until Chapter 3:
mul  $v0,$a0,$v0   # return n * fact (n – 1)Finally, 
fact jumps again to the return address:
jr   $ra           # return to the callerA C variable is generally a location in storage, and its interpretation depends both 

on its 
type
 and 
storage class
. Examples include integers and characters (see Section 
2.9). C has two storage classes: 
automatic
 and 
static
. Automatic variables are local to 
a procedure and are discarded when the procedure exits. Static variables exist across 

exits from and entries to procedures. C variables declared outside all procedures 

are considered static, as are any variables declared using the keyword 
static
 e rest are automatic. To simplify access to static data, MIPS so ware reserves another 

register, called the 
global pointer
, or 
$gp.Figure 2.11 summarizes what is preserved across a procedure call. Note that 
several schemes preserve the stack, guaranteeing that the caller will get the same 

data back on a load from the stack as it stored onto the stac
 e stack above 
$sp is preserved simply by making sure the callee does not write above 
$sp; $sp is Hardware/
Software 
Interfaceglobal pointer
 e register that is reserved to 
point to the static area.
Saved registers: 
$s0Œ$s7Temporary registers: 
$t0Œ$t9Stack pointer register: 
$sp  Argument registers: 
$a0Œ$a3Return address register: 
$ra Return value registers: 
$v0Œ$v1Stack above the stack pointerStack below the stack pointerPreserved
Not preserved
FIGURE 2.11 What is and what is not preserved across a procedure call.
 If the so
 ware relies 
on the frame pointer register or on the global point
er register, discussed in the following subsections, they 
are also preserved.

itself preserved by the callee adding exactly the same amount that was subtracted 
from it; and the other registers are preserved by saving them on the stack (if they 

are used) and restoring them from there.
Allocating Space for New Data on the Stack
 e 
 nal complexity is that the stack is also used to store variables that are local 
to the procedure but do no
 t in registers, such as local arrays or structur
 e segment of the stack containing a procedure’s saved registers and local variables is 

called a procedure frame
 or 
activation record
. Figure 2.12
 shows the state of the 
stack before, during, and a
 er the procedure call.
Some MIPS so
 ware uses a 
frame pointer
 ($fp) to point to th
 rst word of 
the frame of a procedure. A stack pointer might change during the procedure, and 

so references to a local variable in memory might hav
 erent o
 sets depending 
on where they are in the procedure, making the procedure harder to understand. 

Alternatively, a frame pointer o
 ers a stable base register within a procedure for 
local memory-references. Note that an activation record appears on the stack 

whether or not an explicit frame pointer is used. We’ve been avoiding using 
$fp by 
avoiding changes to 
$sp within a procedure: in our examples, the stack is adjusted 

only on entry and exit of the procedure.
procedure frame
 Also 
called activation record
.  e segment of the stack 
containing a procedure’s 
saved registers and local 

variables.
frame pointer
 A value 
denoting the location of 

the saved registers and 

local variables for a given 

procedure.
High addressLow address
(a)(b)(c)Saved argument
registers (if any)
$sp$sp$sp$fp$fp$fpSaved return address
Saved saved
registers (if any)
Local arrays and
structures (if any)
FIGURE 2.12 Illustration of the stack allocation (a) before, (b) during, and (c) after the 
procedure call. e frame pointer (
$fp) points to th rst word of the frame, o
 en a saved argument 
register, and the stack pointer (
$sp) points to the top of the stac
 e stack is adjusted to make room for 
all the saved registers and any memory-resident local variables. Since the stack pointer may change during 
program execution, it’s easier for programmers to reference variables via the stable frame pointer, although it 

could be done just with the stack pointer and a little address arithmetic. If there are no local variables on the 

stack within a procedure, the compiler will save time by 
not setting and restoring the frame pointer. When a 
frame pointer is used, it is initialized using the address in 
$sp on a call, and 
$sp is restored using 
$fp is information is also found in Column 4 of the MIPS Reference Data Card at the front of this book.
 2.8 Supporting Procedures in Computer Hardware 
103
104 Chapter 2 Instructions: Language of the Computer
Allocating Space for New Data on the Heap
In addition to automatic variables that are local to procedures, C programmers 
need space in memory for static variables and for dynamic data structures. 
Figure
 
2.13 shows the MIPS convention for allocation of memory
 e stack starts in the 
high end of memory and grows do
 e 
 rst part of the low end of memory is 
reserved, followed by the home of the MIPS machine code, traditionally called 

the 
text segment
. Above the code is the 
static data segment
, which is the place 
for constants and other static variables. Although arrays tend to b
 xed length 
and thus are a good match to the static data segment, data structures like linked 

lists tend to grow and shrink during their lif
 e segment for such data 
structures is traditionally called the 
heap,
 and it is placed next in memory. Note 
that this allocation allows the stack and heap to grow toward each other, thereby 

allowing th
  cient use of memory as the two segments wax and wane.
text segment
 e segment of a UNIX object 
 le that contains the 

machine language code 

for routines in the source 

 le.
Stack
Dynamic dataStatic dataText
Reserved
$sp7fff fffchex$gp1000 8000hex1000 0000hexpc0040 0000hex0FIGURE 2.13 The MIPS memory allocation for program and data.
 ese addresses are only 
a so
 ware convention, and not part of the MIPS architecture
 e stack pointer is initialized to 
7fff fffchex and grows down toward the data segment. At the other end, the program code (“text”) starts at 
0040 0000hex e static data starts at 
1000 0000hex. Dynamic data, allocated by 
malloc in C and by 
new in Java, is next. It grows up toward the stack in an area called the heap
 e global pointer, 
$gp, is set to 
an address to make it easy to access data. It is initialized to 
1000 8000hex so that it can access from 
1000 0000hex to 
1000 ffffhex using the positive and negative 16-bit o
 sets from 
$gp is information is also 
found in Column 4 of the MIPS Reference Data Card at the front of this book.
C allocates and frees space on the heap with explicit functions. 
malloc() allocates space on the heap and returns a pointer to it, and 
free() releases 
space on the heap to which the pointer points. Memory allocation is controlled by 

programs in C, and it is the source of many common an
  cult bugs. Forgetting 
to free space leads to a “memory leak,” which eventually uses up so much memory 

that the operating system may crash. Freeing space too early leads to “dangling 

pointers,” which can cause pointers to point to things that the program never 

intended. Java uses automatic memory allocation and garbage collection just to 

avoid such bugs.

Figure 2.14 summarizes the register conventions for the MIPS assembly 
lan
guage
 is
 convention is another example of making the
 common case fast
: most procedures can be sa
 ed with up to 4 arguments, 2 registers for a return 
value, 8 saved registers, and 10 temporary registers without ever going to memory.
NameRegister number
UsagePreserved on 
call?$zero0The constant value 0n.a.
$v0Œ$v12Œ3Values for results and expression evaluationno
$a0Œ$a34Œ7Argumentsno
$t0Œ$t7on
seiraropmeT
51Œ8
$s0Œ$s7sey
devaS
32Œ61
$t8Œ$t9on
seiraropmeteroM
52Œ42
$gpsey
retnioplabolG
82
$spsey
retniopkcatS
92
$fpsey
retniopemarF
03
$rasey
sserddanruteR
13
FIGURE 2.14 MIPS register conventions.
 Register 1, called 
$at, is reserved for the assembler (see 
Section 2.12), and registers 26–27, called 
$k0–$k1, are reserved for the operating syst
 is information 
is also found in Column 2 of the MIPS Reference Data Card at the front of this book.
Elaboration: What if there are more than four parameters? The MIPS convention is 
to place the extra parameters on the stack just above the frame pointer. The procedure 
 rst four parameters to be in registers 
$a0 through $a3 and the rest in memory, addressable via the frame pointer.
As mentioned in the caption of Figure 2.12
, the frame pointer is convenient because 
all references to variables in the stack within a procedure will have the same offset. 

The frame pointer is not necessary, however. The GNU MIPS C compiler uses a frame 

pointer, but the C compiler from MIPS does not; it treats register 30 as another save 

register ($s8).Elaboration: Some recursive procedures can be implemented iteratively without using 
recur
 cantly improve performance by removing the overhead 
associated with recursive procedure calls. For example, consider a procedure used to 

accumulate a sum:int sum (int n, int acc) {  if (n >0)
      return sum(n – 1, acc + n);
  else
      return acc;
}Consider the procedure call sum(3,0). This will result in recursive calls to 
sum(2,3), sum(1,5), and 
sum(0,6), and then the result 6 will be returned four 
 2.8 Supporting Procedures in Computer Hardware 
105
106 Chapter 2 Instructions: Language of the Computer
times. This recursive call of sum is referred to as a 
tail call, and this example use of 
tail recursion can be implemented ver ciently (assume 
$a0 = n and $a1 = acc):sum: slti $t0, $a0, 1 # test if n <= 0      bne $t0, $zero, sum_exit # go to sum_exit if n <= 0
      add$a1, $a1, $a0 # add n to acc      addi$a0, $a0, –1 # subtract 1 from n      j sum   
# go to sumsum_exit:
      add$v0, $a1, $zero # return value acc

      jr $ra  
# return to callerWhich of the following statements about C and Java are generally true?
1. C programmers manage data explicitly, while it’s automatic in Java.
2. C leads to more pointer bugs and memory leak bugs than does Java.
 2.9 Communicating with People
Computers were invented to crunch numbers, but as soon as they became 
commercially viable they were used to process text. Most computers today o
 er 8-bit bytes to represent characters, with the 
American Standard Code for Information 
Interchange
 (ASCII) being the representation that nearly everyone follows. 
Figure
 2.15 summarizes ASCII.
Check Yourself
!(@ |   (wow open 

tab at bar is great)
Fourth line of the 
keyboard poem “Hatless 

Atlas,” 1991 (some 

give names to ASCII 

characters: “!” is “wow,” 

“(” is open, “|” is bar, 

and so on).
ASCII valueChar-acterASCII valueChar-acterASCII valueChar-acterASCII valueChar-acterASCII valueChar-acterASCII valueChar-acter096`112p
33!49
097a113q
34"50
098b114r
35#5136
099c115s
36$52
32 space48064@80P
165A81Q
266B82R
7C83S
468D84T100d116t
37%53569E85U101e117u

38&54670F86V102f118v

39'55771G87W103g119w

40(56872H88X104h120x

41)57973I89Y105i121y

42*58:74J90Z106j122z

43+59;75K91[107k123{

44,60<76L92\108l124|

45-61=77M93]109m125}

46.62>78N94^110n126~

47/63?79O95_111o127DEL
FIGURE 2.15 ASCII representation of characters.
 Note that upper- and lowercase letter
 er by exactly 32; this observation can 
lead to shortcuts in checking or changing upper- and lowercase. Values not shown include formatting characters. For example, 8 
represents a 
backspace, 9 represents a tab character, and 13 a carriage return. Another useful value is 0 for null, the value the programmin
g language C uses 
to mark the end of a string
 is information is also found in Column 3 of the MIPS Reference Data Card at the front of this book.

 2.9 Communicating with People 
107ASCII versus Binary Numbers
We could represent numbers as strings of ASCII digits instead of as integers. 
How much does storage increase if the number 1 billion is represented in 

ASCII versus a 32-bit integer?
One billion is 1,000,000,000, so it would take 10 ASCII digits, each 8 bits long. 
 us the storage expansion would be (10 
 8)/32 or 2.5. Beyond the expansion 
in storage, the hardware to add, subtract, multiply, and divide such decimal 

number
  cult and would consume more energy. Suc
  culties explain 
why computing professionals are raised to believe that binary is natural and 

that the occasional decimal computer is bizarre.
A series of instructions can extract a byte from a word, so load word and store 
word ar
  cient for transferring bytes as well as words. Because of the popularity 
of text in some programs, however, MIPS provides instructions to move bytes. 
Load 
byte
 (lb) loads a byte from memory, placing it in the rightmost 8 bits of a register. 
Store byte
 (sb) takes a byte from the rightmost 8 bits of a register and writes it to 
memory
 us, we copy a byte with the sequence
lb $t0,0($sp)        # Read byte from sourcesb $t0,0($gp)        # Write byte to destinationCharacters are normally combined into strings, which have a variable number 
of character
 ere are three choices for representing a string: (1) th
 rst position 
of the string is reserved to give the length of a string, (2) an accompanying variable 
has the length of the string (as in a structure), or (3) the last position of a string is 

indicated by a character used to mark the end of a string. C uses the third choice, 

terminating a string with a byte whose value is 0 (named n
 us, the string “Cal” is represented in C by the following 4 bytes, shown as decimal 

numbers: 67, 97, 108, 0. (As we shall see, Java uses th
 rst option.)
EXAMPLEANSWER
108 Chapter 2 Instructions: Language of the Computer
Compiling a String Copy Procedure, Showing How to Use C Strings
 e procedure 
strcpy copies string 
y to string 
x using the null byte 
termination convention of C:
void strcpy (char x[], char y[]){
    int i;    i = 0;    while ((x[i] = y[i]) != ‘\0’) /* copy & test byte */
    i += 1;
}What is the MIPS assembly code?
Below is the basic MIPS assembly code segment. Assume that base addresses 
for arrays 
x and 
y are found in 
$a0 and 
$a1, while i is in 
$s0. strcpy adjusts the stack pointer and then saves the saved register 
$s0 on the stack:
strcpy: addi $sp,$sp,–4 # adjust stack for 1 more item

 sw $s0, 0($sp) # save $s0
To initialize 
i to 0, the next instruction sets 
$s0 to 0 by adding 0 to 0 and 
placing that sum in 
$s0: add $s0,$zero,$zero # i = 0 + 0
 is is the beginning of the loop
 e address of 
y[i] rst formed by adding 
i to 
y[]:L1: add $t1,$s0,$a1 # address of y[i] in $t1
Note that we don’t have to multiply 
i by 4 since 
y is an array of 
bytes
 and not 
of words, as in prior examples.
To load the character in 
y[i], we use load byte unsigned, which puts the 
character into 
$t2: lbu $t2, 0($t1) # $t2 = y[i]
A similar address calculation puts the address of 
x[i] in $t3, and then the 
character in 
$t2 is stored at that address.
EXAMPLEANSWER
 add $t3,$s0,$a0 # address of x[i] in $t3
 sb $t2, 0($t3) # x[i] = y[i]
Next, we exit the loop if the charact
 at is, we exit if it is the last 
character of the string:
 beq $t2,$zero,L2 # if y[i] == 0, go to L2
If not, we increment i and loop back:
 addi  $s0, $s0,1 # i = i + 1

 j L1 
# go to L1If we don’t loop back, it was the last character of the string; we restore 
$s0 and 
the stack pointer, and then return.
L2: lw $s0, 0($sp) # y[i] == 0: end of string.
   
# Restore old $s0 addi  $sp,$sp,4 # pop 1 word off stack

 jr $ra # return
String copies usually use pointers instead of arrays in C to avoid the operations 
on 
i in the code above. See Section 2.14 for an explanation of arrays versus 
pointers.
Since the procedure 
strcpy above is a leaf procedure, the compiler could 
allocate 
i to a temporary register and avoid saving and restoring 
$s0. Hence, 
instead of thinking of the 
$t registers as being just for temporaries, we can think of 
them as registers that the callee should use whenever convenient. When a compiler 

 nds a leaf procedure, it exhausts all temporary registers before using registers it 
must save.
Characters and Strings in Java
Unicode
 is a universal encoding of the alphabets of most human languages. 
Figure
 2.16 gives a list of Unicode alphabets; there are almost as many 
alphabets
 in Unicode 
as there are useful 
symbols
 in ASCII. To be more inclusive, Java uses Unicode for 
characters. By default, it uses 16 bits to represent a character.
 2.9 Communicating with People 
109
110 Chapter 2 Instructions: Language of the Computer
LatinMalayalamTagbanwa
General PunctuationGreekSinhalaKhmerSpacing Modi˚er Letters
CyrillicThaiMongolianCurrency Symbols
ArmenianLaoLimbuCombining Diacritical Marks
HebrewTibetanTai LeCombining Marks for Symbols
ArabicMyanmarKangxi RadicalsSuperscripts and Subscripts
SyriacGeorgianHiraganaNumber Forms

ThaanaHangul JamoKatakanaMathematical Operators

DevanagariEthiopicBopomofoMathematical Alphanumeric Symbols

BengaliCherokeeKanbunBraille Patterns

GurmukhiUni˚ed Canadian 
Aboriginal SyllabicShavianOptical Character Recognition
GujaratiOghamOsmanyaByzantine Musical Symbols
OriyaRunicCypriot SyllabaryMusical Symbols

TamilTagalogTai Xuan Jing SymbolsArrows
TeluguHanunooYijing Hexagram SymbolsBox Drawing
KannadaBuhidAegean NumbersGeometric Shapes
FIGURE 2.16 Example alphabets in Unicode. Unicode version 4.0 has more than 160 “blocks,” 
which is their name for a collection of symbols. Each block is a multiple of 16. For example, Greek starts at 
0370hex, and Cyrillic at 0400
hex e 
 rst three columns show 48 blocks that correspond to human languages 
in roughly Unicode numerical order
 e last column has 16 blocks that are multilingual and are not in order. 
A 16-bit encoding, called UTF-16, is the default. A variable-length encoding, called UTF-8, keeps the ASCII 

subset as eight bits and uses 16 or 32 bits for the other characters. UTF-32 uses 32 bits per character. To learn 

more, see 
www.unicode.org.
 e MIPS instruction set has explicit instructions to load and store such 16-
bit quantities, called 
halfwords
. Load half
 (lh) loads a halfword from memory, 
placing it in the rightmost 16 bits of a register. Like load byte, 
load half
 (lh) treats 
the halfword as a signed number and thus sign-extends t
 ll th
 most bits 
of the register, while 
load halfword unsigned
 (lhu) works with unsigned integers. 
 us, lhu is the more popular of the two. 
Store half
 (sh) takes a halfword from the 
rightmost 16 bits of a register and writes it to memory. We copy a halfword with 
the sequence
lhu $t0,0($sp) # Read halfword (16 bits) from sourcesh $t0,0($gp)  # Write halfword (16 bits) to destinationStrings are a standard Java class with special built-in support and pr
 ned 
methods for concatenation, comparison, and conversion. Unlike C, Java includes a 
word that gives the length of the string, similar to Java arrays.

Elaboration: MIPS software tries to keep the stack aligned to word addresses, 
allowing the program to always use 
lw and sw (which must be aligned) to access the stack. This convention means that a 
char variable allocated on the stack occupies 4 bytes, even though it needs less. However, a C string variable or an array of bytes 
will 
pack 4 bytes per word, and a Java string variable or array of shorts packs 2 halfwords 

per word.Elaboration:  ecting the international nature of the web, most web pages today 
use Unicode instead of ASCII. I. Which of the following statements about characters and strings in C and 
Java are true?
1. A string in C takes about half the memory as the same string in Java.

2. Strings are just an informal name for single-dimension arrays of 
characters in C and Java.
3. Strings in C and Java use null (0) to mark the end of a string.

4. Operations on strings, like length, are faster in C than in Java.
II. Which type of variable that can contain 1,000,000,000
ten
 takes the most 
memory space?

1. int in C2. string in C3. string in Java
 2.10  MIPS Addressing for 32-bit Immediates and AddressesAlthough keeping all MIPS instructions 32 bits long simp
 es the hardware, there 
are times where it would be convenient to have a 32-bit constant or 32-bit address. 
 is section starts with the general solution for large constants, and then shows the 
optimizations for instruction addresses used in branches and jumps.
Check Yourself
 2.10 MIPS Addressing for 32-bit Immediates and Addresses 
111
112 Chapter 2 Instructions: Language of the Computer
32-Bit Immediate OperandsAlthough constants are frequently short an
 t into the 16-bi
 eld, sometimes they 
are bigger
 e MIPS instruction set includes the instruction 
load upper immediate
 (lui) sp
 cally to set the upper 16 bits of a constant in a register, allowing a 
subsequent instruction to specify the lower 16 bits of the constant. 
Figure 2.17
 shows the operation of 
lui.Loading a 32-Bit ConstantWhat is the MIPS assembly code to load this 32-bit constant into register 
$s0?0000 0000 0011 1101 0000 1001 0000 0000First, we would load the upper 16 bits, which is 61 in decimal, using 
lui:lui $s0, 61   # 61 decimal = 0000 0000 0011 1101 binary e value of register 
$s0 erward is
0000 0000 0011 1101 0000 0000 0000 0000 e next step is to insert the lower 16 bits, whose decimal value is 2304:
ori $s0, $s0, 2304 # 2304 decimal = 0000 1001 0000 0000 e 
 nal value in register 
$s0 is the desired value:
0000 0000 0011 1101 0000 1001 0000 0000EXAMPLEANSWERFIGURE 2.17 The effect of the lui instruction
 e instruction 
lui transfers the 16-bit immediate constant
 eld value into the 
 most 16 bits of the register
 lling the lower 16 bits with 0s.
The machine language version of  lui $t0, 255
Contents of register
 $t0
 after executing 
lui $t0, 255:
0011110000001000
0000 0000 1111 1111
0000 0000 1111 1111
0000 0000 0000 0000
# $t0 is register 8:
 2.10 MIPS Addressing for 32-bit Immediates and Addresses 
113Either the compiler or the assembler must break large constants into pieces and 
then reassemble them into a register. As you might expect, the immediat
 eld’s 
size restriction may be a problem for memory addresses in loads and stores as 

well as for constants in immediate instructions. If this job falls to the assembler, 

as it does for MIPS so
 ware, then the assembler must have a temporary register 
available in which to create the long val
 is need is a reason for the register 
$at (assembler temporary), which is reserved for the assembler.
Hence, the symbolic representation of the MIPS machine language is no longer 
limited by the hardware, but by whatever the creator of an assembler chooses to 

include (see Section 2.12). We stick close to the hardware to explain the architecture 

of the computer, noting when we use the enhanced language of the assembler that 

is not found in the processor.
Elaboration: Creating 32-bit constants needs care. The instruction 
addi copies the  eld of the instruction into the upper 16 bits of a 
word. Logical or immediate
 from Section 2.6 loads 0s into the upper 16 bits and hence is used by the assembler in conjunction with 
lui to create 32-bit constants.Addressing in Branches and Jumps e MIPS jump instructions have the simplest addressin
 ey use th
 nal MIPS 
instruction format, called the 
J-type
, which consists of 6 bits for the operatio
 eld and the rest of the bits for the addr
 eld. 
 us,j   10000   # go to location 10000could be assembled into this format (it’s actually a bit more complicated, as we will 
see):
2100006 bits26 bitswhere the value of the jump opcode is 2 and the jump address is 
10000.Unlike the jump instruction, the condit
ional branch instruction must specify 
two operands in addition to the branch addr
 us,is assembled into this instruction, leaving only 16 bits for the branch address:
51617Exit6 bits5 bits5 bits16 bitsHardware/

Software 

Interface
114 Chapter 2 Instructions: Language of the Computer
If addresses of the program had t
 t in this 16-bi
 eld, it would mean that no 
program could be bigger than 2
16, which is far too small to be a realistic option 
today. An alternative would be to specify a register that would always be added 
to the branch address, so that a branch instruction would calculate the following:
Program counterRegisterBranch address
 is sum allows the program to be as large as 2
32 and still be able to use 
conditional branches, solving the branch address size prob
 en the question 
is, which register?
 e answer comes from seeing how conditional branches are used. Conditional 
branches are found in loops and in 
if statements, so they tend to branch to a 
nearby instruction. For example, about half of all conditional branches in SPEC 

benchmarks go to locations less than 16 instructions away. Since the 
program 
counter
 (PC) contains the address of the current instruction, we can branch within 
215 words of the current instruction if we use the PC as the register to be added 
to the address. Almost all loops and 
if statements are much smaller than 2
16 words, 
so the PC is the ideal choice.
 is form of branch addressing is called 
PC-relative addressing
. As we shall see 
in Chapter 4, it is convenient for the hardware to increment the PC early to point 
to the next instruction. Hence, the MIPS address is actually relative to the address 

of the following instruction (PC 
 4) as opposed to the current instruction (PC). 
It is yet another example of making the
 common case fast
, which in this case is 
addressing nearby instructions.
Like most recent computers, MIPS uses PC-relative addressing for all conditional 
branches, because the destination of these instructions is likely to be close to the 

branch. On the other hand, jump-and-link instructions invoke procedures that 

have no reason to be near the call, so they normally use other forms of addressing. 

Hence, the MIPS architecture o
 ers long addresses for procedure calls by using the 
J-type format for both jump and jump-and-link instructions.
Since all MIPS instructions are 4 bytes long, MIPS stretches the distance of the 
branch by having PC-relative addressing refer to the number of 
words
 to the next 
instruction instead of the number of byt
 us, the 16-bi
 eld can branch four 
times as far by interpreting th
 eld as a relative word address rather than as a 
relative byte address. Similarly, the 26-bit
 eld in jump instructions is also a word 
address, meaning that it represents a 28-bit byte address.
Elaboration: Since the PC is 32 bits, 4 bits must come from somewhere else for 
jumps. The MIPS jump instruction replaces only the lower 28 bits of the PC, leaving 
the upper 4 bits of the PC unchanged. The loader and linker (Section 2.12) must be 

careful to avoid placing a program across an address boundary of 256 MB (64 million 

instructions); otherwise, a jump must be replaced by a jump register instruction preceded 

by other instructions to load the full 32-bit address into a register.
PC-relative 
addressing
 An addressing regime 
in which the address 

is the sum of the 

program counter
 (PC) 
and a constant in the 

instruction.

Showing Branch Offset in Machine Language e while
 loop on pages 92–93 was compiled into this MIPS assembler code:
Loop:sll $t1,$s3,2        # Temp reg $t1 = 4 * i    add  $t1,$t1,$s6      # $t1 = address of save[i]
    lw   $t0,0($t1)       # Temp reg $t0 = save[i]

    addi $s3,$s3,1        # i = i + 1
    j    Loop             # go to Loop
Exit:If we assume we place the loop starting at location 80000 in memory, what is 
the MIPS machine code for this loop?
 e assembled instructions and their addresses are:
EXAMPLEANSWER800000019920
8000409229032

800083598
08001258212

8001681919
1800202
2000080024. . .
Remember that MIPS instructions have byte addresses, so addresses of 
sequential word
 er by 4, the number of bytes in a word
 e bne instruction 
on the fourth line adds 2 words or 8 bytes to the address of the 
following
 instruction (80016), specifying the branch destination relative to that following 

instruction (8 
 80016) instead of relative to the branch instruction (12 
 80012) or using the full destination addr e jump instruction on 

the last line does use the full address (20000 
 4  80000), corresponding to 
the label 
Loop. 2.10 MIPS Addressing for 32-bit Immediates and Addresses 
115
116 Chapter 2 Instructions: Language of the Computer
Most conditional branches are to a nearby location, but occasionally they branch 
far away, farther than can be represented in the 16 bits of the conditional branch 

instructio
 e assembler comes to the rescue just as it did with large addresses 
or constants: it inserts an unconditional jump to the branch target, and inverts the 

condition so that the branch decides whether to skip the jump.
Branching Far Away
Given a branch on register 
$s0 being equal to register 
$s1,beq    $s0, $s1, L1replace it by a pair of instructions that o
 ers a much greater branching distance.
 ese instructions replace the short-address conditional branch:
      bne    $s0, $s1, L2      j      L1
L2:MIPS Addressing Mode Summary
Multiple forms of addressing are generically called 
addressing modes
. Figure 2.18
 shows how operands are iden
 ed for each addressing mode
 e MIPS addressing 
modes are the following:
1. Immediate addressing,
 where the operand is a constant within the instruction 
itself
2. Register addressing,
 where the operand is a register
3. Base or 
displacement addressing,
 where the operand is at the memory location 
whose address is the sum of a register and a constant in the instruction
4. PC-relative addressing,
 where the branch address is the sum of the PC and a 
constant in the instruction
5. Pseudodirect addressing,
 where the jump address is the 26 bits of the 
instruction concatenated with the upper bits of the PC
Hardware/

Software 

InterfaceEXAMPLEANSWERaddressing mode
 One of several addressing 
regimes delimited by their 

varied use of operands 

and/or addresses.

Although we show MIPS as having 32-bit addresses, nearly all microprocessors 
(including MIPS) have 64-bit address extensions (see 
 Appendix E
 and Section 
2.18 ese extensions were in response to the needs of so
 ware for larger 
program
 e process of instruction set extension allows architectures to expand in 
such a way that is able to move so
 ware compatibly upward to the next generation 
of architecture.
Hardware/

Software 

Interface1.  Immediate addressing
2. Register addressing
3.  Base addressing
4.  PC-relative addressing
5.  Pseudodirect addressing
Immediateoprsrt
oprsrt. . .funct
rdRegisterRegistersoprsrtAddress
Word
Memory
+RegisterHalfword
ByteoprsrtAddress
Word
Memory
+PCopWord
Memory
PCAddressFIGURE 2.18 Illustration of the ﬁ ve MIPS addressing modes.
 e operands are shaded in color. 
 e operand of mode 3 is in memory, whereas the operand for mode 2 is a register. Note that versions of 
load and store access bytes, halfwords, or words. For mode 1, the operand is 16 bits of the instruction itself. 
Modes 4 and 5 address instructions in memory, with mode 4 adding a 16-bit addr
 ed 
  2 bits to the 
PC and mode 5 concatenating a 26-bit addr
 ed 
  2 bits with the 4 upper bits of the PC. Note that a 
single operation can use more than one addressing mode. Add, for example, uses both immediate (
addi) and register (
add) addressing.
 2.10 MIPS Addressing for 32-bit Immediates and Addresses 
117
118 Chapter 2 Instructions: Language of the Computer
Decoding Machine LanguageSometimes you are forced to reverse-engineer machine language to create the 
original assembly language. One example is when looking at “core dump.” 
Figure
 
2.19 shows the MIPS encoding of th
 elds for the MIPS machine language
 is  gure helps when translating by hand between assembly language and machine 
language.
Decoding Machine CodeWhat is the assembly language stat
ement corresponding to this machine 
instruction?
00af8020hex e 
 rst step in converting hexadecimal to binary is t
 nd the op
 elds:(Bits: 31 28 26                        5   2 0)       0000 0000 1010 1111 1000 0000 0010 0000We look at the o eld to determine the operation. Referring to 
Figure 2.19
, when bits 31–29 are 000 and bits 28–26 are 000, it is an R-format instruction. 

Let’s reformat the binary instruction into R-forma
 elds, listed in 
Figure 2.20
:op        rs       rt       rd       shamt    funct000000    00101    01111    10000    00000    100000 e bottom portion of 
Figure 2.19
 determines the operation of an R-format 
instruction. In this case, bits 5–3 are 100 and bits 2–0 are 000, which means 
this binary pattern represents an 
add instruction.
We decode the rest of the instruction by looking at th
 eld val
 e decimal values are 5 for the rs
 eld, 15 for rt, and 16 for rd (shamt is unused). 
Figure 2.14 
shows that these numbers represent registers 
$a1, $t7, and 
$s0. Now we can reveal the assembly instruction:
add $s0,$a1,$t7EXAMPLEANSWER
op(31:26)28Œ2631Œ290(000)R-formatBltz/gez
jumpjump&linkbrancheqbranch
neblezbgtz
1(001)addimmediateaddiusetlessthanimm.
setless

thanimm. 

unsignedandiorixoriloadupper

immediate2(010)TLBFlPt3(011)4(100)loadbyteloadhalf
lwlloadwordloadbyte 
unsignedloadhalfunsignedlwr5(101)storebytestorehalf
swlstoreword
swr6(110)load linked 
wordlwc17(111)store cond. 
wordswc1op(31:26)=010000 (TLB), rs(25:21)23Œ2125Œ240(00)mfc0cfc0mtc0ctc01(01)2(10)3(11)op(31:26)=000000 (R-format), funct(5:0)
2Œ05Œ30(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(000)shiftleft
logicalshiftrightlogicalsrasllvsrlvsrav
1(001)jumpregisterjalr
syscallbreak
2(010)mfhimthim˜ o
mtlo3(011)multmultudivdivu
4(100)addaddusubtractsubuandor
xornotor(nor)
5(101)setl.t.setl.t. 
unsigned6(110)7(111)FIGURE 2.19 MIPS instruction encoding.
 is notation gives the value of
 eld by row and by column. For example, the top portion 
of th
 gure shows 
load word
 in row number 4 (100
two
 for bits 31–29 of the instruction) and column number 3 (011
two
 for bits 28–26 of the 
instruction), so the corresponding value of the o
 eld (bits 31–26) is 100011
two
. Underscore means th
 eld is used elsewhere. For example, 
R-format in row 0 and column 0 (op 
 000000two
 ned in the bottom part of th
 gure. Hence, 
subtract in row 4 and column 
2 of the bottom section means that the func
 eld (bits 5–0) of the instruction is 100010
two
 and the o
 eld (bits 31–26) is 000000
two
 e floating point value in row 2, col
 ned in Figure 3.18 in Chapter 3. 
Bltz/gez is the opcode for four instructions found 
in Appendix A: 
bltz, bgez, bltzal, and 
bgezal is chapter describes instructions given in full name using color, while Chapter 3 
describes instructions given in mnemonics using color. Appendix A covers all instructions.
 2.10 MIPS Addressing for 32-bit Immediates and Addresses 
119
120 Chapter 2 Instructions: Language of the Computer
Figure 2.20
 shows all the MIPS instruction formats. 
Figure 2.1
 on page 64 shows 
the MIPS assembly language revealed in this chapter
 e remaining hidden portion 
of MIPS instructions deals mainly with arithmetic and real numbers, which are 
covered in the next chapter.
   I.  What is the range of addresses for conditional branches in MIPS (K 
 1024)?1. Addresses between 0 and 64K 
 12. Addresses between 0 and 256K 
 13. Addresses up to about 32K before the branch to about 32K a
 er4. Addresses up to about 128K before the branch to about 128K a
 er II. What is the range of addresses for jump and jump and link in MIPS 
(M  1024K)?1. Addresses between 0 and 64M 
 12. Addresses between 0 and 256M 
 13. Addresses up to about 32M before the branch to about 32M a
 er4. Addresses up to about 128M before the branch to about 128M a
 er5. Anywhere within a block of 64M addresses where the PC supplies the 
upper 6 bits
6. Anywhere within a block of 256M addresses where the PC supplies the 
upper 4 bits
III. What is the MIPS assembly language instruction corresponding to the 
machine instruction with the value 0000 0000
hex?1. j2. R-format3. addi4. sll5. mfc06. Un
 ned opcode: there is no legal instruction that corresponds to 0
Check Yourself
NameFieldsCommentsField size6 bits5 bits5 bits5 bits5 bits6 bitsAll MIPS instructions are 32 bits long
R-formatoprsrtrd
shamtfunctArithmetic instruction format
I-format oprsrt
address/immediate
Transfer, branch,
imm. format 
Jump instruction formatsserddategrat
potamrof-J
FIGURE 2.20 MIPS instruction formats.

 2.11 Parallelism and Instructions: Synchronization 
121 2.11  Parallelism and Instructions: 
SynchronizationParallel execution
 is easier when tasks are independent, but o
 en they need to 
cooperate. Cooperation usually means some tasks are writing new values that 
others must read. To know when a ta
 nished writing so that it is safe for 
another to read, the tasks need to synchronize. If they don’t synchronize, there is a 

danger of a 
data race
, where the results of the program can change depending on 
how events happen to occur.
For example, recall the analogy of the eight reporters writing a story on page 44 of 
Chapter 1. Suppose one reporter needs to read all the prior sections before writing 

a conclusion. Hence, he or she must know when the other reporters have
 nished their sections, so that there is no danger of sections being changed a
 erwards. 
 at 
is, they had better synchronize the writing and reading of each section so that the 

conclusion will be consistent with what is printed in the prior sections.
In computing, synchronization mechanisms are typically built with user-level 
so
 ware routines that rely on hardware-supplied synchronization instructions. In 
this section, we focus on the implementation of 
lock
 and 
unlock
 synchronization 
operations. Lock and unlock can be used straightforwardly to create regions 

where only a single processor can operate, called a 
mutual exclusion
, as well as to 
implement more complex synchronization mechanisms.
 e critical ability we require to implement synchronization in a multiprocessor 
is a set of hardware primitives with the ability to 
atomically
 read and modify a 
memory locatio
 at is, nothing else can interpose itself between the read and 
the write of the memory location. Without such a capability, the cost of building 

basic synchronization primitives will be high and will increase unreasonably as the 

processor count increases.
 ere are a number of alternative formulations of the basic hardware primitives, 
all of which provide the ability to atomically read and modify a location, together 

with some way to tell if the read and write were performed atomically. In general, 

architects do not expect users to employ the basic hardware primitives, but 

instead expect that the primitives will be used by system programmers to build a 

synchronization library, a process that is o
 en complex and tricky.
Let’s start with one such hardware primitive and show how it can be used to 
build a basic synchronization primitive. One typical operation for building 

synchronization operations is the 
atomic exchange
 or 
atomic swap
, which inter-
changes a value in a register for a value in memory.
To see how to use this to build a basic synchronization primitive, assume that 
we want to build a simple lock where the value 0 is used to indicate that the lock 

is free and 1 is used to indicate that the lock is unavailable. A processor tries to set 

the lock by doing an exchange of 1, which is in a register, with the memory address 

corresponding to the loc
 e value returned from the exchange instruction is 1 
if some other processor had already claimed access, and 0 otherwise. In the latter 
data race
 Two memory 
accesses form a data race 
if they are fro
 erent 
threads to same location, 

at least one is a write, 

and they occur one a
 er another.

122 Chapter 2 Instructions: Language of the Computer
case, the value is also changed to 1, preventing any competing exchange in another 
processor from also retrieving a 0.
For example, consider two processors that each try to do the exchange 
simultaneously: this race is broken, since exactly one of the processors will perform 

the exchang
 rst, returning 0, and the second processor will return 1 when it does 
the exchange e key to using the exchange primitive to implement synchronization 

is that the operation is atomic: the exchange is indivisible, and two simultaneous 

exchanges will be ordered by the hardware. It is impossible for two processors 

trying to set the synchronization variable in this manner to both think they have 

simultaneously set the variable.
Implementing a single atomic memory operation introduces some challenges in 
the design of the processor, since it requires both a memory read and a write in a 

single, uninterruptible instruction.
An alternative is to have a pair of instructions in which the second instruction 
returns a value showing whether the pair of instructions was executed as if the pair 

were ato
 e pair of instruction
 ectively atomic if it appears as if all other 
operations executed by any processor occurred before or a er the pair
 us, when 
an instruction pa
 ectively atomic, no other processor can change the value 
between the instruction pair.
In MIPS this pair of instructions includes a special load called a 
load linked
 and 
a special store called a 
store conditional
 ese instructions are used in sequence: 
if the contents of the memory location sp
 ed by the load linked are changed 
before the store conditional to the same address occurs, then the store conditional 

fa
 e store conditio ned to both store the value of a (presumably 
 erent) register in memory 
and
 to change the value of that register to a 1 if it 
succeeds and to a 0 if it fails. Since the load linked returns the initial value, and the 

store conditional returns 1 only if it succeeds, the following sequence implements 

an atomic exchange on the memory location sp
 ed by the contents of 
$s1:again: addi $t0,$zero,1       ;copy locked value   ll       $t1,0($s1)        ;load linked
   sc       $t0,0($s1)        ;store conditional
   beq      $t0,$zero,again   ;branch if store fails
   add      $s4,$zero,$t1     ;put load value in $s4Any time a processor intervenes and mo
 es the value in memory between the 
ll and 
sc instructions, the 
sc returns 0 in 
$t0, causing the code sequence to try 
again. At the end of this sequence the contents of 
$s4 and the memory location 
sp
 ed by 
$s1 have been atomically exchanged.
Elaboration: Although it was presented for multiprocessor synchronization, atomic 
exchange is also useful for the operating system in dealing with multiple processes in a single processor. To make sure nothing interferes in a single processor, the store 

conditional also fails if the processor does a context switch between the two instructions 

(see Chapter 5).
 2.12 Translating and Starting a Program 
123An advantage of the load linked/store conditional mechanism is that it can be used 
to build other synchronization primitives, such as 
atomic compare and swap
 or atomic fetch-and-increment
, which are used in some parallel programming models. These 

involve more instructions between the 
ll and the sc, but not too many.
Since the store conditional will fail after either another attempted store to the load linked address or any exception, care must be taken in choosing which instructions are 

inserted between the two instructions. In particular, only register-register instructions 

can safely be permitted; otherwise, it is possible to create deadlock situations where 

the processor can never complete the 
sc because of repeated page faults. In addition, 
the number of instructions between the load linked and the store conditional should be 

small to minimize the probability that either an unrelated event or a competing processor 

causes the store conditional to fail frequently.
When do you use primitives like load linked and store conditional?
1. When cooperating threads of a parallel program need to synchronize to get 
proper behavior for reading and writing shared data
2. When cooperating processes on a uniprocessor need to synchronize for 
reading and writing shared data
 2.12 Translating and Starting a Program
 is section describes the four steps in transforming a C progra
 le on disk 
into a program running on a computer. 
Figure 2.21
 shows the translation hierarchy. 
Some systems combine these steps to reduce translation time, but these are the 

logical four phases that programs go throug
 is section follows this translation 
hierarchy.
Compiler e compiler transforms the C program into an 
assembly language program
, a symbolic form of what the machine understands. High-level language programs 

take many fewer lines of code than assembly language, so programmer productivity 

is much higher.
In 1975, many operating systems and assemblers were written in 
assembly 
language
 because memories were small and compilers wer
  cient. 
 e million-fold increase in memory capacity per single DRAM chip has reduced 

program size concerns, and optimizing compilers today can produce assembly 

language programs nearly as well as an assembly language expert, and sometimes 

even better for large programs.
Check Yourself
assembly language
 A symbolic language that 

can be translated into 

binary machine language.

124 Chapter 2 Instructions: Language of the Computer
AssemblerSince assembly language is an interface to higher-level so
 ware, the assembler 
can also treat common variations of machine language instructions as if they 
were instructions in their own righ
 e hardware need not implement these 
instructions; however, their appearance in assembly language simp
 es translation 
and programming. Such instructions are called 
pseudoinstructions
.As mentioned above, the MIPS hardware makes sure that register 
$zero always 
has the val
 at is, whenever register 
$zero is used, it supplies a 0, and the 
programmer cannot change the value of register 
$zero. Register 
$zero is used 
to create the assembly language instruction that copies the contents of one register 

to another.
 us the MIPS assembler accepts this instruction even though it is not 
found in the MIPS architecture:
move $t0,$t1      # register $t0 gets register $t1pseudoinstruction
 A common variation 
of assembly language 

instructions o
 en treated 
as if it were an instruction 

in its own right.
LoaderC program
CompilerAssembly language program
Assembler
Object: Machine language moduleObject: Library routine (machine language)
Linker
Memory
Executable: Machine language program
FIGURE 2.21 A translation hierarchy for C.
 A high-level language progra
 rst compiled into 
an assembly language program and then assemble
d into an object module in machine language
 e linker 
combines multiple modules with library routines to resolve all refer
 e loader then places the machine 
code into the proper memory locations for execution by the processor. To speed up the translation process, 
some steps are skipped or combined. Some compilers produce object modules directly, and some systems use 

linking loaders that perform the last two steps. To identify the type of
 le, UNIX follo
  x convention 
fo les: C sour
 les are named 
x.c, assembly
 les are 
x.s, objec
 les are named 
x.o, statically linked 
library routines are 
x.a, dynamically linked library routes are 
x.so, and executab
 les by default are 
called a.out. MS-DOS uses th
  xes 
.C, .ASM, .OBJ, .LIB, .DLL, and 
.EXE to the sa
 ect.
 e assembler converts this assembly language instruction into the machine 
language equivalent of the following instruction:
add $t0,$zero,$t1 # register $t0 gets 0 + register $t1 e MIPS assembler also converts 
blt (branch on less than) into the two 
instructions 
slt and 
bne mentioned in the example on page 95. Other examples 
include 
bgt, bge, and 
ble. It also converts branches to faraway locations into a 
branch and jump. As mentioned above, the MIPS assembler allows 32-bit constants 
to be loaded into a register despite the 16-bit limit of the immediate instructions.
In summary, pseudoinstructions give MIPS a richer set of assembly language 
instructions than those implemented by the hardware
 e only cost is reserving 
one register, 
$at, for use by the assembler. If you are going to write assembly 

programs, use pseudoinstructions to simplify your task. To understand the MIPS 

architecture and be sure to get best performance, however, study the real MIPS 

instructions found in 
Figures 2.1 and 2.19
.Assemblers will also accept numbers in a variety of bases. In addition to binary 
and decimal, they usually accept a base that is more succinct than binary yet 

converts easily to a bit pattern. MIPS assemblers use hexadecimal.
Such features are convenient, but the primary task of an assembler is assembly 
into machine code
 e assembler turns the assembly language program into an 
obje
 le, which is a combination of machine language instructions, data, and 
information needed to place instructions properly in memory.
To produce the binary version of each instruction in the assembly language 
program, the assembler must determine the 
addresses corresponding to all labels. 
Assemblers keep track of labels used in branches and data transfer instructions 

in a symbol table
. As you might expect, the table contains pairs of symbols and 
addresses.
 e objec
 le for UNIX systems typically contains six distinct pieces:
 e obje
 le header
 describes the size and position of the other pieces of the 
ob
 le.
 e text segment
 contains the machine language code.
 e static data segment
 contains data allocated for the life of the program. 
(UNIX allows programs to use both 
static data,
 which is allocated throughout 
the program, and 
dynamic data
, which can grow or shrink as needed by the 
program. See 
Figure 2.13
.) e relocation information
 iden
 es instructions and data words that depend 
on absolute addresses when the program is loaded into memory.
 e symbol table
 contains the remaining labels that are no
 ned, such as 
external references.
symbol table
 A table 
that matches names of 
labels to the addresses of 

the memory words that 

instructions occupy.
 2.12 Translating and Starting a Program 
125
126 Chapter 2 Instructions: Language of the Computer
 e debugging information
 contains a concise description of how the modules 
were compiled so that a debugger can associate machine instructions with C 
sour
 les and make data structures readable.
 e next subsection shows how to attach such routines that have already been 
assembled, such as library routines.
LinkerWhat we have presented so far suggests that a single change to one line of one 

procedure requires compiling and assembling the whole program. Complete 

retranslation is a terrible waste of computing resour
 is repetition is 
particularly wasteful for standard library routines, because programmers would 

be compiling and assembling routines that b
 nition almost never change. An 
alternative is to compile and assemble each procedure independently, so that a 

change to one line would require compiling and assembling only one procedure. 

 is alternative requires a new systems program, called a 
link editor
 or 
linker
, which takes all the independently assemb
led machine language programs and 
“stitches” them together.
 ere are three steps for the linker:
1. Place code and data modules symbolically in memory.
2. Determine the addresses of data and instruction labels.

3. Patch both the internal and external references.

 e linker uses the relocation information and symbol table in each object 
module to resolv
 ned labels. Such references occur in branch instructions, 
jump instructions, and data addresses, so the job of this program is much like that 
of an editor: it
 nds the old addresses and replaces them with the new addresses. 
Editing is the origin of the name “link editor,” or linker for shor
 e reason a 
linker is useful is that it is much faster to patch code than it is to recompile and 

reassemble.
If all external references are resolved, the linker next determines the memory 
locations each module will occupy. Recall that 
Figure 2.13
 on page 104 shows 

the MIPS convention for allocation of program and data to memory. Since the 

 les were assembled in isolation, the assembler could not know where a module’s 
instructions and data would be placed relative to other modules. When the linker 

places a module in memory, all 
absolute
 references, that is, memory addresses that 
are not relative to a register, must be 
relocated
 to r
 ect its true location.
 e linker produces an 
executable
 le that can be run on a computer. Typically, 
this
 le has the same format as an ob
 le, except that it contains no unresolved 
references. It is possible to have partially linke
 les, such as library routines, that 
still have unresolved addresses and hence result in ob
 les.linker
 Also called 
link editor
. A systems 
program that combines 
independently assembled 

machine language 

programs and resolves all 

 ned labels into an 
executab
 le.
executable
 le A functional program in 

the format of an object 

 le that contains no 
unresolved references. 

It can contain symbol 

tables and debugging 

information. A “stripped 

executable” does not 

contain that information. 

Relocation information 

may be included for the 

loader.

Linking Object Files
Link the two objec
 les below. Show updated addresses of th
 rst few 
instructions of the completed executab
 le. We show the instructions in 
assembly language just to make the example understandable; in reality, the 
instructions would be numbers.
Note that in the objec
 les we have highlighted the addresses and symbols 
that must be updated in the link process: the instructions that refer to the 

addresses of procedures 
A and 
B and the instructions that refer to the addresses 
of data words 
X and 
Y.EXAMPLE le header
NameProcedure AText size
100hexData size20hexText segment
AddressInstruction
0lw
 $a0, 0($gp)4jal 0……Data segment0(X)……Relocation informationAddressInstruction typeDependency
 0lwX4jal BSymbol tableLabelAddressX–B– le headerNameProcedure BText size
200hexData size30hexText segment
AddressInstruction
0sw $a1, 0($gp)4jal 0……Data segment0(Y)……Relocation informationAddressInstruction typeDependency
 0swY4jal ASymbol tableLabelAddressY–A– 2.12 Translating and Starting a Program 
127
128 Chapter 2 Instructions: Language of the Computer
Procedure 
A needs to
 nd the address for the variable labeled 
X to put in the 
load instruction and t
 nd the address of procedure 
B to place in the 
jal instruction. Procedure 
B needs the address of the variable labeled 
Y for the 
store instruction and the address of procedure 
A for its 
jal instruction.
From 
Figure 2.13
 on page 104, we know that the text segment starts 
at address 
40 0000hex and the data segment at 
1000 0000hex e text of 
procedure 
A is placed at th
 rst address and its data at the second
 e object 
 le header for procedure 
A says that its text is 100
hex bytes and its data is 20
hex bytes, so the starting address for procedure 
B text is 
40 0100hex, and its data 
starts at 
1000 0020hex.ANSWER le header
Text size
300hexData size50hexText segment
AddressInstruction
0040 0000hexlw $a0, 8000hex($gp)0040 0004hexjal 40 0100hex……0040 0100hexsw $a1, 8020hex($gp)0040 0104hexjal 40 0000hex……Data segmentAddress1000 0000hex(X)……1000 0020hex(Y)……Figure 2.13
 also shows that the text segment starts at address 
40 0000hex and the data segment at 
1000 0000hex e text of procedure 
A is placed at the 
 rst address and its data at the second
 e objec
 le header for procedure 
A says that its text is 100
hex bytes and its data is 20
hex bytes, so the starting address 
for procedure 
B text is 
40 0100hex, and its data starts at 
1000 0020hex.Now the linker updates the addr
 elds of the instructions. It uses the 
instruction typ
 eld to know the format of the address to be edited. We have 
two types here:
 e jals are easy because they use pseudodirect addressin
 e jal at 
address 
40 0004hex gets 
40 0100hex (the address of procedure 
B) in its 
addr eld, and the 
jal at 
40 0104hex gets 
40 0000hex (the address of 
procedure 
A) in its addr
 eld. e load and store addresses are harder because they are relative to a base 
register
 is example uses the global pointer as the base register. 
Figure 2.13
 
shows that $
gp is initialized to 
1000 8000hex. To get the address 
1000 0000hex 
(the address of word 
X), we place 
8000hex in the addr
 eld of 
lw at address 
40 0000hex. Similarly, we place 
8020hex in the addr
 eld of 
sw at address 
40 0100hex to get the address 
1000 0020hex (the address of word 
Y).
Elaboration: Recall that MIPS instructions are word aligned, so 
jal drops the right two bits to increase the instruction’s address range. Thus, it uses 26 bits to create a 
28-bit byte address. Hence, the actual address in the lower 26 bits of the 
jal instruction 
in this example is 10 0040hex, rather than 40 0100hex.LoaderNow that the executab
 le is on disk, the operating system reads it to memory and 
starts i
 e loader
 follows these steps in UNIX systems:
1. Reads the executab
 le header to determine size of the text and data 
segments.
2. Creates an address space large enough for the text and data.
3. Copies the instructions and data from the executab
 le into memory.
4. Copies the parameters (if any) to the main program onto the stack.

5. Initializes the machine registers and sets the stack pointer to th
 rst free 
location.
6. Jumps to a start-up routine that copies the parameters into the argument 
registers and calls the main routine of the program. When the main routine 
returns, the start-up routine terminates the program with an 
exit system 
call.
Sections A.3 and A.4 in Appendix A describe linkers and loaders in more detail.
Dynamically Linked Libraries e 
 rst part of this section describes the traditional approach to linking libraries 
before the program is run. Although this static approach is the fastest way to call 

library routines, it has a few disadvantages:
 e library routines become part of the executable code. If a new version of 
the library is released that
 xes bugs or supports new hardware devices, the 
statically linked program keeps using the old version.
 It loads all routines in the library that are called anywhere in the executable, 
even if those calls are not executed
 e library can be large relative to the 
program; for example, the standard C library is 2.5 MB.
 ese disadvantages lead to 
dynamically linked libraries (DLLs)
, where the 
library routines are not linked and loaded until the program is run. Both the 
program and library routines keep extra information on the location of nonlocal 

procedures and their names. In the initial version of DLLs, the loader ran a dynamic 

linker, using the extra information in th
 le t
 nd the appropriate libraries and to 
update all external references.
loader
 A systems 
program that places an 
object program in main 

memory so that it is ready 

to execute.
dynamically linked 
libraries (DLLs)
 Library 
routines that are linked 
to a program during 

execution.
 2.12 Translating and Starting a Program 
129Virtually every 
problem in computer 

science can be solved 

by another level of 

indirection.
David Wheeler

130 Chapter 2 Instructions: Language of the Computer
 e downside of the initial version of DLLs was that it still linked all routines 
of the library that might be called, versus only those that are called during the 
running of the progra is observation led to the lazy procedure linkage version 

of DLLs, where each routine is linked only 
 er it is called.
Like many innovations in o
 eld, this trick relies on a level of indirection. 
Figure 2.22
 shows the technique. It starts with the nonlocal routines calling a set of 

dummy routines at the end of the program, with one entry per nonlocal routine. 

 ese dummy entries each contain an indirect jump.
 e 
 rst time the library routine is called, the program calls the dummy entry 
and follows the indirect jump. It points to code that puts a number in a register to 
Text
jal(a) First call to DLL routine(b) Subsequent calls to DLL routine
lwjr......DataText
li    IDj......Text
Data/Text
Dynamic linker/loader
Remap DLL routinej...DLL routinejr...Text
jallwjr......DataDLL routinejr...Text
FIGURE 2.22 Dynamically linked library via lazy procedure linkage.
 (a) Steps for th
 rst time 
a call is made to the DLL routine
 e steps to
 nd the routine, remap it, and link it are skipped on 
subsequent calls. As we will see in Chapter 5, the opera
ting system may avoid copying the desired routine by 
remapping it using virtual memory management.

identify the desired library routine and then jumps to the dynamic linker/loader. 
 e link
 nds the desired routine, remaps it, and changes the address in 
the indirect jump location to point to that routine. It then jumps to it. When the 

routine completes, it returns to the original calling site.
 erea
 er, the call to the 
library routine jumps indirectly to the routine without the extra hops.
In summary, DLLs require extra space for the information needed for dynamic 
linking, but do not require that whole libraries be copied or linked
 ey pay a good 
deal of overhead th
 rst time a routine is called, but only a single indirect jump 
therea
 er. Note that the return from the library pays no extra overhead. Microso
 ’s 
Windows relies extensively on dynamically linked libraries, and it is also the default 

when executing programs on UNIX systems today.
Starting a Java Program
 e discussion above captures the traditional model of executing a program, 
where the emphasis is on fast execution time for a program targeted to a sp
 c instruction set architecture, or even a sp
 c implementation of that architecture. 
Indeed, it is possible to execute Java programs just like C. Java was invented with 

 erent set of goals, however. One was to run safely on any computer, even if it 
might slow execution time.
Figure 2.23
 shows the typical translation and execution steps for Java. Rather 
than compile to the assembly language of a target computer, Java is comp
 rst 
to instructions that are easy to interpret: the 
Java bytecode
 instruction set (see 
 Section 2.15
 is instruction set is designed to be close to the Java language 
so that this compilation step is trivial. Virtually no optimizations are performed. 

Like the C compiler, the Java compiler checks the types of data and produces the 

proper operation for each type. Java programs are distributed in the binary version 

of these bytecodes.
A so
 ware interpreter, called a 
Java Virtual Machine
 (JVM)
, can execute Java 
bytecodes. An interpreter is a program that simulates an instruction set architecture. 
Java bytecode
 Instruction from an 
instruction set designed 

to interpret Java 

programs.
Java Virtual Machine 
(JVM)
 e program that 
interprets Java bytecodes.
Java program
CompilerClass files (Java bytecodes)
Java Virtual Machine
Compiled Java methods (machine language)
Java library routines (machine language)
Just In Time
compilerFIGURE 2.23 A translation hierarchy for Java.
 A Java progra
 rst compiled into a binary 
version of Java bytecodes, with all address ned by the compiler
 e Java program is now ready to run 
on the interpreter, called the 
Java Virtual Machine
 (
 e JVM links to desired methods in the Java 
library while the program is running. To achieve greater performance, the JVM can invoke the JIT compiler, 
which selectively compiles methods into the native machine language of the machine on which it is running.
 2.12 Translating and Starting a Program 
131
132 Chapter 2 Instructions: Language of the Computer
For example, the MIPS simulator used with this book is an interpreter.
 ere is no 
need for a separate assembly step since either the translation is so simple that the 
comp
 lls in the addresses o
 nds them at runtime.
 e upside of interpretation is portability
 e availability of so
 ware Java virtual 
machines meant that most people could write and run Java programs shortly 

 er Java was announced. Today, Java virtual machines are found in hundreds of 
millions of devices, in everything from cell phones to Internet browsers.
 e downside of interpretation is lower performance
 e incredible advances in 
performance of the 1980s and 1990s made interpretation viable for many important 

applications, but the factor of 10 slowdown when compared to traditionally 

compiled C programs made Java unattractive for some applications.
To preserve portability and improve execution speed, the next phase of Java 
development was compilers that translated 
while
 the program was running. Such 
Just In Time compilers (JIT)
 typically
 le the running program t
 nd where 
the “hot” methods are and then compile them into the native instruction set on 

which the virtual machine is runnin e compiled portion is saved for the next 

time the program is run, so that it can run faster each time it is r
 is balance 
of interpretation and compilation evolves over time, so that frequently run Java 

program
 er little of the overhead of interpretation.
As computers get faster so that compilers can do more, and as researchers 
invent betters ways to compile Java on th
 y, the performance gap between Java 
and C or C
 is closing. 
 Section 2.15
 goes into much greater depth on the 
implementation of Java, Java bytecodes, JVM, and JIT compilers.
Which of the advantages of an interpreter over a translator do you think was most 

important for the designers of Java?
1. Ease of writing an interpreter
2. Better error messages

3. Smaller object code

4. Machine independence
 2.13 A C Sort Example to Put It All Together
One danger of showing assembly language code in snippets is that you will have no 
idea what a full assembly language program looks like. In this section, we derive 

the MIPS code from two procedures written in C: one to swap array elements and 

one to sort them.
Just In Time compiler 
(JIT)
 e name 
commonly given to a 
compiler that operates at 

runtime, translating the 

interpreted code segments 

into the native code of the 

computer.
Check Yourself

 2.13 A C Sort Example to Put It All Together 
133The Procedure swapLet’s start with the code for the procedure 
swap in Figure 2.24
 is procedure 
simply swaps two locations in memory. When translating from C to assembly 
language by hand, we follow these general steps:
1. Allocate registers to program variables.
2. Produce code for the body of the procedure.

3. Preserve registers across the procedure invocation.

 is section describes the 
swap procedure in these three pieces, concluding by 
putting all the pieces together.
Register Allocation for swapAs mentioned on pages 98–99, the MIPS convention on parameter passing is to 
use registers 
$a0, $a1, $a2, and 
$a3. Since swap has just two parameters, 
v and 
k, they will be found in registers 
$a0 and 
$a1 e only other variable is 
temp, which we associate with register 
$t0 since swap is a leaf procedure (see page 100). 
 is register allocation corresponds to the variable declarations in th
 rst part of 
the swap procedure in 
Figure 2.24
.Code for the Body of the Procedure swap e remaining lines of C code in swap are
temp = v[k];v[k] = v[k+1];
v[k+1] = temp;Recall that the memory address for MIPS refers to the 
byte
 address, and so 
words are really 4 bytes apart. Hence we need to multiply the index 
k by 4 before 
adding it to the address. 
Forgetting that sequential word addresses
 er by 4 instead 
void swap(int v[], int k) {  int temp;  temp = v[k]; 
 v[k] = v[k+1]; 
 v[k+1] = temp; }FIGURE 2.24 A C procedure that swaps two locations in memory.
 is subsection uses this 
procedure in a sorting example.

134 Chapter 2 Instructions: Language of the Computer
of by 1 is a common mistake in assembly language programming
. Hence th
 rst step 
is to get the address of 
v[k] by multiplying 
k b
  
  by 2:
sll   $t1, $a1,2     # reg $t1 = k * 4add   $t1, $a0,$t1   # reg $t1 = v + (k * 4)
                     # reg $t1 has the address of v[k]Now we load 
v[k] using 
$t1, and then 
v[k+1] by adding 4 to 
$t1:lw    $t0, 0($t1)    # reg $t0 (temp) = v[k]
lw    $t2, 4($t1)    # reg $t2 = v[k + 1]
                     # refers to next element of vNext we store 
$t0 and 
$t2 to the swapped addresses:
sw    $t2, 0($t1)    # v[k] = reg $t2
sw    $t0, 4($t1)    # v[k+1] = reg $t0 (temp)Now we have allocated registers and written the code to perform the operations 
of the procedure. What is missing is the code for preserving the saved registers 
used within 
swap. Since we are not using saved registers in this leaf procedure, 
there is nothing to preserve.
The Full swap ProcedureWe are now ready for the whole routine, which includes the procedure label and 

the return jump. To make it easier to follow, we identify in 
Figure 2.25
 each block 

of code with its purpose in the procedure.
Procedure body
swap: sll $t1,$a1,2  #reg$t
1=k*4 add $t1,$a0,$t1    #reg$t1
=v+(k*4)
 #reg$t1hastheaddressofv[k]
 lw $t0,0($t1)  #reg$t0(temp)=v[k]

 lw $t2,4($t1)  #reg$t2=v[k+1]
 #referstonextelementofv
 sw $t2,0($t1)  #v[k]=reg$t2
 sw $t0,4($t1)  #v[k+1]=reg$t0(temp)
Procedure return
 jr $ra  #returntocallingroutine
FIGURE 2.25 MIPS assembly code of the procedure swap in Figure 2.24
.
The Procedure sortTo ensure that you appreciate the rigor of programming in assembly language, we’ll 
try a second, longer example. In this case, we’ll build a routine that calls the swap 

procedure.
 is program sorts an array of integers, using bubble or exchange sort, 
which is one of the simplest if not the fastest sorts. 
Figure 2.26
 shows the C version 

of the program. Once again, we present this procedure in several steps, concluding 

with the full procedure.
void sort (int v[], int n){
 int i, j;

 for (i = 0; i < n; i += 1) {

  for (j = i Œ 1; j >= 0 && v[j] > v[j + 1]; j =1) {

  swap(v,j);

  }

 }
}FIGURE 2.26 A C procedure that performs a sort on the array 
v.Register Allocation for sort e two parameters of the procedure 
sort, v and 
n, are in the parameter registers 
$a0 and 
$a1, and we assign register 
$s0 to 
i and register 
$s1 to 
j.Code for the Body of the Procedure sort e procedure body consists of two nested 
for loops and a call to 
swap that includes 
parameters. Let’s unwrap the code from the outside to the middle.
 e 
 rst translation step is th
 rst 
for loop:
for (i = 0; i <n; i += 1) {Recall that the C 
for statement has three parts: initialization, loop test, and iteration 
increment. It takes just one instruction to initialize 
i to 0, th
 rst part of the 
for statement:
move    $s0, $zero    # i = 0(Remember that 
move is a pseudoinstruction provided by the assembler for the 
convenience of the assembly language progra
mmer; see page 124.) It also takes just 
one instruction to increment 
i, the last part of the 
for statement:
addi    $s0, $s0, 1     # i += 1 2.13 A C Sort Example to Put It All Together 
135
136 Chapter 2 Instructions: Language of the Computer
 e loop should be exited if 
i < n is not true or, said another way, should be 
exited if 
 e set on less than instruction sets register 
$t0 to 1 if 
$s0 < $a1 and to 0 otherwise. Since we want to test if 
, we branch if register 
$t0 is test takes two instructions:
for1tst:slt  $t0, $s0, $a1      
        beq 
 e bottom of the loop just jumps back to the loop test:
        j  for1tst      # jump to test of outer loop
exit1: e skeleton code of th
 rst 
for loop is then
       move $s0, $zero       # i = 0


            . . .
            (body of first for loop)
            . . .
       addi $s0, $s0, 1      # i += 1
       j    for1tst          # jump to test of outer loop
exit1:Vo
 e exercises explore writing faster code for similar loops.)
 e second 
for loop looks like this in C:
for (j = i – 1; j >= 0 && v[j] > v[j + 1]; j –= 1) { e initialization portion of this loop is again one instruction:
addi     $s1, $s0, –1 # j = i – 1 e decrement of 
j at the end of the loop is also one instruction:
addi     $s1, $s1, –1 # j –= 1 e loop test has two parts. We exit the loop if either condition fails, so th
 rst 
test must exit the loop if it fails (
j  0):for2tst: slti $t0, $s1, 0      # reg $t0 = 1 if $s1 < 0 (j < 0)

         bne  $t0, $zero, exit2 # go to exit2 if $s1 < 0 (j < 0)
 is branch will skip over the second condition test. If it doesn’t skip, 
j  0.
 e second test exits if 
v[j] > v[j + 1] is not true, or exits if 
v[j + 1]. First we create the address by multiplying 
j by 4 (since we need a byte 
address) and add it to the base address of 
v:sll    $t1, $s1, 2   # reg $t1 = j * 4add    $t2, $a0, $t1 # reg $t2 = v + (j * 4)Now we load 
v[j]:lw     $t3, 0($t2)   # reg $t3  = v[j]Since we know that the second element is just the following word, we add 4 to 
the address in register 
$t2 to get 
v[j + 1]:lw     $t4, 4($t2)   # reg $t4  = v[j + 1] e test of 
 is the same as 
, so the 
two instructions of the exit test are

 e bottom of the loop jumps back to the inner loop test:
j    for2tst   # jump to test of inner loopCombining the pieces, the skeleton of the second 
for loop looks like this:
        addi $s1, $s0, –1     # j = i – 1
for2tst:slti $t0, $s1, 0      # reg $t0 = 1 if $s1 < 0 (j < 0)
        bne $t0, $zero, exit2 # go to exit2 if $s1 < 0 (j < 0)
        sll $t1, $s1, 2       # reg $t1 = j * 4
        add $t2, $a0, $t1     # reg $t2 = v + (j * 4)
        lw  $t3, 0($t2)       # reg $t3 = v[j]
        lw  $t4, 4($t2)       # reg $t4 = v[j + 1]


            . . .
            (body of second for loop)
            . . .
        addi $s1, $s1, –1     # j –= 1
        j  for2tst            # jump to test of inner loop
exit2:The Procedure Call in sort e next step is the body of the second 
for loop:
swap(v,j);Calling 
swap is easy enough:
jal    swap 2.13 A C Sort Example to Put It All Together 
137
138 Chapter 2 Instructions: Language of the Computer
Passing Parameters in 
sort e problem comes when we want to pass parameters because the 
sort procedure 
needs the values in registers 
$a0 and 
$a1, yet the swap procedure needs to have its 
parameters placed in those same registers. One solution is to copy the parameters 
for 
sort into other registers earlier in the procedure, making registers 
$a0 and 
$a1 available for the call of 
swap is copy is faster than saving and restoring on 
the stack.) We
 rst copy 
$a0 and 
$a1 into 
$s2 and 
$s3 during the procedure:
move  $s2, $a0     # copy parameter $a0 into $s2move  $s3, $a1     # copy parameter $a1 into $s3 en we pass the parameters to 
swap with these two instructions:
move  $a0, $s2     # first swap parameter is v
move  $a1, $s1     # second swap parameter is jPreserving Registers in 
sort e only remaining code is the saving and restoring of registers. Clearly, we must 
save the return address in register 
$ra, since sort is a procedure and is called 
itse
 e sort procedure also uses the saved registers 
$s0, $s1, $s2, and 
$s3, so they must be saved.
 e prologue of the 
sort procedure is then
addi  $sp,$sp,–20  # make room on stack for 5 registers
sw    $ra,16($sp)  # save $ra on stack
sw    $s3,12($sp)  # save $s3 on stack
sw    $s2, 8($sp)  # save $s2 on stack
sw    $s1, 4($sp)  # save $s1 on stack
sw    $s0, 0($sp)  # save $s0 on stack e tail of the procedure simply reverses all these instructions, then adds a 
jr to 
return.
The Full Procedure sortNow we put all the pieces together in 
Figure 2.27
, being careful to replace references 
to registers 
$a0 and 
$a1 in the 
for loops with references to registers 
$s2 and 
$s3. Once again, to make the code easier to follow, we identify each block of code with 

its purpose in the procedure. In this example, nine lines of the 
sort procedure in 
C became 35 lines in the MIPS assembly language.
Elaboration: One optimization that works with this example is 
procedure inlining
. Instead of passing arguments in parameters and invoking the code with a 
jal instruction, 
the compiler would copy the code from the body of the 
swap procedure where the call to swap appears in the code. Inlining would avoid four instructions in this example. The 
downside of the inlining optimization is that the compiled code would be bigger if the 
inlined procedure is called from several locations. Such a code expansion might turn 

into lower
 performance if it increased the cache miss rate; see Chapter 5.

Saving registers
sort: addi $sp,$sp,Œ20 #makeroomonstackfor5registers
 sw $ra,16($sp)#save$raonstack

 sw $s3,12($sp) #save$s3onstack

 sw $s2,8($sp)#save$s2onstack

 sw $s1,4($sp)#save$s1onstack

 sw $s0,0($sp)#save$s0onstack
Procedure body
Move parameters move $s2,$a0 
#copyparameter$a0into$s2(save$a0)
 move $s3,$a1 
#copyparameter$a1into$s3(save$a1)
Outer loop move $s0,$zero#i=0
for1tst:slt     $t0,$s0,$s3 #reg$t0=0if$s0−$s3(i−n)

 beq $t0,$zero,exit1#gotoexit1if$s0−$s3(i−n)
Inner loop addi $s1,$s0,Œ1
#j=iŒ1
for2tst:slti    $t0,$s1,0   #reg$t0=1if$s1<0(j<0)

 bne $t0,$zero,exit2#gotoexit2if$s
1<0(j<0)

 sll $t1,$s1,2#reg$t
1=j*4
 add $t2,$s2,$t1#reg$t
2=v+(j*4)
 lw $t3,0($t2)#reg$t3 =v[j]
 lw $t4,4($t2)#reg$t4 =v[j+1]

 slt $t0,$t4,$t3#reg$t
0=0if$t4−$t3

 beq $t0,$zero,exit2#gotoexit2if$t4−$t3
Pass parametersand call move $a0,$s2 #1stparameterofswapisv(old$a0)
 move $a1,$s1 
#2ndparameterofswapisj
 jal swap  #swap
 code shown in Figure 2.25
Inner loop addi $s1,$s1,Œ1#jŒ=1

 j for2tst  #jumptotestofinnerloop
Outer loopexit2: addi $s0,$s0,1 #i+=1

 j for1tst  #jumptotestofouterloop
Restoring registers
exit1: lw $s0,0($sp) #restore$s0fromstack

 lw $s1,4($sp)#restore$s1fromstack

 lw $s2,8($sp)#restore$s2fromstack

 lw $s3,12($sp) #restore$s3fromstack

 lw $ra,16($sp) #restore$rafromstack

 addi $sp,$sp,20 #restorestackpointer
Procedure return
 jr $ra  #returntocallingroutine
FIGURE 2.27 MIPS assembly version of procedure 
sort in Figure 2.26
. 2.13 A C Sort Example to Put It All Together 
139
140 Chapter 2 Instructions: Language of the Computer
Figure 2.28
 shows the impact of compiler optimization on sort program 
performance, compile time, clock cycles, instruction count, and CPI. Note that 

unoptimized code has the best CPI, and O1 optimization has the lowest instruction 

count, but O3 is the fastest, reminding us that time is the only accurate measure of 

program performance.
Figure 2.29
 compares the impact of programming languages, compilation 
versus interpretation, and algorithms on performance of sor
 e fourth column 
shows that the unoptimized C program is 8.3 times faster than the interpreted 

Java code for Bubble Sort. Using the JIT compiler makes Java 2.1 times 
faster
 than 
the unoptimized C and within a factor of 1.13 of the highest optimized C code. 

( Section 2.15
 gives more details on interpretation versus compilation of Java and 
the Java and MIPS code for Bubble Sor
 e ratios aren’t as close for Quicksort 
in Column 5, presumably because it is harder to amortize the cost of runtime 

compilation over the shorter execution time
 e last column demonstrates the 
impact of a better algorithm, o
 ering three orders of magnitude a performance 
increases by when sorting 100,000 items. Even comparing interpreted Java in 

Column 5 to the C compiler at highest optimization in Column 4, Quicksort beats 

Bubble Sort by a factor of 50 (0.05 
 2468, or 123 times faster than the unoptimized 
C code versus 2.41 times faster).
Elaboration: The MIPS compilers always save room on the stack for the arguments 
in case they need to be stored, so in reality they always decrement 
$sp by 16 to make 
room for all four argument registers (16 bytes). One reason is that C provides a 
vararg option that allows a pointer to pick, say, the third argument to a procedure. When the 

compiler encounters the rare 
vararg, it copies the four argument registers onto the 

stack into the four reserved locations.
Understanding 
Program 
Performance
gcc optimizationRelative performance
Clock cycles (millions)Instruction count 
(millions)CPI
None1.00158,615114,938 1.38 
O1 (medium)2.3766,990  37,4701.79 
O2 (full)2.3866,52139,9931.66 
O3 (procedure integration)2.4165,74744,9931.46 
FIGURE 2.28 Comparing performance, instruction count, and CPI using compiler 
optimization for Bubble Sort.
 e programs sorted 100,000 words with the array initialized to random 
val
 ese programs were run on a Pentium 4 with a clock rate of 3.06 GHz and a 533 MHz system bus 
with 2 GB of PC2100 DDR SDRAM. It used Linux version 2.4.20.

 2.14 Arrays versus Pointers 
141 2.14 Arrays versus Pointers
A challenge for any new C programmer is understanding pointers. Comparing 
assembly code that uses arrays and array indices to the assembly code that uses 

pointers o
 ers insights about pointer
 is section shows C and MIPS assembly 
versions of two procedures to clear a sequence of words in memory: one using 

array indices and one using pointers. 
Figure 2.30
 shows the two C procedures.
 e purpose of this section is to show how pointers map into MIPS instructions, 
and not to endorse a dated programming style. We’ll see the impact of modern 

compiler optimization on these two procedures at the end of the section.
Array Version of Clear
Let’s start with the array version, 
clear1, focusing on the body of the loop and 
ignoring the procedure linkage code. We assume that the two parameters 
array and 
size are found in the registers 
$a0 and 
$a1, and that 
i is allocated to 
register 
$t0. e initialization of 
i, th
 rst part of the 
for loop, is straightforward:
     move    $t0,$zero      # i = 0 (register $t0 = 0)To set 
array[i] to 0 we must
 rst get its address. Start by multiplying 
i by 4 
to get the byte address:
loop1: sll   $t1,$t0,2      # $t1 = i * 4Since the starting address of the array is in a register, we must add it to the index 
to get the address of 
array[i] using an add instruction:
       add   $t2,$a0,$t1    # $t2 = address of array[i]LanguageExecution methodOptimization
Bubble Sort relative 
performance
Quicksort relative 
performance
Speedup Quicksort 
vs. Bubble Sort
CCompilerNone1.001.002468CompilerO12.371.501562CompilerO22.381.501555CompilerO32.411.911955JavaInterpreter
Œ0.120.051050JIT compilerŒ2.130.29338FIGURE 2.29 Performance of two sort algorithms in C and Java using interpretation and optimizing compilers relative 
to unoptimized C version.
 e last column shows the advantage in performance of Quicksort over Bubble Sort for each language and 
execution optio
 ese programs were run on the same system as in 
Figure 2.28
 e JVM is Sun version 1.3.1, and the JIT is Sun Hotspot 
version 1.3.1.

142 Chapter 2 Instructions: Language of the Computer
Finally, we can store 0 in that address:
        sw   $zero, 0($t2)  # array[i] = 0 is instruction is the end of the body of the loop, so the next step is to increment 
i:        addi $t0,$t0,1      # i = i + 1 e loop test checks if i is less than size:
       slt  $t3,$t0,$a1      # $t3 = (i < size)       bne  $t3,$zero,loop1  # if (i < size) go to loop1We have now seen all the pieces of the procedure. Here is the MIPS code for 
clearing an array using indices:
       move  $t0,$zero       # i = 0
loop1: sll   $t1,$t0,2       # $t1 = i * 4
       add   $t2,$a0,$t1     # $t2 = address of array[i]
       sw    $zero, 0($t2)   # array[i] = 0
       addi  $t0,$t0,1       # i = i + 1
       slt   $t3,$t0,$a1     # $t3 = (i < size)
       bne   $t3,$zero,loop1 # if (i < size) go to loop1 is code works as long as 
size is greater than 0; ANSI C requires a test of size 
before the loop, but we’ll skip that legality here.)
clear1(int array[], int size){
 int i;

 for (i = 0; i < size; i += 1)

  array[i] = 0;

}
clear2(int *array, int size)
{
 int *p;

 for (p = &array[0]; p < &array[size]; p = p + 1)

  *p = 0;

}FIGURE 2.30 Two C procedures for setting an array to all zeros.
 Clear1 uses indices, 
while 
clear2 uses pointer
 e second procedure needs some explanation for those unfamiliar with C. 
 e address of a variable is indicated by &, and the object pointed to by a pointer is indicated b
 e declarations declare that 
array and 
p are pointers to integer
 e 
 rst part of the 
for loop in 
clear2 assigns the address of th
 rst element of 
array to the pointer 
p e second part of the 
for loop tests to see 
if the pointer is pointing beyond the last element of 
array. Incrementing a pointer by one, in the last part of 
the 
for loop, means moving the pointer to the next sequential object of its declared size. Since 
p is a pointer to 
integers, the compiler will generate MIPS instructions to increment 
p by four, the number of bytes in a MIPS 
integer e assignment in the loop places 0 in the object pointed to by 
p.
Pointer Version of Clear
 e second procedure that uses pointers allocates the two parameters 
array and 
size to the registers 
$a0 and 
$a1 and allocates 
p to register 
$t0 e code for 
the second procedure starts with assigning the pointer 
p to the address of th
 rst 
element of the array:
     move  $t0,$a0         # p = address of array[0] e next code is the body of the 
for loop, which simply stores 0 into 
p:loop2: sw  $zero,0($t0)    # Memory[p] = 0 is instruction implements the body of the loop, so the next code is the iteration 
increment, which changes 
p to point to the next word:
     addi  $t0,$t0,4       # p = p + 4Incrementing a pointer by 1 means moving the pointer to the next sequential 
object in C. Since 
p is a pointer to integers, each of which uses 4 bytes, the compiler 
increments p by 4.
 e loop te
 e 
 rst step is calculating the address of the last element 
of 
array. Start with multiplying 
size by 4 to get its byte address:
     sll   $t1,$a1,2       # $t1 = size * 4and then we add the product to the starting address of the array to get the address 
of th
 rst word 
 er the array:
add  $t2,$a0,$t1      # $t2 = address of array[size] e loop test is simply to see if 
p is less than the last element of 
array:slt  $t3,$t0,$t2      # $t3 = (p<&array[size])bne  $t3,$zero,loop2  # if (p<&array[size]) go to loop2With all the pieces completed, we can show a pointer version of the code to zero 
an array:
   move $t0,$a0        # p = address of array[0]
loop2: sw   $zero,0($t0)   # Memory[p] = 0
   addi $t0,$t0,4      # p = p + 4

   sll  $t1,$a1,2      # $t1 = size * 4

   add  $t2,$a0,$t1    # $t2 = address of array[size]

   slt  $t3,$t0,$t2    # $t3 = (p<&array[size])

   bne  $t3,$zero,loop2 # if (p<&array[size]) go to loop2
As in th
 rst example, this code assumes 
size is greater than 0.
 2.14 Arrays versus Pointers 
143
144 Chapter 2 Instructions: Language of the Computer
Note that this program calculates the address of the end of the array in every 
iteration of the loop, even though it does not change. A faster version of the code 
moves this calculation outside the loop:
   move $t0,$a0         # p = address of array[0]   sll  $t1,$a1,2       # $t1 = size * 4
   add  $t2,$a0,$t1     # $t2 = address of array[size]loop2: sw   $zero,0($t0)    # Memory[p] = 0
   addi $t0,$t0,4       # p = p + 4
   slt  $t3,$t0,$t2     # $t3 = (p<&array[size])
   bne  $t3,$zero,loop2 # if (p<&array[size]) go to loop2Comparing the Two Versions of Clear
Comparing the two code sequences side by side illustrates th
 erence between 
array indices and pointers (the changes introduced by the pointer version are 
highlighted):
 e version on th
  must have the “multiply” and add inside the loop because 
i is incremented and each address must be recalculated from the ne
 e memory pointer version on the right increments the pointer 
p directly.
 e pointer 
version moves the scalin
  and the array bound addition outside the loop, 
thereby reducing the instructions executed per iteration from 6 t
 is manual 
optimization corresponds to the compiler optimization of strength reductio
  instead of multiply) and induction variable elimination (eliminating array address 

calculations within loops). 
 Section 2.15
 describes these two and many other 
optimizations.
Elaboration: As mentioned ealier, a C compiler would add a test to be sure that 
size is greater than 0. One wa rst instruction of the 
loop to the slt instruction.
 move $t0,$zero  # i = 0
loop1: sll $t1,$t0,2 # $t1 = i * 4
 add $t2,$a0,$t1 # $t2 = &array[i]
 sw $zero, 0($t2) # array[i] = 0

 addi $t0,$t0,1 # i = i + 1

 slt $t3,$t0,$a1 # $t3 = (i < size)

 bne $t3,$zero,loop1# if () go to loop1
 move $t0,
$a0 # p = & array[0] sll $t1,
$a1,2 # $t1 = 
size * 4 add $t2,$a0,$t1 # $t2 = &array[
size]loop2: sw $zero,0($t0) # Memory[p] = 0 addi $t0,$t0,4 # p = p + 4 slt $t3,$t0,
$t2     
# $t3=(p<&array[size]) bne $t3,$zero,loop2# if () go to loop2

  Advanced Material: Compiling C and 
Interpreting Java
 is section gives a brief overview of how the C compiler works and how Java 
is executed. Because the comp
 cantly a
 ect the performance of a 
computer, understanding compiler technology today is critical to understanding 

performance. Keep in mind that the subject of compiler construction is usually 

taught in a one- or two-semester course, so our introduction will necessarily only 

touch on the basics.
 e second part of this section, starting on page 2.15-15, is for readers interested 
in seeing how an objected-oriented language like Java executes on the MIPS 

architecture. It shows the Java bytecodes used for interpretation and the MIPS code 

for the Java version of some of the C segments in prior sections, including Bubble 

Sort. It covers both the Java virtual machine and just-in-time (JIT) compilers.
Compiling C is 
 rst part of the section introduces the internal 
anatomy
 of a compiler. To 
start, Figure 2.15.1 shows the structure of recent compilers, and we describe the 

optimizations in the order of the passes of that structure.
DependenciesLanguage dependent;machine independentSomewhat language dependent;
largely machine independentSmall language dependencies;machine dependencies slight(e.g., register counts/types)Highly machine dependent;language independentFront end perlanguageFunctionTransform language to
common intermediate formFor example, looptransformations andprocedure inlining
(also called procedure integration)Including global and localoptimizations  registerallocationDetailed instruction selectionand machine-dependentoptimizations; may includeor be followed by assemblerHigh-leveloptimizationsGlobaloptimizerCode generatorIntermediaterepresentationFIGURE 2.15.1 The structure of a modern optimizing compiler consists of a number of 
passes or phases. Logically, each pass can be thought of as running to completion before the next occurs. 
In practice, some passes may handle one procedure at a time, essentially interleaving with another pass.
5.92.15
 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-3To illustrate the concepts in this part of this section, we will use the C version of 
a while
 loop from page 92:
while (save[i] == k) i += 1;
The Front End
 e function of the front end is to read in a source program; check the syntax 
and semantics; and translate the source program to an intermediate form that 

interprets most of the language-spe
 c operation of the program. As we will see, 
intermediate forms are usually simple, and some are in fact similar to the Java 

bytecodes (see Figure 2.15.8).
 e front end is usually broken into four separate functions:
1. Scanning
 reads in individual characters and creates a string of tokens. 
Examples of 
tokens
 are reserved words, names, operators, and punctuation 
symbols. In the above example, the token sequence is 
while, (, save, [, i, ], ==, k, ), i, +=, 1. A word like 
while is recognized as a reserved 
word in C, but 
save, i, and 
j are recognized as names, and 
1 is recognized 
as a number.
2. Parsing
 takes the token stream, ensures the syntax is correct, and produces 
an 
abstract syntax tree
, which is a representation of the syntactic structure of 
the program. Figure 2.15.2 shows what the abstract syntax tree might look 

like for this program fragment.
3. Semantic analysis
 takes the abstract syntax tree and checks the program for 
semantic correctness. Semantic checks normally ensure that variables and 

types are properly declared and that the types of operators and objects match, 

a step called 
type checking
. During this process, a symbol table representing 
all the named objects—classes, variables, and functions—is usually created 

and used to type-check the program.
4. Generation of the intermediate representation
 (IR) takes the symbol table and 
the abstract syntax tree and generates the intermediate representation that is 

the output of the front end. Intermediate representations usually use simple 

operations on a small set of primitive types, such as integers, characters, and 

reals. Java bytecodes represent one type of intermediate form. In modern 

compilers, the most common intermediate form looks much like the MIPS 

instruction set but with an
 nite number of virtual registers; later, we describe 
how to map these virtual registers to
 nite set of real registers. Figure 2.15.3 
shows how our example might be represented in such an intermediate form. We 

capitalize the MIPS instructions in this section when they represent IR forms.
 e intermediate form sp
 es the functionality of the program in a manner 
independent of the original source
 er this front end has created the intermediate 
form, the remaining passes are largely language independent.

2.15-4 2.15 Advanced Material: Compiling C and Interpreting Java
while statement while ydob tnemetats noitidnoc expression  assignment  comparison left-hand side 
expression identifier factor l number 1  k yarraexpression 
expression expression 
factor factor 
array access identifier 
identifier factor 
save identifier 
i FIGURE 2.15.2 An abstract syntax tree for the while
 example.
 e roots of the tree consist of 
the informational tokens such as numbers and names. Long chains of straight-line descendents are o
 en 
omitted in constructing the tree.
High-Level Optimizations
High-level optimizations are transformations that are done at something close to 
the source level.
 e most common high-level transformation is probably 
procedure inlining
, which replaces a call to a function by the body of the function, substituting the 

caller’s arguments for the procedure’s parameters. Other high-level optimizations 

involve loop transformations that can reduce loop overhead, improve memory 

access, and exploit the hardware more
 ectively. For example, in loops that 
execute many iterations, such as those traditionally controlled by a 
for statement, 
the optimization of 
loop-unrolling
 is o
 en useful. Loop-unrolling involves taking 
a loop, replicating the body multiple times, and executing the transformed loop 

fewer times. Loop-unrolling reduces the loop overhead and provides opportunities 

for many other optimizations. Other types of high-level transformations include 
loop-unrolling
 A technique to get more 

performance from loops 

that access arrays, in 

which multiple copies of 

the loop body are made 

and instructions from 

 erent iterations are 
scheduled together.

 # comments are written like this--source code often included
 # while (save[i] == k) 
loop:     
  LI R1,save    
# loads the starting address of save into R1
 LW R2,i
 MULT R3,R2,4 # Multiply R2 by 4

 ADD R4,R3,R1
 LW R5,0(R4) # load save[i]

 LW R6,k

 BNE R5,R6,endwhileloop
 # i += 1

 LW R6, i
 ADD R7,R6,1   
# increment SW R7,i
 branch loop # next iteration
endwhileloop:FIGURE 2.15.3 The while
 loop example is shown using a typical intermediate 
representation. In practice, the names 
save, i, and 
k would be replaced by some sort of address, such 
as a reference to either the local stack pointer or a global pointer, and an o
 set, similar to the way 
save[i] is accessed. Note that the format of the MIPS instructions
 erent, because they are intermediate 
representations here: the operations are capitalized and the registers use 
RXX notation.
 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-5sophisticated loop transformations such as interchanging nested loops and 
blocking loops to obtain better memory behavior; see Chapter 5 for examples.
Local and Global OptimizationsWithin the pass dedicated to local and global optimization, three classes of 

optimizations are performed:
1. Local optimization
 works within a single basic block. A local optimization 
pass is o
 en run as a precursor and successor to global optimization to 
“clean up” the code before and a
 er global optimization.
2. Global optimization
 works across multiple basic blocks; we will see an 
example of this shortly.
3. Global register allocation
 allocates variables to registers for regions of the 
code. Register allocation is crucial to getting good performance in modern 

processors.
Several optimizations are performed both locally and globally, including common 
subexpression elimination, constant propagation, copy propagation, dead store 

elimination, and strength reduction. Let’s look at some simple examples of these 

optimizations.

2.15-6 2.15 Advanced Material: Compiling C and Interpreting Java
Common subexpression elimination
  nds multiple instances of the same 
expression and replaces the second one by rst. Consider, 

for example, a code segment to add 4 to an array element:
x[i] = x[i] + 4 e address calculation for 
x[i] occurs twice and is identical since neither the 
starting address of 
x nor the value of 
i chang
 us, the calculation can be reused. 
Let’s look at the intermediate code for this fragment, since it allows several other 

optimizations to be performed.
 e unoptimized intermediate code is on th
 . On 
the right is the optimized code, using comm
on subexpression elimination to replace 
the second address calculation with th
 rst. Note that the register allocation has 
not yet occurred, so the compiler is using virtual register numbers like 
R100 here.
# x[i] + 4 # x[i] + 4
li R100,x li R100,x

lw R101,i lw R101,i

mult R102,R101,4 mult R102,R101,4

add R103,R100,R102 add R103,R100,R102

lw R104,0(R103) lw R104,0(R103)

add R105,R104,4 # value of x[i] is in R104

# x[i] = li R106,x add R105,R104,4

lw R107,i # x[i] =

mult R108,R107,4 sw R105,0(R103)

add R109,R106,R107
sw R105,0(R109)If the same optimization were possible across two basic blocks, it would then be an 
instance of 
global common subexpression elimination.
Let’s consider some of the other optimizations:
 Strength reduction
 replaces complex operations by simpler ones and can be 
applied to this code segment, replacing the MULT by
  
 . Constant propagation
 and its sibling 
constant folding
 nd constants in code 
and propagate them, collapsing constant values whenever possible.
 Copy propagation
 propagates values that are simple copies, eliminating the 
need to reload values and possibly enabling other optimizations, such as 

common subexpression elimination.
 Dead store elimination
 nds stores to values that are not used again and 
eliminates the store; its “cousin” is 
dead code elimination
, whic
 nds unused 
code—code that cannot a
 ect the result of the program—and eliminates it. 
With the heavy use of macros, templates, and the similar techniques designed 

to reuse code in high-level languages, dead code occurs surprisingly o
 en.
Compilers must be 
conservative
 e 
 rst task of a compiler is to produce correct 
code; its second task is usually to produce fast code, although other factors, such as 

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-7code size, may sometimes be important as well. Code that is fast but incorrect—for 
any possible combination of inputs—is simply wron
 us, when we say a compiler 
is “conservative,” we mean that it performs an optimization only if it knows with 

100% certainty that, no matter what the inputs, the code will perform as the user 

wrote it. Since most compilers translate and optimize one function or procedure 

at a time, most compilers, especially at lower optimization levels, assume the worst 

about function calls and about their own parameters.
Programmers concerned about performance of
 critical loops, especially in real-time 
or embedded applications, o
 en 
 nd themselves staring at the assembly language 
produced by a compiler and wondering why the compiler failed to perform some 

global optimization or to allocate a variable to a register throughout a loop
 e 
answer o
 en lies in the dictate that the compiler be conservative
 e opportunity for 
improving the code may seem obvious to the programmer, but then the programmer 

 en has knowledge that the compiler does not have, such as the absence of aliasing 
between two pointers or the absence of
 ects by a function call
 e compiler 
may indeed be able to perform the transformation with a little help, which could 

eliminate the worst-case behavior that it must assume
 is insight also illustrates 
an important observation: programmers who use pointers to try to improve 

performance in accessing variables, especially pointers to values on the stack that also 

have names as variables or as elements of arrays, are likely to disable many compiler 

optimization
 e end result is that the lower-level pointer code may run no better, 
or perhaps even worse, than the higher-level code optimized by the compiler.
Global Code OptimizationsMany global code optimizations have the same aims as those used in the local 

case, including common subexpression e
limination, constant propagation, copy 
propagation, and dead store and dead code elimination.
 ere are two other important global optimizations: code motion and induction 
variable elimination. Both are loop optimizations; that is, they are aimed at code 

in loops. 
Code motion
 nds code that is loop invariant: a particular piece of 
code computes the same value on every iteration of the loop and, hence, may be 

computed once outside the loop. 
Induction variable elimination
 is a combination of 
transformations that reduce overhead on indexing arrays, essentially replacing array 

indexing with pointer accesses. Rather than
 examine induction variable elimination 
in depth, we point the reader to Section 2.14, which compares the use of array 

indexing and pointers; for most loops, the transformation from the more obvious 

array code to the pointer code can be performed by a modern optimizing compiler.
Understanding 

Program 

Performance

2.15-8 2.15 Advanced Material: Compiling C and Interpreting Java
Implementing Local OptimizationsLocal optimizations are implemented on basic blocks by scanning the basic block 
in instruction execution order, looking for optimization opportunities. In the 

assignment statement example on page 2.15-6, 
the duplication of the entire address 
calculation is recognized by a series of sequential passes over the code. Here is how 

the process might proceed, including a description of the checks that are needed:
1. Determine that the two 
li operations return the same result by observing 
that the operand 
x is the same and that the value of its address has not been 
changed between the two 
li operations.
2. Replace all uses of 
R106 in the basic block by 
R101.3. Observe that 
i cannot change between the two 
LWs that reference it. So 
replace all uses of 
R107 with 
R101.4. Observe that the 
mult instructions now have the same input operands, so 
that 
R108 may be replaced by 
R102.5. Observe that now the two 
add instructions have identical input operands 
(R100 and 
R102), so replace the 
R109 with 
R103.6. Use dead store code elimination to delete the second set of 
li, lw, mult, and 
add instructions since their results are unused.
 roughout this process, we need to know when two instances of an operand have 
the same value.
 is is easy to determine when they refer to virtual registers, since 
our intermediate representation uses such registers only once, but the problem can 

be trickier when the operands are variables in memory, even though we are only 

considering references within a basic block.
It is reasonably easy for the compiler to make the common subexpression 
elimination determination in a conservative fashion in this case; as we will see in 

the next subsection, this is more
  cult when branches intervene.
Implementing Global OptimizationsTo understand the challenge of implementing global optimizations, let’s consider 
a few examples:
 Consider the case of an opportunity for common subexpression elimination, 
say, of an IR statement like 
ADD Rx, R20, R50. To determine whether two 
such statements compute the same value, we must determine whether the 

values of 
R20 and 
R50 are identical in the two statements. In practice, this 
means that the values of 
R20 and 
R50 have not changed between th
 rst 
statement and the second. For a single basic block, this is easy to decide; it is 

more
  cult for a more complex program structure involving multiple basic 
blocks and branches.
 Consider the second 
LW of 
i into 
R107 within the earlier example: how do 
we know whether its value is used again? If we consider only a single basic 

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-9block, and we know that all uses of 
R107 are within that block, it is easy to 
see. As optimization proceeds, however,
 common subexpression elimination 
and copy propagation may create other uses of a value. Determining that a 

value is unused and the code is dead is more
  cult in the case of multiple 
basic blocks.
 Finally, consider the load of 
k in our loop, which is a candidate for code 
motion. In this simple example, we might argue it is easy to see that 
k is 
not changed in the loop and is, hence, loop invariant. Imagine, however, a 

more complex loop with multiple nestings and 
if statements within the body. 
Determining that the load of 
k is loop invariant is harder in such a case.
 e information we need to perform these global optimizations is similar: we 
need to know where each operand in an IR statement could have been changed or 

 ned
 (us
 nition informatio
 e dual of this information is also needed: 
that
 nding all the uses of that changed opera
 nition-use information). 
Data
 ow analysis
 obtains both types of information.
Global optimizations and data
 ow analysis operate on a 
contro
 ow graph
, where 
the nodes represent basic blocks and the arcs represent contro
 ow between basic 
blocks. Figure 2.15.4 shows the contro
 ow graph for our simple loop example, 
with one important transformation introduced. We describe the transformation in 

the caption, but see if you can discover it, and why it was done, on your own!
8.      LW R6,i9.      ADD R7,R6,110.    SW R7,i1.      LI R1,save
2.      LW R2,i3.      SLL R3,R2,24.      ADD R4,R3,R1
5.      LW R5,0(R4)
6.      LW R6,k7.      BEQ R5,R6,startwhileloopFIGURE 2.15.4 A control ﬂ ow graph for the
 while
 loop example.
 Each node represents a basic 
block, which terminates with a branch or by sequential fall-through into another basic block that is also 

the target of a branc
 e IR statements have been numbered for ease in referring to th
 e important 
transformation performed was to move the 
while
 test and conditional branch to the end.
 is eliminates the 
unconditional branch that was formerly inside the loop and places it before the loop
 is transformation 
is so important that many compilers do it during the generation of the IR
 e 
MULT was also replaced with 
(“strength-reduced to”) an 
SLL.
2.15-10 2.15 Advanced Material: Compiling C and Interpreting Java
Suppose we have computed the us
 nition information for the control 
 ow graph in Figure 2.15.4. How does this information allow us to perform code 
motion? Consider IR statements number 1 and 6: in both cases, the us
 nition 
information tells us that there ar
 nitions (changes) of the operands of these 
statements within the loop
 us, these IR statements can be moved outside the 
loop. Notice that if the 
LI of 
save and the 
LW of 
k are executed once, just prior 
to the loop entrance, the computatio
 ect is the same, but the program now 
runs faster since these two statements are outside the loop. In contrast, consider 

IR statement 2, which loads the value of 
i e 
 nitions of 
i that a
 ect this 
statement are both outside the loop, where 
i is initiall
 ned, and inside the loop 
in statement 10 where it is stored. Hence, this statement is not loop invariant.
Figure 2.15.5 shows the code a
 er performing both code motion and induction 
variable elimination, which simp
 es the address calculatio
 e variable 
i can 
still be register allocated, eliminating the need to load and store it every time, and 

we will see how this is done in the next subsection.
Before we turn to register allocation, we need to mention a caveat that also 
illustrates the complexity an
  culty of optimizers. Remember that the compiler 
must be conservative. To be conservative, a compiler must consider the following 

question: Is there 
any way
 that the variable 
k could possibly ever change in this 
loop? Unfortunately, there is one way. Suppose that the variable 
k and the variable 
i actually refer to the same memory location, which could happen if they were 
accessed by pointers or reference parameters.
LW R2,iADD R7,R2,1ADD R4,R4,4SW R7,iLI R1,save
LW R6,k
LW R2,i
SLL R3,R2,2
ADD R4,R3,R1LW R5,0(R4)BEQ R5,R6,startwhileloopFIGURE 2.15.5 The control ﬂ ow graph showing the representation of the
 while
 loop 
example after code motion and induction variable elimination.  e number of instructions in 
the inner loop has been reduced from 10 to 6.

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-11I am sure that many readers are saying, “Well, that would certainly be a stupid 
piece of code!” Alas, this response is not open to the compiler, which must 

translate the code as it is written. Recall too that the aliasing information must 

also be conservative; thus, compilers o
 en 
 nd themselves negating optimization 
opportunities because of a possible alias that exists in one place in the code or 

because of incomplete information about aliasing.
Register AllocationRegister allocation is perhaps the most important optimization for modern 

load-store architectures. Eliminating a load or a store eliminates an instruction. 

Furthermore, register allocation enhances the value of other optimizations, such as 

common subexpression elimination. Fortunat
ely, the trend toward larger register 
counts in modern architectures has made register allocation simpler and more 

 ective. Register allocation is done on both
 a local basis and a gl
obal basis, that is, 
across multiple basic blocks but within a single function. Local register allocation 

is usually done late in compilation, as th
 nal code is generated. Our focus here is 
on the more challenging and more opportunistic global register allocation.
Modern global register allocation uses a region-based approach, where a 
region (sometimes called a 
live range
) represents a section of code during which 
a particular variable could be allocated to a particular register. How is a region 

select
 e process is iterative:
1. Choose
 nition (change) of a variable in a given basic block; add that 
block to the region.
2. Find any uses of that
 nition, which is a data
 ow analysis problem; add 
any basic blocks that contain such uses, as well as any basic block that the 

value passes through to reach a use, to the region.
3. Find any ot
 nitions that also can a
 ect a use found in the previous 
step and add the basic blocks containing thos
 nitions, as well as the 
blocks th
 nitions pass through to reach a use, to the region.
4. Repeat steps 2 and 3 using th
 nitions discovered in step 3 until 
convergence.
 e set of basic blocks found by this technique has a special property: if the 
designated variable is allocated to a register in all these basic blocks, then there is 

no need for loading and storing the variable.
Modern global register allocators start by constructing the regions for every 
virtual register in a function. Once the regions are constructed, the key question 

is how to allocate a register to each region: the challenge is that certain regions 

overlap and may not use the same register. Regions that do not overlap (i.e., share 

no common basic blocks) can share the same register. One way to represent 

2.15-12 2.15 Advanced Material: Compiling C and Interpreting Java
the interference among regions is with an 
interference graph,
 where each node 
represents a region, and the arcs between nodes represent that the regions have 

some basic blocks in common.
Once an interference graph has been constructed, the problem of allocating 
registers is equivalent to a famous problem called 
graph coloring:
 nd a color for 
each node in a graph such that no two adjacent nodes have the same color. If the 

number of colors equals the number of registers, then coloring an interference 

graph is equivalent to allocating a register for each regio
 is insight was the 
initial motivation for the allocation method now known as region-based allocation, 

but originally called the graph-coloring approach. Figure 2.15.6 shows th
 ow 
graph representation of the 
while
 loop example a
 er register allocation.
What happens if the graph cannot be colored using the number of registers 
availab
 e allocator must spill registers until it can complete the coloring. By 
doing the coloring based on a priority function that takes into account the number 

of memory references saved and the cost of tying up the register, the allocator 

attempts to avoid spilling for the most important candidates.
Spilling is equivalent to splitting up a region (or live range); if the region is split, 
fewer other regions will interfere with the two separate nodes representing the 

original region. A process of splitting regions and successive coloring is used to 

allow the allocation process to complete, at which point all candidates will have 

been allocated a register. Of course, whenever a region is split, loads and stores 
ADD $t2,$t2,1ADD $t4,$t4,4LI $t0,save
LW $t1,k

LW $t2,i
SLL $t3,$t2,2
ADDU $t4,$t3,$t0LW $t3,0($t4)BEQ $t3,$t1,startwhileloopFIGURE 2.15.6  The control ﬂ
 ow graph showing the representation of the 
while
 loop example after code motion and induction variable elimination and register allocation, 

using the MIPS register names. e number of IR statements in the inner loop has now dropped to 
only four from six before register allocation and ten before any global optimization
 e value of 
i resides 
in $t2 at the end of the loop and may need to be stored eventually to maintain the program semantics. If 
i were unused
 er the loop, not only could the store be avoided, but also the increment inside the loop could 
be eliminated completely!

Hardware/
Software 

Interface 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-13must be introduced to get the value from memory or to store it there.
 e location 
chosen to split a region must balance the cost of the loads and stores that must be 

introduced against the advantage of freeing up a register and reducing the number 

of interferences.
Modern register allocators are incredibly
 ective in using the large register 
counts available in modern processors. In many programs, th
 ectiveness of 
register allocation is limited not by the availability of registers but by the possibilities 

of aliasing that cause the compiler to be conservative in its choice of candidates.
Code Generation e 
 nal steps of the compiler are code generation and assembly. Most compilers 
do not use a stand-alone assembler that accepts assembly language source code; 

to save time, they instead perform most of the same function
 lling in symbolic 
values and generating the binary code as th
 nal stage of code generation.
In modern processors, code generation is reasonably straightforward, since the 
simple architectures make the choice of instruction relatively obvious. For more 

complex architectures, such as the x86, code generation is more complex since 

multiple IR instructions may collapse into a single machine instruction. In modern 

compilers, this compilation process uses pattern matching with either a tree-based 

pattern matcher or a pattern matcher driven by a parser.
During code generation, th
 nal stages of machine-dependent optimization 
are also performed.
 ese include some constant folding optimizations, as well as 
localized instruction scheduling (see Chapter 4).
Optimization Summary
Figure 2.15.7 gives examples of typical optimizations, and the last column indicates 

where the optimization is performed in the gcc compiler. It is so
  cult 
to separate some of the simpler optimizations—local and processor-dependent 

optimizations—from transformations done in the code generator, and some 

optimizations are done multiple times, especially local optimizations, which may be 

performed before and
 er global optimization as well as during code generation.
Today, essentially all programming for desktop and server applications is done 
in high-level languages, as is most programming for embedded applications. 

 is development means that since most instructions executed are the output 
of a compiler, an instruction set architecture is essentially a compiler target. 

With 
Moore’s Law
 comes the temptation of adding sophisticated operations 
in an instruction s
 e challenge is that they may not exactly match what the 
compiler needs to produce or may be so general that they aren’t fast. For example, 

consider special loop instructions found in some computers. Suppose that instead 

2.15-14 2.15 Advanced Material: Compiling C and Interpreting Java
of decrementing by one, the compiler wanted to increment by four, or instead 
of branching on not equal zero, the compiler wanted to branch if the index was 

less than or equal to the limi
 e loop instruction may be a mismatch. When 
faced with such objections, the instruction set designer might then generalize the 

operation, adding another operand to specify the increment and perhaps an option 

on which branch condition to use.
 en the danger is that a common case, say, 
incrementing by one, will be slower than a sequence of simple operations.
Elaboration: Some more sophisticated compilers, and many research compilers, use 
an analysis technique called interprocedural analysis
 to obtain more information about 
functions and how they are called. Interprocedural analysis attempts to discover what 

properties remain true across a function call. For example, we might discover that a 

function call can never change any global variables, which might be useful in optimizing 

a loop that calls such a function. Such information is called 
may-information
 or 
ﬂ ow-
insensitive information ciently, although analyzing 
level ccgnoitanalpxEeman noitazimitpO edni rossecorp ;level ecruos eht raen ro tAlevel hgiHpendent3Oydob erudecorp yb llac erudecorp ecalpeRnoitargetni erudecorPedoc enil-thgiarts nihtiWlacoLCommon subexpression eliminationReplace two instances of the same computa 1
Oypoc elgnis yb noitConstant propagationReplace all instances of a variable that is as 
signed a constant with the 
constantO1Stack height reductionRearrange expression tree to minimize re 
sources needed for ex pression  
evaluationO1
hcnarb a ssorcAlabolGGlobal common subexpression 
elimi nation2Osehcnarb sessorc noisrev siht tub ,lacol sa emaS elbairav a fo secnatsni lla ecalpeRnoitagaporp ypoCA that has been assigned 
X (i.e., 
A = X) with XO22Opool eht fo noitareti hcae eulav emas setupmoc taht pool a morf edoc evomeRnoitom edoCInduction variable elimina tionSimplify/eliminate array addressing calcula 2
Ospool nihtiw snoitProcessor dependentDepends on processor knowledge

Strength reductionMany examples; replace multiply by a con 1
Ostfihs htiw tnatsPipeline schedulingReorder instructions to improve pipeline per 1
Oecnamrof1Otegrat sehcaer taht tnemecalpsid hcnarb tsetrohs eht esoohCnoitazimitpo tesffo hcnarBFIGURE 2.15.7  Major types of optimizations and explanation of each class.
 e third column shows when these occur 
at
 erent levels of optimization in g
 e GNU organization calls the three optimization levels medium (O1), full (O2), and full with 
integration of small procedures (O3).

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-15a call to a function F requires analyzing all the functions that F calls, which makes 
the process somewhat time consuming for large programs. A more costly property to 

discover is that a function 
must always change some variable; such information is called 
must-information or 
ﬂ ow-sensitive information.
 Recall the dictate to be conservative: 
may-information can never be used as must-information—just because a function 
may
 change a variable does not mean that it must change it. It is conservative, however, to 
use the negation of may-information, so the compiler can rely on the fact that a function 

will never change a variable in optimizations around the call site of that function.
One of the most important uses of interprocedural analysis is to obtain so-
called alias information. An 
alias
 occurs when two names may designate the same 
variable. For example, it is quite helpful to know that two pointers passed to a 

function may never designate the same variable. Alias information is usuall
 ow-
insensitive and must be used conservatively.
Interpreting Java
 is second part of the section is for readers interested in seeing how an 
object-
oriented language
 like Java executes on a MIPS architecture. It shows the Java 
bytecodes used for interpretation and the MIPS code for the Java version of some 

of the C segments in prior sections, including Bubble Sort.
Let’s quickly review the Java lingo to make sure we are all on the same page
 e 
big idea of object-oriented programming is for programmers to think in terms of 

abstract objects, and operations are associated with each 
type
 of object. New types 
can o
 en be thought of as re
 nements to existing types, and so some operations 
for the existing types are used by the new type without change
 e hope is that 
the programmer thinks at a higher level, and that code can be reused more readily 

if the programmer implements the common operations on many
 erent types.
 is 
 erent perspective led to
 erent set of term
 e type of an object 
is a 
class
, which is th
 nition of a new data type together with the operations 
that ar
 ned to work on that data type. A particular object is then an 
instance
 of a class, and creating an object from a class is called 
instantiation
 e operations 
in a class are called 
methods
, which are similar to C procedures. Rather than call 
a procedure as in C, you 
invoke
 a method in Ja
 e other members of a class 
are 
 elds
, which correspond to variables in C. Variables inside objects are called 
instance
 elds
. Rather than access a structure with a pointer, Java uses an 
object 
reference
 to access an objec
 e syntax for method invocation is 
x.y, where 
x is 
an object reference and y is the method name.
 e parent–child relationship between older and newer classes is captured 
by the verb “extends”: a child class 
extends
 (or sub classes) a parent cl
 e 
child class typically will re
 ne some of the methods found in the parent to match 
the new data type. Some methods wo
 ne, and the child class 
inherits
 those 
methods.
To reduce the number of errors associated with pointers and explicit memory 
deallocation, Java automatically frees unused storage, using a separate garbage 
object-oriented 
language
 A programming language 

that is oriented around 

objects rather than 

actions, or data versus 

logic.
2.15-16 2.15 Advanced Material: Compiling C and Interpreting Java
collector that frees memory when it is full. Hence, 
new creates a new instance of a 
dynamic object on the heap, but there is no 
free in Java. Java also requires array 
bounds to be checked at runtime to catch another class of errors that can occur in 

C programs.
Interpretation
As mentioned before, Java programs are distributed as Java bytecodes, and the Java 

Virtual Machine (JVM) executes Java byte co
 e JVM understands a binary 
format called the 
clas
 le
 format. A cl
 le is a stream of bytes for a single class, 
containing a table of valid methods with their bytecodes, a pool of constants that 

acts in part as a symbol table, and other information such as the parent class of this 

class.
When th
 rst started, it looks for the class method 
main. To start any 
Java class, the JVM dynamically loads, links, and initializes a cl
 e JVM loads 
a class by
 rst 
 nding the binary representation of the proper class (c
 le) and 
then creating a class from that binary representation. Linking combines the class 

into the runtime state of the JVM so that it can be executed. Finally, it executes the 

class initialization method that is included in every class.
Figure 2.15.8 shows Java bytecodes and their corresponding MIPS instructions, 
illustratin
 ve majo
 erences between the two:
1. To simplify compilation, Java uses a stack instead of registers for operands. 
Operands are pushed on the stack, operated on, and then popped o
  the 
stack.
 e designers of the JVM were concerned about code size, so bytecodes 
vary in length between one an
 ve bytes, versus the 4-byte,
 xed-size 
MIPS instructions. To save space, the JVM even has redundant instructions 

of
 erent lengths whose only
 erence is size of the immediate.
 is 
decision illustrates a code size variation of our third design principle: make 

the common case 
small. e JVM has safety features embedded in the architecture. For example, 
array data transfer instructions check to be sure that th
 rst operand is a 
reference and that the second index operand is within bounds.
4. To allow garbage collectors to
 nd all live pointers, the JVM us
 erent 
instructions to operate on addresses versus integers so that the JVM can 

know what operands contain addresses. MIPS generally lumps integers and 

addresses together.
5. Finally, unlike MIPS, there are Java-spe
 c instructions that perform complex 
operations, like allocating an array on the heap or invoking a method.

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-17edocetyb avaJnoitarepOyrogetaCSize (bits)MIPS instr. Meaning
pop ;SON+SOT=SONdda8ddaiddacitemhtirApop ;SONŒSOT=SONbus8busitcartbusb8I + ]a8I[emarF =]a8I[emarFidda8b8I a8I cniitnemercniData transferload local integer/addressiload I8/aload I816lw TOS=Frame[I8]
load local integer/addressiload_/aload_{0,1,2,3}8lwTOS=Frame[{0,1,2,3}]
store local integer/addressistore I8/astore I816swFrame[I8]=TOS; pop
load integer/address from arrayiaload/aaload8lwNOS=*NOS[TOS]; pop

store integer/address into arrayiastore/aastore8sw*NNOS[NOS]=TOS; pop2
pop ;]SOT[SON*=SONhl8daolasyarra morf flah daol2pop ;SOT=]SON[SONN*hs8erotsasyarra otni flah erotspop ;]SOT[SON*=SONbl8daolabyarra morf etyb daol2pop ;SOT=]SON[SONN*bs8erotsabyarra otni etyb erotsload immediatebipush I8, sipush I1616, 24addipush; TOS=I8 or I16

load immediateiconst_{Œ1,0,1,2,3,4,5}8addipush; TOS={Œ1,0,1,2,3,4,5}
pop ;SON&SOT=SONdna8dnaidnalacigoLpop ;SON|SOT=SONro8roiropop ;SOT << SON=SONlls8lhsitfel tfihspop ;SOT >> SON=SONlrs8rhsuithgir tfihsConditional branchbranch on equalif_icompeq I1624beqif TOS == NOS, go to I16; pop2
branch on not equalif_icompne I1624bneif TOS != NOS, go to I16; pop2
2pop ;61I ot og ,SON }=>,>,=<,<{ SOT fitls4261I }eg,tg,el,tl{pmoci_fierapmocUnconditional jump61I ot ogj4261I otogpmujrj8nruteri ,ternruter3+CP=SOT ;hsup ;61I ot oglaj4261I rsjenituorbus ot pmujStack managementremove from stackpop, pop28pop, pop2
SON=SOT ;hsup8pudkcats no etacilpudT=SOT ;SOT=SON ;SON=T8pawskcats no snoitisop 2 pot pawsSafety checkcheck for null referenceifnull I16, ifnotnull I1624if TOS {==,!=} null, go to I16
get length of arrayarraylength8push; TOS = length of array
check if object a typeinstanceof I1624TOS = 1 if TOS matches type of 
Const[I16]; TOS = 0 otherwiseInvocationinvoke methodinvokevirtual I1624Invoke method in Const[I16], dispatching 
on typeAllocationcreate new class instancenew I1624Allocate object type Const[I16] on heap
create new arraynewarray I1624Allocate array type Const[I16] on heap
FIGURE 2.15.8  Java bytecode architecture versus MIPS.
 Although many bytecodes are simple, those in the last half-dozen rows 
above are complex and sp
 c to Java. Bytecodes are one to
 ve bytes in length, hence their name
 e Java mnemonics use the pr
 x 
i for 
32-bit integer, 
a for reference (address), 
s for 16-bit integers (short), and 
b for 8-bit bytes. We use 
I8 for an 8-bit constant and 
I16 for a 
16-bit constant. MIPS uses registers for operands, but the JVM uses a stac
 e compiler knows the maximum size of the operand stack for 
each method and simply allocates space for it in the current frame. Here is the notation in the Meaning column: 
TOS: top of stack; 
NOS: next 
position below 
TOS; NNOS: next position below 
NOS; pop: remove 
TOS; pop2: remove 
TOS and 
NOS; and 
push: add a position to the stack. 
*NOS and 
*NNOS mean access the memory location pointed to by the address in the stack at those positions. 
Const[] refers to the runtime 
constant pool of a class created by the JVM, and 
Frame[] refers to the variables of the local method frame.
 e only missing MIPS instructions 
from Figure 2.1 are 
nor, andi, ori, slti, and 
lui e missing bytecodes are a few arithmetic and logical operators, some tricky stack 
management, compares to 0 and branch, support for branch tables, type conversions, more variations of the complex, Java-spe
 c instructions 
plus operations on
 oating-point data, 64-bit integers (longs), and 16-bit characters.

2.15-18 2.15 Advanced Material: Compiling C and Interpreting Java
Compiling a while
 Loop in Java Using Bytecodes
Compile the 
while
 loop from page 92, this time using Java bytecodes:
while (save[i] == k)i += 1;Assume that 
i, k, and 
save are th
 rst three local variables. Show the 
addresses of the byteco
 e MIPS version of the C loop in Figure 
2.15.3 took six instructions and twenty-four bytes. How big is the bytecode 

version?
 e 
 rst step is to put the array reference in 
save on the stack:
0 aload_3 # Push local variable 3 (save[]) onto stack is 1-byte instruction informs the JVM that an address in local variable 3 is 
being put on the stac
 e 0 on th
  of this instruction is the byte address 
of this
 rst instruction; bytecodes for each method start at
 e next step is 
to put the index on the stack:
1 iload_1 # Push local variable 1 (i) onto stackLike the prior instruction, this 1-byte instruction is a short version of a more 

general instruction that takes 2 bytes to load a local variable onto the stac
 e 
next instruction is to get the value from the array element:
2 iaload # Put array element (save[i]) onto stack is 1-byte instruction checks the prior two operands, pops them o
  the stack, 
and then puts the value of the desired array element onto the new top of the 

stack. Next, we place 
k on the stack:
3 iload_2 # Push local variable 2 (k) onto stackWe are now ready for the 
while
 test:
4 if_icompne, Exit # Compare and exit if not equal is 3-byte instruction compares the top two elements of the stack, pops them 
  the stack, and branches if they are not equal. We ar
 nally ready for the 
body of the loop:
7 iinc, 1, 1 # Increment local variable 1 by 1 (i+=1)EXAMPLEANSWER
 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-19 is unusual 3-byte instruction increments a local variable by 1 without using 
the operand stack, an optimization that again saves space. Finally, we return to 

the top of the loop with a 3-byte jump:
10 go to 0 # Go to top of Loop (byte address 0) us, the bytecode version takes seven instructions and thirteen bytes, almost 
half the size of the MIPS C code. (As before, we can optimize this code to jump 

less.)Compiling for Java
Since Java is derived from C and Java has the same built-in types as C, the assignment 

statement examples in Sections 2.2 to 2.6 of Chapter 2 are the same in Java as they 

ar
 e same is true for the 
if statement example in Section 2.7.
 e Java version of the 
while
 loop
 erent, however.
 e designers of C 
leave it up to the programmers to be sure that their code does not exceed the array 

bo
 e designers of Java wanted to catch array bound bugs, and thus require 
the compiler to check for such violations. To check bounds, the compiler needs to 

know what they are. Java includes an extra word in every array that holds the upper 

bound.
 e lower bo
 ned as 0.
Compiling a while
 Loop in Java
Modify the MIPS code for the 
while
 loop on page 94 to include the array 
bounds checks that are required by Java. Assume that the length of the array is 

located just before th
 rst element of the array.
Let’s assume that Java arrays reserved th
 rst two words of arrays before the 
data starts. We’ll see the use of th
 rst word soon, but the second word has the 
array length. Before we enter the loop, let’s load the length of the array into a 

temporary register:
lw $t2,4($s6) # Temp reg $t2 = length of array save
Before we multiply 
i by 4, we must test to see if it’s less than 0 or greater than 
the last element of the array.
 e 
 rst step is to check if 
i is less than 0:
Loop: slt $t0,$s3,$zero # Temp reg $t0 = 1 if i < 0
Register 
$t0 is set to 1 if 
i is less than 0. Hence, a branch to see if register 
$t0 is 
not equal to
 zero will give us th
 ect of branching if 
i is less than 
 is pair of instructions, 
slt and 
bne, implements branch on less than. 
EXAMPLEANSWER
2.15-20 2.15 Advanced Material: Compiling C and Interpreting Java
Register 
$zero always contains 0, so th
 nal test is accomplished using the 
bne instruction and comparing register 
$t0 to register 
$zero:bne $t0,$zero,IndexOutOfBounds  # if i<0, goto Error
Since the array starts at 0, the index of the last array element is one less than the 
length of the array.
 us, the test of the upper array bound is to be sure that 
i is 
less than the length of the array.
 e second step is to set a temporary register 
to 1 if 
i is less than the array length and then branch to an error if it’s not less. 
 at is, we branch to an error if the temporary register is 
equal to
 zero:
slt $t0,$s3,$t2 # Temp reg $t0 = 0 if i >= length
beq $t0,$zero,IndexOutOfBounds  #if i>=length, goto ErrorNote that these two instructions implement branch on greater than or equal to. 
 e next two lines of the MIPS 
while
 loop are unchanged from the C version:
sll $t1,$s3,2 # Temp reg $t1 = 4 * i
add $t1,$t1,$s6 # $t1 = address of save[i]
We need to account for th
 rst 8 bytes that are reserved in Java. We do that by 
changing the addr
 eld of the load from 0 to 8:
lw $t0,8($t1) # Temp reg $t0 = save[i]
 e rest of the MIPS code from the C 
while
 loop
 ne as is:
bne $t0,$s5, Exit # go to Exit if save[i] ? k
add $s3,$s3,1 # i = i + 1

j Loop # go to Loop

Exit: (See the exercises for an optimization of this sequence.)
Invoking Methods in Java
 e compiler picks the appropriate method depending on the type of the object. In a 
few cases, it is unambiguous, and the method can be invoked with no more overhead 

than a C procedure. In general, however, the compiler knows only that a given variable 

contains a pointer to an object that belongs to some subtype of a general class. Since 

it doesn’t know at compile time which subclass the object is, and thus which method 

should be invoked, the compiler will generate code that
 rst tests to be sure the pointer 
isn’t null and then uses the code to load a pointer to a table with all the legal methods 

for that type
 e 
 rst word of the object has the method table address, which is why 
Java arrays reserve two words. Let’s say it’s using th
 h method that was declared for 
that cl
 e method order is the same for all subclass
 e compiler then takes 
the
 h address from that table and invokes the method at that address.

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-21public class sort {   public static void sort (int[] v) {  for (int i = 0; i < 
v.length; i += 1) {   for (int j = i - 1; j >= 0 && v[j] > v[j + 1]; j Œ= 1) {
 swap(v, j);
   }

 }   protected static void swap(int[] v, int k) {  int temp = v[k];

  v[k] = v[k+1];
  v[k+1] = temp;
   }}FIGURE 2.15.9 An initial Java procedure that performs a sort on the array v.
 Changes from 
Figures 2.24 and 2.26 are highlighted.
 e cost of object orientation in general is that method invocation includes 1) a 
conditional branch to be sure that the pointer to the object is valid; 2) a load to get 

the address of the table of available methods; 3) another load to get the address of 

the proper method; 4) placing a return address into the return register, an
 nally 
5) a jump register to invoke the method
 e next subsection gives a concrete 
example of method invocation.
A Sort Example in Java
Figure 2.15.9 shows the Java version of exchange sort. A simp
 erence is that 
there is no need to pass the length of the array as a separate parameter, since Java 

arrays include their length: 
v.length denotes the length of 
v.A more
 cant 
 erence is that Java methods are prepended with keywords 
not found in the C procedur
 e 
sort method is declared 
public static while 
swap is declared 
protected static. Public
 means that 
sort can be 
invoked from any other method, while 
protected
 means 
swap can only be called by 
other methods within the same 
package
 and from methods within derived classes. 
A static method
 is another name for a class method—methods that perform 
classwide operations and do not apply to an individual object. Static methods are 

essentially the same as C procedures.
 is straightforward translation from C into static methods means there is no 
ambiguity on method invocation, and so it can be ju
  cient as C. It also is limited 
to sorting integers, which mean
 erent sort has to be written for each data type.
To demonstrate the object orientation of Java, Figure 2.15.10 shows the 
new version with the changes highlighted. First, we declare 
v to be of the type 
Comparable and replace 
v[j] > v[j + 1] with an invocation of 
compareTo. By changing 
v to this new class, we can use this code to 
sort many data types.
public
 A Java keyword 
that allows a method to 

be invoked by any other 

method.
protected
 A Java key 
word that restricts 

invocation of a method 

to other methods in that 

package.
package
 Basically a 
directory that contains a 

group of related classes.
static method
 A method that applies to 

the whole class rather to 

an individual object. It is 

unrelated to static in C.

2.15-22 2.15 Advanced Material: Compiling C and Interpreting Java
public class sort {   public static void sort (Comparable[] v) {  for (int i = 0; i < v.length; i += 1) {

       for (int j = i Œ 1; j >= 0 && v[j].compareTo(v[j + 1]);  j Œ= 1) {                    swap(v, j);
           }
   }
     
   protected static void swap(Comparable[] v, int k) {  Comparable temp = v[k];  v[k] = v[k+1];
  v[k+1] = temp;
   }}public class Comparable {
  public int(compareTo (int x)
  { return value Œ x; }

  public int value;
}FIGURE 2.15.10 A revised Java procedure that sorts on the array v that can take on more types.
 Changes from Figure 
2.15.9 are highlighted.
 e method 
compareTo compares two elements and returns a value greater than 
0 if the parameter is larger than the object, 0 if it is equal, and a negative number 

if it is smaller than the objec
 ese two changes generalize the code so it can 
sort integers, characters, strings, and so on, if there are subclasses of 
Comparable with each of these types and if there is a version of 
compareTo for each type. 
For pedagogic purposes, we re
 ne the class 
Comparable and the method 
compareTo here to compare integer
 e actu
 nition of 
Comparable in the 
Java library is considerably
 erent.
Starting from the MIPS code that we generated for C, we show what changes we 
made to create the MIPS code for Java.
For 
swap, the only
 cant 
 erences are that we must check to be sure the 
object reference is not null and that each array reference is within bo
 e 
 rst 
test checks that the address in th
 rst parameter is not zero:
swap: beq $a0,$zero,NullPointer #if $a0==0,goto Error
Next, we load the length of v into a register and check that index 
k is OK.
lw $t2,4($a0) # Temp reg $t2 = length of array v
slt $t0,$a1,$zero # Temp reg $t0 = 1 if k < 0

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-23bne $t0,$zero,IndexOutOfBounds  # if k < 0, goto Errorslt $t0,$a1,$t2 # Temp reg $t0 = 0 if k >= length

beq $t0,$zero,IndexOutOfBounds  #if k>=length,goto Error is check is followed by a check that 
k+1 is within bounds.
addi $t1,$a1,1 # Temp reg $t1 = k+1 
slt $t0,$t1,$zero  
# Temp reg $t0 = 1 if k+1 < 0bne $t0,$zero,IndexOutOfBounds  # if k+1 < 0, goto Error slt $t0,$t1,$t2  
# Temp reg $t0 = 0 if k+1 >= lengthbeq $t0,$zero,IndexOutOfBounds  #if k+1>=length,goto ErrorFigure 2.15.11 highlights the extra MIPS instructions in 
swap that a Java 
compiler might produce. We again must adjust the o
 set in the load and store to 
account for two words reserved for the method table and length.
Figure 2.15.12 shows the method body for those new instructions for 
sort. (We 
can take the saving, restoring, and return from Figure 2.27.)
 e 
 rst test is again to make sure the pointer to 
v is not null:
beq $a0,$zero,NullPointer #if $a0==0,goto Error
Next, we load the length of the array (we use register 
$s3 to keep it similar to the 
code for the C version of swap):
1w $s3,4($aO) #$s3 = length of array v
Bounds checkswap: beq $a0,$zero,NullPointer #if $a0==0,goto Error
 lw $t2,-4($a0)  # Temp reg $t2 = length of array v

 slt $t0,$a1,$zero  # Temp reg $t0 = 1 if k < 0

 bne $t0,$zero,IndexOutOfBounds # if k < 0, goto Error

 slt $t0,$a1,$t2  # Temp reg $t0 = 0 if k >= length

 beq $t0,$zero,IndexOutOfBounds # if k >= length, goto Error

 addi $t1,$a1,1  # Temp reg $t1 = k+1

 slt $t0,$t1,$zero  # Temp reg $t0 = 1 if k+1 < 0

 bne $t0,$zero,IndexOutOfBounds # if k+1 < 0, goto Error

 slt $t0,$t1,$t2  # Temp reg $t0 = 0 if k+1 >= length

 beq $t0,$zero,IndexOutOfBounds # if k+1 >= length, goto Error
Method body sll $t1, $a1, 2  # reg $t1 = k * 4 

 add $t1, $a0, $t1  # reg $t1 = v + (k * 4) 
]k[v fo sserdda eht sah 1t$ ger #     lw $t0, 
8($t1)  # reg $t0 (temp) = v[k]
 lw $t2,
 12
($t1)  # reg $t2 = v[k + 1]
v fo tnemele txen ot srefer #     sw $t2, 
8($t1)  # v[k] = reg $t2
 sw $t0, 
12($t1)  # v[k+1] = reg $t0 (temp)
Procedure return
 jr $ra  # return to calling routine
FIGURE 2.15.11 MIPS assembly code of the procedure swap in Figure 2.24
.
2.15-24 2.15 Advanced Material: Compiling C and Interpreting Java
Now we must ensure that the index is within bounds. Since th
 rst test of the inner 
loop is to test if 
j is negative, we can skip that initial bound te
 at leaves the test 
for too big:
slt $t0,$s1,$s3    
# Temp reg $t0 = 0 if j >= lengthbeq $t0,$zero,IndexOutOfBounds  #if j>=length, goto ErrorMethod bodyMove parameters
)0a$ evas( 2s$ otni 0a$ retemarap ypoc # 0a$ ,2s$ evom Test ptr null
 beq  $a0,$zero,NullPointer # if $a0==0, goto Error
Get array length
v yarra fo htgnel = 3s$ # )0a$(4,3s$ wl Outer loop0 = i # orez$ ,0s$ evom for1tst: slt $t0, $s0, $s3  # reg $t0 = 0 if $s0 − $s3  (i − n)

 beq $t0, $zero, exit1 # go to exit1 if $s0 − $s3  (i − n)
Inner loop start
 addi $s1, $s0, Œ1 # j = i Œ 1

for2tst: slti $t0, $s1, 0 # reg $t0 = 1 if $s1 < 0 (j < 0)

 bne $t0, $zero, exit2 # go to exit2 if $s1 < 0 (j < 0)
Test if 
j too big
 slt $t0,$s1,$s3 # Temp reg $t0 = 0 if j >= length
 beq $t0,$zero,IndexOutOfBounds # if j >= length, goto Error
Get v[j] sll $t1, $s1, 2 # reg $t1 = j * 4 
 add $t2, $s2, $t1 # reg $t2 = v + (j * 4) 

 lw $t3, 0($t2) # reg $t3 = v[j]
Test if 
j+1 < 0or if j+1 too big1+j = 1t$ ger pmeT # 1,1s$,1t$ idda  slt $t0,$t1,$zero # Temp reg $t0 = 1 if j+1 < 0

 bne $t0,$zero,IndexOutOfBounds # if j+1 < 0, goto Error

 slt $t0,$t1,$s3 # Temp reg $t0 = 0 if j+1 >= length

 beq $t0,$zero,IndexOutOfBounds # if j+1 >= length, goto Error
Get v[j+1] lw $t4, 4($t2) # reg $t4   = v[j + 1]
Load method tableelbat dohtem fo sserdda = 5t$ # )0a$(0,5t$ wl Get method addr˚ fo sserdda = 5t$ # )5t$(8,5t$ wl  rst methodPass parameters
]j[v si oTerapmoc fo retemarap ts1 #  3t$ ,0a$ evom  ]1+j[v si oTerapmoc fo .marap dn2 # 4t$ ,1a$ evom Set return addr
sserdda nruter daol #  1L,ar$ al Call indirectlyoTerapmoc rof edoc llac #  5t$ rj Test if should skip 
swap
L1: slt $t0, $zero, $v0  # reg $t0 = 0 if 0 − $v0 
 beq $t0, $zero, exit2 # go to exit2 if $t4 − $t3 
Pass parameters
and call swap
 v si paws fo retemarap ts1 # 2s$ ,0a$ evom  j si paws fo retemarap dn2 # 1s$ ,1a$ evom 43.2 erugiF ni nwohs edoc paws #  paws laj Inner loop end addi $s1, $s1, Œ1 # j Œ= 1
 j for2tst p
ool renni fo tset ot pmuj # Outer loopexit2: addi $s0, $s0, 1 # i += 1
 j for1tst p
ool retuo fo tset ot pmuj # FIGURE 2.15.12 MIPS assembly version of the method body of the Java version of 
sort e new code is highlighted in this 
 gure. We must still add the code to save and restore registers and the return from the MIPS code found in Figure 2.27. To keep
 the code similar 
to that
 gure, we load 
v.length into 
$s3 instead of into a temporary register. To reduce the number of lines of code, we make the simplifying 
assumption that 
compareTo is a leaf procedure and we do not need to push registers to be saved on the stack.

 2.15 Advanced Material: Compiling C and Interpreting Java 
2.15-25 e code for testing 
j + 1 is quite similar to the code for checking 
k + 1 in 
swap, so we skip it here.
 e ke
 erence is the invocation of 
compareTo. We
 rst load the address of 
the table of legal methods, which we assume is two words before the beginning of 

the array:
lw $t5,0($a0) # $t5 = address of method table
Given the address of the method table for this object, we then get the desired 

method. Let’s assume 
compareTo is the third method in the 
Comparable class. To 
pick the address of the third method, we load that address into a temporary register:
lw $t5,8($t5) # $t5 = address of third method
We are now ready to call 
compareTo e next step is to save the necessary 
registers on the stack. Fortunately, we don’t need the temporary registers or 

argument registers a
 er the method invocation, so there is nothing to save
 us, 
we simply pass the parameters for 
compareTo:move $a0, $t3 # 1st parameter of compareTo is v[j]
move $a1, $t4 # 2nd parameter of compareTo is v[j+1]
Since we are using a jump register to invoke 
compareTo, we need to pass the 
return address explicitly. We use the pseudoinstruction load address (
la) and label 
where we want to return, and then do the indirect jump:
la $ra,L1 # load return address
jr $t5 # to code for compareTo
 e method returns, with 
$v0 determining which of the two elements is larger. 
If 
$v0 > 0, then 
v[j] >v[j+1], and we need to 
swap us, to skip the 
swap, we need to test if 
$v0 ð 0, which is the same as 
0 š $v0. We also need to include 
the label for the return address:
L1: slt $t0, $zero, $v0 # reg $t0 = 0 if 0 š $v0
beq $t0, $zero, exit2  
# go to exit2 if v[j+1] š v[j] e MIPS code for 
compareTo  as an exercise.
 e main changes for the Java versions of 
sort and 
swap are testing for null object 
references and index out-of-bounds errors, and the extra method invocation to 

give a more general compare.
 is method invocation is more expensive than a 
C procedure call, since it requires a load, a conditional branch, a pair of chained 

loads, and an indirect jump. As we see 
in Chapter 4, dependent loads and indirect 
jumps can be relatively slow on modern processor
 e increasing popularity 
Hardware/ 
Software 

Interface
2.15-26 2.15 Advanced Material: Compiling C and Interpreting Java
of Java suggests that many programmers today are willing to leverage the high 
performance of modern processors to pay for error checking and code reuse.
Elaboration: Although we test each reference to 
j and 
j  1 to be sure that these 
indices are within bounds, an assembly language programmer might look at the code 

and reason as follows: e inner 
for loop is only executed if 
j  0 and since 
j  1 
 j, there is no 
need to test 
j  1 to see if it is less than 0.
2. Since 
i takes on the val. , (data.length 
 1) and since 
j takes on 
the values i 
 1, i 
. , 2, 1, 0, there is no need to test if 
j  data.length 
since the largest value 
j can be is data.length 
 2.3. Following the same reasoning, there is no need to test whether 
j  1 
 data.
length since the largest value of 
j  1 is data.length 
 1.There are coding tricks in Chapter 2 and superscalar execution in Chapter 4 that 
lower the effective cost of such bounds checking, but only high optimizing compilers 

can reason this way. Note that if the compiler inlined the swap method into sort, many 

checks would be unnecessary.
Elaboration: Look carefully at the code for swap in Figure 2.15.11. See anything 
wrong in the code, or at least in the explanation of how the code works? It implicitly 

assumes that each Comparable element in 
v is 4 bytes long. Surely, you need much more 
than 4 bytes for a complex subclass of 
Comparable, which could contain any number 
 elds. Surprisingly, this code does work, because an important property of Java’s 

semantics forces the use of the same, small representation for all variables, elds, and 

array elements that belong to 
Comparable or its subclasses.Java types are divided into 
primitive types ned types for numbers, 
characters, and Booleans—and 
reference types
—the built-in classes like String, 
 ned classes, and arrays. Values of reference types are pointers (also called 
references
) to anonymous objects that are themselves allocated in the heap. For the 
programmer, this means that assigning one variable to another does not create a new 

object, but instead makes both variables refer to the same object. Because these 

objects are anonymous and programs therefore have no way to refer to them directly, 

a program must use indirection through a variable to read or write an elds 

(variables). Thus, because the data structure allocated for the array 
v consists entirely 
of pointers, it is safe to assume they are all the same size, and the same swapping code 

works for all of 
Comparable’s subtypes.
To write sorting and swapping functions for arrays of primitive types requires that 
we write new versions of the functions, one for each type. This replication is for two 

reasons. First, primitive type values do not include the references to dispatching tables 

that we used on 
Comparables to determine at runtime how to compare values. Second, 
primitive values come in different sizes: 1, 2, 4, or 8 bytes.
The pervasive use of pointers in Java is elegant in its consistency, with the penalty 
being a level of indirection and a requirement that objects be allocated on the heap. 

Furthermore, in any language where the lifetimes of the heap-allocated anonymous 

 2.21 Historical Perspective and Further Reading 
2.15-27objects are independent of the lifetimes of the named variables, elds, and array 
elements that reference them, programmers must deal with the problem of deciding 

when it is safe to deallocate heap-allocated storage. Java’s designers chose to use 

garbage collection. Of course, use of garbage collection rather than explicit user memory 

management also improves program safety.
C provides an interesting contrast. Although programmers can write essentially 
the same pointer-manipulating solution in C, there is another option. In C
, programmers can elect to forgo the level of indirection and directly manipulate an array 

of objects, rather than an array of pointers to those objects. To do so, C
 programmers 
would typically use the template capability, which allows a class or function to be 

parameterized by the 
type of data on which it acts. Templates, however, are compiled 
using the equivalent of macro expansion. That is, if we declared an instance of sort 

capable of sorting types X and Y, C
 would create two copies of the code for the class: 
one for sort
X and one for sort
Y, each specialized accordingly. This solution 
increases code size in exchange for making comparison faster (since the function calls 
would not be indirect, and might even be subject to inline expansion). Of course, the 

speed advantage would be canceled if swapping the objects required moving large 

amounts of data instead of just single pointers. As always, the best design depends on 

the details of the problem.
 2.16 Real Stuff: ARMv7 (32-bit) Instructions 
145People used to be taught to use pointers in C to get great
  ciency than that 
available with arrays: “Use pointers, even if you can’t understand the code.” Modern 
optimizing compilers can produce code for the array version that is just as good. 

Most programmers today prefer that the compiler do the heav
 ing.
   
Advanced Material: Compiling C and Interpreting Java
 is section gives a brief overview of how the C compiler works and how Java 
is executed. Because the comp
 cantly a
 ect the performance of a 
computer, understanding compiler technology today is critical to understanding 
performance. Keep in mind that the subject of compiler construction is usually 

taught in a one- or two-semester course, so our introduction will necessarily only 

touch on the basics.
 e second part of this section is for readers interested in seeing how an 
object 
oriented language
 like Java executes on a MIPS architecture. It shows the Java 
byte-codes used for interpretation and the MIPS code for the Java version of some 
of the C segments in prior sections, including Bubble Sort. It covers both the Java 

Virtual Machine and JIT compilers.
 e rest of 
 Section 2.15
 can be found online.
 2.16 Real Stuff: ARMv7 (32-bit) Instructions
ARM is the most popular instruction set architecture for embedded devices, with 

more than 9 billion devices in 2011 using ARM, and recent growth has been 2 

billion per year. Standing originally for the Acorn RISC Machine, later changed 

to Advanced RISC Machine, ARM came out the same year as MIPS and followed 

similar philosophies. 
Figure 2.31
 lists the similari e princi
 erence is 
that MIPS has more registers and ARM has more addressing modes.
 ere is a similar core of instruction sets for arithmetic-logical and data transfer 
instructions for MIPS and ARM, as 
Figure 2.32
 shows.
Addressing ModesFigure 2.33
 shows the data addressing modes supported by ARM. Unlike MIPS, 

ARM does not reserve a register to contain 0. Although MIPS has just three simple 

data addressing modes (see 
Figure 2.18
), ARM has nine, including fairly complex 

calculations. For example, ARM has an addressing mode that ca
  one register 
Understanding 

Program 

Performance
2.15object oriented 
language
 A programming language 
that is oriented around 

objects rather than 

actions, or data versus 

logic.
146 Chapter 2 Instructions: Language of the Computer
ARM MIPS Date announced19851985Instruction size (bits)
3232Address space (size, model)
32 bits, ˜ at
32 bits, ˜ at
Data alignmentAlignedAlignedData addressing modes
93Integer registers (number, model, size)15 GPR 
32 bits 31 GPR 32 bits  I/OMemory mapped
Memory mapped
FIGURE 2.31 Similarities in ARM and MIPS instruction sets.
Register-register
ddA
buS
luM
iviD
dnA
rO
roX
oC
Data transferaoL
rotS
rotS
Instruction name 
ARMMIPSaddAdd (trap if over˜ ow)
adds; swivs
addaddu, addiusubtcart
Subtract (trap if over˜ ow)subs; swivs
subsubumult, multumulylpit
div, divuŠed
andandororrxoreorLoad high part registerŠ
luiShift left logicallsl1sllv, sllShift right logicallsr
1srlv, srlShift right arithmeticasr
1srav, sra slt/i,slt/iucmp, cmn, tst, teqerapm
Load byte signedldrsblb

Load byte unsignedldrblbu

Load halfword signedldrshlh

Load halfword unsignedldrhlhu
lwldrdrowd
sbstrbetybe
Store halfword
strhshswstrdrowe
Read, write special registersmrs, msr
move Atomic Exchangeswp, swpbll;scFIGURE 2.32 ARM register-register and data transfer instructions equivalent to MIPS 
core. Dashes mean the operation is not available in that architecture or not synthesized in a few instructions. 
If there are several choices of instructions equivalent to the MIPS core, they are separated by commas. ARM 

incl
 s as part of every data operation instruction, so th
 s with superscript 1 are just a variation 
of a move instruction, such as 
lsr1. Note that ARM has no divide instruction.

by any amount, add it to the other registers to form the address, and then update 
one register with this new address.
Addressing mode
MIPSRegister operandXXImmediate operandXXRegister + offset (displacement or based)
XXRegister + register (indexed)
ŠXRegister + scaled register (scaled)
ŠXRegister + offset and update register
ŠXRegister + register and update register
ŠXAutoincrement, autodecrement
ŠXPC-relative data
ŠXARMFIGURE 2.33 Summary of data addressing modes.
 ARM has separate register indirect and register 
 set addressing modes, rather than just putting 0 in the o
 set of the latter mode. To get greater addressing 
range
 s the o
 set 
  1 or 2 bits if the data size is halfword or word.
Compare and Conditional BranchMIPS uses the contents of registers to evaluate conditional branches. ARM uses the 

traditional four condition code bits stored in the program status word: 
negative, 
zero, carry,
 and 
over
 ow ey can be set on any arithmetic or logical instruction; 
unlike earlier architectures, this setting is optional on each instruction. An 

explicit option leads to fewer problems in a pipelined implementation. ARM uses 

conditional branches to test condition codes to determine all possible unsigned 

and signed relations.
CMP subtracts one operand from the other and th
 erence sets the condition 
codes. 
Compare negative
 (CMN) adds
 one operand to the other, and the sum sets 
the condition codes. TST performs logical AND on the two operands to set all 

condition codes but over ow, while TEQ uses exclusive OR to set th
 rst three 
condition codes.
One unusual feature of ARM is that every instruction has the option of executing 
conditionally, depending on the condition codes. Every instruction starts with a 

4-bit
 eld that determines whether it will act as a no operation instruction (nop) 
or as a real instruction, depending on the condition codes. Hence, conditional 

branches are properly considered as conditionally executing the unconditional 

branch instruction. Conditional execution allows avoiding a branch to jump over a 

single instruction. It takes less code space and time to simply conditionally execute 

one instruction.
Figure 2.34
 shows the instruction formats for ARM an
 e principal 
 erences are the 4-bit conditional executio
 eld in every instruction and the 
smaller regist
 eld, because ARM has half the number of registers.
 2.16 Real Stuff: ARMv7 (32-bit) Instructions 
147
148 Chapter 2 Instructions: Language of the Computer
Unique Features of ARM
Figure 2.35
 shows a few arithmetic-logical instructions not found in MIPS. Since 
ARM does not have a dedicated register for 0, it has separate opcodes to perform 

some operations that MIPS can do with 
$zero. In addition, ARM has support for 
multiword arithmetic.
ARM’s 12-bit immediate
 eld has a novel interpretatio e eight least-
 cant bits are zero-extended to a 32-bit value, then rotated right the number 
of bits sp
 ed in th
 rst four bits of th
 eld multiplied by two. One advantage is 
that this scheme can represent all powers of two in a 32-bit word. Whether this split 

actually catches more immediates than a simple 12-bit
 eld would be an interesting 
study.
Opera
 ing is not limited to immediat e second register of all 
arithmetic and logical processing operations has the option of bein
 ed before 
being operated o
 e 
  options ar
  
  logical
  right logical
  right arithmetic, and rotate right.
Register
Constant
OpcodeARMRegister-register
Opx
4312827
2827
2827
2827
191615
1615
1615
1615
1615
1112
430
Op8Rs14Rd4Rs24Opx
8Data transfer
ARMOpx
4311112
0Op8Rs14Rd4Const
12BranchARMJump/Call
Opx
4312324
0Op4Const
24ARMOpx
4312324
0Op4Const
24MIPS312526
202120
2526
2120

2120
19201110650
Const
5Rs15Rs25Rd5Opx
6Op6MIPS310Const
16Rs15Rd5Op6MIPS312526
2526
0Rs15Opx
5/Rs25Const
16Op6310Op6MIPSConst
26FIGURE 2.34 Instruction formats, ARM and MIPS.
 e 
 erences result from whether the 
architecture has 16 or 32 registers.

 2.17 Real Stuff: x86 Instructions 
149ARM also has instructions to save groups of registers, called 
block loads and 
stores
. Under control of a 16-bit mask within the instructions, any of the 16 registers 
can be loaded or stored into memory in a single instructio
 ese instructions can 
save and restore registers on procedure entry and retur
 ese instructions can 
also be used for block memory copy, and today block copies are the most important 
use of such instructions.
 2.17 Real Stuff: x86 Instructions
Designers of instruction sets sometimes provide more powerful operations than 

those found in ARM an
 e goal is generally to reduce the number of 
instructions executed by a progra
 e danger is that this reduction can occur at 
the cost of simplicity, increasing the time a program takes to execute because the 

instructions are slower
 is slowness may be the result of a slower clock cycle time 
or of requiring more clock cycles than a simpler sequence.
 e path toward operation complexity is thus fraught with peril. Section 2.19 
demonstrates the pitfalls of complexity.
Evolution of the Intel x86ARM and MIPS were the vision of single small groups in 1985; the pieces of these 

architectur
 t nicely together, and the whole architecture can be described 
succinctly. Such is not the case for the x86; it is the product of several independent 

groups who evolved the architecture over 35 years, adding new features to the 

original instruction set as someone might add clothing to a packed bag. Here are 

important x86 milestones.
Beauty is altogether in 

the eye of the beholder.
Margaret Wolfe 
Hungerford, 
Molly 
Bawn
, 1877Name DeÞ nition ARM   MIPS
Load immediateRd = Immmovaddi $0,NotRd = ~(Rs1)mvnnor $0,
MoveRd = Rs1movor $0,
Rotate rightRd = Rs i >>  iRd0. . . iŒ1 = Rs31Œi. . . 31ror
And notRd = Rs1 & ~(Rs2)bic
Reverse subtractRd = Rs2 Œ Rs1rsb, rsc

Support for multiword 
integer addCarryOut, Rd = Rd + Rs1 + 

OldCarryOut
adcsŠSupport for multiword 

integer subCarryOut, Rd = Rd Œ Rs1 + 

OldCarryOut
sbcsŠFIGURE 2.35 ARM arithmetic/logical instructions not found in MIPS.

150 Chapter 2 Instructions: Language of the Computer
 1978 e Intel 8086 architecture was announced as an assembly 
language–compatible extension of the then successful Intel 8080, an 8-bit 
microprocessor.
 e 8086 is a 16-bit architecture, with all internal registers 
16 bits wide. Unlike MIPS, the registers have dedicated uses, and hence the 

8086 is not considered a 
general-purpose register
 architecture.
 1980 e Inte oating-point coprocessor is announced
 is archi-
tecture extends the 8086 with abou
 oating-point instructions. Instead of 
using registers, it relies on a stack (see 
 Section 2.21
 and Section 3.7).
 1982 e 80286 extended the 8086 architecture by increasing the address 

space to 24 bits, by creating an elaborate memory-mapping and protection 

model (see Chapter 5), and by adding a few instructions to round out the 

instruction set and to manipulate the protection model.
 1985 e 80386 extended the 80286 architecture to 32 bits. In addition to 
a 32-bit architecture with 32-bit registers and a 32-bit address space, the 

80386 added new addressing modes and additional operation
 e added 
instructions make the 80386 nearly a general-purpose register machine
 e 80386 also added paging support in addition to segmented addressing (see 

Chapter 5). Like the 80286, the 80386 has a mode to execute 8086 programs 

without change.
 1989–95 e subsequent 80486 in 1989, Pentium in 1992, and Pentium 

Pro in 1995 were aimed at higher performance, with only four instructions 

added to the user-visible instruction set: three to help with multiprocessing 

(Chapter 6) and a conditional move instruction.
 1997 er the Pentium and Pentium Pro were shipping, Intel announced that 
it would expand the Pentium and the Pentium Pro architectures with MMX 

(Multi Media Extension
 is new set of 57 instructions uses th
 oating-
point stack to accelerate multimedia and communication applications. MMX 

instructions typically operate on multiple short data elements at a time, in 

the tradition of 
single instruction, multiple data
 (SIMD) architectures (see 
Chapter 6). Pentium II did not introduce any new instructions.
 1999: Intel added another 70 instructions, labeled SSE (
Streaming SIMD 
Extensions
) as part of Penti
 e primary changes were to add eight 
separate registers, double their width to 128 bits, and add a single precision 

 oating-point data type. Hence, four 32-bit
 oating-point operations can be 
performed in parallel. To improve memory performance, SSE includes cache 

prefetch instructions plus streaming store instructions that bypass the caches 

and write directly to memory.
 2001: Intel added yet another 144 instructions, this time labeled SS
 e new data type is double precision arithmetic, which allows pairs of 64-bit 

 oating-point operations in parallel. Almost all of these 144 instructions are 
versions of existing MMX and SSE instructions that operate on 64 bits of data 
general-purpose 
register (GPR)
 A register that can be 
used for addresses or for 

data with virtually any 

instruction.

in parallel. Not only does this change enable more multimedia operations; 
it gives the comp
 erent target fo
 oating-point operations than 
the unique stack architecture. Compilers can choose to use the eight SSE 

register
 oating-point registers like those found in other computer
 is change boosted the
 oating-point performance of the Pentium 4, th
 rst 
microprocessor to include SSE2 instructions.
 2003: A company other than Intel enhanced the x86 architecture this time. 

AMD announced a set of architectural extensions to increase the address 

space from 32 to 64 bits. Similar to the transition from a 16- to 32-bit address 

space in 1985 with the 80386, AMD64 widens all registers to 64 bits. It also 

increases the number of registers to 16 and increases the number of 128-

bit SSE registers t e primary ISA change comes from adding a new 

mode called 
long mode
 that re
 nes the execution of all x86 instructions 
with 64-bit addresses and data. To address the larger number of registers, it 

adds a new pr
 x to instructions. Depending how you count, long mode also 
adds four to ten new instructions and drops 27 old ones. PC-relative data 

addressing is another extension. AMD64 still has a mode that is identical 

to x86 (
legacy mode
) plus a mode that restricts user programs to x86 but 
allows operating systems to use AMD64 (
compatibility mode
 ese modes 
allow a more graceful transition to 64-bit addressing than the HP/Intel IA-64 

architecture.
 2004: Intel capitulates and embraces AMD64, relabeling it 
Extended Memory 
64 Technology
 e majo
 erence is that Intel added a 128-bit 
atomic compare and swap instruction, which probably should have been 

included in AMD64. At the same time, Intel announced another generation of 

media extensions. SSE3 adds 13 instructions to support complex arithmetic, 

graphics operations on arrays of structures, video encodin
 oating-point 
conversion, and thread synchronization (see Section 2.11). AMD added SSE3 

in subsequent chips and the missing atomic swap instruction to AMD64 to 

maintain binary compatibility with Intel.
 2006: Intel announces 54 new instructions as part of the SSE4 instruction set 

extension
 ese extensions perform tweaks like sum of absolut
 erences, 
dot products for arrays of structures, sign or zero extension of narrow data to 

wider sizes, population count, and so o
 ey also added support for virtual 
machines (see Chapter 5).
 2007: AMD announces 170 instructions as part of SSE5, including 46 

instructions of the base instruction set that adds three operand instructions 

like MIPS.
 2011: Intel ships the Advanced Vector Extension that expands the SSE 

register width from 128 to 256 bits, thereby re
 ning about 250 instructions 
and adding 128 new instructions.
 2.17 Real Stuff: x86 Instructions 
151
152 Chapter 2 Instructions: Language of the Computer
 is history illustrates the impact of the “golden handc
 s” of compatibility on 
the x86, as the existing so
 ware base at each step was too important to jeopardize 
with
 cant architectural changes.
Whatever the artistic failures of the x86, keep in mind that this instruction set 
largely drove the PC generation of computers and still dominates the cloud portion 
of the PostPC Era. Manufacturing 350M x86 chips per year may seem small 

compared to 9 billion ARMv7 chips, but many companies would love to control 

such a market. Nevertheless, this checkered ancestry has led to an architecture that 

  cult to explain and impossible to love.
Brace yourself for what you are about to see! Do 
not try to read this section 
with the care you would need to write x86 programs; the goal instead is to give you 

familiarity with the strengths and weaknesses of the world’s most popular desktop 

architecture.
Rather than show the entire 16-bit, 32-bit, and 64-bit instruction set, in this 
section we concentrate on the 32-bit subset that originated with the 80386. We start 

our explanation with the registers and addressing modes, move on to the integer 

operations, and conclude with an examination of instruction encoding.
x86 Registers and Data Addressing Modes
 e registers of the 80386 show the evolution of the instruction set (
Figure 2.36
).  e 80386 extended all 16-bit registers (except the segment registers) to 32 bits, 
pre
 xing an 
E to their name to indicate the 32-bit version. We’ll refer to them 
generically as GPRs (
general-purpose registers
 e 80386 contains only eight 
 is means MIPS programs can use four times as many and ARMv7 twice 
as many.
Figure 2.37
 shows the arithmetic, logical, and data transfer instructions are 
two-operand instruction
 ere are two importan
 erences here
 e x86 
arithmetic and logical instructions must have one operand act as both a source 

and a destination; ARMv7 and MIPS allow separate registers for source and 

destinatio
 is restriction puts more pressure on the limited registers, since one 
source register must be mo
 ed. 
 e second importan
 erence is that one of 
the operands can be in memory
 us, virtually any instruction may have one 
operand in memory, unlike ARMv7 and MIPS.
Data memory-addressing modes, described in detail below, o
 er two sizes of 
addresses within the instructio
 ese so-called 
displacements
 can be 8 bits or 32 
bits.
Although a memory operand can use any addressing mode, there are restrictions 
on which 
registers
 can be used in a mode. 
Figure 2.38
 shows the x86 addressing 
modes and which GPRs cannot be used with each mode, as well as how to get the 

sa
 ect using MIPS instructions.
x86 Integer Operations e 8086 provides support for both 8-bit (
byte
) and 16-bit (
word
) data typ
 e 80386 adds 32-bit addresses and data (
double words
) in the x86. (AMD64 adds 64-

GPR 0GPR 1GPR 2GPR 3GPR 4GPR 5GPR 6GPR 7Code segment pointerStack segment pointer (top of stack)
Data segment pointer 0Data segment pointer 1Data segment pointer 2Data segment pointer 3Instruction pointer (PC)
Condition codesUse031NameEAXECXEDXEBXESPEBPESIEDICSSSDSESFSGSEIPEFLAGS
FIGURE 2.36 The 80386 register set. Starting with the 80386, the top eight registers were extended 
to 32 bits and could also be used as general-purpose registers.
Source/destination operand type
Second source operand
RegisterRegisterRegisterImmediate
RegisterMemory
MemoryRegister

MemoryImmediate
FIGURE 2.37 Instruction types for the arithmetic, logical, and data transfer instructions.
  e x86 allows the combinations sho
 e only restriction is the absence of a memory-memory mode. 
Immediates may be 8, 16, or 32 bits in length; a register is any one of the 14 major registers in 
Figure 2.36
 (not EIP or EFLAGS).
 2.17 Real Stuff: x86 Instructions 
153
154 Chapter 2 Instructions: Language of the Computer
bit addresses and data, called 
quad words
; we’ll stick to the 80386 in this section.) 
 e data type distinctions apply to register operations as well as memory accesses.
Almost every operation works on both 8-bit data and on one longer data size. 
 at size is determined by the mode and is either 16 bits or 32 bits.
Clearly, some programs want to operate on data of all three sizes, so the 80386 
architects provided a convenient way to specify each version without expanding 
co
 cantly. 
 ey decided that either 16-bit or 32-bit data dominates 
most programs, and so it made sense to be able to set a default large size
 is default data size is set by a bit in the code segment register. To override the default 

data size, an 8-bit 
pre
 x is attached to the instruction to tell the machine to use the 
other large size for this instruction.
 e pr
 x solution was borrowed from the 8086, which allows multiple pr
 xes 
to modify instruction behavior
 e three original pr xes override the default 
segment register, lock the bus to support synchronization (see Section 2.11), or 

repeat the following instruction until the register ECX counts down t
 is last 
pre
 x was intended to be paired with a byte move instruction to move a variable 
number of byt
 e 80386 also added a pr
 x to override the default address size.
 e x86 integer operations can be divided into four major classes:
1. Data movement instructions, including move, push, and pop
2. Arithmetic and logic instructions, including test, integer, and decimal 
arithmetic operations
3. Control 
 ow, including conditional branches, unconditional jumps, calls, 
and returns
4. String instructions, including string move and string compare
ModeDescriptionRegister restrictions
MIPS equivalentRegister indirect
Address is in a register.
Not ESP or EBPlw$s0,0($s1
)Based mode with 8- or 32-bit displacementAddress is contents of base register plus 
displacement.Not ESP lw$s0,100($s1
) #<= 16-bit

               #displacement
Base plus scaled indexThe address is
Base + (2Scale x Index) where Scale has the value 0, 1, 2, or 3.
Base: any GPRIndex: not ESPmul$t0,$s2,4

add$t0,$t0,$s1

lw$s0,0($t0
)Base plus scaled index with
8- or 32-bit displacementThe address is
Base + (2Scale x Index) + displacementwhere Scale has the value 0, 1, 2, or 3.
Base: any GPRIndex: not ESPmul$t0,$s2,4

add$t0,$t0,$s1

lw$s0,100($t0
) #<=16-bit
#displacement
FIGURE 2.38 x86 32-bit addressing modes with register restrictions and the equivalent MIPS code. e Base plus Scaled 
Index addressing mode, not found in ARM or MIPS, is included to avoid the multiplies by 4 (scale factor of 2) to turn an index 
in a register 
into a byte address (see 
Figures 2.25 and 2.27
). A scale factor of 1 is used for 16-bit data, and a scale factor of 3 for 64-bi
t data. A scale factor 
of 0 means the address is not scaled. If the displacement is longer than 16 bits in the second or fourth modes, then the MIPS e
quivalent mode 
would need two more instructions: a 
lui to load the upper 16 bits of the displacement and an 
add to sum the upper address with the base 
register 
$s1. (Intel gives tw
 erent names to what is called Based addressing mode—Based and Indexed—but they are essentially identical 
and we combine them here.)

 e 
 rst two categories are unremarkable, except that the arithmetic and logic 
instruction operations allow the destination to be either a register or a memory 
location. 
Figure 2.39
 shows some typical x86 instructions and their functions.
Conditional branches on the x86 are based on 
condition codes
 or 
 ags
, like 
ARMv7. Condition codes are s
 ect of an operation; most are used 
to compare the value of a result to 0. Branches then test the condition codes. PC-
Instruction
Functionje nameifequal(conditioncode){EIP=name};
EIPŒ128<=name<EIP+128
jmp nameEIP=namecall nameSP=SPŒ4;M[SP]=EIP+5;EIP=name;
movwEBX,[EDI+45]
EBX=M[EDI+45]pushESI
SP=SPŒ4;M[SP]=ESI
popEDI
EDI=M[SP];SP=SP+4
addEAX,#6765
EAX=EAX+6765
testEDX,#42
Set condition code (˜ ags) with EDX and 42
movslM[EDI]=M[ESI];
EDI=EDI+4;ESI=ESI+4
FIGURE 2.39 Some typical x86 instructions and their functions.
 A list of frequent operations 
appears in 
Figure 2.40
 e CALL saves the EIP of the next instruction on the stack. (EIP is the Intel PC.)
relative branch addresses must be sp
 ed in the number of bytes, since unlike 
ARMv7 and MIPS, 80386 instructions are not all 4 bytes in length.
String instructions are part of the 8080 ancestry of the x86 and are not commonly 
executed in most program
 ey are o
 en slower than equivalent so
 ware routines 
(see the fallacy on page 159).
Figure 2.40
 lists some of the integer x86 instructions. Many of the instructions 
are available in both byte and word formats.
x86 Instruction Encoding
Saving the worst for last, the encoding of instructions in the 80386 is complex, with 
many
 erent instruction formats. Instructions for the 80386 may vary from 1 
byte, when there are no operands, up to 15 bytes.
Figure 2.41
 shows the instruction format for several of the example instructions in 
Figure 2.39
 e opcode byte usually contains a bit saying whether the operand is 8 bits 
or 32 bits. For some instructions, the opcode may include the addressing mode and 

the register; this is true in many instructions that have the form “register 
 register op 
immediate.” Other instructions use a “postbyte” or extra opcode byte, labeled “mod, reg, 

r/m,” which contains the addressing mode informatio
 is postbyte is used for many 
 2.17 Real Stuff: x86 Instructions 
155
156 Chapter 2 Instructions: Language of the Computer
of the instructions that address memory
 e base plus scaled index mode uses a second 
postbyte, labeled “sc, index, base.”
Figure 2.42
 shows the encoding of the two postbyte address sp
 ers for 
both 16-bit and 32-bit mode. Unfortunately, to understand fully which registers 
and which addressing modes are available, you need to see the encoding of all 

addressing modes and sometimes even the encoding of the instructions.
x86 ConclusionIntel had a 16-bit microprocessor two years before its competitors’ more elegant 

architectures, such as the Motorola 68000, and this head start led to the selection 

of the 8086 as the CPU for the IBM PC. Intel engineers generally acknowledge that 

the x86 is more
  cult to build than computers like ARMv7 and MIPS, but the 
large market meant in the PC Era that AMD and Intel could a
 ord more resources 
Instruction
MeaningControlConditional and unconditional branches
jnz,jz
Jump if condition to EIP + 8-bit offset; 
JNE (forJNZ), JE (for JZ) are   
alternative names
jmpUnconditional jumpŠ8-bit or 16-bit offset 
callSubroutine callŠ16-bit offset; return address pushed onto stack
retPops return address from stack and jumps to it
loopLoop branchŠdecrement ECX; jump to EIP + 8-bit displacement if ECX
0  Data transferMove data between registers or between register and memory
moveMove between two registers or between register and memory
push,pop
Push source operand on stack; pop operand from stack top to a register
lesLoad ES and one of the GPRs from memory
Arithmetic, logicalArithmetic and logical operations using the data registers and memory
add,sub
Add source to destination; subtract source from destination; register-memory 
format
cmpCompare source and destination; register-memory format
shl,shr,rcr
Shift left; shift logical right; rotate right with carry condition code as ˚ ll
cbwConvert byte in eight rightmost bits of EAX to 16-bit word in right of EAX
testLogical AND of source and destination sets condition codes
inc,dec
Increment destination, decrement destination
or,xor
Logical OR; exclusive OR; register-memory format
String Move between string operands; length given by a repeat preÞ
 xmovsCopies from string source to destination by incrementing ESI and EDI; may be 

repeated
lodsLoads a byte, word, or doubleword of a string into the EAX register
FIGURE 2.40 Some typical operations on the x86. Many operations use register-memory format, 
where either the source or the destination may be memory and the other may be a register or immediate 
operand.

to help overcome the added complexity. What the x86 lacks in style, it made up for 
in market size, making it beautiful from the right perspective.
Its saving grace is that the most frequently used x86 architectural components 
are not to
  cult to implement, as AMD and Intel have demonstrated by rapidly 
improving performance of integer programs since 1978. To get that performance, 
FIGURE 2.41 Typical x86 instruction formats.
 Figure 2.42
 shows the encoding of the postbyte. 
Many instructions contain the 1-bi
 eld w, which says whether the operation is a byte or a double word
 e  eld in 
MOV is used in instructions that may move to or from memory and shows the direction of the move. 
 e ADD instruction requires 32 bits for the immediat
 eld, because in 32-bit mode, the immediates are 
either 8 bits or 32 bi
 e immediate
 eld in the 
TEST is 32 bits long because there is no 8-bit immediate for 
test in 32-bit mode. Overall, instructions may vary from 1 to 15 bytes in lengt
 e long length comes from 
extra 1-byte pr
 xes, having both a 4-byte immediate and a 4-byte displacement address, using an opcode of 
2 bytes, and using the scaled index mode sp er, which adds another byte.
 2.17 Real Stuff: x86 Instructions 
157a. JE EIP + displacement
b. CALL
c. MOV      EBX, [EDI + 45]
d. PUSH ESI
e. ADD EAX, #6765
f. TEST EDX, #42
ImmediatePostbyte
TESTADDPUSHMOVCALLJEwwImmediateRegRegwd
Displacementr/mPostbyte
OffsetDisplacementCondi-tion448
83268118
534323173218
158 Chapter 2 Instructions: Language of the Computer
compilers must avoid the portions of the architecture that are hard to implement 
fast.In the PostPC Era, however, despite considerable architectural and manufacturing 
expertise, x86 has not yet been competitive in the personal mobile device.
 2.18 Real Stuff: ARMv8 (64-bit) Instructions
Of the many potential problems in an instruction set, the one that is almost impossible 

to overcome is having too small a memory address. While the x86 was successfully 

ext
 rst to 32-bit addresses and then later to 64-bit addresses, many of its 
brethren wer
  behind. For example, the 16-bit address MOStek 6502 powered the 
Apple II, but even given this headstart with th
 rst commercially successful personal 
computer, its lack of address bits condemned it to the dustbin of history.
ARM architects could see the writing on the wall of their 32-bit address 
computer, and began design of the 64-bit address version of ARM in 2007. It was 

 nally revealed in 2013. Rather than some minor cosmetic changes to make all 
the registers 64 bits wide, which is basically what happened to the x86, ARM did a 

complete overhaul
 e good news is that if you know MIPS it will be very easy to 
pick up ARMv8, as the 64-bit version is called.
First, as compared to MIPS, ARM dropped virtually all of the unusual features 
of v7:
 ere is no conditional executio
 eld, as there was in nearly every instruction 
in v7.regw = 0w = 1r/mmod = 0
mod = 1mod = 2mod = 316b32b16b
32b16b32b16b32b
0ALAXEAX0addr=BX+SI=EAX
samesamesamesamesame
1CLCXECX1addr=BX+DI=ECX
addr as addr as addr as addr as as
2DLDXEDX2addr=BP+SI=EDX
mod=0mod=0mod=0mod=0reg
3BLBXEBX3addr=BP+SI=EBX
+ disp8+ disp8+ disp16+ disp32Þ eld
4AHSPESP4addr=SI=
(sib)SI+disp8(sib)+disp8SI+disp8
(sib)+disp32ﬁ
5CHBPEBP5addr=DI=disp32DI+disp8EBP+disp8DI+disp16EBP+disp32ﬁ
6DHSIESI6addr=disp16=ESIBP+disp8ESI+disp8BP+disp16ESI+disp32ﬁ

7BHDIEDI7addr=BX=EDIBX+disp8EDI+disp8BX+disp16EDI+disp32ﬁ
FIGURE 2.42 The encoding of the ﬁ rst address speciﬁ er of the x86: mod, reg, r/m.
 e 
 rst four columns show the encoding 
of the 3-bit r
 eld, which depends on the w bit from the opcode and whether the machine is in 16-bit mode (8086) or 32-bit mode (80386). 
 e remaining columns explain the mod an
 elds. 
 e meaning of the 3-bi eld depends on the value in the 2-bit mo eld and the 
address size. Basically, the registers used in the address calculation are listed in the sixth and seventh columns, under mod 
 0, with mod 
 1 adding an 8-bit displacement and mod 
 2 adding a 16-bit or 32-bit displacement, depending on the address mode
 e exceptions are 1) r/m 
 6 when mod 
 1 or mod 
 2 in 16-bit mode selects BP plus the displacement; 2) r/m 
 5 when mod 
 1 or mod 
 2 in 32-bit mode selects 
EBP plus displacement; and 3) r/m 
 4 in 32-bit mode when mod does not equal 3, where (sib) means use the scaled index mode shown in 
Figure 2.38
. When mod 
 3, th
 eld indicates a register, using the same encoding as the r
 eld combined with the w bit.

 2.19 Fallacies and Pitfalls 
159 e immediate
 eld is simply a 12 bit constant, rather than essentially an 
input to a function that produces a constant as in v7.
 ARM dropped Load Multiple and Store Multiple instructions.
 e PC is no longer one of the registers, which resulted in unexpected 
branches if you wrote to it.
Second, ARM added missing features that are useful in MIPS
 V8 has 32 general-purpose registers, which compiler writers surely love. Like 
MIPS, one register is hardwired to 0, although in load and store instructions 
it instead refers to the stack pointer.
 Its addressing modes work for all word sizes in ARMv8, which was not the 
case in ARMv7.
 It includes a divide instruction, which was omitted from ARMv7.
 It adds the equivalent of MIPS branch if equal and branch if not equal.
As the philosophy of the v8 instruction set is much closer to MIPS than it is to 
v7, our conclusion is that the main similarity between ARMv7 and ARMv8 is the 

name.
 2.19 Fallacies and Pitfalls
Fallacy: More powerful instructions mean higher performance.
Part of the power of the Intel x86 is the pr
 xes that can modify the execution of 
the following instruction. One pr
 x can repeat the following instruction until a 
counter counts down t
 us, to move data in memory, it would seem that the 
natural instruction sequence is to use move with the repeat pr
 x to perform 32-bit 
memory-to-memory moves.
An alternative method, which uses the standard instructions found in all 
computers, is to load the data into the registers and then store the registers back to 

memory
 is second version of this program, with the code replicated to reduce 
loop overhead, copies at about 1.5 times as fast. A third version, which uses the 

larg
 oating-point registers instead of the integer registers of the x86, copies at 
about 2.0 times as fast than the complex move instruction.
Fallacy: Write in assembly language to obtain the highest performance.
At one time compilers for programming languages produced naïve instruction 

sequences; the increasing sophistication of compilers means the gap between 

compiled code and code produced by hand is closing fast. In fact, to compete 

with current compilers, the assembly la
nguage programmer needs to understand 
the concepts in Chapters 4 and 5 thoroughly (processor pipelining and memory 

hierarchy).

160 Chapter 2 Instructions: Language of the Computer
 is battle between compilers and assembly language coders is another situation 
in which humans are losing ground. For example, C o
 ers the programmer a 
chance to give a hint to the compiler about which variables to keep in registers 
versus spilled to memory. When compilers were poor at register allocation, such 

hints were vital to performance. In fact, some old C textbooks spent a fair amount 

of time giving examples that
 ectively use register hints. Today’s C compilers 
generally ignore such hints, because the compiler does a better job at allocation 

than the programmer does.
Even 
if writing by hand resulted in faster code, the dangers of writing in assembly 
language are the longer time spent coding and debugging, the loss in portability, 

and th
  culty of maintaining such code. One of the few widely accepted axioms 
of s
 ware engineering is that coding takes longer if you write more lines, and it 
clearly takes many more lines to write a program in assembly language than in C 

or Java. Moreover, once it is coded, the next danger is that it will become a popular 

program. Such programs always live longer than expected, meaning that someone 

will have to update the code over several years and make it work with new releases 

of operating systems and new models of machines. Writing in higher-level language 

instead of assembly language not only allows future compilers to tailor the code 

to future machines; it also makes the so
 ware easier to maintain and allows the 
program to run on more brands of computers.
Fallacy:
 e importance of commercial binary compatibility means successful 
instruction sets don’t change.
While backwards binary compatibility is sacrosanct, 
Figure 2.43
 shows that the x86 

architecture has grown dramatically.
 e average is more than one instruction per 
month over its 35-year lifetime!
Pitfall: Forgetting that sequential word addresses in machines with byte addressing 

do not di
 er by one.
Many an assembly language programmer 
has toiled over errors made by assuming 
that the address of the next word can be
 found by incrementing the address in a 
register by one instead of by the word size in bytes. Forewarned is forearmed!
Pitfall: Using a pointer to an automatic variable outside i
 ning procedure.
A common mistake in dealing with pointers is to pass a result from a procedure 

that includes a pointer to an array that is local to that procedure. Following the 

stack discipline in 
Figure 2.12
, the memory that contains the local array will be 

reused as soon as the procedure returns. Pointers to automatic variables can lead 

to chaos.

 2.20 Concluding Remarks 
161 2.20 Concluding Remarks
 e two principles of the 
stored-program
 computer are the use of instructions that 
are indistinguishable from numbers and the use of alterable memory for programs. 
 ese principles allow a single machine to aid environmental scien
 nancial 
advisers, and novelists in their specialt
 e selection of a set of instructions that 
the machine can understand demands a delicate balance among the number of 

instructions needed to execute a program, the number of clock cycles needed by an 

instruction, and the speed of the clock. As illustrated in this chapter, three design 

principles guide the authors of instruction sets in making that delicate balance:
1. Simplicity favors regularity.
 Regularity motivates many features of the MIPS 
instruction set: keeping all instructions a single size, always requiring three 

register operands in arithmetic instructions, and keeping the regist
 elds in the same place in each instruction format.
2. Smaller is faster.
 e desire for speed is the reason that MIPS has 32 registers 
rather than many more.
3. Good design demands good compromises.
 One MIPS example was the 
compromise between providing for larger addresses and constants in 

instructions and keeping all instructions the same length.
Less is more.
Robert Browning, 
Andrea del Sarto
, 18550100200300
400500600
700
8009001000197819801982198419861988199019921994199619982000200220042006200820102012YearNumber of InstructionsFIGURE 2.43 Growth of x86 instruction set over time.
 While there is clear technical value to 
some of these extensions, this rapid change also increases th
  culty for other companies to try to build 
compatible processors.

162 Chapter 2 Instructions: Language of the Computer
We also saw the great idea of making the
 common cast fast
 applied to instruction 
sets as well as computer architecture. Examples of making the common MIPS 
case fast include PC-relative addressing for conditional branches and immediate 

addressing for larger constant operands.
Above this machine level is assembly language, a language that humans can read. 
 e assembler translates it into the binary numbers that machines can understand, 
and it even “extends” the instruction set by creating symbolic instructions that 

aren’t in the hardware. For instance, constants or addresses that are too big are 

broken into properly sized pieces, common variations of instructions are given 

their own name, and so on. 
Figure 2.44
 lists the MIPS instructions we have covered 
 MIPS instructionsNameFormatPseudo MIPSNameFormat
addaddRmove
moveRsubtractsubRmultiply
multRadd immediateaddiImultiply immediate
multiIload word
lwIload immediate
liIstore word
swIbranch less than
bltIload halflhIbranch less than 
or equalbleIload half unsignedlhuIstore half
shIbranch greater than
bgtIload bytelbIbranch greater than 
or equalbgeIload byte unsignedlbuIstore byte
sbIload linkedllIstore conditional
scIload upper immediateluiIand andRororRnornorRand immediateandiIor immediateoriIshift left logicalsllR
shift right logicalsrlR
branch on equalbeqIbranch on not equalbneIset less thansltRset less than immediatesltiIset less than immediate unsignedsltiuIjumpjJjump register
jrRjump and linkjalJFIGURE 2.44 The MIPS instruction set covered so far, with the real MIPS instructions 
on the left and the pseudoinstructions on the right.
 Appendix A (Section A.10) describes the 
full MIPS architecture.
 Figure 2.1
 shows more details of the MIPS architecture revealed in this chapter
 e information given here is also found in Columns 1 and 2 of the MIPS Reference Data Card at the front of 

the book.

 2.21 Historical Perspective and Further Reading 
163so far, both real and pseudoinstructions. Hiding details from the higher level is 
another example of the great idea of 
abstraction
.Each category of MIPS instructions is associated with constructs that appear in 
programming languages:
 Arithmetic instructions correspond to the operations found in assignment 
statements.
 Transfer instructions are most likely to occur when dealing with data 
structures like arrays or structures.
 Conditional branches are used in 
if statements and in loops.
 Unconditional jumps are used in procedure calls and returns and for 
case/
switch
 statements.
 ese instructions are not born equal; the popularity of the few dominates the 
many. For example, 
Figure 2.45
 shows the popularity of each class of instructions 

for SP
 e varying popularity of instructions plays an important role 
in the chapters about datapath, control, and pipelining.
Instruction classMIPS examples
HLL correspondence
Frequency
IntegerFt. pt.
Arithmeticadd,sub,addi
Operations in assignment statements
Data transferlw,sw,lb, lbu, lh, 
lhu,sb,lui
Logicaland,or,nor,andi,ori,
sll,srl
0perations in assignment statements
Conditional branchbeq,bne,slt,slti, 
sltiuIf statements and loopsJumpj,jr,jal
Procedure calls, r
eturns, and 
case/switch statements16%35%12%
34%2%48%36%4%8%
0%References to data structures, such as arraysFIGURE 2.45 MIPS instruction classes, examples, correspondence to high-level program language constructs, and 
percentage of MIPS instructions executed by category for the average integer and ﬂ oating point SPEC CPU2006 
benchmarks.
 Figure 3.26 in Chapter 3 shows average percentage of the individual MIPS instructions executed.
 er we explain computer arithmetic in Chapter 3, we reveal the rest of the 
MIPS instruction set architecture.
   Historical Perspective and Further 
Reading is section surveys the history of 
instruction set architectures
 (ISAs) over time, 
and we give a short history of programming languages and compilers. ISAs 
2.21
164 Chapter 2 Instructions: Language of the Computer
include accumulator architectures, general-purpose register architectures, 
stack architectures, and a brief history of ARM and the x86. We also review the 

controversial subjects of high-level-language computer architectures and reduced 

instruction set computer architectur
 e history of programming languages 
includes Fortran, Lisp, Algol, C, Cobol, Pascal, Simula, Smalltalk, C
, and Java, 
and the history of compilers includes the key milestones and the pioneers who 

achieved th
 e rest of 
 Section 2.21
 is found online.
 2.22 ExercisesAppendix A describes the MIPS simulator, which is helpful for these exercises. 

Although the simulator accepts pseudoinstructions, try not to use pseudoinstructions 

for any exercises that ask you to produce MIPS code. Your goal should be to learn 

the real MIPS instruction set, and if you are asked to count instructions, your 

count should r
 ect the actual instructions that will be executed and not the 
pseudoinstructions.
 ere are some cases where pseudoinstructions must be used (for example, the 
la instruction when an actual value is not known at assembly time). In many cases, 

they are quite convenient and result in more readable code (for example, the 
li and 
move instructions). If you choose to use pseudoinstructions for these reasons, 
please add a sentence or two to your solution stating which pseudoinstructions you 

have used and why.
2.1 [5] <§2.2> For the following C statement, what is the corresponding MIPS 
assembly code? Assume that the variables 
f, g, h, and 
i are given and could be 
considered 32-bit integers as declared in a C program. Use a minimal number of 
MIPS assembly instructions.
f = g + (h − 5);2.2 [5] <§2.2> For the following MIPS assembly instructions above, what is a 
corresponding C statement?
add  f, g, hadd  f, i, f
 2.22 Exercises 1652.3 [5] <§§2.2, 2.3> For the following C statement, what is the corresponding 
MIPS assembly code? Assume that the variables 
f, g, h, i, and 
j are assigned to 
registers 
$s0, $s1, $s2, $s3, and 
$s4, respectively. Assume that the base address 
of the arrays 
A and 
B are in registers 
$s6 and 
$s7, respectively.
B[8] = A[i−j];2.4 [5] <§§2.2, 2.3> For the MIPS assembly instructions below, what is the 
corresponding C statement? Assume that the variables 
f, g, h, i, and 
j are assigned 
to registers 
$s0, $s1, $s2, $s3, and 
$s4, respectively. Assume that the base address 
of the arrays A and B are in registers 
$s6 and 
$s7, respectively.
sll  $t0, $s0, 2     # $t0 = f * 4add  $t0, $s6, $t0   # $t0 = &A[f]
sll  $t1, $s1, 2     # $t1 = g * 4
add  $t1, $s7, $t1   # $t1 = &B[g]
lw   $s0, 0($t0)     # f = A[f]
addi $t2, $t0, 4
lw   $t0, 0($t2)
add  $t0, $t0, $s0
sw   $t0, 0($t1)2.5 [5] <§§2.2, 2.3> For the MIPS assembly instructions in Exercise 2.4, rewrite 
the assembly code to minimize the number if MIPS instructions (if possible) 
needed to carry out the same function.
2.6  e table below shows 32-bit values of an array stored in memory.
AddressData
242

384

323

366

401

166 Chapter 2 Instructions: Language of the Computer
2.6.1 [5] <§§2.2, 2.3> For the memory locations in the table above, write C 
code to sort the data from lowest to highest, placing the lowest value in the 

smallest memory location shown in the figure. Assume that the data shown 

represents the C variable called 
Array, which is an array of type 
int, and that 
the first number in the array shown is the first element in the array. Assume 

that this particular machine is a byte-addressable machine and a word consists 

of four bytes.
2.6.2 [5] <§§2.2, 2.3> For the memory locations in the table above, write MIPS 
code to sort the data from lowest to highest, placing the lowest value in the smallest 

memory location. Use a minimum number of MIPS instructions. Assume the base 

address of 
Array is stored in register 
$s6.2.7 [5] <§2.3> Show how the value 
0xabcdef12 would be arranged in memory 
of a little-endian and a big-endian machine. Assume the data is stored starting at 

address 0.
2.8 [5] <§2.4> Translate 
0xabcdef12 into decimal.
2.9 [5] <§§2.2, 2.3> Translate the following C code to MIPS. Assume that the 
variables 
f, g, h, i, and 
j are assigned to registers 
$s0, $s1, $s2, $s3, and 
$s4, respectively. Assume that the base address of the arrays 
A and 
B are in registers 
$s6 and 
$s7, respectively. Assume that the elements of the arrays 
A and 
B are 4-byte 
words:
B[8] = A[i] + A[j];2.10 [5] <§§2.2, 2.3> Translate the following MIPS code to C. Assume that the 
variables 
f, g, h, i, and 
j are assigned to registers 
$s0, $s1, $s2, $s3, and 
$s4, respectively. Assume that the base address of the arrays 
A and 
B are in registers 
$s6 and 
$s7, respectively.
addi $t0, $s6, 4add  $t1, $s6, $0
sw   $t1, 0($t0)
lw   $t0, 0($t0)
add  $s0, $t1, $t02.11 [5] <§§2.2, 2.5> For each MIPS instruction, show the value of the opcode 
(OP), source register (RS), and target register (R
 elds. For the I-type instructions, 
show the value of the immediat
 eld, and for the R-type instructions, show the 
value of the destination regist
 eld.
 2.22 Exercises 1672.12 Assume that registers 
$s0 and 
$s1 hold the values 
0x80000000 and 
0xD0000000, respectively. 
2.12.1 [5] <§2.4> What is the value of 
$t0 for the following assembly code?
add $t0, $s0, $s12.12.2 [5] <§2.4> Is the result in 
$t0 the desired result, or has there been over
 ow?
2.12.3 [5] <§2.4> For the contents of registers 
$s0 and 
$s1 as sp
 ed above, 
what is the value of 
$t0 for the following assembly code?
sub $t0, $s0, $s12.12.4 [5] <§2.4> Is the result in 
$t0 the desired result, or has there been over
 ow?
2.12.5 [5] <§2.4> For the contents of registers 
$s0 and 
$s1 as sp
 ed above, 
what is the value of 
$t0 for the following assembly code?
add $t0, $s0, $s1add $t0, $t0, $s02.12.6 [5] <§2.4> Is the result in 
$t0 the desired result, or has there been 
over
 ow?
2.13 Assume that 
$s0 holds the value 128
ten
.2.13.1 [5] <§2.4> For the instruction 
add $t0, $s0, $s1, what is the range(s) of 
values for 
$s1 that would result in over
 ow?
2.13.2 [5] <§2.4> For the instruction 
sub $t0, $s0, $s1, what is the range(s) of 
values for 
$s1 that would result in over
 ow?
2.13.3 [5] <§2.4> For the instruction 
sub $t0, $s1, $s0, what is the range(s) of 
values for
 $s1 that would result in over
 ow?
2.14 [5] <§§2.2, 2.5> Provide the type and assembly language instruction for the 
following binary value: 
0000 0010 0001 0000 1000 0000 0010 0000two2.15 [5] <§§2.2, 2.5> Provide the type and hexadecimal representation of 
following instruction: 
sw $t1, 32($t2)
168 Chapter 2 Instructions: Language of the Computer
2.16 [5] <§2.5> Provide the type, assembly language instruction, and binary 
representation of instruction described by the followin
 elds:op=0, rs=3, rt=2, rd=3, shamt=0, funct=342.17 [5] <§2.5> Provide the type, assembly language instruction, and binary 
representation of instruction described by the followin
 elds:op=0x23, rs=1, rt=2, const=0x42.18 Assume that we would like to expand the MIPS regist
 le to 128 registers 
and expand the instruction set to contain four times as many instructions.
2.18.1 [5] <§2.5> How this would this a
 ect the size of each of the bit
 elds in 
the R-type instructions?

2.18.2 [5] <§2.5> How this would this a
 ect the size of each of the bit
 elds in 
the I-type instructions?

2.18.3 [5] <§§2.5, 2.10> How could each of the two proposed changes decrease 
the size of an MIPS assembly program? On the other hand, how could the proposed 

change increase the size of an MIPS assembly program?
2.19 Assume the following register contents:
$t0 = 0xAAAAAAAA, $t1 = 0x123456782.19.1 [5] <§2.6> For the register values shown above, what is the value of 
$t2 for the following sequence of instructions?
sll $t2, $t0, 44or  $t2, $t2, $t12.19.2 [5] <§2.6> For the register values shown above, what is the value of 
$t2 for the following sequence of instructions?
sll  $t2, $t0, 4
andi $t2, $t2, −12.19.3 [5] <§2.6> For the register values shown above, what is the value of 
$t2 for the following sequence of instructions?
srl  $t2, $t0, 3
andi $t2, $t2, 0xFFEF
 2.22 Exercises 1692.20 [5] <§2.6> Find the shortest sequence of MIPS instructions that extracts bits 
16 down to 11 from register 
$t0 and uses the value of t
 eld to replace bits 31 
down to 26 in register 
$t1 without changing the other 26 bits of register 
$t1.2.21 [5] <§2.6> Provide a minimal set of MIPS instructions that may be used to 
implement the following pseudoinstruction:
not $t1, $t2      // bit-wise invert2.22 [5] <§2.6> For the following C statement, write a minimal sequence of MIPS 
assembly instructions that does the identical operation. Assume 
$t1 = A, $t2 = B, and 
$s1 is the base address of C.
A = C[0] << 4;2.23 [5] <§2.7> Assume 
$t0 holds the value 
0x00101000. What is the value of 
$t2 er the following instructions?
slt  $t2, $0,  $t0bne  $t2, $0,  ELSE
j    DONEELSE: addi $t2, $t2, 2

DONE:2.24 [5] <§2.7> Suppose the program counter (PC) is set to 
0x2000 0000. Is it 
possible to use the jump (j) MIPS assembly instruction to set the PC to the address 
as 0x4000 0000? Is it possible to use the branch-on-equal (beq) MIPS assembly 
instruction to set the PC to this same address?
2.25  e following instruction is not included in the MIPS instruction set:
rpt $t2, loop # if(R[rs]>0) R[rs]=R[rs]−1, PC=PC+4+BranchAddr2.25.1 [5] <§2.7> If this instruction were to be implemented in the MIPS 
instruction set, what is the most appropriate instruction format?
2.25.2 [5] <§2.7> What is the shortest sequence of MIPS instructions that 
performs the same operation?

170 Chapter 2 Instructions: Language of the Computer
2.26 Consider the following MIPS loop:
LOOP: slt  $t2, $0,  $t1beq  $t2, $0,  DONEsubi $t1, $t1, 1
addi $s2, $s2, 2
j    LOOPDONE:2.26.1 [5] <§2.7> Assume that the register 
$t1 is initialized to the value 10. What 
is the value in register 
$s2 assuming 
$s2 is initially zero?
2.26.2 [5] <§2.7> For each of the loops above, write the equivalent C code 
routine. Assume that the registers 
$s1, $s2, $t1, and 
$t2 are integers 
A, B, i, and 
temp, respectively.
2.26.3 [5] <§2.7> For the loops written in MIPS assembly above, assume that 
the register 
$t1 is initialized to the value N. 
How many MIPS instructions are 
executed?
2.27 [5] <§2.7> Translate the following C code to MIPS assembly code. Use a 
minimum number of instructions. Assume that the values of 
a, b, i, and 
j are in 
registers 
$s0, $s1, $t0, and 
$t1, respectively. Also, assume that register 
$s2 holds 
the base address of the array D.
for(i=0; i<a; i++)for(j=0; j<b; j++)D[4*j] = i + j;2.28 [5] <§2.7> How many MIPS instructions does it take to implement the C 
code from Exercise 2.27? If the variables a and b are initialized to 10 and 1 and all 
elements of D are initially 0, what is the total number of MIPS instructions that is 

executed to complete the loop?
2.29 [5] <§2.7> Translate the following loop into C. Assume that the C-level 
integer 
i is held in register 
$t1, $s2 holds the C-level integer called 
result, and 
$s0 holds the base address of the integer 
MemArray.      addi $t1, $0, $0LOOP: lw   $s1, 0($s0)
      add  $s2, $s2, $s1
      addi $s0, $s0, 4
 2.22 Exercises 171      addi $t1, $t1, 1      slti $t2, $t1, 100
      bne  $t2, $s0, LOOP2.30 [5] <§2.7> Rewrite the loop from Exercise 2.29 to reduce the number of 
MIPS instructions executed.
2.31 [5] <§2.8> Implement the following C code in MIPS assembly. What is the 
total number of MIPS instructions needed to execute the function?
int fib(int n){    if (n==0)
       return 0;
    else if (n == 1)
       return 1;
    else
       return fib(n−1) + fib(n−2);2.32 [5] <§2.8> Functions can o
 en be implemented by compilers “in-line.” An 
in-line function is when the body of the function is copied into the program space, 
allowing the overhead of the function call to be eliminated. Implement an “in-line” 

version of the C code above in MIPS assembly. What is the reduction in the total 

number of MIPS assembly instructions needed to complete the function? Assume 

that the C variable n is initialized to 5.
2.33 [5] <§2.8> For each function call, show the contents of the stack a
 er the 
function call is made. Assume the stack pointer is originally at addr

 c, and follow the register conventions as sp
 ed in Figure 2.11.
2.34 Translate function 
f into MIPS assembly language. If you need to use 
registers 
$t0 through 
$t7, use the lower-numbered register
 rst. Assume the 
function declaration for 
func is “int f(int a, int b);”
 e code for function 
f is as follows:
int f(int a, int b, int c, int d){  return func(func(a,b),c+d);
}
172 Chapter 2 Instructions: Language of the Computer
2.35 [5] <§2.8> Can we use the tail-call optimization in this function? If no, 
explain why not. If yes, what is th
 erence in the number of executed instructions 
in f with and without the optimization?
2.36 [5] <§2.8> Right before your function 
f from Exercise 2.34 returns, what do 
we know about contents of registers 
$t5, $s3, $ra, and 
$sp? Keep in mind that 
we know what the entire function 
f looks like, but for function 
func we only know 
its declaration.

2.37 [5] <§2.9> Write a program in MIPS assembly language to convert an ASCII 
number string containing positive and negative integer decimal strings, to an 
integer. Your program should expect register 
$a0 to hold the address of a null-
terminated string containing some combination of the digits 0 through 9. Your 

program should compute the integer value equivalent to this string of digits, then 

place the number in register 
$v0. If a non-digit character appears anywhere in the 
string, your program should stop
 with the value −1 in register 
$v0. For example, 
if register 
$a0 points to a sequence of three bytes 50ten, 52ten, 0ten (the null-
terminated string “24”), then when the program stops, register 
$v0 should contain 
the value 24
ten
.2.38 [5] <§2.9> Consider the following code:
lbu $t0, 0($t1)sw  $t0, 0($t2)Assume that the register 
$t1 contains the address 
0x1000 0000 and the register 
$t2 contains the address 
0x1000 0010. Note the MIPS architecture utilizes 
big-endian addressing. Assume that the data (in hexadecimal) at address 
0x1000 0000 is: 0x11223344. What value is stored at the address pointed to by register 
$t2?2.39 [5] <§2.10> Write the MIPS assembly code that creates the 32-bit constant 
0010 0000 0000 0001 0100 1001 0010 0100two and stores that value to 
register 
$t1.2.40 [5] <§§2.6, 2.10> If the current value of the PC is 
0x00000000, can you use 
a single jump instruction to get to the PC address as shown in Exercise 2.39?
2.41 [5] <§§2.6, 2.10> If the current value of the PC is 
0x00000600, can you use 
a single branch instruction to get to the PC address as shown in Exercise 2.39?

 2.22 Exercises 1732.42 [5] <§§2.6, 2.10> If the current value of the PC is 
0x1FFFf000, can you use 
a single branch instruction to get to the PC address as shown in Exercise 2.39?
2.43 [5] <§2.11> Write the MIPS assembly code to implement the following C 
code:
      lock(lk);      shvar=max(shvar,x);
      unlock(lk);Assume that the address of the 
lk variable is in 
$a0, the address of the shvar 
variable is in 
$a1, and the value of variable 
x is in $a2. Your critical section should 
not contain any function calls. Use 
ll/sc instructions to implement the 
lock() operation, and the 
unlock() operation is simply an ordinary store instruction.
2.44 [5] <§2.11> Repeat Exercise 2.43, but this time use 
ll/sc to perform 
an atomic update of the 
shvar variable directly, without using 
lock() and 
unlock(). Note that in this problem there is no variable 
lk.2.45 [5] <§2.11> Using your code from Exercise 2.43 as an example, explain what 
happens when two processors begin to execute this critical section at the same 
time, assuming that each processor executes exactly one instruction per cycle.
2.46 Assume for a given processor the CPI of arithmetic instructions is 1, 
the CPI of load/store instructions is 10, and the CPI of branch instructions is 
3. Assume a program has the following instruction breakdowns: 500 million 

arithmetic instructions, 300 million load/store instructions, 100 million branch 

instructions.
2.46.1 [5] <§2.19> Suppose that new, more powerful arithmetic instructions are 
added to the instruction set. On average, through the use of these more powerful 

arithmetic instructions, we can reduce the number of arithmetic instructions 

needed to execute a program by 25%, and the cost of increasing the clock cycle 

time by only 10%. Is this a good design choice? Why?
2.46.2 [5] <§2.19> Suppose that w
 nd a way to double the performance of 
arithmetic instructions. What is the overall speedup of our machine? What if we 
 nd a way to improve the performance of arithmetic instructions by 10 times?
2.47 Assume that for a given program 70% of the executed instructions are 
arithmetic, 10% are load/store, and 20% are branch.

174 Chapter 2 Instructions: Language of the Computer
2.47.1 [5] <§2.19> Given this instruction mix and the assumption that an 
arithmetic instruction requires 2 cycles, a load/store instruction takes 6 cycles, and 

a branch instruction takes 3 cyc
 nd the average CPI.
2.47.2 [5] <§2.19> For a 25% improvement in performance, how many cycles, on 

average, may an arithmetic instruction take
 if load/store and branch instructions 
are not improved at all?
2.47.3 [5] <§2.19> For a 50% improvement in performance, how many cycles, on 
average, may an arithmetic instruction take
 if load/store and branch instructions 
are not improved at all?
§2.2, page 66: MIPS, C, Java
§2.3, page 72: 2) Very slow

§2.4, page 79: 2) 
8ten
§2.5, page 87: 4) sub 
$t2, $t0, $t1§2.6, page 89: Both. AND with a mask pattern of 1s will leaves 0s everywhere but 

the desir
 eld. Shi
 ing 
  by the correct amount removes the bits from th
  of th
 eld. Shi
 ing right by the appropriate amount puts th
 eld into the right-
most bits of the word, with 0s in the rest of the word. Note that AND leaves the 

 eld where it was originally, and th  pair moves th
 eld into the rightmost 
part of the word.

§2.7, page 96: I. All are true. II. 1).

§2.8, page 106: Both are true.

§2.9, page 111: I. 1) and 2) II. 3)

§2.10, page 120: I. 4) 
128K. II. 6) a block of 256M. III. 4) 
sll
§2.11, page 123: Both are true.

§2.12, page 132: 4) Machine independence.
Answers to 
Check Yourself


3Numerical precision 
is the very soul of 

science.
Sir D’arcy Wentworth Thompson 
On Growth and Form,
 1917Arithmetic for Computers
3.1 Introduction 1783.2 Addition and Subtraction 
1783.3 Multiplication 1833.4 Division 1893.5 Floating Point 
1963.6 Parallelism and Computer Arithmetic: 
Subword Parallelism 
2223.7 Real Stuff:  Streaming SIMD Extensions and 
Advanced Vector Extensions in x86 
224Computer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
3.8 Going Faster:  Subword Parallelism and Matrix Multiply 
2253.9 Fallacies and Pitfalls 
2293.10 Concluding Remarks 
2323.11 Historical Perspective and Further Reading 
2363.12 Exercises 237The Five Classic Components of a Computer

178 Chapter 3 Arithmetic for Computers
 3.1 IntroductionComputer words are composed of bits; thus, words can be represented as binary 
numbers. Chapter 2 shows that integers can be represented either in decimal or 

binary form, but what about the other numbers that commonly occur? For example:
 What about fractions and other real numbers?
 What happens if an operation creates a number bigger than can be represented?
 And underlying these questions is a mystery: How does hardware really 
multiply or divide numbers?
 e goal of this chapter is to unravel these mysteries including representation of 
real numbers, arithmetic algorithms, hardware that follows these algorithms, and 

the implications of all this for instruction s
 ese insights may explain quirks 
that you have already encountered with computers. Moreover, we show how to use 

this knowledge to make arithmetic-in
tensive programs go much faster.
 3.2 Addition and Subtraction
Addition is just what you would expect in computers. Digits are added bit by bit 

from right to , with carries passed to the next digit to th
 , just as you would 
do by hand. Subtraction uses addition: the appropriate operand is simply negated 

before being added.
Binary Addition and Subtraction
Let’s try adding 6
ten
 to 7
ten
 in binary and then subtracting 6
ten
 from 7
ten
 in binary.
 0000 0000 0000 0000 0000 0000 0000 0111
two = 7ten+ 0000 0000 0000 0000 0000 0000 0000 0110
two = 6ten= 0000 0000 0000 0000 0000 0000 0000 1101
two = 13ten e 4 bits to the right have all the action; 
Figure 3.1
 shows the sums and 
carr
 e carries are shown in parentheses, with the arrows showing how 
they are passed.
Subtracting 6
ten
 from 7
ten
 can be done directly:
Subtraction: Addition’s 
Tricky Pal
No. 10, Top Ten 
Courses for Athletes at a 

Football Factory, David 

Letterman et al.,
 Book of 
Top Ten Lists,
 1990EXAMPLEANSWER
 3.2 Addition and Subtraction 
179 0000 0000 0000 0000 0000 0000 0000 0111
two = 7ten– 0000 0000 0000 0000 0000 0000 0000 0110
two = 6ten= 0000 0000 0000 0000 0000 0000 0000 0001
two = 1tenor via addition using the two’s complement representation of 
6: 0000 0000 0000 0000 0000 0000 0000 0111
two = 7ten+ 1111 1111 1111 1111 1111 1111 1111 1010
two = –6ten= 0000 0000 0000 0000 0000 0000 0000 0001
two = 1ten(0)00
0(0)(0)0
0
0(0)(1)0
0
1(1)(1)1
1
1(1)(0)1
1
0(0)(Carries)

1
0
1(0). . .. . .. . .FIGURE 3.1 Binary addition, showing carries from right to left.
 e rightmost bit adds 1 
to 0, resulting in the sum of this bit being 1 and the carry out from this bit being 0. Hence, the operation 
for the second digit to the right is 0 
 1  is generates a 0 for this sum bit and a carry out o
 e third digit is the sum of 1 
 1  1, resulting in a carry out of 1 and a sum bit o
 e fourth bit is 1 
 0  0, yielding a 1 sum and no carry.
Recall that ov
 ow occurs when the result from an operation cannot be 
represented with the available hardware, in this case a 32-bit word. When can 
over
 ow occur in addition? When adding operands wit
 erent signs, over
 ow 
cannot occur.
 e reason is the sum must be no larger than one of the operands. 
For example, 
10  4  6. Since the opera
 t in 32 bits and the sum is no 
larger than an operand, the sum mu
 t in 32 bits as well
 erefore, no over
 ow 
can occur when adding positive and negative operands.
 ere are similar restrictions to the occurrence of over
 ow during subtract, but 
it’s just the opposite principle: when the signs of the operands are the 
same
, over
 ow 
cannot occur. To see this, remember that 
c  a  c  (a) because we subtract by 
negating the second operand and then add
 erefore, when we subtract operands 
of the same sign we end up by 
adding
 operands of 
 erent
 signs. From the prior 
paragraph, we know that over
 ow cannot occur in this case either.
Knowing when over
 ow cannot occur in addition and subtraction is all well and 
good, but how do we detect it when it 
does occur? Clearly, adding or subtracting 
two 32-bit numbers can yield a result that needs 33 bits to be fully expressed.
 e lack of a 33rd bit means that when over
 ow occurs, the sign bit is set with 
the 
value
 of the result instead of the proper sign of the result. Since we need just one 
extra bit, only the sign bit can be wrong. Hence, over
 ow occurs when adding two 
positive numbers and the sum is negative, or vice vers
 is spurious sum means 
a carry out occurred into the sign bit.
Over
 ow occurs in subtraction when we subtract a negative number from a 
positive number and get a negative result, or when we subtract a positive number 

from a negative number and get a positive result. Such a ridiculous result means a 

borrow occurred from the sign bit. 
Figure 3.2
 shows the combination of operations, 

operands, and results that indicate an over
 ow.

180 Chapter 3 Arithmetic for Computers
We have just seen how to detect over
 ow for two’s complement numbers in a 
computer. What about over
 ow with unsigned integers? Unsigned integers are 
commonly used for memory addresses where over
 ows are ignored.
 e computer designer must therefore provide a way to ignore over
 ow in 
some cases and to recognize it in other
 e MIPS solution is to have two kinds of 
arithmetic instructions to recognize the two choices:
 Add (
add), add immediate (
addi), and subtract (
sub) cause exceptions on 
over
 ow.
 Add unsigned (
addu), add immediate unsigned (
addiu), and subtract 
unsigned (
subu) do not cause exceptions on over
 ow.
Because C ignores over
 ows, the MIPS C compilers will always generate the 
unsigned versions of the arithmetic instructions 
addu, addiu, and 
subu, no matter what the type of the variab
 e MIPS Fortran compilers, however, pick 
the appropriate arithmetic instructions, depending on the type of the operands.
 Appendix B
 describes the hardware that performs addition and subtraction, 
which is called an 
Arithmetic Logic Unit
 or 
ALU
.Elaboration: A constant source of confusion for 
addiu is its name and what happens  eld. The u stands for unsigned, which means addition cannot cause an 
o ow exception. However, eld is sign extended to 32 bits, just 

like 
addi, slti, and 
sltiu. Thus, eld is signed, even if the operation 
is “unsigned.”
 e computer designer must decide ho
w to handle arithmetic over
 ows. Although 
some languages like C and Java ignore integer over
 ow, languages like Ada and 
Fortran require that the program be no
 ed. 
 e programmer or the programming 
environment must then decide what to do when over
 ow occurs.
MIPS detects over
 ow with an 
exception
, also called an 
interrupt
 on many 
computers. An exception or interrupt is essentially an unscheduled procedure 
call
 e address of the instruction that over
 owed is saved in a register, and the 
computer jumps to a pr
 ned address to invoke the appropriate routine for that 
exceptio
 e interrupted address is saved so that in some situations the program 
can continue a
 er corrective code is executed. (Section 4.9 covers exceptions in 
Arithmetic Logic 
Unit (ALU)
 Hardware 
that performs addition, 
subtraction, and usually 

logical operations such as 

AND and OR.
Hardware/
Software 
Interfaceexception
 Also 
called interrupt 
on 
many computers
. An 
unscheduled event 

that disrupts program 

execution; used to detect 

over
 ow.
FIGURE 3.2 Overﬂ ow conditions for addition and subtraction.
OperationOperand AOperand B
Result indicating overßowA + B 0 0< 0A + B< 0< 0 0A Œ B 0< 0< 0
A Œ B< 0 0 0
 3.2 Addition and Subtraction 
181more detail; Chapter 5 describes other situations where exceptions and interrupts 
occur.)
MIPS includes a register called the 
exception program counter
 (EPC) to contain 
the address of the instruction that caused the exceptio
 e instruction 
move from 
system control
 (mfc0) is used to copy EPC into a general-purpose register so that 
MIPS so
 ware has the option of returning to the o
 ending instruction via a jump 
register instruction.
Summary
A major point of this section is that, independent of the representation, th
 nite 
word size of computers means that arithmetic operations can create results that 

are too large to
 t in t
 xed word size. It’s easy to detect over ow in unsigned 
numbers, although these are almost always ignored because programs don’t want to 

detect over
 ow for address arithmetic, the most common use of natural numbers. 
Two’s complement presents a greater challenge, yet some so
 ware systems require 
detection of over
 ow, so today all computers have a way to detect it.
Some programming languages allow two’s complement integer arithmetic 
on variables declared byte and half, whereas MIPS only has integer arithmetic 

operations on full words. As we recall from Chapter 2, MIPS does have data transfer 

operations for bytes and halfwords. What MIPS instructions should be generated 

for byte and halfword arithmetic operations?
1. Load with 
lbu, lhu; arithmetic with 
add, sub, mult, div; then store using 
sb, sh.2. Load with 
lb, lh; arithmetic with 
add, sub, mult, div; then store using 
sb, sh.3. Load with 
lb, lh; arithmetic with 
add, sub, mult, div, using 
AND to mask 
result to 8 or 16 bits a
 er each operation; then store using 
sb, sh.Elaboration: One feature not generally found in general-purpose microprocessors is 
saturating operations. Saturation means that when a calculation o ows, the result 
is set to the largest positive number or most negative number, rather than a modulo 

calculation as in two’s complement arithmetic. Saturation is likely what you want for media 

operations. For example, the volume knob on a radio set would be frustrating if, as you 

turned it, the volume would get continuously louder for a while and then immediately very 

soft. A knob with saturation would stop at the highest volume no matter how far you turned 

it. Multimedia extensions to standard instruction sets often offer saturating arithmetic.
Elaboration: MIPS can trap on o ow, but unlike many other computers, there is 
no conditional branch to test o ow. A sequence of MIPS instructions can discover 
interrupt
 An exception 
that comes from outside 
of the processor. (Some 

architectures use the 

term 
interrupt
 for all 
exceptions.)
Check Yourself

182 Chapter 3 Arithmetic for Computers
o ow. For signed addition, the sequence is the following (see the 
Elaboration on page 89 in Chapter 2 for a description of the xor instruction):
addu $t0, $t1, $t2 # $t0 = sum, but don’t trapxor  $t3, $t1, $t2 # Check if signs differ
slt  $t3, $t3, $zero # $t3 = 1 if signs differ

                             # so no overflow
xor $t3, $t0, $t1 # signs =; sign of sum match too?
                  # $t3 negative if sum sign different
slt $t3, $t3, $zero # $t3 = 1 if sum sign different
For unsigned addition (
$t0 = $t1 + $t2), the test is
addu $t0, $t1, $t2     # $t0 = sum
nor $t3, $t1, $zero    # $t3 = NOT $t1
                       # (2’s comp – 1: 232 – $t1 – 1)sltu $t3, $t3, $t2     # (232 – $t1 – 1) < $t2                       #  232 – 1 < $t1 + $t2bne $t3,$zero,Overflow # if(232–1<$t1+$t2) goto overflowElaboration: In the preceding text, we said that you copy EPC into a register via 
mfc0 and then return to the interrupted code via jump register. This directive leads to 
 rst transfer EPC to a register to use with jump 

register, how can jump register return to the interrupted code 
and restore the original values of all registers? Either you restore the old register rst, thereby destroying your 

return address from EPC, which you placed in a register for use in jump register, or you 

restore all registers but the one with the return address so that you can jump—meaning 

an exception would result in changing that one register at any time during program 

execution! Neither option is satisfactory.
To rescue the hardware from this dilemma, MIPS programmers agreed to reserve 
registers 
$k0 and $k1 for the operating system; these registers are 
not restored on exceptions. Just as the MIPS compilers avoid using register 
$at so that the assembler can use it as a temporary register (see 
Hardware
/Software Interface
 in Section 2.10), 

compilers also abstain from using registers 
$k0 and $k1 to make them available for the 

operating system. Exception routines place the return address in one of these registers 

and then use jump register to restore the instruction address.
Elaboration: The speed of addition is increased by determining the carry in to the 
high-order bits sooner. There are a variety of schemes to anticipate the carry so that 

the worst-case scenario is a function of the log
2 of the number of bits in the adder. 
These anticipatory signals are faster because they go through fewer gates in sequence, 

but it takes many more gates to anticipate the proper carry. The most popular is 
carry 
lookahead, which Section B.6 in 
 Appendix B describes.
 3.3 Multiplication 183 3.3 MultiplicationNow that we have completed the explanation of addition and subtraction, we are 
ready to build the more vexing operation of multiplication.
First, let’s review the multiplication of decimal numbers in longhand to remind 
ourselves of the steps of multiplication and the names of the operands. For reasons 

that will become clear shortly, we limit this decimal example to using only the 

digits 0 and 1. Multiplying 1000
ten
 by 1001
ten
:Multiplicand
1000tenMultiplier
x 1001ten1000000000001000Product
1001000ten e 
 rst operand is called the 
multiplicand
 and the second the 
multiplier
.  e 
 nal result is called the 
product
. As you may recall, the algorithm learned in 
grammar school is to take the digits of the multiplier one at a time from right to 

 , multiplying the multiplicand by the single digit of the multiplier, an
 ing 
the intermediate product one digit to th
  of the earlier intermediate products.
 e 
 rst observation is that the number of digits in the product is considerably 
larger than the number in either the multiplicand or the multiplier. In fact, if we 

ignore the sign bits, the length of the multiplication of an 
n-bit multiplicand and an 
m-bit multiplier is a product that is 
n  m bits long
 at is, 
n  m bits are required 
to represent all possible products. Hence, like add, multiply must cope with 

over
 ow because we frequently want a 32-bit product as the result of multiplying 
two 32-bit numbers.
In this example, we restricted the decimal digits to 0 and 1. With only two 
choices, each step of the multiplication is simple:
1. Just place a copy of the multiplicand (1 
 multiplicand) in the proper place 
if the multiplier digit is a 1, or
2. Place 0 (0 
 multiplicand) in the proper place if the digit is 0.
Although the decimal example above happens to use only 0 and 1, multiplication 
of binary numbers must always use 0 and 1, and thus always o
 ers only these two 
choices.
Now that we have reviewed the basics of multiplication, the traditional next 
step is to provide the highly optimized multiply hardware. We break with tradition 

in the belief that you will gain a better understanding by seeing the evolution of 

the multiply hardware and algorithm through multiple generations. For now, let’s 

assume that we are multiplying only positive numbers.
Multiplication is 

vexation, Division is 

as ba
 e rule of 
three doth puzzle me, 

And practice drives me 

mad.
Anonymous, 
Elizabethan manuscript, 

1570
184 Chapter 3 Arithmetic for Computers
Sequential Version of the Multiplication Algorithm and 
Hardware
 is design mimics the algorithm we learned in grammar school; 
Figure 3.3
 shows 
the hardware. We have drawn the hardware so that dat
 ows from top to bottom 
to resemble more closely the paper-and-pencil method.
Let’s assume that the multiplier is in the 32-bit Multiplier register and that the 64-
bit Product register is initialized to 0. From the paper-and-pencil example above, 
it’s clear that we will need to move the multiplican
  one digit each step, as it may 
be added to the intermediate products. Over 32 steps, a 32-bit multiplicand would 

move 32 bits to th
 . Hence, we need a 64-bit Multiplicand register, initialized 
with the 32-bit multiplicand in the right half and zero in th
 
 is register 
is th
 ed 
  1 bit each step to align the multiplicand with the sum being 
accumulated in the 64-bit Product register.
Figure 3.4
 shows the three basic steps needed for each bi e le
 cant 
bit of the multiplier (Multiplier0) determines whether the multiplicand is added to 

the Product register
 e 
  
  in step 2 has th
 ect of moving the intermediate 
operands to th
 , just as when multiplying with paper and pencil
 e 
  right 
in step 3 gives us the next bit of the multiplier to examine in the following iteration. 

 ese three steps are repeated 32 times to obtain the product. If each step took a 
clock cycle, this algorithm would require almost 100 clock cycles to multiply two 

32-bit number
 e relative importance of arithmetic operations like multiply 
varies with the program, but addition and subtraction may be anywhere from 5 to 

100 times more popular than multiply. Accordingly, in many applications, multiply 

can take multiple clock cycles withou
 cantly a
 ecting performance. Yet 
Amdahl’s Law (see Section 1.10) reminds us that even a moderate frequency for a 

slow operation can limit performance.
MultiplicandShift left64 bits64-bit ALUProductWrite
64 bitsControl testMultiplierShift right
32 bitsFIGURE 3.3 First version of the multiplication hardware.
  e Multiplicand register, ALU, 
and Product register are all 64 bits wide, with only the Multiplier register containing 32 bits. (Appendix B 
describes ALU
 e 32-bit multiplicand starts in the right half of the Multiplicand register an
 ed 
  1 bit on each step
 e multip
 ed in the opposite direction at each step
 e algorithm starts with 
the product initialized to 0. Control decides when to
  the Multiplicand and Multiplier registers and when 
to write new values into the Product register.

 3.3 Multiplication 185 is algorithm and hardware are easily r
 ned to take 1 clock cycle per step. 
 e speed-up comes from performing the operations in parallel: the multiplier 
and multiplicand ar
 ed while the multiplicand is added to the product if the 
multiplier bi
 e hardware just has to ensure that it tests the right bit of 
the multiplier and gets the pr
 ed version of the multiplicand.
 e hardware is 
usually further optimized to halve the width of the adder and registers by noticing 
where there are unused portions of registers and adders. 
Figure 3.5
 shows the 

revised hardware.
32nd repetition?1a.  Add multiplicand to product and
place the result in Product registerMultiplier0 = 01.  Test
Multiplier0Start
Multiplier0 = 12.  Shift the Multiplicand register left 1 bit
3.  Shift the Multiplier register right 1 bit
No: < 32 repetitions
Yes: 32 repetitions
DoneFIGURE 3.4 The ﬁ rst multiplication algorithm, using the hardware shown in Figure 3.3.
 If 
the le
 cant bit of the multiplier is 1, add the multiplicand to the product. If not, go to the next step. 
S
  the multiplican
  and the multiplier right in the next two st
 ese three steps are repeated 32 
times.
186 Chapter 3 Arithmetic for Computers
Replacing arithmetic b
 s can also occur when multiplying by constants. Some 
compilers replace multiplies by short constants with a series o
 s and adds. 
Because one bit to th
  represents a number twice as large in bas
 ing 
the bi
  has the same e
 ect as multiplying by a power of 2. As mentioned in 
Chapter 2, almost every compiler will perform the strength reduction optimization 
of substitutin
  
  for a multiply by a power of 2.
A Multiply AlgorithmUsing 4-bit numbers to save space, multiply 2
ten
  3ten
, or 0010
two
  0011two
.Figure 3.6
 shows the value of each register for each of the steps labeled 

according to 
Figure 3.4
, with th
 nal value of 0000 0110
two
 or 6
ten
. Color is 
used to indicate the register values that change on that step, and the bit circled 

is the one examined to determine the operation of the next step.
Hardware/
Software 
InterfaceEXAMPLEANSWERMultiplicand32 bits32-bit ALUProductWrite
64 bitsControltestShift right
FIGURE 3.5 Reﬁ ned version of the multiplication hardware.
 Compare with th
 rst version in 
Figure 3.3
 e Multiplicand register, ALU, and Multiplier register are all 32 bits wide, with only the Product 
regist
  at 64 bits. Now the produc
 ed righ
 e separate Multiplier register also disappeared
 e multiplier is placed instead in the right half of the Product register
 ese changes are highlighted in color. 
 e Product register should really be 65 bits to hold the carry out of the adder, but it’s shown here as 64 bits 
to highlight the evolution from 
Figure 3.3
.)
 3.3 Multiplication 187Signed MultiplicationSo far, we have dealt with positive number
 e easiest way to understand how 
to deal with signed numbers is to
 rst convert the multiplier and multiplicand to 
positive numbers and then remember the original sign
 e algorithms should 
then be run for 31 iterations, leaving the signs out of the calculation. As we learned 
in grammar school, we need negate the product only if the original signs disagree.
It turns out that the last algorithm will work for signed numbers, provided that 
we remember that we are dealing with numbers that hav
 nite digits, and we are 
only representing them with 32 bits. Hence, th
 ing steps would need to extend 
the sign of the product for signed numbers. When the algorithm completes, the 

lower word would have the 32-bit product.
Faster Multiplication
Moore’s Law
 has provided so much more in resources that hardware designers can 

now build much faster multiplication hardware. Whether the multiplicand is to be 

added or not is known at the beginning of the multiplication by looking at each of 

the 32 multiplier bits. Faster multiplications are possible by essentially providing 

one 32-bit adder for each bit of the multiplier: one input is the multiplicand ANDed 

with a multiplier bit, and the other is the output of a prior adder.
A straightforward approach would be to connect the outputs of adders on the 
right to the inputs of adders on th
 , making a stack of adders 32 high. An 
alternative way to organize these 32 additions is in a parallel tree, as 
Figure 3.7
 
shows. Instead of waiting for 32 add times, we wait just the log
2 (32) o
 ve 32-bit 
add times.IterationStepMultiplierMultiplicandProduct
0 Initial values
00110000 0010
0000 000011a: 1 Prod = Prod + Mcand00110000 0010
0000 00102: Shift left Multiplicand
00110000 01000000 00103: Shift right Multiplier00010000 01000000 001021a: 1  Prod = Prod + Mcand00010000 0100
0000 01102: Shift left Multiplicand
00010000 10000000 01103: Shift right Multiplier00000000 10000000 011031: 0 No operation00000000 1000
0000 01102: Shift left Multiplicand0000
0001 00000000 01103: Shift right Multiplier00000001 00000000 011041: 0 No operation00000001 0000
0000 01102: Shift left Multiplicand0000
0010 00000000 01103: Shift right Multiplier00000010 00000000 0110FIGURE 3.6 Multiply example using algorithm in Figure 3.4
. e bit examined to determine the 
next step is circled in color.

188 Chapter 3 Arithmetic for Computers
In fact, multiply can go even faster tha
 ve add times because of the use of 
carry 
save adders
 (see Section B.6 in 
 Appendix B
) and because it is easy to 
pipeline
 such a design to be able to support many multiplies simultaneously (see Chapter 4).
Multiply in MIPSMIPS provides a separate pair of 32-bit registers to contain the 64-bit product, 
called Hi and 
Lo. To produce a properly signed or unsigned product, MIPS has two 
instructions: multiply (
mult) and multiply unsigned (
multu). To fetch the integer 
32-bit product, the programmer uses 
move from lo
 (mflo e MIPS assembler 
generates a pseudoinstruction for multiply that sp
 es three general-purpose 
registers, generating 
mflo and 
mfhi instructions to place the product into registers.
Summary
Multiplication hardware simpl
 s and add, as derived from the paper-and-
pencil method learned in grammar school. Compilers even us
  instructions 
for multiplications by powers of 2. With much more hardware we can do the adds 

in parallel
, and do them much faster.
Both MIPS multiply instructions ignore over
 ow, so it is up to the so
 ware to 
check to see if the product is too big t
 t in 32 bi
 ere is no over
 ow if Hi is 
0 for 
multu or the replicated sign of Lo for 
mult e instruction 
move from hi
 (mfhi) can be used to transfer Hi to a general-purpose register to test for over
 ow.
Hardware/
Software 
InterfaceProduct1
Product0
Product63Product62
Product47..16
1 bit1 bit1 bit1 bit. . .. . .. . .. . .. . .. . .32 bits32 bits32 bits32 bits32 bits32 bits32 bitsMplier31 † McandMplier30 † McandMplier29 † McandMplier28 † McandMplier3 † McandMplier2 † McandMplier1 † McandMplier0 † Mcand
FIGURE 3.7 Fast multiplication hardware.
 Rather than use a single 32-bit adder 31 times, this hardware “unrolls the loop” to use 31 
adders and then organizes them to minimize delay.

 3.4 Division 189 3.4 Division e reciprocal operation of multiply is divide, an operation that is even less frequent 
and even more quirky. It even o
 ers the opportunity to perform a mathematically 
invalid operation: dividing by 0.
Let’s start with an example of long division using decimal numbers to recall the 
names of the operands and the grammar school division algorithm. For reasons 
similar to those in the previous section, we limit the decimal digits to just 0 or 1. 

 e example is dividing 1,001,010
ten
 by 1000
ten
:1001tenQuotient Divisor 1000ten1001010tenDividend−100010101
1010−100010tenRemainderDivide’s two operands, called the 
dividend
 and 
divisor
, and the result, called 
the 
quotient
, are accompanied by a second result, called the 
remainder
. Here is 
another way to express the relationship between the components:
Dividend 
 Quotient 
 Divisor 
 Remainder
where the remainder is smaller than the divisor. Infrequently, programs use the 
divide instruction just to get the remainder, ignoring the quotient.
 e basic grammar school division algorithm tries to see how big a number 
can be subtracted, creating a digit of the quotient on each attempt. Our carefully 

selected decimal example uses only the numbers 0 and 1, so it’s easy t
 gure out 
how many times the divisor goes into the portion of the dividend: it’s either 0 times 

or 1 time. Binary numbers contain only 0 or 1, so binary division is restricted to 

these two choices, thereby simplifying binary division.
Let’s assume that both the dividend and the divisor are positive and hence the 
quotient and the remainder are nonnegative e division operands and both 

results are 32-bit values, and we 
will ignore the sign for now.
A Division Algorithm and Hardware
Figure 3.8
 shows hardware to mimic our grammar school algorithm. We start with 

the 32-bit Quotient register set to 0. Each iteration of the algorithm needs to move 

the divisor to the right one digit, so we start with the divisor placed in th
  half 
of the 64-bit Divisor register an
  it right 1 bit each step to align it with the 
dividend
 e Remainder register is initialized with the dividend.
Divide et impera.
Latin for “Divide and 
rule,” ancient political 

maxim cited by 

Machiavelli, 1532
dividend 
A number 
being divided.
divisor
 A number that 
the dividend is divided by.
quotient
  e primary 
result of a division; 
a number that when 

multiplied by the 

divisor and added to the 

remainder produces the 

dividend.
remainder
  e secondary result of 

a division; a number 

that when added to the 

product of the quotient 

and the divisor produces 

the dividend

190 Chapter 3 Arithmetic for Computers
Figure 3.9
 shows three steps of th
 rst division algorithm. Unlike a human, the 
computer isn’t smart enough to know in 
advance whether the divisor is smaller 
than the dividend. It mu
 rst subtract the divisor in step 1; remember that this is 
how we performed the comparison in the set on less than instruction. If the result 
is positive, the divisor was smaller or equal to the dividend, so we generate a 1 in 

the quotient (step 2a). If the result is negative, the next step is to restore the original 

value by adding the divisor back to the remainder and generate a 0 in the quotient 

(st e diviso
 ed right and then we iterate aga
 e remainder and 
quotient will be found in their namesake registers a
 er the iterations are complete.
A Divide AlgorithmUsing a 4-bit version of the algorithm to save pages, let’s try dividing 7
ten
 by 2
ten
, or 0000 0111
two
 by 0010
two
.Figure 3.10
 shows the value of each register for each of the steps, with the 

quotient being 3
ten
 and the remainder 1
ten
. Notice that the test in step 2 of whether 
the remainder is positive or negative simply tests whether the sign bit of the 

Remainder register is a 0 or
 e surprising requirement of this algorithm is 
that it takes 
n + 1 steps to get the proper quotient and remainder.
EXAMPLEANSWERDivisorShift right
64 bits64-bit ALURemainderWrite
64 bitsControltestQuotientShift left32 bitsFIGURE 3.8 First version of the division hardware.
 e Divisor register, ALU, and Remainder 
register are all 64 bits wide, with only the Quotient register being 32 bi
 e 32-bit divisor starts in the 
  half of the Divisor register an
 ed right 1 bit each iteratio
 e remainder is initialized with the 
dividend. Control decides when t  the Divisor and Quotient registers and when to write the new value 
into the Remainder register.

 3.4 Division 19133rd repetition?2a.  Shift the Quotient register to the left,
setting the new rightmost bit to 1
Remainder < 0Remainder  0Test Remainder
Start
3.  Shift the Divisor register right 1 bit
No: < 33 repetitions
Yes: 33 repetitions
Done1.  Subtract the Divisor register from the
Remainder register and place the result in the Remainder register2b.  Restore the original value by adding
the Divisor register to the Remainderregister and placing the sum in theRemainder register. Also shift the
Quotient register to the left, setting thenew least significant bit to 0
FIGURE 3.9 A division algorithm, using the hardware in 
Figure 3.8
. If the remainder is positive, 
the divisor did go into the dividend, so step 2a generates a 1 in the quotient. A negative remainder a
 er step 1 means that the divisor did not go into the dividend, so step 2b generates a 0 in the quotient and adds 
the divisor to the remainder, thereby reversing the subtraction of st
 e 
 nal 
 , in step 3, aligns the 
divisor properly, relative to the dividend for the next iteratio
 ese steps are repeated 33 times.
 is algorithm and hardware can be r
 ned to be faster and cheaper.
 e speed-
up comes from
 ing the operands and the quotient simultaneously with the 
subtractio
 is re
 nement halves the width of the adder and registers by noticing 
where there are unused portions of registers and adders. 
Figure 3.11
 shows the 
revised hardware.

192 Chapter 3 Arithmetic for Computers
Signed DivisionSo far, we have ignored signed numbers in divisio
 e simplest solution is to 
remember the signs of the divisor and dividend and then negate the quotient if the 
signs disagree.
IterationStepQuotientDivisorRemainder
0 Initial values00000010 0000
0000 011111: Rem = Rem Œ Div
00000010 0000
1110 01112b: Rem < 0 
 +Div, sll Q, Q0 = 0
00000010 00000000 01113: Shift Div right
00000001 00000000 011121: Rem = Rem Œ Div00000001 0000
1111 01112b: Rem < 0 
 +Div, sll Q, Q0 = 0
00000001 00000000 01113: Shift Div right
00000000 10000000 011131: Rem = Rem Œ Div00000000 1000

1111 11112b: Rem < 0 
 +Div, sll Q, Q0 = 0
00000000 10000000 01113: Shift Div right
00000000 01000000 011141: Rem = Rem Œ Div00000000 0100

0000 00112a: Rem  0  sll Q, Q0 = 1
00010000 01000000 00113: Shift Div right
00010000 00100000 001151: Rem = Rem Œ Div00010000 0010
0000 00012a: Rem  0  sll Q, Q0 = 1
00110000 00100000 00013: Shift Div right
00110000 00010000 0001FIGURE 3.10 Division example using the algorithm in Figure 3.9
. e bit examined to determine 
the next step is circled in color.
Divisor32 bits32-bit ALURemainderWrite
64 bitsControltestShift leftShift right
FIGURE 3.11 An improved version of the division hardware.
 e Divisor register, ALU, and 
Quotient register are all 32 bits wide, with only the Remainder regist
  at 64 bits. Compared to 
Figure 3.8
, the ALU and Divisor registers are halved and the rema
 ed 
 . 
 is version also combines the 
Quotient register with the right half of the Remainder register. (As in 
Figure 3.5
, the Remainder register 
should really be 65 bits to make sure the carry out of the adder is not lost.)

 3.4 Division 193Elaboration: The one complication of signed division is that we must also set the sign 
of the remainder. Remember that the following equation must always hold:
Dividend  Quotient  Divisor  RemainderTo understand how to set the sign of the remainder, let’s look at the example of dividing 
all the combinations of 7ten by 
2ten rst case is easy:
7  2: Quotient  3, 
 Remainder  1Checking the results:7  3  2  (1)  61If we change the sign of the dividend, the quotient must change as well:
7  2: Quotient  3Rewriting our basic formula to calculate the remainder:
Remainder   (Dividend  Quotient  Divisor)  7  (3x  2)  7  (6)  1So,7  2: Quotient  3, Remainder 
 1Checking the results again:7  3  2  (1)  61The reason the answer isn’t a quotient of 
4 and a remainder of 1, which would also 
 t this formula, is that the absolute value of the quotient would then change depending 

on the sign of the dividend and the divisor! Clearly, if
(x  y)  (x)  yprogramming would be an even greater challenge. This anomalous behavior is avoided 

by following the rule that the dividend and remainder must have the same signs, no 

matter what the signs of the divisor and quotient.We calculate the other combinations by following the same rule:
7  2: Quotient  3, Remainder 
 17  2: Quotient  3, Remainder 
 1
194 Chapter 3 Arithmetic for Computers
Thus the correctly signed division algorithm negates the quotient if the signs of the 
operands are opposite and makes the sign of the nonzero remainder match the dividend.
Faster Division
Moore’s Law
 applies to division hardware as well as multiplication, so we would 
like to be able to speed up division by throwing hardware at it. We used many 

adders to speed up multiply, but we cannot do the same trick for divide
 e reason 
is that we need to know the sign of th
 erence before we can perform the next 
step of the algorithm, whereas with multiply we could calculate the 32 partial 

products immediately.
 ere are techniques to produce more than one bit of the quotient per step. 
 e SRT division
 technique tries to 
predict
 several quotient bits per step, using a 
table lookup based on the upper bits of the dividend and remainder. It relies on 

subsequent steps to correct wrong predictions. A typical value today is 4 bi
 e key is guessing the value to subtract. With binary division, there is only a single 

choice ese algorithms use 6 bits from the remainder and 4 bits from the divisor 

to index a table that determines the guess for each step.
 e accuracy of this fast method depends on having proper values in the lookup 
table.
 e fallacy on page 231 in Section 3.9 shows what can happen if the table is 
incorrect.
Divide in MIPSYou may have already observed that the same sequential hardware can be used for 

both multiply and divide in 
Figures 3.5 and 3.11
 e only requirement is a 64-bit 
register that ca
  
  or right and a 32-bit ALU that adds or subtracts. Hence, 
MIPS uses the 32-bit Hi and 32-bit Lo registers for both multiply and divide.
As we might expect from the algorithm above, Hi contains the remainder, and 
Lo contains the quotient a
 er the divide instruction completes.
To handle both signed integers and unsigned integers, MIPS has two instructions: 
divide
 (div) and 
divide unsigned
 (divu e MIPS assembler allows divide 
instructions to specify three registers, generating the 
mflo or 
mfhi instructions to 
place the desired result into a general-purpose register.
Summary
 e common hardware support for multiply and divide allows MIPS to provide a 
single pair of 32-bit registers that are used both for multiply and divide. We accelerate 

division by predicting multliple quotient bits and then correcting mispredictions 

later, 
Figure 3.12
 summarizes the enhancements to the MIPS architecture for the 

last two sections.

 3.4 Division 195MIPS assembly languageCategory Instruction
ExampleMeaningCommentsArithmeticadd add     $s1,$s2,$s3$s1
 = $s2 + $s3Three operands; over˜ow detected
subtractsub     $s1,$s2,$s3$s1
 = $s2 Œ $s3Three operands; over˜ow detected
add immediateaddi    $s1,$s2,100$s1
 = $s2 + 100+ constant; over˜ow detected
add unsignedaddu    $s1,$s2,$s3$s1
 = $s2 + $s3Three operands; over˜ow undetected
subtract unsignedsubu    $s1,$s2,$s3$s1
 = $s2 Œ $s3Three operands; over˜ow undetected
add immediate unsignedaddiu   $s1,$s2,100$s1
 = $s2 + 100+ constant; over˜ow undetected
move from coprocessor 
register 
mfc0    $s1,$epc$s1
 = $epcCopy Exception PC + special regs
multiply mult    $s2,$s3Hi, Lo = $s2 × $s364-bit signed product in Hi, Lo
multiply unsignedmultu   $s2,$s3Hi, Lo = 
$s2 × $s364-bit unsigned product in Hi, Lo
divide div     $s2,$s3Lo = $s2 / $s3, 
Hi = $s2 mod $s3Lo = quotient, Hi = remainder
divide unsigneddivu    $s2,$s3Lo = $s2 / $s3, 
Hi = $s2 mod $s3Unsigned quotient and remaindermove from Hi
mfhi    $s1$s1 = HiUsed to get copy of Hi
move from Lo
mßo     $s1$s1 = LoUsed to get copy of Lo
Data 
transferload wordlw      $s1,20($s2)$s1 = Memory
[$s2 + 20]Word from memory to register
store wordsw      $s1,20($s2)Memory
[$s2 + 20] = $s1Word from register to memory
load half unsignedlhu     $s1,20($s2)$s1
 = Memory[
$s2 + 20]Halfword memory to register
store halfsh      $s1,20($s2)Memory[
$s2 + 20] = $s1Halfword register to memory
load byte unsigned
lbu     $s1,20($s2)$s1
 = Memory[
$s2 + 20]Byte from memory to register
store byte
sb      $s1,20($s2)Memory[
$s2 + 20] = $s1Byte from register to memory
load linked word
ll      $s1,20($s2)$s1
 = Memory[
$s2 + 20]Load word as 1st half of atomic swap 
store conditional wordsc      $s1,20($s2)Memory[
$s2+20]=$s1;$s1=0 
or 1Store word as 2nd half atomic swap 
load upper immediatelui     $s1,100$s1 = 
100 * 216Loads constant in upper 16 bitsLogicalAND AND     $s1,$s2,$s3$s1 = $s2 & $s3
Three reg. operands; bit-by-bit AND
OROR      $s1,$s2,$s3$s1 = $s2 | $s3
Three reg. operands; bit-by-bit OR
NORNOR     $s1,$s2,$s3$s1 = ~ ($s2 |$s3)
Three reg. operands; bit-by-bit NOR
AND immediateANDi    $s1,$s2,100$s1 = $s2 & 100
Bit-by-bit AND with constant
OR immediateORi     $s1,$s2,100$s1 = $s2 | 100
Bit-by-bit OR with constant
shift left logicalsll     $s1,$s2,10$s1 = $s2 
<< 10Shift left by constant
shift right logicalsrl     $s1,$s2,10$s1 = $s2 
>> 10Shift right by constant
Condi- 
tional 
branchbranch on equalbeq     $s1,$s2,25if ($s1 == $s2) go to PC + 4 + 100Equal test; PC-relative branchbranch on not equalbne     $s1,$s2,25if ($s1 !=  $s2) go to PC + 4 + 100Not equal test; PC-relative set on less thanslt     $s1,$s2,$s3if ($s2 < $s3)  $s1 = 1;
else $s1 = 0Compare less than; two™s 

complementset less than immediateslti    $s1,$s2,100if ($s2 < 100)  $s1 = 1; 
else $s1=0Compare < constant; two™s 

complementset less than unsignedsltu    $s1,$s2,$s3if ($s2 < $s3)  $s1 = 1; 
else $s1=0Compare less than; natural numbers
set less than immediate 

unsignedsltiu   $s1,$s2,100if ($s2 < 100)  $s1 = 1; 
else $s1 = 0Compare < constant; natural numbers
Uncondi- 
tional  
jumpjumpj       2500go to 10000Jump to target addressjump registerjr      $rago to $raFor switch, procedure return
jump and linkjal     2500$ra = PC + 4; go to 10000For procedure call
FIGURE 3.12 MIPS core architecture. e memory and registers of the MIPS architecture are not included for space reasons, but this 
section added the Hi and Lo registers to support multiply and divide. MIPS machine language is listed in the MIPS Reference Dat
a Card at the 
front of this book.

196 Chapter 3 Arithmetic for Computers
MIPS divide instructions ignore over
 ow, so so
 ware must determine whether the 
quotient is too large. In addition to over ow, division can also result in an improper 
calculation: division by 0. Some computers distinguish these two anomalous events. 

MIPS so
 ware must check the divisor to discover division by 0 as well as over
 ow.
Elaboration: An even faster algorithm does not immediately add the divisor back 
if the remainder is negative. It simply adds
 the dividend to the shifted remainder in the following step, since (
r  d)  2  d  r  2  d  2  d  r  2  d. This nonrestoring
 division algorithm, which takes 1 clock cycle per step, is explored further 
in the exercises; the algorithm above is called 
restoring
 division. A third algorithm that doesn’t save the result of the subtract if it’s negative is called a 
nonperforming division algorithm. It averages one-third fewer arithmetic operations.
 3.5 Floating Point
Going beyond signed and unsigned inte
gers, programming languages support 
numbers with fractions, which are called 
reals
 in mathematics. Here are some 
examples of reals:
3.14159265… ten
 (pi)
2.71828… ten
 (e)0.000000001ten
 or 1.0
ten
 × 10−9 (seconds in a nanosecond)
3,155,760,000ten
 or 3.15576
ten
 × 109 (seconds in a typical century)
Notice that in the last case, the number didn’t represent a small fraction, but it 
was bigger than we could represent with a 32-bit signed integer
 e alternative 
notation for the last two numbers is called 
scienti
 c notation
, which has a single 
digit to th
  of the decimal point. A number in scien
 c notation that has no 
leading 0s is called a 
normalized
 number, which is the usual way to write it. For 
example, 1.0
ten
  109 is in normalized scien
 c notation, but 0.1
ten
  108 and 
10.0ten
  1010 are not.
Just as we can show decimal numbers in scien
 c notation, we can also show 
binary numbers in scien
 c notation:
1.0two
  21To keep a binary number in normalized form, we need a base that we can increase 

or decrease by exactly the number of bits the number must b
 ed to have one 
nonzero digit to th
  of the decimal point. Only a base of 2 fu lls our need. Since 
the base is not 10, we also need a new name for decimal point; 
binary point
 ne.Hardware/
Software 
InterfaceSpeed gets you 

nowhere if you’re 

headed the wrong way.
American proverb
scienti
 c notation
 A notation that renders 
numbers with a single 

digit to th
  of the 
decimal point.
normalized
 A number 
 oating-point notation 
that has no leading 0s.

 3.5 Floating Point 
197Computer arithmetic that supports such numbers is called 
 oating point
 because it represents numbers in which the binary point is no
 xed, as it is for 
integer
 e programming language C uses the name 
 oat
 for such numbers. Just 
as in scien
 c notation, numbers are represented as a single nonzero digit to the 
  of the binary point. In binary, the form is
1.xxxxxxxxxtwo
  2yyyy(Although the computer represents the exponent in base 2 as well as the rest of the 
number, to simplify the notation we show the exponent in decimal.)
A standard scien
 c notation for reals in normalized form o
 ers three 
advantages. It simp
 es exchange of data that incl
 oating-point numbers; 
it simp es th
 oating-point arithmetic algorithms to know that numbers will 
always be in this form; and it increases the accuracy of the numbers that can be 

stored in a word, since the unnecessary leading 0s are replaced by real digits to the 

right of the binary point.
Floating-Point Representation
A designer o
 oating-point representation mu
 nd a compromise between the 
size of the 
fraction
 and the size of the 
exponent
, becaus
 xed word size means 
you must take a bit from one to add a bit to the other
 is tradeo
  is between 
precision and range: increasing the size of the fraction enhances the precision 

of the fraction, while increasing the size of the exponent increases the range of 

numbers that can be represented. As our design guideline from Chapter 2 reminds 

us, good design demands good compromise.
Floating-point numbers are usually a multiple of the size of a word
 e representation o
 oating-point number is shown below, where 
s is the sign 
of th
 oating-point number (1 meaning negative), 
exponent
 is the value of the 
8-bit exponen
 eld (including the sign of the exponent), and 
fraction
 is the 23-bit 
number. As we recall from Chapter 2, this representation is 
sign and magnitude
, since the sign is a separate bit from the rest of the number.
313029282726252423222120191817161514131211109876543210
sexponentfraction
1 bit 8 bits23 bitsIn general
 oating-point numbers are of the form
(1)S  F  2EF involves the value in the fractio
 eld and E involves the value in the exponent 
 eld; the exact relationship to thes elds will be spelled out soon. (We will shortly 
see that MIPS does something slightly more sophisticated.)
 oating point
  Computer arithmetic that 
represents numbers in 

which the binary point is 

no
 xed.
fraction
  e value, 
generally between 0 and 

1, placed in the fraction 

 eld. 
 e fraction is also 
called the 
mantissa
.exponent
 In the 
numerical representation 

system of
 oating-point 
arithmetic, the value that 

is placed in the exponent 

 eld.
198 Chapter 3 Arithmetic for Computers
 ese chosen sizes of exponent and fraction give MIPS computer arithmetic 
an extraordinary range. Fractions almost as small as 2.0
ten
  1038 and numbers 
almost as large as 2.0
ten
  1038 can be represented in a computer. Alas, extraordinary 
 ers fro
 nite, so it is still possible for numbers to be too large
 us, over
 ow 
interrupts can occ oating-point arithmetic as well as in integer arithmetic. 
Notice that 
 ow here means that the exponent is too large to be represented 
in the exponen
 eld.Floating point o
 ers a new kind of exceptional event as well. Just as programmers 
will want to know when they have calculated a number that is too large to be 

represented, they will want to know if the nonzero fraction they are calculating 

has become so small that it cannot be represented; either event could result in a 

program giving incorrect answers. To distinguish it from over
 ow, we call this 
event 
und
 ow is situation occurs when the negative exponent is too large to 
 t in the exponen
 eld.One way to reduce chances of under
 ow or over
 ow is to o
 er another format 
that has a larger exponent. In C this number is called 
double
, and operations on 
doubles are called 
double precision
 oating-point arithmetic; 
single precision
  oating point is the name of the earlier format.
 e representation of a double precisio
 oating-point number takes two MIPS 
words, as shown below, where 
s is still the sign of the number, 
exponent
 is the value 
of the 11-bit exponen
 eld, and 
fraction
 is the 52-bit number in the fractio
 eld. ow 
 oating-
point)
 A situation in 
which a positive exponent 
becomes too large t
 t in 
the exponen
 eld.und
 ow 
 oating-
point) 
A situation 
in which a negative 

exponent becomes too 

large to
 t in the exponent 
 eld.double precision
  oating-point value 
represented in two 32-bit 

words.
single precision
  oating-point value 
represented in a single 32-

bit word.
MIPS double precision allows numbers almost as small as 2.0
ten
  10308 and almost 
as large as 2.0
ten
  10308. Although double precision does increase the exponent 
range, its primary advantage is its greater precision because of the much larger 
fraction.
 ese formats go beyo
 ey are part of the 
 oating-point 
standard
, found in virtually every computer invent
 is standard has 
greatly improved both the ease of portin
 oating-point programs and the quality 
of computer arithmetic.
To pack even more bits into th
 cand, IEEE 754 makes the leading 1-bit 
of normalized binary numbers implicit. Hence, the number is actually 24 bits long 

in single precision (implied 1 and a 23-bit fraction), and 53 bits long in double 

precision (1 
 52). To be precise, we use the term 
signi
 cand
 to represent the 24- 
or 53-bit number that is 1 plus the fraction, and 
fraction
 when we mean the 23- or 
52-bit number. Since 0 has no leading 1, it is given the reserved exponent value 0 so 

that the hardware won’t attach a leading 1 to it.
313029282726252423222120191817161514131211109876543210
fractionexponents1 bit11 bits20 bitsfraction (continued)32 bits
 3.5 Floating Point 
199 us 00 … 00
two
 represents 0; the representation of the rest of the numbers uses 
the form from before with the hidden 1 added:
(1)S   (1  Fraction) 
 2Ewhere the bits of the fraction represent a number between 0 and 1 and E sp
 es the value in the exponen
 eld, to be given in detail shortly. If we number the bits 
of the fraction from 
  to right
 s1, s2, s3, …, then the value is
(1)S  (1  (s1  21)  (s2  22)  (s3  23)  (s4  24) ...)  2EFigure 3.13
 shows the encodings o
 oating-point numbers. Other 
features of IEEE 754 are special symbols to represent unusual events. For example, 
instead of interrupting on a divide by 0, so
 ware can set the result to a bit pattern 
representing 
r 
he largest exponent is reserved for these special symbols. 
When the programmer prints the results, the program will print a
 nity symbol. 
(For the mathematically trained, the purpose of
 nity is to form topological 
closure of the reals.)
IEEE 754 even has a symbol for the result of invalid operations, such as 0/0 
or subtractin
 nity fro
 nity. 
 is symbol is 
NaN
, for 
Not a Number
 e purpose of NaNs is to allow programmers to postpone some tests and decisions to 

a later time in the program when they are convenient.
 e designers of IEEE 754 also want
 oating-point representation that could 
be easily processed by integer comparisons, especially for sortin
 is desire is 
why the sign is in th
 cant bit, allowing a quick test of less than, greater 
than, or equal to 0. (It’s a little more complicated than a simple integer sort, since 

this notation is essentially sign and magnitude rather than two’s complement.)
Placing the exponent before th
 cand also simp
 es the sorting of 
 oating-point numbers using integer comparison instructions, since numbers with 
bigger exponents look larger than numbers with smaller exponents, as long as both 

exponents have the same sign.
Single precision
Double precision
Object represented
ExponentFractionExponentFraction
00000
0 Nonzero0 Nonzero± denormalized number
1Œ254Anything1Œ2046Anything± ˜oating-point number
255020470± in˚nity255Nonzero2047Nonzero
NaN (Not a Number)FIGURE 3.13  EEE 754 encoding of ﬂ oating-point numbers.
 A separate sign bit determines the 
sign. Denormalized numbers are described in the 
Elaboration
 on pag
 is information is also found in 
Column 4 of the MIPS Reference Data Card at the front of this book.

200 Chapter 3 Arithmetic for Computers
Negative exponents pose a challenge to simp
 ed sorting. If we use two’s 
complement or any other notation in which negative exponents have a 1 in the 
 cant bit of the exponen
 eld, a negative exponent will look like a big 
number. For example, 1.0
two
  21 would be represented as
313029282726252423222120191817161514131211109876543210
01111111100000000000000000000...
(Remember that the leading 1 is implicit in th
 cand.) 
 e value 1.0
two
  21 would look like the smaller binary number
313029282726252423222120191817161514131211109876543210
00000000100000000000000000000...
 e desirable notation must therefore represent the most negative exponent as 
00 … 00two
 and the most positive as 11 … 11
two
 is convention is called 
biased 
notation
, with the bias being the number subtracted from the normal, unsigned 
representation to determine the real value.
IEEE 754 uses a bias of 127 for single precision, so an exponent of 
1 is represented by the bit pattern of the value 
1  127ten
, or 126
ten
  0111 1110two
, and 
1 is represented by 1 
 127, or 128
ten
  1000 0000two
 e exponent bias for 
double precision is 1023. Biased exponent means that the value represented by a 

 oating-point number is really
(1)S   (1  Fraction) 
 2(Exponent 
 Bias)
 e range of single precision numbers is then from as small as
1.00000000000000000000000two
   2126to as large as
1.11111111111111111111111two
   2127.Let’s demonstrate.

 3.5 Floating Point 
201Floating-Point Representation
Show the IEEE 754 binary representation of the number 
0.75ten
 in single and 
double precision.
 e number 
0.75ten
 is also
3/4ten
 or 
 3/22ten
It is also represented by the binary fraction
11two
 /22ten
 or 
 0.11two
In scien
 c notation, the value is
 0.11two
  20and in normalized scien
 c notation, it is
1.1two
  21 e general representation for a single precision number is
(1)S  (1  Fraction) 
 2(Exponent
127)Subtracting the bias 127 from the exponent of 
1.1two
  21 yields
(1)1  (1  .1000 0000 0000 0000 0000 000two
)  2(126127) e single precision binary representation of 
0.75ten
 is then
313029282726252423222120191817161514131211109876543210
10111111010000000000000000000000
1 bit 8 bits23 bits e double precision representation is
EXAMPLEANSWER(1)1  (1  .1000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000two
)  2(10221023)313029282726252423222120191817161514131211109876543210
10111111111010000000000000000000
1 bit 11 bits20 bits00000000000000000000000000000000
32 bits
202 Chapter 3 Arithmetic for Computers
Now let’s try going the other direction.
Converting Binary to Decimal Floating Point
What decimal number is represented by this single precisio
 oat?
EXAMPLE313029282726252423222120191817161514131211109876543210
11000000101000000000000000000...
 e sign bit is 1, the exponen eld contains 129, and the fractio
 eld contains 
1  22  1/4, or 0.25. Using the basic equation,
(1)S  (1  Fraction) 
 2(Exponent
Bias)
  (1)1  (1  0.25)  2(129127)    1  1.25  22    1.25  4    5.0In the next few subsections, we will give the algorithms fo
 oating-point 
addition and multiplication. At their core, they use the corresponding integer 
operations on th
 cands, but extra bookkeeping is necessary to handle the 
exponents and normalize the result. W
 rst give an intuitive derivation of the 
algorithms in decimal and then give a more detailed, binary version in th
 gures.
Elaboration: Following IEEE guidelines, the IEEE 754 committee was reformed 20 
years after the standard to see what changes, if any, should be made. The revised 
standard IEEE 754-2008 includes nearly all the IEEE 754-1985 and adds a 16-bit format 

(“half precision”) and a 128-bit format (“quadruple precision”). No hardware has yet been 

built that supports quadruple precision, but it will surely come. The revised standard 

 oating point arithmetic, which IBM mainframes have implemented.
Elaboration: In an attempt to increase range without remo
 cand, 
some computers before the IEEE 754 standard used a base other than 2. For example, 

the IBM 360 and 370 mainframe computers use base 16. Since changing the IBM 

exponent b cand by 4 bits, “normalized” base 16 numbers 

can have up to 3 leading bits of 0s! Hence, hexadecimal digits mean that up to 3 bits must 

 cand, which leads to surprising problems in the accuracy of 

 oating-point arithmetic. IBM mainframes now support IEEE 754 as well as the hex format.
ANSWER
 3.5 Floating Point 
203Floating-Point Addition
Let’s add numbers in scien
 c notation by hand to illustrate the problems in 
 oating-point addition: 9.999
ten
  101  1.610ten
  101. Assume that we can store 
only four decimal digits of th cand and two decimal digits of the exponent.
 Step 1. To be able to add these numbers properly, we must align the decimal 
point of the number that has the smaller exponent. Hence, we need 
a form of the smaller number, 1.610
ten
  101, that matches the 
larger exponent. We obtain this by observing that there are multiple 

representations of an unnor
 oating-point number in 
scien
 c notation:
 1.610ten
  101  0.1610ten
  100  0.01610ten
  101  
 e number on the right is the version we desire, since its exponent 
matches the exponent of the larger number, 9.999
ten
  101 us, the 
 rst st
 s th
 cand of the smaller number to the right until 
its corrected exponent matches that of the larger number. But we can 

represent only four decimal digits so, a
 er 
 ing, the number is 
really
 0.016  101 Step 2. Next comes the addition of th
 cands:
 9.999ten
 + 0.016ten
 10.015ten
  
 e sum is 10.015
ten
  101. Step 3. 
 is sum is not in normalized scien
 c notation, so we need to 
adjust it:
 10.015ten
  101  1.0015ten
  102  
 us, a
 er the addition we may have t  the sum to put it into 
normalized form, adjusting the exponent appropriately
 is example 
sho
 ing to the right, but if one number were positive and the 
other were negative, it would be possible for the sum to have many 

leading 0s, requirin
 s. Whenever the exponent is increased 
or decreased, we must check for over
 ow or under ow—that is, we 
must make sure that the exponent s
 ts in i
 eld. Step 4. Since we assumed that th
 cand can be only four digits long 
(excluding the sign), we must round the number. In our grammar 

school algorithm, the rules truncate the number if the digit to the 

right of the desired point is between 0 and 4 and add 1 to the digit if 

the number to the right is between 5 an
 e number
 1.0015ten
  102
204 Chapter 3 Arithmetic for Computers
  is rounded to four digits in th
 cand to
 1.002ten
  102  since the fourth digit to the right of the decimal point was between 5 
and 9. Notice that if we have bad luck on rounding, such as adding 1 
to a string of 9s, the sum may no longer be normalized and we would 

need to perform step 3 again.
Figure 3.14
 shows the algorithm for binar
 oating-point addition that follows 
this decimal example. Steps 1 and 2 are similar to the example just discussed: 

adjust th
 cand of the number with the smaller exponent and then add the 
tw
 cands. Step 3 normalizes the results, forcing a check for over
 ow or 
under
 ow. 
 e test for over ow and under ow in step 3 depends on the precision 
of the operands. Recall that the pattern of all 0 bits in the exponent is reserved and 

used for th
 oating-point representation of zero. Moreover, the pattern of all 1 bits 
in the exponent is reserved for indicating values and situations outside the scope of 

normal
 oating-point numbers (see the 
Elaboration
 on page 222). For the example 
below, remember that for single precision, the maximum exponent is 127, and the 

minimum exponent is 
126.Binary Floating-Point Addition
Try adding the numbers 0.5
ten
 and 
0.4375ten
 in binary using the algorithm in 
Figure 3.14
.Let’
 rst look at the binary version of the two numbers in normalized scien
 c notation, assuming that we keep 4 bits of precision:
 0.5ten
  1/2ten 
  1/21ten
  0.1two
  0.1two
  20  1.000two
  210.4375ten
  7/16ten
  7/24ten
  0.0111two
  0.0111two
  20  1.110two
  22Now we follow the algorithm:
 Step 1. 
 e 
 cand of the number with the lesser exponent (
1.11two
  22 ed right until its exponent matches the larger number:
1.110two
  22  0.111two
  21 Step 2. Add th
 cands:
1.000two
  21  (0.111two
  21)  0.001two
  21EXAMPLEANSWER
 3.5 Floating Point 
205Still normalized?
4. Round the significand to the appropriate
number of bits
Yes
Overflow or
underflow?
Start
NoYes
Done1.  Compare the exponents of the two numbers;
shift the smaller number to the right until its
exponent would match the larger exponent
2. Add the significands
3. Normalize the sum, either shifting right and
incrementing the exponent or shifting left
and decrementing the exponent
NoExceptionFIGURE 3.14 Floating-point addition. e normal path is to execute steps 3 and 4 once, but if 
rounding causes the sum to be unnormalized, we must repeat step 3.

206 Chapter 3 Arithmetic for Computers
 Step 3. Normalize the sum, checking for over
 ow or under
 ow:
0.001two
  21  0.010two
  22  0.100two
  23  1.000two
  24  Since 127 
 4  126, there is no over
 ow or under
 ow. 
 e biased exponent would be 
4  127, or 123, which is between 1 and 
254, the smallest and largest unreserved biased exponents.)
 Step 4. Round the sum:
 1.000two
  24  
 e sum alread
 ts exactly in 4 bits, so there is no change to the bits 
due to rounding.
  
 is sum is then
1.000two 
 24  0.0001000two
  0.0001two
  1/24ten
  1/16ten
  0.0625ten
 is sum is what we would expect from adding 0.5
ten
 to 
0.4375ten
.Many computers dedicate hardware to r
 oating-point operations as fast as possible. 
Figure 3.15
 sketches the basic organization of hardware fo
 oating-point addition.
Floating-Point Multiplication
Now that we have expla
 oating-point addition, let’s tr
 oating-point 
multiplication. We start by multiplying decimal numbers in scien
 c notation by 
hand: 1.110
ten
  1010  9.200ten
  105. Assume that we can store only four digits 
of th
 cand and two digits of the exponent.
 Step 1. Unlike addition, we calculate the exponent of the product by simply 
adding the exponents of the operands together:
New exponent 
 10  (5)  5  Let’s do this with the biased exponents as well to make sure we obtain 
the same result: 10 + 127 = 137, and 
5 + 127 = 122, so
New exponent 
 137  122 259  
 is result is too large for the 8-bit exponen
 eld, so something is 
am
 e problem is with the bias because we are adding the biases 
as well as the exponents:
New exponent 
 (10  127)  (5  127)  (5  2  127)  259  Accordingly, to get the correct biased sum when we add biased numbers, 
we must subtract the bias from the sum
:
 3.5 Floating Point 
207Compareexponents
Small ALUExponentdifference
ControlExponentSignFraction
Big ALUExponentSignFraction
010101Shift right
0101Increment ordecrementShift left or right
Rounding hardware
ExponentSignFraction
Shift smaller
number right
AddNormalize
RoundFIGURE 3.15 Block diagram of an arithmetic unit dedicated to ﬂ oating-point addition.
 e steps of 
Figure 3.14
 correspond 
to each block, from top to bottom. First, the exponent of one operand is subtracted from the other using the small ALU to determine which is 
larger and by how muc
 is 
 erence controls the three multiplexors; fro
  to right, they select the larger exponent, th
 cand of the 
smaller number, and th
 cand of the larger number
 
 ca
 ed right, and then th
 cands are added together 
using the big ALU
 e normalization step th s th
  or right and increments or decrements the exponent. Rounding then creates 
th
 nal result, which may require normalizing again to produce the actu
 nal result.

208 Chapter 3 Arithmetic for Computers
New exponent 
 137  122  127  259  127  132  (5  127)  and 5 is indeed the exponent we calculated initially.
 Step 2. Next comes the multiplication of th
 cands:
     1.110ten
   ×  9.200ten
     0000    0000   2220  9990 10212000ten
  
 ere are three digits to the right of the decimal point for each 
operand, so the decimal point is placed six digits from the right in the 
produc
 cand:
 10.212000ten
  Assuming that we can keep only three digits to the right of the decimal 
point, the product is 10.212 
 105. Step 3. 
 is product is unnormalized, so we need to normalize it:
10.212ten
  105  1.0212ten
  106  
 us, a
 er the multiplication, the product can b
 ed right one digit 
to put it in normalized form, adding 1 to the exponent. At this point, 

we can check for over
 ow and under
 ow. Under
 ow may occur if both 
operands are small—that is, if both have large negative exponents.
 Step 4. We assumed that th
 cand is only four digits long (excluding the 
sign), so we must round the number
 e number
1.0212ten
  106  is rounded to four digits in th
 cand to
1.021ten
  106 Step 5. 
 e sign of the product depends on the signs of the original operands. 
If they are both the same, the sign is positive; otherwise, it’s negative. 

Hence, the product is
1.021ten
  106  
 e sign of the sum in the addition algorithm was determined by 
addition of th
 cands, but in multiplication, the sign of the 
product is determined by the signs of the operands.

 3.5 Floating Point 
2095. Set the sign of the product to positive if the
signs of the original operands are the same;
if they differ make the sign negative
Still normalized?
4. Round the significand to the appropriate
number of bits
Yes
Overflow or
underflow?
Start
NoYes
Done1.  Add the biased exponents of the two
numbers, subtracting the bias from the sum
to get the new biased exponent
2. Multiply the significands
3. Normalize the product if necessary, shifting
it right and incrementing the exponent
NoExceptionFIGURE 3.16 Floating-point multiplication. e normal path is to execute steps 3 and 4 once, but if 
rounding causes the sum to be unnormalized, we must repeat step 3.

210 Chapter 3 Arithmetic for Computers
Once again, as 
Figure 3.16
 shows, multiplication of binar
 oating-point numbers 
is quite similar to the steps we have just completed. We start with calculating 
the new exponent of the product by adding the biased exponents, being sure to 

subtract one bias to get the proper result. Next is multiplication o
 cands, 
followed by an optional normalization step
 e size of the exponent is checked 
for over
 ow or under
 ow, and then the product is rounded. If rounding leads to 
further normalization, we once again check for exponent size. Finally, set the sign 

bit to 1 if the signs of the operands wer
 erent (negative product) or to 0 if they 
were the same (positive product).
Binary Floating-Point Multiplication
Let’s try multiplying the numbers 0.5
ten
 and 
0.4375ten
, using the steps in 
Figure 3.16
.In binary, the task is multiplying 1.000
two
  21 by 
1.110two
  22. Step 1. Adding the exponents without bias:
1  (2)  3  or, using the biased representation:
(1  127)  (2  127)  127  (1  2)  (127  127  127)  3  127  124  Step 2. Multiplying th
 cands:
     1.000two
    1.110two
     0000    1000   1000  1000 1110000two
  
 e product is 1.110000
two
  23, but we need to keep it to 4 bits, so it 
is 1.110two
  23. Step 3. Now we check the product to make sure it is normalized, and then 
check the exponent for over
 ow or under
 ow. 
 e product is already 
normalized and, since 127 
 3  126, there is no over
 ow or 
under
 ow. (Using the biased representation, 254 
 124  1, so the 
exponen
 ts.) Step 4. Rounding the product makes no change:
1.110two
  23EXAMPLEANSWER
 3.5 Floating Point 
211 Step 5. Since the signs of the original opera
 er, make the sign of the 
product negative. Hence, the product is
1.110two
  23  Converting to decimal to check our results:
1.110two
  23  0.001110two
  0.00111two
  7/25ten
  7/32ten
  0.21875ten
  
 e product of 0.5
ten
 and 
0.4375ten
 is indeed 0.21875ten
.Floating-Point Instructions in MIPS
MIPS supports the IEEE 754 single precision and double precision formats with 
these instructions:
 Floating-point 
addition, single
 (add.s) and 
addition, double
 (add.d) Floating-point 
subtraction, single
 (sub.s) and 
subtraction, double
 (sub.d) Floating-point 
multiplication, single
 (mul.s) and 
multiplication, double
 (mul.d) Floating-point 
division, single
 (div.s) and 
division, double
 (div.d) Floating-point 
comparison, single
 (c.x.s) and 
comparison, double
 (c.x.d), 
where 
x may be 
equal
 (eq), not equal
 (neq), less than
 (lt), less than or equal
 (le), greater than
 (gt), or 
greater than or equal
 (ge) Floating-point 
branch, true
 (bc1t) and 
branch, false
 (bc1f)Floating-point comparison sets a bit to true or false, depending on the comparison 

condition, an
 oating-point branch then decides whether or not to branch, 
depending on the condition.
 e MIPS designers decided to add separate
 oating-point registers—called 
$f0, $f1, $f2, …—used either for single precision or double precision. Hence, 
they included separate loads and stores fo
 oating-point registers: 
lwc1 and 
swc1 e base registers fo
 oating-point data transfers which are used for 
addresses remain integer register
 e MIPS code to load two single precision 
numbers from memory, add them, and then store the sum might look like this:
lwc1      $f4,c($sp)  # Load 32-bit F.P. number into F4lwc1      $f6,a($sp)  # Load 32-bit F.P. number into F6
add.s     $f2,$f4,$f6 # F2 = F4 + F6 single precision
swc1      $f2,b($sp)  # Store 32-bit F.P. number from F2A double precision register is really an even-odd pair of single precision registers, 
using the even register number as its name
 us, the pair of single precision 
registers 
$f2 and 
$f3 also form the double precision register named 
$f2.Figure 3.17
 summarizes th
 oating-point portion of the MIPS architecture revealed 
in this chapter, with the additions to suppor
 oating point shown in color. Similar to 
Figure 2.19 in Chapter 2, 
Figure 3.18
 shows the encoding of these instructions.

212 Chapter 3 Arithmetic for Computers
MIPS ˜oating-point operandsNameExampleComments32 ˜oating- point registers
$f0, $f1, $f2, . . . , $f31MIPS ˜oating-point registers are used in pairs for double precision numbers.
230 memory words
Memory[0],  

Memory[4], . . . , 

Memory[4294967292]
Accessed only by data transfer instructions. MIPS uses byte addresses, so 

sequential word addresses differ by 4. Memory holds data structures, such 

as arrays, and spilled registers, such as those saved on procedure calls. 
MIPS ˜oating-point assembly languageCategory Instruction
ExampleMeaningCommentsArithmeticFP add singleadd.s   $f2,$f4,$f6
$f2 = $f4 + $f6FP add (single precision)FP subtract singlesub.s   $f2,$f4,$f6
$f2 = $f4 Ð $f6FP sub (single precision)FP multiply singlemul.s   $f2,$f4,$f6
$f2 = $f4 × $f6FP multiply (single precision)FP divide singlediv.s   $f2,$f4,$f6
$f2 = $f4 / $f6FP divide (single precision)FP add doubleadd.d   $f2,$f4,$f6
$f2 = $f4 + $f6FP add (double precision)FP subtract doublesub.d   $f2,$f4,$f6
$f2 = $f4 Ð $f6FP sub (double precision)FP multiply doublemul.d   $f2,$f4,$f6
$f2 = $f4 × $f6FP multiply (double precision)FP divide doublediv.d   $f2,$f4,$f6
$f2 = $f4 / $f6FP divide (double precision)Data 
transferload word copr. 1lwc1    $f1,100($s2)
$f1 = Memory[$s2 + 100]32-bit data to FP registerstore word copr. 1swc1    $f1,100($s2)
Memory[
$s2 + 100] = $f132-bit data to memory
Condi- tional branchbranch on FP truebc1t    25
if (cond == 1) go to PC + 4 
+ 100PC-relative branch if FP  
cond.branch on FP falsebc1f    25
if (cond == 0) go to PC + 4 
+ 100PC-relative branch if not  
cond.FP compare single 
(eq,ne,lt,le,gt,ge)c.lt.s $f2,$f4if ($f2 < $f4) 
    cond = 1; else cond = 0FP compare less than 
single precisionFP compare double (eq,ne,lt,le,gt,ge)c.lt.d $f2,$f4if ($f2 < $f4) 
    cond = 1; else cond = 0FP compare less than 
double precisionMIPS ˜oating-point machine languageNameFormat
ExampleCommentsadd.s R17166420
add.s  $f2,$f4,$f6sub.s R17166421
sub.s  $f2,$f4,$f6mul.s R17166422
mul.s  $f2,$f4,$f6div.s R17166423
div.s  $f2,$f4,$f6add.d R17176420
add.d  $f2,$f4,$f6sub.d R17176421
sub.d  $f2,$f4,$f6mul.d R17176422
mul.d  $f2,$f4,$f6div.d R17176423
div.d  $f2,$f4,$f6lwc1 I4920
2100lwc1   $f2,100($s4)swc1 I5720
2100swc1   $f2,100($s4)bc1t I1781
25
bc1t   25bc1f I1780
25
bc1f   25c.lt.sR171642060
c.lt.s $f2,$f4c.lt.dR171742060
c.lt.d $f2,$f4Field size
6 bits5 bits5 bits5 bits5 bits6 bitsAll MIPS instructions 32 bits
FIGURE 3.17 MIPS ﬂ oating-point architecture revealed thus far.
 See Appendix A, Section A.10, for more detail
 is information 
is also found in column 2 of the MIPS Reference Data Card at the front of this book.

 3.5 Floating Point 
213op(31:26):28Œ2631Œ290(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(000)RfmtBltz/gezj
jalbeqbneblezbgtz
1(001)addiaddiusltisltiuANDiORi xORilui
2(010)TLBFlPt3(011)4(100)lblhlwllw
lbulhulwr
5(101)sbshswlsw
swr6(110)lwc0lwc17(111)swc0swc1op(31:26) = 010001 (FlPt), (rt(16:16) = 0 => 
c = f, rt(16:16) = 1 => 
c = t), rs(25:21):23Œ2125Œ240(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(00)mfc1cfc1mtc1ctc11(01)bc1.c2(10)f = singlef= double3(11)op(31:26) = 010001 (FlPt), (f above: 10000 => f = s, 10001 => f = d), funct(5:0):2Œ0
5Œ30(000)1(001)2(010)3(011)4(100)5(101)6(110)7(111)
0(000)add.fsub.fmul.fdiv.fabs.fmov.fneg.f1(001)2(010)3(011)4(100)cvt.s.fcvt.d.fcvt.w.f5(101)
6(110)c.f.fc.un.fc.eq.fc.ueq.fc.olt.fc.ult.fc.ole.fc.ule.f7(111)c.sf.fc.ngle.fc.seq.fc.ngl.fc.lt.fc.nge.fc.le.fc.ngt.fFIGURE 3.18 MIPS ﬂ oating-point instruction encoding.
 is notation gives the value of
 eld by row and by column. For example, 
in the top portion of th
 gure, 
lw is found in row number 4 (100
two
 for bits 31–29 of the instruction) and column number 3 (011
two
 for bits 
28–26 of the instruction), so the corresponding value of the o
 eld (bits 31–26) is 100011
two
. Underscore means th
 eld is used elsewhere. 
For example, FlPt in row 2 and column 1 (op 
 010001two
 ned in the bottom part of th
 gure. Hence 
sub.f in row 0 and column 1 of 
the bottom section means that the func
 eld (bits 5–0) of the instruction) is 000001
two
 and the o eld (bits 31–26) is 010001
two
. Note that the 
5-bit rs
 eld, sp
 ed in the middle portion of th
 gure, determines whether the operation is single precision (
f  s, so rs 
 10000) or double 
precision (
f  d, so rs 
 10001). Similarly, bit 16 of the instruction determines if the 
bc1.c instruction tests for true (bit 16 
 1  bc1.t) or false (bit 16 
 0   bc1.f). Instructions in color are described in Chapter 2 or this chapter, with Appendix A covering all instructions. 
 is information is also found in column 2 of the MIPS Reference Data Card at the front of this book.

214 Chapter 3 Arithmetic for Computers
One issue that architects face in supportin
 oating-point arithmetic is whether 
to use the same registers used by the integer instructions or to add a special set 
fo
 oating point. Because programs normally perform integer operations and 
 oating-point operations on
 erent data, separating the registers will only 
slightly increase the number of instructions needed to execute a progra
 e major impact is to create a separate set of data transfer instructions to move data 

betw
 oating-point registers and memory.
 e be
 ts of separate
 oating-point registers are having twice as many 
registers without using up more bits in the instruction format, having twice the 

register bandwidth by having separate integer an
 oating-point register sets, and 
being able to customize registers to
 oating point; for example, some computers 
convert all sized operands in registers into a single internal format.
Compiling a Floating-Point C Program into MIPS Assembly Code
Let’s convert a temperature in Fahrenheit to Celsius:
   float f2c (float fahr)           {
                  return ((5.0/9.0) *(fahr – 32.0));
           }Assume that the
 oating-point argument 
fahr is passed in 
$f12 and the 
result should go in 
$f0. (Unlike integer register
 oating-point register 0 can 
contain a number.) What is the MIPS assembly code?
We assume that the compiler places the thre
 oating-point constants in 
memory within easy reach of the global pointer 
$gp e 
 rst two instruc-
tions load the constants 5.0 and 9.0 int
 oating-point registers:
f2c:    lwc1 $f16,const5($gp) # $f16 = 5.0 (5.0 in memory)
    lwc1 $f18,const9($gp) # $f18 = 9.0 (9.0 in memory) ey are then divided to get the fraction 5.0/9.0:
   div.s $f16, $f16, $f18 # $f16 = 5.0 / 9.0Hardware/
Software 
InterfaceEXAMPLEANSWER
 3.5 Floating Point 
215(Many compilers would divide 5.0 by 9.0 at compile time and save the single 
constant 5.0/9.0 in memory, thereby avoiding the divide at runtime.) Next, we 

load the constant 32.0 and then subtract it from 
fahr ($f12):   lwc1 $f18, const32($gp)# $f18 = 32.0   sub.s $f18, $f12, $f18 # $f18 = fahr – 32.0Finally, we multiply the two intermediate results, placing the product in $
f0 as the return result, and then return
   mul.s $f0, $f16, $f18 # $f0 = (5/9)*(fahr – 32.0)
   jr $ra                # returnNow let’s perfor
 oating-point operations on matrices, code commonly 
found in scien
 c programs.
Compiling Floating-Point C Procedure with Two-Dimensional 
Matrices into MIPSMost
 oating-point calculations are performed in double precision. Let’s per-
form matrix multiply of C 
 C  A * B. It is commonly called DGEMM, 
for Double precision, General Matrix Multiply. We’ll see versions of DGEMM 
again in Section 3.8 and subsequently in Chapters 4, 5, and 6. Let’s assume C, 

A, and B are all square matrices with 32 elements in each dimension.
   void mm (double c[][], double a[][], double b[][])   {
           int i, j, k;
           for (i = 0; i != 32; i = i + 1)
           for (j = 0; j != 32; j = j + 1)
           for (k = 0; k != 32; k = k + 1)
             c[i][j] = c[i][j] + a[i][k] *b[k][j];
   } e array starting addresses are parameters, so they are in 
$a0, $a1, and 
$a2. Assume that the integer variables are in 
$s0, $s1, and 
$s2, respectively. 
What is the MIPS assembly code for the body of the procedure?
Note that 
c[i][j] is used in the innermost loop above. Since the loop index 
is k, the index does not a
 ect c[i][j], so we can avoid loading and storing 
c[i][j] each iteration. Instead, the compiler loads 
c[i][j] into a register 
outside the loop, accumulates the sum of the products of 
a[i][k] and 
EXAMPLEANSWER
216 Chapter 3 Arithmetic for Computers
b[k][j] in that same register, and then stores the sum into 
c[i][j] upon 
termination of the innermost loop.
We keep the code simpler by using the assembly language pseudoinstructions 
li (which loads a constant into a register), and 
l.d and 
s.d (which the 
assembler turns into a pair of data transfer instructions, 
lwc1 or 
swc1, to a 
pair of
 oating-point registers).
 e body of the procedure starts with saving the loop termination value of 
32 in a temporary register and then initializing the three 
for loop variables:
   mm:...       li     $t1, 32  # $t1 = 32 (row size/loop end)
      li     $s0, 0   # i = 0; initialize 1st for loop
L1:   li     $s1, 0   # j = 0; restart 2nd for loop
L2:   li     $s2, 0   # k = 0; restart 3rd for loopTo calculate the address of 
c[i][j], we need to know how a 32 
 32, two-
dimensional array is stored in memory. As you might expect, its layout is the 
same as if there were 32 single-dimension arrays, each with 32 elements. So the 

 rst step is to skip over the 
i “single-dimensional arrays,” or rows, to get the 
one we wan
 us, we multiply the index in th
 rst dimension by the size of 
the row, 32. Since 32 is a power of 2, we can us
  instead:
sll  $t2, $s0, 5      # $t2 = i * 25 (size of row of c)Now we add the second index to select the jth element of the desired row:
   addu  $t2, $t2, $s1   # $t2 = i * size(row) + jTo turn this sum into a byte index, we multiply it by the size of a matrix element 

in bytes. Since each element is 8 bytes for double precision, we can inste
    by 3:
   sll  $t2, $t2, 3      # $t2 = byte offset of [i][j]Next we add this sum to the base address of 
c, giving the address of 
c[i][j], and then load the double precision number 
c[i][j] into 
$f4:addu  $t2, $a0, $t2   # $t2 = byte address of c[i][j]l.d   $f4, 0($t2)     # $f4 = 8 bytes of c[i][j] e followin
 ve instructions are virtually identical to th
 ve: calculate 
the address and then load the double precision number 
b[k][j].L3: sll $t0, $s2, 5    # $t0 = k * 2
5 (size of row of b)    addu $t0, $t0, $s1 # $t0 = k * size(row) + j
    sll $t0, $t0, 3    # $t0 = byte offset of [k][j]
    addu $t0, $a2, $t0 # $t0 = byte address of b[k][j]
    l.d $f16, 0($t0)   # $f16 = 8 bytes of b[k][j]Similarly, th
 ve instructions are like th
 ve: calculate the address 
and then load the double precision number 
a[i][k].
 3.5 Floating Point 
217sll     $t0, $s0, 5    # $t0 = i * 2
5 (size of row of a)addu    $t0, $t0, $s2  # $t0 = i * size(row) + ksll     $t0, $t0, 3    # $t0 = byte offset of [i][k]
addu    $t0, $a1, $t0  # $t0 = byte address of a[i][k]
l.d     $f18, 0($t0)   # $f18 = 8 bytes of a[i][k]Now that we have loaded all the data, we ar nally ready to do so
 oating-
point operations! We multiply elements of 
a and 
b located in registers 
$f18 and 
$f16, and then accumulate the sum in 
$f4.mul.d $f16, $f18, $f16 # $f16 = a[i][k] * b[k][j]
add.d $f4, $f4, $f16   # f4 = 
c[i][j] + a[i][k] * b[k][j] e 
 nal block increments the index 
k and loops back if the index is not 32. 
If it is 32, and thus the end of the innermost loop, we need to store the sum 
accumulated in $
f4 into 
c[i][j].addiu  $s2, $s2, 1     # $k = k + 1bne    $s2, $t1, L3    # if (k != 32) go to L3
s.d    $f4, 0($t2)     # c[i][j] = $f4Similarly, thes
 nal four instructions increment the index variable of the 
middle and outermost loops, looping back if the index is not 32 and exiting if 
the index is 32.
addiu  $s1, $s1, 1     # $j = j + 1bne    $s1, $t1, L2    # if (j != 32) go to L2
addiu  $s0, $s0, 1     # $i = i + 1
bne    $s0, $t1, L1    # if (i != 32) go to L1
…Figure 3.22
 below shows the x86 assembly language code for a slightl
 erent 
version of DGEMM in 
Figure 3.21
.Elaboration: The array layout discussed in the example, called 
row-major order,
 is used by C and many other programming languages. Fortran instead uses 
column-major order,
 whereby the array is stored column by column.
Elaboration:  oating-point registers could originally be used 
for double precision operations: $f0, $f2, $f4, …, 
$f30. Double precision is computed 
using pairs of these single precision register
 oating-point registers 
w oating-point numbers. MIPS-32 

added l.d and s.d to the instruction set. MIPS-32 also added “paired single” versions of 
 oating-point instructions, where a single instr
 oating-point 
operations on two 32-bit operands inside 64-bit registers (see Section 3.6). For example, 

add.ps $f0, $f2, $f4 is equivalent to add.s $f0, $f2, $f4 followed by 
add.s 
$f1, $f3, $f5.
218 Chapter 3 Arithmetic for Computers
Elaboration: Another reason for separate integer
 oating-point registers is that 
microprocessors in the 1980s didn’t have enough transistor
 oating-point unit 
on the same chip as the integer unit. Hence, oating-point unit,
 oating-point registers, was optionally available as a second chip. Such optional accelerator 
chips are called coprocessors,
 and explain the acron
 oating-point loads in MIPS: 
lwc1 means load word to coprocessor 1, oating-point unit. (Coprocessor 0 deals 

with virtual memory, described in Chapter 5.) Since the early 1990s, microprocessors 

ha oating point (and just about everything else) on chip, and hence the term 

coprocessor joins accumulator and core memory
 as quaint terms that date the speaker.
Elaboration: As mentioned in Section 3.4, accelerating division is more challenging 
than multiplication. In addition to SRT, another technique to leverage a fast multiplier 

is Newton’s iteration
,  nd 
the reciprocal 1/c, which is then multiplied by the other operand. Iteration techniques 

cannot be rounded properly without calculating many extra bits. A TI chip solved this 

problem by calculating an extra-precise reciprocal.
Elaboration: Java embraces IEEE 754 b nition of Ja oating-point 
data types and operations. Thus, rst example could have well been 

generated for a class method that converted Fahrenheit to Celsius.
The second example above uses multiple dimensional arrays, which are not explicitly 
supported in Java. Java allows arrays of arrays, but each array may have its own length, 

unlike multiple dimensional arrays in C. Like the examples in Chapter 2, a Java version 

of this second example would require a good deal of checking code for array bounds, 

including a new length calculation at the end of row access. It would also need to check 

that the object reference is not null.Accurate ArithmeticUnlike integers, which can represent exactly every number between the smallest and 
largest number
 oating-point numbers are normally approximations for a number 
they can’t really represen
 e reason is that an
 nite variety of real numbers 
exists between, say, 0 and 1, but no more than 2
53 can be represented exactly in 
double precisio
 oating poin
 e best we can do is getting th
 oating-point 
representation close to the actual number
 us, IEEE 754 o ers several modes of 
rounding to let the programmer pick the desired approximation.
Rounding sounds simple enough, but to round accurately requires the hardware 
to include extra bits in the calculation. In the preceding examples, we were vague 

on the number of bits that an intermediate representation can occupy, but clearly, 

if every intermediate result had to be truncated to the exact number of digits, there 

would be no opportunity to round. IEEE 754, therefore, always keeps two extra bits 

on the right during intermediate additions, called 
guard
 and 
round
, respectively. 
Let’s do a decimal example to illustrate their value.
guard
  e 
 rst of two 
extra bits kept on the 
right during intermediate 

calculations of
 oating-
point numbers; used 

to improve rounding 

accuracy.
round
 Method to 
make the intermediate 

 oating-point result
 t th
 oating-point format; 
the goal is typically to
 nd the nearest number that 

can be represented in the 

format.

 3.5 Floating Point 
219Rounding with Guard DigitsAdd 2.56
ten
  100 to 2.34
ten
  102, assuming that we have thre
 cant 
decimal digits. Round to the nearest decimal number with thre
 cant 
decimal digi
 rst with guard and round digits, and then without them.
First we mu
  the smaller number to the right to align the exponents, so 
2.56ten
  100 becomes 0.0256
ten
  102. Since we have guard and round digits, 
we are able to represent the two le
 cant digits when we align expo-
nen
 e guard digit holds 5 and the round digit ho
 e sum is
2.3400ten
+ 0.0256ten
2.3656ten
 us the sum is 2.3656
ten
  102. Since we have two digits to round, we want 
values 0 to 49 to round down and 51 to 99 to round up, with 50 being the 
tiebreaker. Rounding the sum up with thre cant digits yields 2.37
ten
  102.Doing this 
without
 guard and round digits drops two digits from the 
calculatio
 e new sum is then
2.34ten
+ 0.02ten
2.36ten
 e answer is 2.36
ten
  102, o
  by 1 in the last digit from the sum above.
Since the worst case for rounding would be when the actual number is halfway 

between tw
 oating-point representations, accurac
 oating point is normally 
measured in terms of the number of bits in error in the le
 cant bits of the 
 cand; the measure is called the number of 
units in the last place, or 
ulp
. If 
a number were o
  by 2 in the le
 cant bits, it would be called o
  by 2 ulps. 
Provided there is no over
 ow, under
 ow, or invalid operation exceptions, IEEE 
754 guarantees that the computer uses the number that is within one-half ulp.
Elaboration: Although the example above really needed just one extra digit, multiply 
can need two. A binary product may have one leading 0 bit; hence, the normalizing step 
 cant bit 
of the product, leaving the round bit to help accurately round the product.
IEEE 754 has four rounding modes: always round up (tow always round down 
(toward 
 truncate, and round to nearest e nal mode determines what to 
do if the number is exactly halfway in between. The U.S. 
Internal Revenue Service
 (IRS) always rounds 0.50 dollars up, t of the IRS. A more equitable way 

would be to round up this case half the time and round down the other half. IEEE 754 
sa cant bit retained in a halfway case would be odd, add one; 
EXAMPLEANSWERunits in the last place (ulp)
  e number of 
bits in error in the least 
 cant bits of the 
 cand between 
the actual number and 

the number that can be 

represented.

220 Chapter 3 Arithmetic for Computers
if it’s even, truncate. This method alwa cant bit in the 
tie-breaking case, giving the rounding mode its name. This mode is the most commonly 

used, and the only one that Java supports.
The goal of the extra rounding bits is to allow the computer to get the same results as if the intermediate results w nite precision and then rounded. To 

support this goal and round to the nearest even, the standard has a third bit in addition 

to guard and round; it is set whenever there are nonzero bits to the right of the round 
bit. This sticky bit
 allows the computer to see the difference between 0.50 … 00 
ten and 0.50 … 01ten when rounding.
The sticky bit may be set, for example, during addition, when the smaller number is 
shifted to the right. Suppose we added 5.01
ten  101 to 2.34ten  102 in the example above. Even with guard and round, we would be adding 0.0050 to 2.34, with a sum of 
2.3450. The sticky bit would be set, since there are nonzero bits to the right. Without the 

sticky bit to remember whether any 1s were shifted off, we would assume the number 

is equal to 2.345000 … 00 and round to the nearest even of 2.34. With the sticky bit 

to remember that the number is larger than 2.345000 … 00, we round instead to 2.35.
Elaboration: PowerPC, SPARC64, AMD SSE5, and Intel AVX architectures provide a 
single instruction that does a multiply and add on three registers: 
a  a  (b  c). Obviously, this instr oating-point performance for this 

common operation. Equally important is that instead of performing two roundings—after 

the multiply and then after the add—which would happen with separate instructions, 

the multiply add instruction can perform a single rounding after the add. A single 

rounding step increases the precision of multiply add. Such operations with a single rounding are called fused multiply add. It was added to the IEEE 754-2008  standard 
(see  Section 3.11).Summary
 e Big Picture
 that follows reinforces the stored-program concept from Chapter 2; 
the meaning of the information cannot be determined just by looking at the bits, for 
the same bits can represent a variety of objec
 is section shows that computer 
arit
 nite and thus can disagree with natural arithmetic. For example, the 
IEEE 754 standard
 oating-point representation
(1)5  (1  Fraction) 
 2(Exponent 
Bias)
is almost always an approximation of the real number. Computer systems must 

take care to minimize this gap between computer arithmetic and arithmetic in the 

real world, and programmers at times need to be aware of the implications of this 

approximation.
sticky bit
 A bit used in 
rounding in addition to 
guard and round that is 

set whenever there are 

nonzero bits to the right 

of the round bit.
fused multiply add
  oating-point 
instruction that performs 

both a multiply and an 

add, but rounds only once 

 er the add.
Bit patterns have no inherent meanin
 ey may represent signed integers, 
unsigned integer
 oating-point numbers, instructions, and so on. What is 
represented depends on the instruction that operates on the bits in the word.
The BIGPicture
 3.5 Floating Point 
221C typeJava typeData transfers
Operationsintintlw, sw, lui
addu, addiu, subu, mult, div, AND, ANDi, OR, ORi, NOR, slt, sltiunsigned int—lw, sw, lui
addu, addiu, subu, multu, divu, AND, 
ANDi, OR, ORi, NOR, sltu, sltiuchar—lb, sb, lui
add, addi, sub, mult, div AND, ANDi, 
OR, ORi, NOR, slt, slti—charlh, sh, lui
addu, addiu, subu, multu, divu, AND, 
ANDi, OR, ORi, NOR, sltu, sltiufloatfloatlwc1, swc1
add.s, sub.s, mult.s, div.s, c.eq.s, 
c.lt.s, c.le.sdoubledoublel.d, s.d
add.d, sub.d, mult.d, div.d, c.eq.d, 
c.lt.d, c.le.dIn the last chapter, we presented the storage classes of the programming language C 
(see the 
Hardware/So
 ware Interface
 section in Sectio
 e table above shows 
some of the C and Java data types, the MIPS data transfer instructions, and instructions 

that operate on those types that appear in Chapter 2 and this chapter. Note that Java 

omits unsigned integers.
 e revised IEEE 754-2008 standard added a 16-bi
 oating-point format wi
 ve exponent bits. What do you think is the likely range of numbers it could represent?
1.     1.0000 00 
 20             to     1.1111 1111 11 
 231, 02. 1.0000 0000 0  214   to 
1.1111 1111 1  215, 0, aN
3. 1.0000 0000 00  214 to 
1.1111 1111 11  215, 0, aN
4. 1.0000 0000 00  215 to 
1.1111 1111 11  214, 0, aN
Elaboration: To accommodate comparisons that may include NaNs, the standard 
includes ordered
 and unordered
 as options for compares. Hence, the full MIPS instruction 
set has man avors of compares to support NaNs. (Java does not support unordered 
compares.)Hardware/ 
Software 

InterfaceCheck Yourself
 e majo
 erence between computer numbers and numbers in the 
real world is that computer numbers have limited size and hence limited 
precision; it’s possible to calculate a number too big or too small to be 

represented in a word. Programmers must remember these limits and 

write programs accordingly.

222 Chapter 3 Arithmetic for Computers
In an attempt to squeeze ever oating-point operation, 
the standard allows some numbers to be represented in unnormalized form. Rather than 
having a gap between 0 and the smallest normalized number, IEEE allows 
denormalized  
numbers
 (also known as denorms or subnormals). They have the same exponent as 
zero but a nonzero fraction. The cance until it 

becomes 0, called 
gradual underﬂ ow
. For example, the smallest positive single precision 

normalized number is
1.0000 0000 0000 0000 0000 000two  2126but the smallest single precision denormalized number is
0.0000 0000 0000 0000 0000 001two  2126, or 1.0two  2149For double precision, the denorm gap goes from 1.0 
 21022 to 1.0  21074.The possibility of an occasional unnormalized operand has given headaches to 
 oating-point designers who are tr oating-point units. Hence, many 

computers cause an exception if an operand is denormalized, letting software complete 

the operation. Although software implementations are perfectly valid, their lower 

performance has lessened the popularity of denorms in por oating-point software. 

Moreover, if programmers do not expect denorms, their programs may surprise them.
 3.6  Parallelism and Computer Arithmetic: 
Subword Parallelism
Since every desktop microprocessor by
 nition has its own graphical displays, 
as transistor budgets increased it was inevitable that support would be added for 
graphics operations.
Many graphics systems originally used 8 bits to represent each of the three 
primary colors plus 8 bits for a location of a pixel
 e addition of speakers and 
microphones for teleconferencing and video games suggested support of sound as 

well. Audio samples need more than 8 bits of precision, but 16 bits ar
  cient.
Every microprocessor has special support so that bytes and halfwords take up 
less space when stored in memory (see Section 2.9), but due to the infrequency of 

arithmetic operations on these data sizes in typical integer programs, there was 

little support beyond data transfers. Architects recognized that many graphics 

and audio applications would perform the same operation on vectors of this data. 

By partitioning the carry chains within a 128-bit adder, a processor could use 

parallelism
 to perform simultaneous operations on short vectors of sixteen 8-bit 
operands, eight 16-bit operands, four 32-bit operands, or two 64-bit opera
 e cost of such partitioned adders was small.
Given that the parallelism occurs within a wide word, the extensions are 
cl
 ed as 
subword parallelism
. It is also cl
 ed under the more general name 
of 
data level parallelism
 ey have been also called vector or SIMD, for single 
instruction, multiple data (see Sectio
 e rising popularity of multimedia 

 3.6 Parallelism and Computer Arithemtic: Subword Parallelism 
223applications led to arithmetic instructions that support narrower operations that 
can easily operate in parallel.
For example, ARM added more than 100 instructions in the NEON multimedia 
instruction extension to support subword parallelism, which can be used either 

with ARMv7 or ARMv8. It added 256 bytes of new registers for NEON that can be 

viewed as 32 registers 8 bytes wide or 16 registers 16 bytes wide. NEON supports 

all the subword data types you can imagine 
except
 64-bi
 oating point numbers:
 8-bit, 16-bit, 32-bit, and 64-bit signed and unsigned integers
 32-bit 
 oating point numbers
Figure 3.19
 gives a summary of the basic NEON instructions.
FIGURE 3.19 Summary of ARM NEON instructions for subword parallelism. 
We use the curly brackets {} to show optional 
variations of the basic operations: {S8,U8,8} stand for signed and unsigned 8-bit integers or 8-bit data where type doesn’t mat
ter, of which 16 
 t in a 128-bit register; {S16,U16,16} stand for signed and unsigned 16-bit integers or 16-bit type-less data, of whic
 t in a 128-bit register; 
{S32,U32,32} stand for signed and unsigned 32-bit integers or 32-bit type-less data, of whic
 t in a 128-bit register; {S64,U64,64} stand for 
signed and unsigned 64-bit integers or type-less 64-bit data, of whic
 t in a 128-bit register; {F32} stan
d for signed and unsigned 32-bit 
 oating point numbers, of whic
 t in a 128-bit register. Vector Load reads one n-element structure from memory into 1, 2, 3, or 4 NEON 
registers. It loads a single n-element structure to one lane (See Section 6.6), and elements of the register that are not loade
d are unchanged. 
Vector Store writes one n-element structure into memory from 1, 2, 3, or 4 NEON registers.
Elaboration: In addition to signed and unsigned integers, xed-point” 
format of four sizes called I8, I16, I32, and I64, of which 16, 8, 4, t in a 128-
bit register, respectively. A por xed point is for the fraction (to the right of 

the binary point) and the rest of the data is the integer portion (to the left of the binary 

point).  The location of the binary point is up to the software. Many ARM processors do 

not ha oating point hardw oating point operations must be performed by 

library routines. F cantly faster than softw oating 

point routines, but more work for the programmer.
Data transferArithmeticLogical/Compare821.DNAV,46.DNAV}23U,23S,61U,61S,8U,8S{}W,L{DDAV,23F.DDAV
23F.RDLV
821.RROV,46.RROV}23U,23S,61U,61S,8U,8S{}W,L{BUSV,23F.BUSV
23F.RTSV
VLD{1,2,3.4}.{I8,I16,I32}VMUL.F32, VMULL{S8,U8,S16,U16,S32,U32}VEOR.64, VEOR.128
VST{1,2,3.4}.{I8,I16,I32}VMLA.F32, VMLAL{S8,U8,S16,U16,S32,U32}VBIC.64, VBIC.128
VMOV.{I8,I16,I32,F32}, #immVMLS.F32, VMLSL{S8,U8,S16,U16,S32,U32}VORN.64, VORN.128
VMVN.{I8,I16,I32,F32}, #immVMAX.{S8,U8,S16,U16,S32,U32,F32}
VCEQ.{I8,I16,I32,F32}VMOV.{I64,I128}
VMIN.{S8,U8,S16,U16,S32,U32,F32}VCGE.{S8,U8,S16,U16,S32,U32,F32}}23F,23U,23S,61U,61S,8U,8S{.TGCV
}23F,23S,61S,8S{.SBAV
}821I,46I{.NVMV

224 Chapter 3 Arithmetic for Computers
 3.7  Real Stuff:  Streaming SIMD Extensions and Advanced Vector Extensions in x86
 e original MMX (
MultiMedia eXtension
) and SSE (
Streaming SIMD Extension
) instructions for the x86 included similar operations to those found in ARM NEON. 
Chapter 2 notes that in 2001 Intel added 144 instructions to its architecture as 

part of SSE2, including double precisio
 oating-point registers and operations. It 
includes eight 64-bit registers that can be used fo
 oating-point operands. AMD 
expanded the number to 16 registers, called XMM, as part of AMD64, which 

Intel relabeled EM64T for its use. 
Figure 3.20
 summarizes the SSE and SSE2 

instructions.
In addition to holding a single precision or double precision number in a 
register, Intel allows multip
 oating-point operands to be packed into a single 
128-bit SSE2 register: four single precision or two double precisio
 us, the 16 
 oating-point registers for SSE2 are actually 128 bits wide. If the operands can be 
arranged in memory as 128-bit aligned data, then 128-bit data transfers can load 

and store multiple operands per instructio
 is pack
 oating-point format is 
supported by arithmetic operations that can operate simultaneously on four singles 

(PS) or two doubles (PD). 
Data transferArithmeticCompare
MOV{A/U}{SS/PS/SD/PD} xmm, mem/xmmADD{SS/PS/SD/PD} xmm,mem/xmm  CMP{SS/PS/SD/PD}
SUB{SS/PS/SD/PD} xmm,mem/xmm  MOV {H/L} {PS/PD}  xmm, mem/xmm MUL{SS/PS/SD/PD} xmm,mem/xmm DIV{SS/PS/SD/PD} xmm,mem/xmm SQRT{SS/PS/SD/PD} mem/xmmMAX {SS/PS/SD/PD} mem/xmm
MIN{SS/PS/SD/PD} mem/xmmFIGURE 3.20 The SSE/SSE2 ﬂ oating-point instructions of the x86.
 xmm means one operand is 
a 128-bit SSE2 register, and mem/xmm means the other operand is either in memory or it is an SSE2 register. 
We use the curly brackets {} to show optional variations of the basic operations: {SS} stands for 
Scalar Single 
precisio
 oating point, or one 32-bit operand in a 128-bit register; {PS} stands for 
Packed Single
 precision 
 oating point, or four 32-bit operands in a 128-bit register; {SD} stands for Scalar Double precisio
 oating 
point, or one 64-bit operand in a 128-bit register; {PD} stands for 
Packed Double
 precisio
 oating point, or 
two 64-bit operands in a 128-bit register; {A} means the 128-bit operand is aligned in memory; {U} means 

the 128-bit operand is unaligned in memory; {H} mean
s move the high half of the 128-bit operand; and {L} 
means move the low half of the 128-bit operand.

 3.8  
Going Faster:  Subword Parallelism and Matrix Multiply 
225In 2011 Intel doubled the width of the registers again, now called YMM, with 
Advanced Vector Extensions (AVX)
 us, a single operation can now specify eight 
32-bi
 oating-point operations or four 64-bit
 oating-point operation
 e legacy SSE and SSE2 instructions now operate on the lower 128 bits of the YMM 
register
 us, to go from 128-bit and 256-bit operations, you prepend the letter 
“v” (for vector) in front of the SSE2 assembly language operations and then use the 

YMM register names instead of the XMM register name. For example, the SSE2 

instruction to perform two 64-bi
 oating-point multiplies
addpd  %xmm0, %xmm4It becomes
vaddpd  %ymm0, %ymm4 which now produces four 64-bi
 oating-point multiplies.
Elaboration: AVX also added three address instructions to x86. For example, 
vaddpd can now specify vaddpd %ymm0, %ymm1, %ymm4 # %ymm4 = %ymm1 + %ymm2instead of the standard two address version
addpd  %xmm0, %xmm4 # %xmm4 = %xmm4 + %xmm0(Unlike MIPS, the destination is on the right in x86.) Three addresses can reduce the 
number of registers and instructions needed for a computation.
 3.8  Going Faster:  Subword Parallelism and 
Matrix MultiplyTo demonstrate the performance impact of subword parallelism, we’ll run the same 
code on the Intel Cor rst without AVX and then with it. 
Figure 3.21
 shows an 

unoptimized version of a matrix-matrix multiply written in C.  As we saw in Section 

3.5, this program is commonly called 
DGEMM
, which stands for Double precision 
GEneral Matrix Multiply. Starting with this edition, we have added a new section 

entitled “Going Faster” to demonstrate the performance b
 t of adapting so
 ware 
to the underlying hardware, in this case the Sandy Bridge version of the Intel Core 

i7 microprocessor
 is new section in Chapters 3, 4, 5, and 6 will incrementally 
improve DGEMM performance using the ideas that each chapter introduces.
Figure 3.22
 shows the x86 assembly language output for the inner loop of 
Figure
 
3.21 e 
 ve 
 oating point-instructions start with a 
v like the AVX instructions, 
but note that they use the XMM registers instead of YMM, and they include 
sd in 
the name, which stands for scalar double precision. We’l
 ne the subword parallel 
instructions shortly.

226 Chapter 3 Arithmetic for Computers
FIGURE 3.22 The x86 assembly language for the body of the nested loops generated by compiling the 
optimized C code in Figure 3.21
. Although it is dealing with just 64-bits of data, the compiler uses the AVX version of 
the instructions instead of SSE2 presumably so that it can use three address per instruction instead of two (see the Elaboratio
n in Section 3.7).
FIGURE 3.21 Unoptimized C version of a double precision matrix multiply, widely known as DGEMM for 

Double-precision GEneral Matrix Multiply (GEMM). Because we are passing the matrix dimension as the parameter 
n, this version of DGEMM uses single dimensional versions of matrices 
C, A, and 
B and address arithmetic to get better 
performance instead of using the more intuitive two-dimensional arrays that we saw in Sectio
 e comments remind 
us of this more intuitive notation.
 1.  
void dgemm (int n, double* A, double* B, double* C) 2. {
 3. for (int i = 0; i < n; ++i)

 4. for (int j = 0; j < n; ++j) 

 5. {

 6. double cij = C[i+j*n]; /* cij = C[i][j] */

 7. for( int k = 0; k < n; k++ )

 8.  
cij += A[i+k*n] * B[k+j*n]; /* cij += A[i][k]*B[k][j] */ 9. C[i+j*n] = cij; /* C[i][j] = cij */

10. }11. } 1.  
vmovsd (%r10),%xmm0 # Load 1 element of C into %xmm0 2. mov    %rsi,%rcx 
# register %rcx = %rsi 3. xor    %eax,%eax 
# register %eax = 0 4.  
vmovsd (%rcx),%xmm1 # Load 1 element of B into %xmm1 5. add    %r9,%rcx 
# register %rcx = %rcx + %r9 6.  
vmulsd (%r8,%rax,8),%xmm1,%xmm1 # Multiply %xmm1, element of A 7. add    $0x1,%rax 
# register %rax = %rax + 1 8. cmp    %eax,%edi 
# compare %eax to %edi 9. vaddsd %xmm1,%xmm0,%xmm0 # Add %xmm1, %xmm0
10. jg     30 <dgemm+0x30> # jump if %eax > %edi

11. add    $0x1,%r11d 
# register %r11 = %r11 + 112. vmovsd %xmm0,(%r10) 
# Store %xmm0 into C element
 3.8 Going Faster: Subword Parallelism and Matrix Multiply 
227FIGURE 3.23 Optimized C version of DGEMM using C intrinsics to generate the AVX subword-parallel 
instructions for the x86. 
Figure 3.24
 shows the assembly language produced by the compiler for the inner loop.
While compiler writers may eventually be able to routinely produce high-
quality code that uses the AVX instructions of the x86, for now we must “cheat” by 
using C intrinsics that more or less tell the compiler exactly how to produce good 

code. 
Figure 3.23
 shows the enhanced version of 
Figure 3.21
 for which the Gnu C 

compiler produces AVX code. 
Figure 3.24
 shows annotated x86 code that is the 

output of compiling using gcc with the –O3 level of optimization.
 e declaration on line 6 of 
Figure 3.23
 uses the 
__m256d data type, which tells 
the compiler the variable will hold 4 double-precisio
 oating-point val
 e intrinsic 
_mm256_load_pd() also on line 6 uses AVX instructions to load 4 
double-precisio
 oating-point numbers in parallel (
_pd) from the matrix 
C into 
c0 e address calculation 
C+i+j*n on line 6 represents element 
C[i+j*n]. Symmetrically, th
 nal step on line 11 uses the intrinsic 
_mm256_store_pd() to store 4 double-precisio
 oating-point numbers from 
c0 into the matrix 
C. As we’re going through 4 elements each iteration, the outer 
for loop on line 4 
increments 
i by 4 instead of by 1 as on line 3 of 
Figure 3.21
.Inside the loops, on line 9 w
 rst load 4 elements of A again using 
_mm256_load_pd(). To multiply these elements by one element of B, on line 10 we
 rst 
use the intrinsic 
_mm256_broadcast_sd(), which makes 4 identical copies 
of the scalar double precision number—in this case an element of 
B—in one of the 
YMM registers. We then use 
_mm256_mul_pd() on line 9 to multiply the four 
double-precision results in parallel. Finally, 
_mm256_add_pd() on line 8 adds 
the 4 products to the 4 sums in 
c0.Figure 3.24
 shows resulting x86 code for the body of the inner loops produced 
by the compiler. You can see th
 ve AVX instructions—they all start with 
v and 
 1. #include <x86intrin.h>
 2. void dgemm (int n, double* A, double* B, double* C)

 3. {

 4.   for ( int i = 0; i < n; i+=4 )

 5.     for ( int j = 0; j < n; j++ ) {

 6.  
      __m256d c0 = _mm256_load_pd(C+i+j*n); /* c0 = C[i][j] */ 7.       for( int k = 0; k < n; k++ )

 8. c0 = _mm256_add_pd(c0, /* c0 += A[i][k]*B[k][j] */

 9. _mm256_mul_pd(_mm256_load_pd(A+i+k*n), 

10. _mm256_broadcast_sd(B+k+j*n)));
11.  _mm256_store_pd(C+i+j*n, c0); /* C[i][j] = c0 */12.     }

13. }
228 Chapter 3 Arithmetic for Computers
four of th
 ve use 
pd for parallel double precision—that correspond to the C 
intrinsics mentioned above
 e code is very similar to that in 
Figure 3.22
 above: 
both use 12 instructions, the integer instructions are nearly identical (bu
 erent 
registers), and th
 oating-point instructio
 erences are generally just going 
from 
scalar double
 (sd) using XMM registers to 
parallel double
 (pd) with YMM 
register
 e one exception is line 4 of 
Figure 3.24
. Every element of A must be 
multiplied by one element of B. One solution is to place four identical copies of the 
64-bit B element side-by-side into the 256-bit YMM register, which is just what the 

instruction 
vbroadcastsd does.
For matrices of dimensions of 32 by 32, the unoptimized DGEMM in 
Figure 3.21
 runs at 1.7 GigaFLOPS (FLoating point Operations Per Second) on one core of a 

2.6 GHz Intel Core i7 (Sandy Bridg
 e optimized code in 
Figure 3.23
 performs 
at 6.4 GigaFLO
 e AVX version is 3.85 times as fast, which is very close to the 
factor of 4.0 increase that you might hope for from performing 4 times as many 

operations at a time by using 
subword parallelism
.Elaboration: As mentioned in the Elaboration in Section 1.6, Intel offers Turbo mode 
that temporarily runs at a higher clock rate until the chip gets too hot. This Intel Core i7 
(Sandy Bridge) can increase from 2.6 GHz to 3.3 GHz in Turbo mode. The results above 

are with Turbo mode turned off. If we turn it on, we improve all the results by the increase 

in the clock rate of 3.3/2.6 = 1.27 to  2.1 GFLOPS for unoptimized DGEMM and 8.1 
GFLOPS with AVX. Turbo mode works particularly well when using only a single core of 

an eight-core chip, as in this case, as it lets that single core use much more than its fair 

share of power since the other cores are idle.
FIGURE 3.24 The x86 assembly language for the body of the nested loops generated by compiling 
the optimized C code in Figure 3.23
. Note the similarities to 
Figure 3.22
, with the primar
 erence being that the 
 ve 
 oating-point operations are now using YMM registers and using the pd versions of the instructions for parallel double 
precision instead of the sd version for scalar double precision.
 1.  
vmovapd (%r11),%ymm0 # Load 4 elements of C into %ymm0 2. mov    %rbx,%rcx # register %rcx = %rbx
 3. xor    %eax,%eax  # register %eax = 0

 4.  
vbroadcastsd (%rax,%r8,1),%ymm1 # Make 4 copies of B element
 5. add    $0x8,%rax # register %rax = %rax + 8

 6.  
vmulpd (%rcx),%ymm1,%ymm1 # Parallel mul %ymm1,4 A elements
 7. add    %r9,%rcx # register %rcx = %rcx + %r9

 8. cmp    %r10,%rax # compare %r10 to %rax

 9.  
vaddpd %ymm1,%ymm0,%ymm0 # Parallel add %ymm1, %ymm0
10.  jne    50 <dgemm+0x50> # jump if not %r10 != %rax11. add    $0x1,%esi 
# register % esi = % esi + 112.  vmovapd %ymm0,(%r11) # Store %ymm0 into 4 C elements
 3.9 Fallacies and Pitfalls 
229 3.9 Fallacies and Pitfalls
Arithmetic fallacies and pitfalls generally stem from th
 erence between the 
limited precision of computer arithmetic and the unlimited precision of natural 
arithmetic.
 Fallacy: Just
  
  instruction can replace an integer multiply by a 
power of 2, a right shi
  is the same as an integer division by a power of 2.
Recall that a binary number 
c, where 
xi means the 
ith bit, represents the number
…  (x3  23)  (x2  22) 1 (x1  21)  (x0  20)S
 ing the bits of 
c right by 
n bits would seem to be the same as dividing by 
2n. And this 
is true for unsigned integer
 e problem is with signed integers. For 
example, suppose we want to divide 
5ten
 by 4
ten
; the quotient should be 
1ten
 e two’s complement representation of 
5ten
 is1111  1111  1111  1111  1111  1111  1111  1011two
According to this fallacy
 ing right by two should divide by 4
ten
 (22):0011  1111  1111  1111  1111  1111  1111  1110two
With a 0 in the sign bit, this result is clearly wron
 e value created by th
  right is actually 1,073,741,822
ten
 instead of 
1ten
.A solution would be to have an arithmetic righ
  that extends the sign bit 
instead of
 ing in 0s. A 2-bit arit
  right of 
5ten
 produces
1111  1111  1111  1111  1111  1111  1111  1110two
 e result is 
2ten
 instead of 
1ten
; close, but no cigar.
Pitfall: Floating-point addition is not associative. 
Associativity holds for a sequence of two’s complement integer additions, even if the 

computation over
 ows. Alas, becaus
 oating-point numbers are approximations 
of real numbers and because computer arithmetic has limited precision, it does 

not hold fo
 oating-point numbers. Given the great range of numbers that can be 
represente
 oating point, problems occur when adding two large numbers of 
opposite signs plus a small number. For example, let’s see if 
c  (a  b)  (c  a)  b. Assume 
c  1.5ten
  1038, a  1.5ten
  1038, and 
b  1.0, and that these are 
all single precision numbers.
 us mathematics 
ma
 ned as the 
subject in which we 

never know what we 

are talking about, nor 

whether what we are 

saying is true.
Bertrand Russell, 
Recent 
Words on the Principles 
of Mathematics,
 1901
230 Chapter 3 Arithmetic for Computers
c()1.510(1.5101.0)
1.510(1.5
ten
38ten
38ten
38teabnn
38ten
38ten
38ten
10)
0.0c()(1.5101.510)1.0
(0.0ab)
)1.0
1.0Si
 oating-point numbers have limited precision and result in approximations 
of real results, 1.5
ten
  1038 is so much larger than 1.0
ten
 that 1.5
ten
  1038  1.0 is still 
1.5ten
  1038 at is why the sum of 
c, a, and 
b is 0.0 or 1.0, depending on the order 
of th
 oating-point additions, so 
c  (a  b)  (c  a)  b erefore, 
 oating-
point addition is 
not associative.
 Fallacy: Parallel execution strategies that work for integer data types also work 
for
 oating-point data types.
 Programs have typically been writt rst to run sequentially before being rewritten 

to run concurrently, so a natural question is, “Do the two versions get the same 

answer?” If the answer is no, you presume there is a bug in the parallel version that 

you need to track down.
 is approach assumes that computer arithmetic does not a
 ect the results when 
going from sequential to parallel
 at is, if you were to add a million numbers 
together, you would get the same results whether you used 1 processor or 1000 

processor
 is assumption holds for two’s complement integers, since integer 
addition is associative
 oating-point addition is not associative, the 
assumption does not hold.
A more vexing version of this fallacy occurs on a parallel computer where the 
operating system scheduler may us erent number of processors depending 

on what other programs are running on a parallel computer. As the varying 

number of processors from each run would cause th
 oating-point sums to be 
calculat
 erent orders,  getting slightl erent answers each time  despite 
running identical code with identical input ma
 ummox unaware parallel 
programmers.
Given this quandary, programmers who write parallel code with
 oating-point 
numbers need to verify whether the results are credible even if they don’t give the 

same exact answer as the sequential code
 e 
 eld that deals with such issues is 
called numerical analysis, which is the subject of textbooks in its own right. Such 

concerns are one reason for the popularity of numerical libraries such as LAPACK 

and SCALAPAK, which have been validated in both their sequential and parallel 

forms.
 Pitfal
 e MIPS instruction add immediate unsigned 
(addiu) sign-extends 
its 16-bit immediat
 eld.

 3.9 Fallacies and Pitfalls 
231Despite its name, add immediate unsigned (
addiu) is used to add constants to 
signed integers when we don’t care about over
 ow. MIPS has no subtract immediate 
instruction, and negative numbers need sign extension, so the MIPS architects 
decided to sign-extend the immediate
 eld. Fallacy: Only theoretical mathematicians care abou
 oating-point accuracy.
Newspaper headlines of November 1994 prove this statement is a fallacy (see 

Figure 3.25
 e following is the inside story behind the headlines.
 e Pentium used a standar
 oating-point divide algorithm that generates 
multiple quotient bits per step, using th
 cant bits of divisor and 
dividend to guess the next 2 bits of the quotien
 e guess is taken from a lookup 
table containing 
2, 1, 0, 1, or 
 e guess is multiplied by the divisor and 
subtracted from the remainder to generate a new remainder. Like nonrestoring 

division, if a previous guess gets too large a remainder, the partial remainder is 

adjusted in a subsequent pass.
Evidently, there wer
 ve elements of the table from the 80486 that Intel 
engineers thought could never be accessed, and they optimized the logic to return 

0 instead of 2 in these situations on the Pentium. Intel was wrong: while th
 rst 11 
FIGURE 3.25 A sampling of newspaper and magazine articles from November 1994, 
including the New York Times, San Jose Mercury News, San Francisco Chronicle, and 
Infoworld.
 e Penti
 oating-point divide bug even made the “Top 10 List” of the 
David Letterman 
Late Show
 on television. Intel eventually took a $300 million write-o
  to replace the buggy chips.

232 Chapter 3 Arithmetic for Computers
bits were always correct, errors would show up occasionally in bits 12 to 52, or the 
4th to 15th decimal digits.
A math professor at Lynchburg College in Virg
 omas Nicely, discovered the 
bug in Septemb
 er calling Intel technical support and getting no o
  cial reaction, he posted his discovery on the Intern
 is post led to a story in a trade 
magazine, which in turn caused Intel to issue a 
press release. It called the bug a glitch 
that would a
 ect only theoretical mathematicians, with the average spreadsheet 
user seeing an error every 27,000 years. IBM Research soon counterclaimed that the 

average spreadsheet user would see an error every 24 days. Intel soon threw in the 

towel by making the following announcement on December 21:
“We at Intel wish to sincerely apologize for our handling of the recently publicized 

Pentium processo
 aw. 
 e Intel Inside symbol means that your computer has 
a microprocessor second to none in quality and performanc
 ousands of Intel 
employees work very hard to ensure that this is true. But no microprocessor is 

ever perfect. What Intel continues to believe is technically an extremely minor 

problem has taken on a life of its own. Although Inte
 rmly stands behind the 
quality of the current version of the Pentium processor, we recognize that many 

users have concerns. We want to resolve these concerns. Intel will exchange the 

current version of the Pentium processor for an updated version, in which this 

 oating-point div
 aw is corrected, for any owner who requests it, free of 
charge anytime during the life of their computer.”
Analysts estimate that this recall cost Intel $500 million, and Intel engineers did not 

get a Christmas bonus that year.
 is story brings up a few points for everyone to ponder. How much cheaper 
would it have been t
 x the bug in July 1994? What was the cost to repair the 
damage to Intel’s reputation? And what is the corporate responsibility in disclosing 

bugs in a product so widely used and relied upon as a microprocessor?
 3.10 Concluding Remarks
Over the decades, computer arithmetic has become largely standardized, greatly 
enhancing the portability of programs. Two’s complement binary integer arithmetic is 

found in every computer sold today, and if it incl
 oating point support, it o
 ers 
the IEEE 754 binar
 oating-point arithmetic.
Computer arithmetic is distinguished from
 paper-and-pencil arithmetic by the 
constraints of limited precisio
 is limit may result in invalid operations through 
calculating numbers larger or smaller than the pr
 ned limits. Such anomalies, called 
“over
 ow” or “under
 ow,” may result in exceptions or interrupts, emergency events 
similar to unplanned subroutine calls. Chapters 4 and 5 discuss exceptions in more detail.
Floating-point arithmetic has the added challenge of being an approximation 
of real numbers, and care needs to be taken to ensure that the computer number 

 3.10 Concluding Remarks 
233selected is the representation closest to the actual number
 e challenges of 
imprecision and limited representation o
 oating point are part of the inspiration 
for th
 eld of numerical anal
 e recent switch to 
parallelism
 shines the 
searchlight on numerical analysis again, as solutions that were long considered 
safe on sequential computers must be reconsidered when trying to
 nd the fastest 
algorithm for parallel computers that still achieves a correct result.
Data-level parallelism, sp
 cally subword parallelism, o
 ers a simple path to 
higher performance for programs that are intensive in arithmetic operations for 

either integer o
 oating-point data. We showed that we could speed up matrix 
multiply nearly fourfold by using instructions that could execute fo
 oating-
point operations at a time.
With the explanation of computer arithmetic in this chapter comes a description 
of much more of the MIPS instruction set. One point of confusion is the instructions 

covered in these chapters versus instructions executed by MIPS chips versus the 

instructions accepted by MIPS assemblers. Tw
 gures try to make this clear.
Figure 3.26
 lists the MIPS instructions covered in this chapter and Chapter 2. 
We call the set of instructions on th
 -hand side of th
 gure the 
MIPS core
 e instructions on the right we call the 
MIPS arithmetic core
. On th  of 
Figure 3.27
 are the instructions the MIPS processor executes that are not found in 
Figure 3.26
. 
We call the full set of hardware instructions 
MIPS-32. On the right of 
Figure 3.27
 are the instructions accepted by the assembler that are not part of MIPS-32. We call 

this set of instructions 
Pseudo MIPS.
Figure 3.28
 gives the popularity of the MIPS instructions for SPEC CPU2006 
integer an
 oating-point benchmarks. All instructions are listed that were 
responsible for at least 0.2% of the instructions executed.
Note that although programmers and compiler writers may use MIPS-32 to 
have a richer menu of options, MIPS core instructions dominate integer SPEC 

CPU2006 execution, and the integer core plus arithmetic core dominate SPEC 

 oating point, as the table below shows.
Instruction subset
IntegerFl. pt.
MIPS core98%31%
MIPS arithmetic core2%66%
Remaining MIPS-320%3%
For the rest of the book, we concentrate on the MIPS core instructions—the integer 

instruction set excluding multiply and divide—to make the explanation of computer 

design easier. As you can see, the MIPS core includes the most popular MIPS 

instructions; be assured that understanding a computer that runs the MIPS core 

will give you su
  cient background to understand even more ambitious computers. 
No matter what the instruction set or its size—MIPS, ARM, x86—never forget that 

bit patterns have no inherent meanin
 e same bit pattern may represent a signed 
integer, unsigned integer, oating-point number, string, instruction, and so on. In 

stored program computers, it is the operation on the bit pattern that determines its 

meaning.

234 Chapter 3 Arithmetic for Computers
 MIPS core instructionsNameFormatMIPS arithmetic coreNameFormat
addaddRmultiply
multRadd immediateaddiImultiply unsigned
multuRadd unsignedadduRdivide
divRadd immediate unsignedaddiuIdivide unsigned
divuRsubtractsubRmove from Hi
mfhiRsubtract unsignedsubuRmove from Lo
mßoRANDANDRmove from system control (EPC)
mfc0RAND immediateANDiI˜oating-point add single
add.sRORORR˜oating-point add double
add.dROR immediateORiI˜oating-point subtract single
sub.sRNORNORR˜oating-point subtract double
sub.dRshift left logicalsllR˜oating-point multiply single
mul.sRshift right logicalsrlR˜oating-point multiply double
mul.dRload upper immediateluiI˜oating-point divide single
div.sRload wordlwI˜oating-point divide double
div.dRstore wordswIload word to ˜oating-point single
lwc1Iload halfword unsignedlhuIstore word to ˜oating-point single
swc1Istore halfwordshIload word to ˜oating-point double
ldc1Iload byte unsigned
lbuIstore word to ˜oating-point double
sdc1Istore byte
sbIbranch on ˜oating-point true
bc1tIload linked (
atomic update)llIbranch on ˜oating-point falsebc1fIstore cond. (atomic update)scI˜oating-point compare single
c.x.sRbranch on equalbeqI(x = eq, neq, lt, le, gt, ge)branch on not equalbneI˜oating-point compare double
c.x.dRjumpjJ(x = eq, neq, lt, le, gt, ge)jump and linkjalJjump registerjrRset less thansltRset less than immediatesltiIset less than unsignedsltuRset less than immediate unsignedsltiuIFIGURE 3.26 The MIPS instruction set.
 is book concentrates on the instructions in th
  col
 is information is also found 
in columns 1 and 2 of the MIPS Reference Data Card at the front of this book.

 3.10 Concluding Remarks 
235Remaining MIPS-32NameFormatPseudo MIPSName
Format
exclusive or (rs rt)xorRabsolute value
absrd,rs
exclusive or immediatexoriInegate (signed or unsigned)negsrd,rs
shift right arithmeticsraRrotate left
rolrd,rs,rt
shift left logical variablesllvRrotate right
rorrd,rs,rt
shift right logical variablesrlvRmultiply and don™t check o˜w 
(signed or uns.)mulsrd,rs,rt
shift right arithmetic variablesravRmultiply and check o˜w (signed or uns.)mulosrd,rs,rt
move to Hi
mthiRdivide and check over˜ow
divrd,rs,rt
move to Lo
mtloRdivide and don™t check over˜ow
divurd,rs,rt
load halfwordlhIremainder (signed or unsigned)remsrd,rs,rt
load byte
lbIload immediate
lird,immload word left (unaligned)lwlIload address
lard,addrload word right (unaligned)lwrIload double
ldrd,addrstore word left (unaligned)swlIstore double
sdrd,addrstore word right (unaligned)swrIunaligned load word
ulwrd,addrload linked (
atomic update)llIunaligned store word
uswrd,addrstore cond. (atomic update)scIunaligned load halfword (signed or uns.)ulhsrd,addrmove if zero
movzRunaligned store halfword
ushrd,addrmove if not zero
movnRbranch 
bLabelmultiply and add (S or uns.)maddsRbranch on equal zero
beqzrs,L
multiply and subtract (S or uns.)msubsIbranch on compare (signed or unsigned)bxsrs,rt,L
branch on  zero and linkbgezalI(x = lt, le, gt, ge)branch on < zero and linkbltzalIset equal
seqrd,rs,rt
jump and link registerjalrRset not equal
snerd,rs,rt
branch compare to zerobxzIset on compare (signed or unsigned)sxsrd,rs,rt
branch compare to zero likely
bxzlI(x = lt, le, gt, ge)(x = lt, le, gt, ge)load to ˜oating point (s or d)l.frd,addrbranch compare reg likely
bxlIstore from ˜oating point (s or d)s.frd,addrtrap if compare regtxRtrap if compare immediatetxiI(x = eq, neq, lt, le, gt, ge)return from exception
rfeRsystem callsyscallIbreak (cause exception)breakImove from FP to integer
mfc1Rmove to FP from integer
mtc1RFP move 
(s or d)mov.fRFP move if zero 
(s or d)movz.fRFP move if not zero 
(s or d)movn.fRFP square root (s or d)sqrt.fRFP absolute value (s or d)abs.fRFP negate (s or d)neg.fRFP convert
 (w, s, or d)
cvt.f.fRFP compare un (s or d)c.xn.fRFIGURE 3.27 Remaining MIPS-32 and Pseudo MIPS instruction sets.
 f means single (
s) or double (
d) precisio
 oating-point 
instructions, and 
s means signed and unsigned (
u) versions. MIPS-32 also has FP instructions for multiply and add/sub (
madd.f/ msub.f), ceiling (
ceil.f), truncate (
trunc.f), round (
round.f), and reciprocal (
recip.f e underscore represents the letter to include to represent 
that datatype.

236 Chapter 3 Arithmetic for Computers
Core MIPS NameIntegerFl. pt.Arithmetic core + MIPS-32 NameIntegerFl. pt.
addadd0.0%0.0%FP add double
add.d0.0%10.6%
add immediateaddi0.0%0.0%FP subtract double
sub.d0.0%4.9%
add unsignedaddu5.2%3.5%FP multiply double
mul.d0.0%15.0%
add immediate unsignedaddiu9.0%7.2%FP divide double
div.d0.0%0.2%
subtract unsignedsubu2.2%0.6%FP add single
add.s0.0%1.5%
ANDAND0.2%0.1%FP subtract single
sub.s0.0%1.8%
AND immediateANDi0.7%0.2%FP multiply single
mul.s0.0%2.4%
OROR4.0%1.2%FP divide single
div.s0.0%0.2%
OR immediateORi1.0%0.2%load word to FP double
l.d0.0%17.5%
NORNOR0.4%0.2%store word to FP double
s.d0.0%4.9%
shift left logicalsll4.4%1.9%load word to FP single
l.s0.0%4.2%
shift right logicalsrl1.1%0.5%store word to FP single
s.s0.0%1.1%
load upper immediatelui3.3%0.5%branch on ˜oating-point true
bc1t0.0%0.2%
load wordlw18.6%5.8%branch on ˜oating-point false
bc1f0.0%0.2%
store wordsw7.6%2.0%˜oating-point compare double
c.x.d0.0%0.6%
load byte
lbu3.7%0.1%multiply
mul0.0%0.2%
store byte
sb0.6%0.0%shift right arithmetic
sra0.5%0.3%
branch on equal (zero)beq8.6%2.2%load half
lhu1.3%0.0%
branch on not equal (zero)bne8.4%1.4%store half
sh0.1%0.0%
jump and linkjal0.7%0.2%
jump registerjr1.1%0.2%
set less thanslt9.9%2.3%set less than immediateslti3.1%0.3%
set less than unsignedsltu3.4%0.8%
set less than imm. uns.sltiu1.1%0.1%
FIGURE 3.28 The frequency of the MIPS instructions for SPEC CPU2006 integer and ﬂ
 oating point.
 All instructions that 
accounted for at least 0.2% of the instructions are included in the table. Pseudoinstructions are converted into MIPS-32 before
 execution, and 
hence do not appear here.
 3.11  Historical Perspective and Further 
ReadingThis section surveys the history of the floating point going back to von 
Neumann, including the surprisingly controversial IEEE standards effort, plus 

the rationale for the 80-bit stack architecture for floating point in the x86. See 

the rest of 
 Section 3.11 
online.
Gresham’s Law (“Bad 
money drives out 

Good”) for computers 

would say, “
 e Fast 
drives out the Slow 

even if the Fast is 

wrong.”
W. Kahan, 
1992
 3.12 Exercises 237 3.12 Exercises3.1 [5] <§3.2> What is 5ED4 
 07A4 when these values represent unsigned 16-
bit hexadecimal number e result should be written in hexadecimal. Show your 

work.
3.2 [5] <§3.2> What is 5ED4 
 07A4 when these values represent signed 16-
bit hexadecimal numbers stored in sign-magnitude forma
 e result should be 
written in hexadecimal. Show your work.
3.3 [10] <§3.2> Convert 5ED4 into a binary number. What makes base 16 
(hexadecimal) an attractive numbering system for representing values in 

computers?
3.4 [5] <§3.2> What is 4365 
 3412 when these values represent unsigned 12-bit 
octal number
 e result should be written in octal. Show your work.
3.5 [5] <§3.2> What is 4365 
 3412 when these values represent signed 12-bit 
octal numbers stored in sign-magnitude forma e result should be written in 

octal. Show your work.
3.6 [5] <§3.2> Assume 185 and 122 are unsigned 8-bit decimal integers. Calculate 
185 – 122. Is there over
 ow, under
 ow, or neither?
3.7 [5] <§3.2> Assume 185 and 122 are signed 8-bit decimal integers stored in 

sign-magnitude format. Calculate 185 
 122. Is there over
 ow, under
 ow, or 
neither?
3.8 [5] <§3.2> Assume 185 and 122 are signed 8-bit decimal integers stored in 
sign-magnitude format. Calculate 185 
 122. Is there over
 ow, under
 ow, or 
neither?
3.9 [10] <§3.2> Assume 151 and 214 are signed 8-bit decimal integers stored in 
two’s complement format. Calculate 151 
 214 using saturating arit
 e result should be written in decimal. Show your work.
3.10 [10] <§3.2> Assume 151 and 214 are signed 8-bit decimal integers stored in 
two’s complement format. Calculate 151 
 214 using saturating arit
 e result should be written in decimal. Show your work.
3.11 [10] <§3.2> Assume 151 and 214 are unsigned 8-bit integers. Calculate 151 
 214 using saturating arit
 e result should be written in decimal. Show 
your work.
3.12 [20] <§3.3> Using a table similar to that shown in 
Figure 3.6
, calculate the 
product of the octal unsigned 6-bit integers 62 and 12 using the hardware described 

in Figure 3.3
. You should show the contents of each register on each step.
Never give in, never 

give in, never, never, 

never—in nothing, 

great or small, large or 

petty—never give in.
Winston Churchill, 
address at Harrow 

School, 1941

238 Chapter 3 Arithmetic for Computers
3.13 [20] <§3.3> Using a table similar to that shown in 
Figure 3.6
, calculate the 
product of the hexadecimal unsigned 8-bit integers 62 and 12 using the hardware 

described in 
Figure 3.5
. You should show the contents of each register on each step.
3.14 [10] <§3.3> Calculate the time necessary to perform a multiply using the 
approach given in 
Figures 3.3 and 3.4
 if an integer is 8 bits wide and each step 

of the operation takes 4 time units. Assume that in step 1a an addition is always 

performed—either the multiplicand will be added, or a zero will be. Also assume 

that the registers have already been initialized (you are just counting how long it 

takes to do the multiplication loop itself). If this is being done in hardware, the 

 s of the multiplicand and multiplier can be done simultaneously. If this is being 
done in so
 ware, they will have to be done one a
 er the other. Solve for each case.
3.15 [10] <§3.3> Calculate the time necessary to perform a multiply using the 

approach described in the text (31 adders stacked vertically) if an integer is 8 bits 

wide and an adder takes 4 time units.
3.16 [20] <§3.3> Calculate the time necessary to perform a multiply using the 
approach given in 
Figure 3.7
 if an integer is 8 bits wide and an adder takes 4 time 

units.
3.17 [20] <§3.3> As discussed in the text, one possible performance enhancement 
is to
  and add instead of an actual multiplication. Since 9 
 6, for example, 
can be written (2 
 2  2  1)  6, we can calculate 9 
 6 by
 ing 6 to th
  3 
times and then adding 6 to that result. Show the best way to calculate 0
33  055 
usin
 s and adds/subtracts. Assume both inputs are 8-bit unsigned integers.
3.18 [20] <§3.4> Using a table similar to that shown in 
Figure 3.10
, calculate 

74 divided by 21 using the hardware described in 
Figure 3.8
. You should show 

the contents of each register on each step. Assume both inputs are unsigned 6-bit 

integers.
3.19 [30] <§3.4> Using a table similar to that shown in 
Figure 3.10
, calculate 
74 divided by 21 using the hardware described in 
Figure 3.11
. You should show 

the contents of each register on each step. Assume A and B are unsigned 6-bit 

integer
 is algorithm requires a slightl
 erent approach than that shown in 
Figure 3.9
. You will want to think hard about this, do an experiment or two, or else 

go to the web t
 gure out how to make this work correctly. (Hint: one possible 
solution involves using the fact that 
Figure 3.11
 implies the remainder register can 

be
 ed either direction.)
3.20 [5] <§3.5> What decimal number does the bit pattern 
0×0C000000 represent if it is a two’s complement integer? An unsigned integer?
3.21 [10] <§3.5> If the bit pattern 
0×0C000000 is placed into the Instruction 
Register, what MIPS instruction will be executed?

3.22 [10] <§3.5> What decimal number does the bit pattern 
0×0C000000 represent if i
 oating point number? Use the IEEE 754 standard.

 3.12 Exercises 2393.23 [10] <§3.5> Write down the binary representation of the decimal number 
63.25 assuming the IEEE 754 single precision format.
3.24 [10] <§3.5> Write down the binary representation of the decimal number 
63.25 assuming the IEEE 754 double precision format.
3.25 [10] <§3.5> Write down the binary representation of the decimal number 
63.25 assuming it was stored using the single precision IBM format (base 16, 

instead of base 2, with 7 bits of exponent).
3.26 [20] <§3.5> Write down the binary bit pattern to represent 
1.5625  101 assuming a format similar to that employed by the DEC PDP-8 (t
 most 12 
bits are the exponent stored as a two’s complement number, and the rightmost 24 
bits are the fraction stored as a two’s complement number). No hidden 1 is used. 

Comment on how the range and accuracy of this 36-bit pattern compares to the 

single and double precision IEEE 754 standards.
3.27 [20] <§3.5> IEEE 754-2008 contains a half precision that is only 16 bits 
wide
 e 
 most bit is still the sign bit, the exponent is 5 bits wide and has a bias 
of 15, and the mantissa is 10 bits long. A hidden 1 is assumed. Write down the 

bit pattern to represent 
1.5625  101 assuming a version of this format, which 
uses an excess-16 format to store the exponent. Comment on how the range and 

accuracy of this 16-bi
 oating point format compares to the single precision IEEE 
754 standard.
3.28  e Hewlett-Packard 2114, 2115, and 2116 used a format 
with th
 most 16 bits being the fraction stored in two’s complement format, 
followed by another 16-bit
 eld which had th
 most 8 bits as an extension of the 
fraction (making the fraction 24 bits long), and the rightmost 8 bits representing 

the exponent. However, in an interesting twist, the exponent was stored in sign-

magnitude format with the sign bit on the far right! Write down the bit pattern to 

represent 
1.5625  101 assuming this format. No hidden 1 is used. Comment on 
how the range and accuracy of this 32-bit pattern compares to the single precision 

IEEE 754 standard.
3.29 [20] <§3.5> Calculate the sum of 2.6125 
 101 and 4.150390625 
 101 by hand, assuming A and B are stored in the 16-bit half precision described in 
Exercise 3.27. Assume 1 guard, 1 round bit, and 1 sticky bit, and round to the 

nearest even. Show all the steps.
3.30 [30] <§3.5> Calculate the product of –8.0546875 
 100 and 
1.79931640625  10–1 by hand, assuming A and B are stored in the 16-bit half precision format 
described in Exercise 3.27. Assume 1 guard, 1 round bit, and 1 sticky bit, and round 
to the nearest even. Show all the steps; however, as is done in the example in the 

text, you can do the multiplication in human-readable format instead of using the 

techniques described in Exercises 3.12 through 3.14. Indicate if there is over
 ow 
or under
 ow. Write your answer in both the 16-bi
 oating point format described 
in Exercise 3.27 and also as a decimal number. How accurate is your result? How 

does it compare to the number you get if you do the multiplication on a calculator?

240 Chapter 3 Arithmetic for Computers
3.31 [30] <§3.5> Calculate by hand 8.625 
 101 divided by 
4.875  100. Show 
all the steps necessary to achieve your answer. Assume there is a guard, a round bit, 
and a sticky bit, and use them if necessary. Write th
 nal answer in both the 16-bit 
 oating point format described in Exercise 3.27 and in decimal and compare the 
decimal result to that which you get if you use a calculator.
3.32 [20] <§3.9> Calculate (3.984375 
 101  3.4375  101)  1.771  103 by hand, assuming each of the values are stored in the 16-bit half precision format 
described in Exercise 3.27 (and also described in the text).  Assume 1 guard, 1 

round bit, and 1 sticky bit, and round to the nearest even. Show all the steps, and 

write your answer in both the 16-bi
 oating point format and in decimal.
3.33 [20] <§3.9> Calculate 3.984375 
 101  (3.4375  101  1.771  103) by hand, assuming each of the values are stored in the 16-bit half precision format 

described in Exercise 3.27 (and also described in the text). Assume 1 guard, 1 

round bit, and 1 sticky bit, and round to the nearest even. Show all the steps, and 

write your answer in both the 16-bi
 oating point format and in decimal.
3.34 [10] <§3.9> Based on your answers to 3.32 and 3.33, does (3.984375 
 101  3.4375  101)  1.771  103 = 3.984375  101  (3.4375  101  1.771  103)?3.35 [30] <§3.9> Calculate (3.41796875 10
3  6.34765625  103)  1.05625  102 by hand, assuming each of the values are stored in the 16-bit half precision 
format described in Exercise 3.27 (and also described in the text). Assume 1 guard, 

1 round bit, and 1 sticky bit, and round to the nearest even. Show all the steps, and 

write your answer in both the 16-bi
 oating point format and in decimal.
3.36 [30] <§3.9> Calculate 3.41796875 10
3  (6.34765625  103  1.05625  102) by hand, assuming each of the values are stored in the 16-bit half precision 
format described in Exercise 3.27 (and also described in the text). Assume 1 guard, 

1 round bit, and 1 sticky bit, and round to the nearest even. Show all the steps, and 

write your answer in both the 16-bi
 oating point format and in decimal.
3.37 [10] <§3.9> Based on your answers to 3.35 and 3.36, does (3.41796875 10
3  6.34765625  103)  1.05625  102 = 3.41796875  103  (6.34765625  103  1.05625  102)?3.38 [30] <§3.9> Calculate 1.666015625 
 100 (1.9760  104  1.9744  104) by hand, assuming each of the values are stored in the 16-bit half precision 
format described in Exercise 3.27 (and also described in the text). Assume 1 guard, 

1 round bit, and 1 sticky bit, and round to the nearest even. Show all the steps, and 

write your answer in both the 16-bi
 oating point format and in decimal.
3.39 [30] <§3.9> Calculate (1.666015625 
 100  1.9760  104)  (1.666015625 100  1.9744  104) by hand, assuming each of the values are stored in the 
16-bit half precision format described in Exercise 3.27 (and also described in the 

text). Assume 1 guard, 1 round bit, and 1 sticky bit, and round to the nearest even. 

Show all the steps, and write your answer in both the 16-bi
 oating point format 
and in decimal.

 3.12 Exercises 2413.40 [10] <§3.9> Based on your answers to 3.38 and 3.39, does (1.666015625 
 100  1.9760  104)  (1.666015625  100  1.9744  104) = 1.666015625  100  (1.9760  104  1.9744  104)?3.41 [10] <§3.5> Using th
 oating point format, write down the bit 
pattern that would represent 
1/4. Can you represent 
1/4 exactly?
3.42 [10] <§3.5> What do you get if you add 
1/4 to itself 4 times? What is 
1/4  4? Are they the same? What should they be?
3.43 [10] <§3.5> Write down the bit pattern in the fraction of value 1/3 assuming 
 oating point format that uses binary numbers in the fraction. Assume there are 
24 bits, and you do not need to normalize. Is this representation exact?
3.44 [10] <§3.5> Write down the bit pattern in the fraction assumin
 oating 
point format that uses Binary Coded Decimal (base 10) numbers in the fraction 
instead of base 2. Assume there are 24 bits, and you do not need to normalize. Is 

this representation exact?
3.45 [10] <§3.5> Write down the bit pattern assuming that we are using base 15 
numbers in the fraction instead of base 2. (Base 16 numbers use the symbols 0–9 

and A–F. Base 15 numbers would use 0–9 and A–E.) Assume there are 24 bits, and 

you do not need to normalize. Is this representation exact?
3.46 [20] <§3.5> Write down the bit pattern assuming that we are using base 30 
numbers in the fraction instead of base 2. (Base 16 numbers use the symbols 0–9 

and A–F. Base 30 numbers would use 0–9 and A–T.) Assume there are 20 bits, and 

you do not need to normalize. Is this representation exact?
3.47  e following C code implements a four-ta
 lter on 
input array sig_in.  Assume that all arrays are 16-bi
 xed-point values.
for (i3;i<128;i)
sig_out[i]sig_in[i-3] * f[0]sig_in[i-
22] * f[1]
sig_in[i-1] * f[2]sig_in[i] * f[3];
Assume you are to write an optimized implementation this code in assembly 
language on a processor that has SIMD instructions and 128-bit registers.  Without 

knowing the details of the instruction set, br
 y describe how you would 
implement this code, maximizing the use of sub-word operations and minimizing 

the amount of data that is transferred between registers and memory.  State all your 

assumptions about the instructions you use.
§3.2, page 182: 2.
§3.5, page 221: 3.
Answers to 
Check Yourself

4In a major matter, no 
details are small.
French Proverb
The Processor4.1 Introduction 2444.2 Logic Design Conventions 
2484.3 Building a Datapath 
2514.4 A Simple Implementation Scheme 
2594.5 An Overview of Pipelining 
2724.6 Pipelined Datapath and Control 
2864.7 Data Hazards: Forwarding versus 
Stalling 3034.8 Control Hazards 
3164.9 Exceptions 3254.10 Parallelism via Instructions 
332Computer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
4.11 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Pipelines 
3444.12 Going Faster:  Instruction-Level Parallelism and Matrix 
Multiply 3514.13 Advanced Topic: An Introduction to Digital Design Using a
Hardware Design Language to Describe and Model a Pipeline
and More Pipelining Illustrations 3544.14 Fallacies and Pitfalls 
3554.15 Concluding Remarks 
3564.16 Historical Perspective and Further Reading 
3574.17 Exercises 357The Five Classic Components of a Computer

244 Chapter 4 The Processor
 4.1 IntroductionChapter 1 explains that the performance of a computer is determined by three key 
factors: instruction count, clock cycle time, and 
clock cycles per instruction
 (CPI). Chapter 2 explains that the compiler and the instruction set architecture determine 

the instruction count required for a given program. However, the implementation 

of the processor determines both the clock cycle time and the number of clock 

cycles per instruction. In this chapter, we construct the datapath and control unit 

for tw
 erent implementations of the MIPS instruction set.
 is chapter contains an explanation of the principles and techniques used in 
implementing a processor, starting with a highly abstract and simp
 ed overview 
in this section. It is followed by a section that builds up a datapath and constructs a 

simple version of a processo
  cient to implement an instruction set like MIPS. 
 e bulk of the chapter covers a more realistic 
pipelined
 MIPS implementation, 
followed by a section that develops the concepts necessary to implement more 

complex instruction sets, like the x86.
For the reader interested in understanding the high-level interpretation of 
instructions and its impact on program performance, this initial section and Section 

4.5 present the basic concepts of pipelining. Recent trends are covered in Section 

4.10, and Section 4.11 describes the recent Intel Core i7 and ARM Cortex-A8 

architectures. Section 4.12 shows how to use instruction-level parallelism to more 

than double the performance of the matrix multiply from Sectio
 ese sections 
provide enough background to understand the pipeline concepts at a high level.
For the reader interested in understanding the processor and its performance in 
more depth, Sections 4.3, 4.4, and 4.6 will be useful
 ose interested in learning 
how to build a processor should also cover 4.2, 4.7, 4.8, and 4.9. For readers with 

an interest in modern hardware design, 
 Section 4.13
 describes how hardware 
design languages and CAD tools are used to implement hardware, and then how 
to use a hardware design language to describe a pipelined implementation. It also 

gives several more illustrations of how pipelining hardware executes.
A Basic MIPS ImplementationWe will be examining an implementation that includes a subset of the core MIPS 

instruction set:
 e memory-reference instructions 
load word
 (lw) and 
store word
 (sw) e arithmetic-logical instructions 
add, sub, AND, OR, and 
slt e instructions 
branch equal
 (beq) and
 jump (j), which we add last
 is subset does not include all the integer instructions (for example
 , multiply, and divide are missing), nor does it include an
 oating-point instructions. 

 4.1 Introduction 245However, it illustrates the key principles used in creating a datapath and designing 
the control
 e implementation of the remaining instructions is similar.
In examining the implementation, we will have the opportunity to see how the 
instruction set architecture determines many aspects of the implementation, and 

how the choice of various implementation strategies a
 ects the clock rate and CPI 
for the computer. Many of the key design principles introduced in Chapter 1 can 

be illustrated by looking at the implementation, such as 
Simplicity favors regularity
. In addition, most concepts used to implement the MIPS subset in this chapter are 

the same basic ideas that are used to construct a broad spectrum of computers, 

from high-performance servers to general-purpose microprocessors to embedded 

processors.
An Overview of the Implementation
In Chapter 2, we looked at the core MIPS instructions, including the integer 

arithmetic-logical instructions, the memor
y-reference instructions, and the branch 
instructions. Much of what needs to be done to implement these instructions is the 

same, independent of the exact class of instruction. For every instruction, th
 rst 
two steps are identical:
1. Send the 
program counter
 (PC) to the memory that contains the code and 
fetch the instruction from that memory.
2. Read one or two registers, usin
 elds of the instruction to select the registers 
to read. For the load word instruction, we need to read only one register, but 

most other instructions require reading two registers.
 er these two steps, the actions required to complete the instruction depend 
on the instruction class. Fortunately, for each of the three instruction classes 

(memory-reference, arithmetic-logical, and branches), the actions are largely the 

same, independent of the exact instructio
 e simplicity and regularity of the 
MIPS instruction set simp
 es the implementation by making the execution of 
many of the instruction classes similar.
For example, all instruction classes, except jump, use the arithmetic-logical unit 
(ALU) a
 er reading the register
 e memory-reference instructions use the ALU 
for an address calculation, the arithmetic-log
ical instructions for the operation 

execution, and branches for compariso
 er using the ALU, the actions required 
to complete various instruction class
 er. A memory-reference instruction 
will need to access the memory either to read data for a load or write data for a 

store. An arithmetic-logical 
or load instruction must write the data from the ALU 
or memory back into a register. Lastly, for a branch instruction, we may need to 

change the next instruction address based on the comparison; otherwise, the PC 

should be incremented by 4 to get the address of the next instruction.
Figure 4.1
 shows the high-level view of a MIPS implementation, focusing on 
the various functional units and their interconnection. Although t
 gure shows 
most of th
 ow of data through the processor, it omits two important aspects of 
instruction execution.

246 Chapter 4 The Processor
First, in several places, 
Figure 4.1
 shows data going to a particular unit as coming 
from tw
 erent sources. For example, the value written into the PC can come 
from one of two adders, the data written into the regist
 le can come from either 
the ALU or the data memory, and the second input to the ALU can come from 
a register or the immediat
 eld of the instruction. In practice, these data lines 
cannot simply be wired together; we must add a logic element that chooses from 

among the multiple sources and steers one of those sources to its destinatio
 is selection is commonly done with a device called a 
multiplexor
, although this device 
might better be called a 
data selector
. Appendix B describes the multiplexor, which 
selects from among several inputs based on the setting of its contro
 e control lines are set based primarily on information taken from the instruction 

being executed.
 e second omission in 
Figure 4.1
 is that several of the units must be controlled 
depending on the type of instruction. For example, the data memory must read 
FIGURE 4.1 An abstract view of the implementation of the MIPS subset showing the 
major functional units and the major connections between them. 
All instructions start by using 
the program counter to supply the instruction address to the instruction memory
 er the instruction is 
fetched, the register operands used by an instruction are sp
 ed by
 elds of that instruction. Once the 
register operands have been fetched, they can be operated on to compute a memory address (for a load or 

store), to compute an arithmetic result (for an inte
ger arithmetic-logical instruction), or a compare (for a 
branch). If the instruction is an arithmetic-logical instruction, the result from the ALU must be written to 

a register. If the operation is a load or store, the ALU result is used as an address to either store a value from 

the registers or load a value from memory into the register
 e result from the ALU or memory is written 
back into the regist
 le. Branches require the use of the ALU output to determine the next instruction 
address, which comes either from the ALU (where the PC and branch o
 set are summed) or from an adder 
that increments the current PC b
 e thick lines interconnecting the functional units represent buses, 
which consist of multip e arrows are used to guide the reader in knowing how informatio
 ows. 
Since signal lines may cross, we explicitly show when cros
sing lines are connected by the presence of a dot 
where the lines cross.
DataPCAddressInstruction
Instructionmemory
Registers
ALUAddressDataDatamemory
AddAdd4Register #Register #
Register #
 4.1 Introduction 247on a load and written on a store
 e regist
 le must be written only on a load 
or an arithmetic-logical instruction. And, of course, the ALU must perform one 
of several operations. (Appendix B describes the detailed design of the ALU.) 

Like the multiplexors, control lines that are set on the basis of variou
 elds in the 
instruction direct these operations.
Figure 4.2
 shows the datapath of 
Figure 4.1
 with the three required multiplexors 
added, as well as control lines for the major functional units. A 
control unit
, which has the instruction as an input, is used to determine how to set the control 

lines for the functional units and two of the multiplexor
 e third multiplexor, 
DataPCAddressInstruction
Instructionmemory
Registers
ALUAddressDataDatamemory
AddAdd4MemWrite
MemReadMuxMuxMuxControl
RegWrite
ZeroBranch
ALU operation
Register #Register #Register #FIGURE 4.2 The basic implementation of the MIPS subset, including the necessary multiplexors and control lines.
  e top multiplexor (“Mux”) controls what value replaces the PC (PC + 4 or the branch destination address); the multiplexor is c
ontrolled 
by the gate that “ANDs” together the Zero output of the ALU and a control signal that indicates that the instruction is a branc
 e middle 
multiplexor, whose output returns to the regist le, is used to steer the output of the ALU (in 
the case of an arithmetic-logical instruction) or 
the output of the data memory (in the case of a load) for writing into the regist le. Finally, the bottommost multiplexor is used to determine 
whether the second ALU input is from the registers (for an arithmetic-logical instruction or a branch) or from the o
 set 
 eld of the instruction 
(for a load or stor
 e added control lines are straightforward and determine the operation performed at the ALU, whether the data memory 
should read or write, and whether the registers should perform a write operatio
 e control lines are shown in color to make them easier to 
see.

248 Chapter 4 The Processor
which determines whether PC + 4 or the branch destination address is written 
into the PC, is set based on the Zero output of the ALU, which is used to perform 

the comparison of a beq instructio
 e regularity and simplicity of the MIPS 
instruction set means that a simple decoding process can be used to determine how 

to set the control lines.
In the remainder of the chapter, we re
 ne this view to
 ll in the details, which 
requires that we add further functional units, increase the number of connections 

between units, and, of course, enhance a control unit to control what actions 

are taken fo
 erent instruction classes. Sections 4.3 and 4.4 describe a simple 
implementation that uses a single long clock cycle for every instruction and follows 

the general form of 
Figures 4.1 and 4.2
. In t
 rst design, every instruction begins 
execution on one clock edge and completes execution on the next clock edge.
While easier to understand, this approach is not practical, since the clock cycle 
must be severely stretched to accommodate the longest instructio
 er designing 
the control for this simple computer, we will look at pipelined implementation with 

all its complexities, including exceptions.
How many of th
 ve classic components of a computer—shown on page 243—do 
Figures 4.1 and 4.2
 include?
 4.2 Logic Design Conventions
To discuss the design of a computer, we must decide how the hardware logic 
implementing the computer will operate and how the computer is clocked
 is section reviews a few key ideas in digital logic that we will use extensively in this 

chapter. If you have little or no background in digital logic, yo
 nd it helpful 
to read 
 Appendix B
 before continuing.
 e datapath elements in the MIPS implementation consist of tw
 erent types 
of logic elements: elements that operate on
 data values and elements that contain 
state.
 e elements that operate on data values are all 
combinational
, which means 
that their outputs depend only on the current inputs. Given the same input, a 
combinational element always produces the same outpu
 e ALU shown in 
Figure
 
4.1 and discussed in 
 Appendix B
 is an example of a combinational element. Given 
a set of inputs, it always produces the same output because it has no internal storage.
Other elements in the design are not combinational, but instead contain 
state
. An element contains state if it has some internal storage. We call these elements 
state 
elements because, if we pulled the power plug on the computer, we could restart it 
accurately by loading the state elements with the values they contained before we 
pulled the plug. Furthermore, if we saved and restored the state elements, it would 

be as if the computer had never lost power.
 us, these state elements completely 
characterize the computer. In 
Figure 4.1
, the instruction and data memories, as 

well as the registers, are all examples of state elements.
Check Yourself
combinational 
element
 An operational 
element, such as an AND 
gate or an ALU.
state element
 A memory 
element, such as a register 

or a memory.

 4.2 Logic Design Conventions 
249A state element has at least two inputs and one outpu
 e required inputs are 
the data value to be written into the element and the clock, which determines when 
the data value is writt
 e output from a state element provides the value that 
was written in an earlier clock cycle. For example, one of the logically simplest state 

elements is a D-typ
 i
 op (see 
 Appendix B
), which has exactly these two 
inputs (a value and a clock) and one output. In addition t
 i
 ops, our MIPS 
implementation uses two other types of state elements: memories and registers, 

both of which appear in 
Figure 4.1
 e clock is used to determine when the state 
element should be written; a state element can be read at any time.
Logic components that contain state are also called 
sequential
, because their 
outputs depend on both their inputs and the contents of the internal state. For 

example, the output from the functional unit representing the registers depends 

both on the register numbers supplied and on what was written into the registers 

previously
 e operation of both the combinational and sequential elements and 
their construction are discussed in more detail in 
 Appendix B
.Clocking Methodology
A clocking methodology
 nes when signals can be read and when they can be 
written. It is important to specify the timing of reads and writes, because if a signal 

is written at the same time it is read, the value of the read could correspond to the 

old value, the newly written value, or even some mix of the two! Computer designs 

cannot tolerate such unpredictability. A clocking methodology is designed to make 

hardware predictable.
For simplicity, we will assume an 
edge-triggered clocking
 methodology. An 
edge-triggered clocking methodology means that any values stored in a sequential 
logic element are updated only on a clock edge, which is a quick transition from 

low to high or 
vice versa
 (see 
Figure 4.3
). Because only state elements can store a 
data value, any collection of combinational logic must have its inputs come from a 

set of state elements and its outputs written into a set of state elemen
 e inputs 
are values that were written in a previous clock cycle, while the outputs are values 

that can be used in a following clock cycle.
clocking 
methodology
 e approach used to 
determine when data is 

valid and stable relative to 

the clock.
edge-triggered 
clocking
 A clocking 
scheme in which all state 
changes occur on a clock 

edge.
Stateelement1Stateelement2Combinational logicClock cycle
FIGURE 4.3 Combinational logic, state elements, and the clock are closely related.
In a synchronous digital system, the clock determines when elements with state will write values into internal 
storage. Any inputs to a state element must reach a stable value (that is, have reached a value from which they 

will not change until a
 er the clock edge) before the active clock edge causes the state to be updated. All state 
elements in this chapter, including memory, are assumed to be positive edge-triggered; that is, they change 

on the rising clock edge.

250 Chapter 4 The Processor
Figure 4.3
 shows the two state elements surrounding a block of combinational 
logic, which operates in a single clock cycle: all signals must propagate from state 
element 1, through the combinational logic, and to state element 2 in the time of 

one clock cycle.
 e time necessary for the signals to reach state elemen
 nes the length of the clock cycle.
For simplicity, we do not show a write 
control signal
 when a state element is 
written on every active clock edge. In contrast, if a state element is not updated on 
every clock, then an explicit write control signal is required. Both the clock signal 

and the write control signal are inputs, and the state element is changed only when 

the write control signal is asserted and a clock edge occurs.
We will use the word 
asserted
 to indicate a signal that is logically high and 
assert
 to specify that a signal shou
ld be driven logically high, and 
deassert
 or 
deasserted
 to represent logically low. We use the terms assert and deassert because when 
we implement hardware, at times 1 represents logically high and at times it can 

represent logically low.
An edge-triggered methodology allows us to read the contents of a register, 
send the value through some combinational logic, and write that register in the 

same clock cycle. 
Figure 4.4
 gives a generic example. It doesn’t matter whether we 

assume that all writes take place on the rising clock edge (from low to high) or on 

the falling clock edge (from high to low), since the inputs to the combinational 

logic block cannot change except on the chosen clock edge. In this book we use 

the rising clock edge. With an edge-triggered timing methodology, there is 
no feedback within a single clock cycle, and the logic in 
Figure 4.4
 works correctly. In 
 Appendix B
, we br 
y discuss additional timing constraints (such as setup and 
hold times) as well as other timing methodologies.
For the 32-bit MIPS architecture, nearly all of these state and logic elements will 
have inputs and outputs that are 32 bits wide, since that is the width of most of the 

data handled by the processor. We will make it clear whenever a unit has an input 

or output that is other than 32 bits in widt
 e 
 gures will indicate 
buses
, which 
are signals wider than 1 bit, with thicker lines. At times, we will want to combine 

several buses to form a wider bus; for example, we may want to obtain a 32-bit bus 

by combining two 16-bit buses. In such cases, labels on the bus lines will make it 
control signal
 A signal 
used for multiplexor 
selection or for directing 

the operation of a 

functional unit; contrasts 

with a 
data signal
, which 
contains information 

that is operated on by a 

functional unit.
asserted
 e signal is 
logically high or true.
deasserted
 e signal is 
logically low or false.
StateelementCombinational logicFIGURE 4.4 An edge-triggered methodology allows a state element to be read and 
written in the same clock cycle without creating a race that could lead to indeterminate 
data values. Of course, the clock cycle still must be long enough so that the input values are stable when 

the active clock edge occurs. Feedback cannot occur within one clock cycle because of the edge-triggered 

update of the state element. If feedback were poss
ible, this design could not 
work properly. Our designs 
in this chapter and the next rely on the edge-triggered timing methodology and on structures like the one 

shown in th
 gure.

 4.3 Building a Datapath 
251clear that we are concatenating buses to form a wider bus. Arrows are also added 
to help clarify the direction of th
 ow of data between elements. Finally, 
color
 indicates a control signal as opposed to a signal that carries data; this distinction 
will become clearer as we proceed through this chapter.
True or false: Because the regist
 le is both read and written on the same clock 
cycle, any MIPS datapath using edge-triggered writes must have more than one 
copy of the regist
 le.
Elaboration: There is also a 64-bit version of the MIPS architecture, and, naturally 
enough, most paths in its implementation would be 64 bits wide. 
 4.3 Building a Datapath
A reasonable way to start a datapath design is to examine the major components 

required to execute each class of MIPS instructions. Let’s start at the top by looking 

at which 
datapath elements
 each instruction needs, and then work our way down 
through the levels of 
abstraction
. When we show the datapath elements, we will 
also show their control signals. We use abstraction in this explanation, starting 

from the bottom up.
Figure 4.5a
 shows th rst element we need: a memory unit to store the 
instructions of a program and supply instructions given an address. 
Figure
 
4.5b also shows the 
program counter (PC)
, which as we saw in Chapter 2 
is a register that holds the address of the current instruction. Lastly, we will 
need an adder to increment the PC to the address of the next instructio
 is adder, which is combinational, can be built from the ALU described in detail 

in  Appendix B
 simply by wiring the control lines so that the control always 
sp es an add operation. We will draw such an ALU with the label 
Add
, as in Figure 4.5
, to indicate that it has been permanently made an adder and cannot 

perform the other ALU functions.
To execute any instruction, we must start by fetching the instruction from 
memory. To prepare for executing the next instruction, we must also increment 

the program counter so that it points at the next instruction, 4 bytes later. 
Figure
 
4.6 shows how to combine the three elements from 
Figure 4.5
 to form a datapath 

that fetches instructions and increments the PC to obtain the address of the next 

sequential instruction.
Now let’s consider the R-format instructions (see Figure 2.20 on page 120). 
 ey all read two registers, perform an ALU operation on the contents of the 
registers, and write the result to a register. We call these instructions either 
R-type 
instructions
 or 
arithmetic-logical instructions
 (since they perform arithmetic or 
logical operation
 is instruction class includes 
add, sub, AND, OR, and 
slt, Check Yourself
datapath element
 A unit used to operate 

on or hold data within a 

processor. In the MIPS 

implementation, the 

datapath elements include 

the instruction and data 

memories, the register 

 le, the ALU, and adders.
program counter 
(PC) e register 
containing the address 
of the instruction in the 

program being executed.

252 Chapter 4 The Processor
which were introduced in Chapter 2. Recall that a typical instance of such an 
instruction is 
add $t1,$t2,$t3, which reads 
$t2 and 
$t3 and writes 
$t1. e processor’s 32 general-purpose registers are stored in a structure called a 
register
 le. A regist
 le is a collection of registers in which any register can be 
read or written by specifying the number of the register in th
 le. 
 e regist
 le contains the register state of the computer. In addition, we will need an ALU to 

operate on the values read from the registers.
R-format instructions have three register operands, so we will need to read two 
data words from the regist
 le and write one data word into the regist
 le for 
each instruction. For each data word to be read from the registers, we need an input 

to the regist
 le that sp
 es the 
register number
 to be read and an output from 
the regist
 le that will carry the value that has been read from the registers. To 
write a data word, we will need two inputs: one to specify the register number to be 

written and one to supply the 
data
 to be written into the register
 e regist
 le always outputs the contents of whatever register numbers are on the Read register 

inputs. Writes, however, are controlled by the write control signal, which must be 

asserted for a write to occur at the clock edge. 
Figure 4.7a
 shows the result; we 

need a total of four inputs (three for register numbers and one for data) and two 

outputs (both for dat
 e register number inputs are 5 bits wide to specify one 
of 32 registers (32 = 2
5), whereas the data input and two data output buses are each 
32 bits wide.
Figure 4.7b
 shows the ALU, which takes two 32-bit inputs and produces a 32-bit 
result, as well as a 1-bit signal if the resul
 e 4-bit control signal of the ALU is 
described in detail in 
 Appendix B
; we will review the ALU control shortly when 
we need to know how to set it.
register
 le A state 
element that consists 
of a set of registers that 

can be read and written 

by supplying a register 

number to be accessed.
Instruction
addressInstruction
Instructionmemory
a. Instruction memory
PCb. Program counter
Add
Sumc. Adder
FIGURE 4.5 Two state elements are needed to store and access instructions, and an 
adder is needed to compute the next instruction address.
 e state elements are the instruction 
memory and the program counter
 e instruction memory need only provide read access because the 
datapath does not write instructions. Since the instruction memory only reads, we treat it as combinational 
logic: the output at any time r
 ects the contents of the location sp
 ed by the address input, and no read 
control signal is needed. (We will need to write the 
instruction memory when we load the program; this is 
not hard to add, and we ignore it for simplicity
 e program counter is a 32-bit register that is written at the 
end of every clock cycle and thus does not need a write control signal
 e adder is an ALU wired to always 
add its two 32-bit inputs and place the sum on its output.

 4.3 Building a Datapath 
253PCReadaddressInstructionInstructionmemoryAdd4FIGURE 4.6 A portion of the datapath used for fetching instructions and incrementing 
the program counter.
 e fetched instruction is used by other parts of the datapath.
Read
register 1Registers
ALUDataDataZeroALUresultRegWrite
a. Registers
b. ALU
55
5Registernumbers
Readdata 1Readdata 2ALU operation
4Read
register 2Write
registerWrite
DataFIGURE 4.7 The two elements needed to implement R-format ALU operations are the 
register ﬁ le and the ALU.
 e regist
 le contains all the registers and has two read ports and one write 
por
 e design of multiported regist
 les is discussed in Section B.8 of  
 Appendix B
 e regist
 le always outputs the contents of the registers corresponding to the Read register inputs on the outputs; no 
other control inputs are needed. In contrast, a register write must be explicitly indicated by asserting the 

write control signal. Remember that writes are edge-triggered, so that all the write inputs (i.e., the value to 

be written, the register number, and the write control signal) must be valid at the clock edge. Since writes 

to the regist
 le are edge-triggered, our design can legally read and write the same register within a clock 
cycle: the read will get the value written in an earlier clock cycle, while the value written will be available 

to a read in a subsequent clock cycle
 e inputs carrying the register number to the regist
 le are all 5 
bits wide, whereas the lines carrying data values are 32 bits wide e operation to be performed by the 

ALU is controlled with the ALU operation signal, 
which will be 4 bits wide, using the ALU designed in 
 Appendix B
. We will use the Zero detection output of the ALU shortly to implement branch
 e over
 ow output will not be needed until Section 4.9, when we discuss exceptions; we omit it until then.

254 Chapter 4 The Processor
Next, consider the MIPS load word and store word instructions, which have the 
general form 
lw $t1,offset_value($t2) or 
sw $t1,offset_value ($t2) ese instructions compute a memory address by adding the base register, 
which is 
$t2, to the 16-bit signed o
 set 
 eld contained in the instruction. If the 
instruction is a store, the value to be stored must also be read from the regist
 le where it resides in 
$t1. If the instruction is a load, the value read from memory 
must be written into the regist
 le in the sp
 ed register, which is 
$t1 us, we will need both the regist
 le and the ALU from 
Figure 4.7
.In addition, we will need a unit to 
sign-extend
 the 16-bit o
 set 
 eld in the 
instruction to a 32-bit signed value, and a data memory unit to read from or write 

to
 e data memory must be written on store instructions; hence, data memory 
has read and write control signals, an address input, and an input for the data to be 

written into memory. 
Figure 4.8
 shows these two elements.
 e beq instruction has three operands, two registers that are compared for 
equality, and a 16-bit o
 set used to compute the 
branch target address
 relative 
to the branch instruction address. Its form is 
beq $t1,$t2,offset. To 
implement this instruction, we must compute the branch target address by adding 

the sign-extended o
 set 
 eld of the instruction to th
 ere are two details in 
th
 nition of branch instructions (see Chapter 2) to which we must pay attention:
 e instruction set architecture sp
 es that the base for the branch address 
calculation is the address of the instruction following the branch. Since we 

compute PC + 4 (the address of the next instruction) in the instruction fetch 

datapath, it is easy to use this value as the base for computing the branch 

target address.
 e architecture also states that the o
 set 
 
 ed 
  2 bits so that it 
is a word o
 set; th
  increases th
 ective range of the
 set 
 eld by a 
factor of 4.
To deal with the latter complication, we will need to
  the o
 set 
 eld by 2.
As well as computing the branch target address, we must also determine whether 
the next instruction is the instruction that follows sequentially or the instruction 

at the branch target address. When the condition is true (i.e., the operands are 

equal), the branch target address becomes the new PC, and we say that the 
branch
 is taken. If the operands are not equal, the incremented PC should replace the 
current PC (just as for any other normal instruction); in this case, we say that the 
branch
 is not taken
. us, the branch datapath must do two operations: compute the branch target 
address and compare the register contents. (Branches also a
 ect the instruction 
fetch portion of the datapath, as we will deal with shortly.) 
Figure 4.9
 shows the 

structure of the datapath segment that handles branches. To compute the branch 

target address, the branch datapath includes a sign extension unit, from 
Figure 4.8
 
and an adder. To perform the compare, we need to use the regist
 le shown in 
Figure 4.7a
 to supply the two register operands (although we will not need to write 

into the regist
 le). In addition, the comparison can be done using the ALU we 
sign-extend
 To increase 
the size of a data item by 
replicating the high-order 

sign bit of the original 

data item in the high-

order bits of the larger, 

destination data item.
branch target 
address
 e address 
sp
 ed in a branch, 
which becomes the new 
program counter (PC) 

if the branch is taken. In 

the MIPS architecture the 

branch target is given by 

the sum of the o
 set 
 eld of the instruction and the 

address of the instruction 

following the branch.
branch taken
 A branch where the 

branch condition is 

satis
 ed and the program 
counter (PC) becomes 

the branch target. All 

unconditional jumps are 

taken branches.
branch not taken or 
(untaken branch)
 A branch where the 
branch condition is false 

and the program counter 

(PC) becomes the address 

of the instruction that 

sequentially follows the 

branch.

 4.3 Building a Datapath 
255designed in  Appendix B
. Since that ALU provides an output signal that indicates 
whether the result was 0, we can send the two register operands to the ALU with 
the control set to do a subtract. If the Zero signal out of the ALU unit is asserted, 
we know that the two values are equal. Although the Zero output always signals 

if the result is 0, we will be using it only to implement the equal test of branches. 

Later, we will show exactly how to connect the control signals of the ALU for use 

in the datapath.
 e jump instruction operates by replacing the lower 28 bits of the PC with the 
lower 26 bits of the instructio
 ed 
  by 2 bits. Simply concatenating 00 to the 
jump
 set accomplishes th
 , as described in Chapter 2.
Elaboration: In the MIPS instruction set, 
branches are delayed
, meaning that the 
instruction immediately following the branch is always executed, 
independent of whether the branch condition is true or false. When the condition is false, the execution looks 
like a normal branch. When the condition is true, a dela rst executes the 

instruction immediately following the branch in sequential instruction order before 

 ed branch target address. The motivation for delayed branches 
arises from how pipelining affects branches (see Section 4.8). For simplicity, we generally 

ignore delayed branches in this chapter and implement a nondelayed 
beq instruction.
branch
 A type of branch 
where the instruction 
immediately following the 

branch is always executed, 

independent of whether 

the branch condition is 

true or false.
AddressReaddataDatamemory
a. Data memory unit
Write
dataMemReadMemWrite
b. Sign extension unit
Sign-extend
1632FIGURE 4.8 The two units needed to implement loads and stores, in addition to the 
register ﬁ le and ALU of 
Figure 4.7
, are the data memory unit and the sign extension unit.
  e memory unit is a state element with inputs for the address and the write data, and a single output for 
the read resul
 ere are separate read and write controls, although only one of these may be asserted on 
any given cloc
 e memory unit needs a read signa
l, since, unlike the regist
 le, reading the value of 
an invalid address can cause problems, as we will see in Chapt
 e sign extension unit has a 16-bit 
input that is sign-extended into a 32-bit result appearing on the output (see Chapter 2). We assume the 
data memory is edge-triggered for writes. Standard memory chips actually have a write enable signal that is 

used for writes. Although the write enable is not edg
e-triggered, our edge-triggered design could easily be 
adapted to work with real memory chips. See Section B.8 of 
 Appendix B
 for further discussion of how 
real memory chips work.

256 Chapter 4 The Processor
Creating a Single DatapathNow that we have examined the datapath components needed for the individual 
instruction classes, we can combine them into a single datapath and add the control 

to complete the implementatio
 is simplest datapath will attempt to execute all 
instructions in one clock cycle
 is means that no datapath resource can be used 
more than once per instruction, so any element needed more than once must be 

duplicated. We therefore need a memory for instructions separate from one for 

data. Although some of the functional units will need to be duplicated, many of the 

elements can be shared b
 erent instructio
 ows.
To share a datapath element between tw
 erent instruction classes, we may 
need to allow multiple connections to the 
input of an element, using a multiplexor 
and control signal to select among the multiple inputs.
Readregister 1Registers
ALUZeroRegWrite
Readdata 1Readdata 2ALU operation
4To branch

control logicAdd
SumBranch

targetPC+4 from instruction datapath
Sign-extend
1632Instruction
Shiftleft 2Read
register 2Write
registerWrite
dataFIGURE 4.9 The datapath for a branch uses the ALU to evaluate the branch condition and 
a separate adder to compute the branch target as the sum of the incremented PC and the sign-extended, lower 16 bits of the instruction (the branch displacement), shifted left 2 

bits.  e unit labeled 
Shi
  
  2
 is simply a routing of the signals between input and output that adds 00
two
 to the low-order end of the sign-extended o
 set 
 eld; no actu
  hardware is needed, since the amount of 
the “s
 ” is constant. Since we know that the o
 set was sign-extended from 16 bits, th
  will throw away 
only “sign bits.” Control logic is used to decide whether the incremented PC or branch target should replace 

the PC, based on the Zero output of the ALU.

 4.3 Building a Datapath 
257Building a Datapath e operations of arithmetic-logical (or R-type) instructions and the memory 
instructions datapath are quite similar.
 e ke
 erences are the following:
 e arithmetic-logical instructions use the ALU, with the inputs coming 
from the two register
 e memory instructions can also use the ALU 
to do the address calculation, although the second input is the sign-
extended 16-bit o
 set 
 eld from the instruction.
 e value stored into a destination register comes from the ALU (for an 
R-type instruction) or the memory (for a load).
Show how to build a datapath for the operational portion of the memory-
reference and arithmetic-logical instructions that uses a single regist
 le and a single ALU to handle both types of instructions, adding any necessary 

multiplexors.
To create a datapath with only a single regist
 le and a single ALU, we must 
support tw
 erent sources for the second ALU input, as well as tw
 erent 
sources for the data stored into the regist
 le. 
 us, one multiplexor is placed 
at the ALU input and another at the data input to the regist
 le. 
Figure 4.10
 shows the operational portion of the combined datapath.
Now we can combine all the pieces to make a simple datapath for the core 
MIPS architecture by adding the datapath for instruction fetch (
Figure 4.6
), the 
datapath from R-type and memory instructions (
Figure 4.10
), and the datapath 

for branches (
Figure 4.9
). Figure 4.11
 shows the datapath we obtain by composing 

the separate pi
 e branch instruction uses the main ALU for comparison of 
the register operands, so we must keep the adder from 
Figure 4.9
 for computing 

the branch target address. An additional multiplexor is required to select either the 

sequentially following instruction address (PC + 4) or the branch target address to 

be written into the PC.
Now that we have completed this simple datapath, we can add the control unit. 
 e control unit must be able to take inputs and generate a write signal for each 
state element, the selector control for each multiplexor, and the ALU control
 e ALU contro
 erent in a number of ways, and it will be useful to design i
 rst 
before we design the rest of the control unit.
I. Which of the following is correct for a load instruction? Refer to 
Figure 4.10
.a. MemtoReg should be set to cause the data from memory to be sent to the 
regist
 le.
EXAMPLEANSWERCheck Yourself

258 Chapter 4 The Processor
Readregister 1Readregister 2Write
registerWrite
dataWrite

dataRegisters
ALUZeroRegWrite
MemReadMemWrite
MemtoRegReaddata 1Readdata 2ALU operation
4Sign-extend
1632Instruction
ALUresultMux01Mux1
0ALUSrcAddressDatamemory
ReaddataFIGURE 4.10 The datapath for the memory instructions and the R-type instructions. 
 is example shows how a single 
datapath can be assembled from the pieces in 
Figures 4.7 and 4.8
 by adding multiplexors. Two multiplexors are needed, as descri
bed in the 
example.
Readregister 1Write

dataRegisters
ALUAdd
ZeroRegWrite
MemReadMemWrite
PCSrcMemtoRegReaddata 1Readdata 2ALU operation
4Sign-extend
1632Instruction
ALUresultAddALUresultMuxMuxMuxALUSrcAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PCRead
register 2Write
registerWrite
dataFIGURE 4.11 The simple datapath for the core MIPS architecture combines the elements required by different 
instruction classes.
 e components come from 
Figures 4.6, 4.9, and 4.10
 is datapath can execute the basic instructions (load-store 
word, ALU operations, and branches) in a single clock cycle. Just one additional multiplexor is needed to integrate branch
 e support for 
jumps will be added later.

 4.4 A Simple Implementation Scheme 
259b. MemtoReg should be set to cause the correct register destination to be 
sent to the regist
 le.
c. We do not care about the setting of MemtoReg for loads.
 e single-cycle datapath conceptually described in this section 
must
 have 
separate instruction and data memories, because
a. the formats of data and instructions ar
 erent in MIPS, and hence 
 erent memories are needed.
b. having separate memories is less expensive.
c. the processor operates in one cycle and cannot use a single-ported 
memory for tw
 erent accesses within that cycle
 4.4 A Simple Implementation Scheme
In this section, we look at what might be thought of as the simplest possible 
implementation of our MIPS subset. We build this simple implementation using 

the datapath of the last section and adding a simple control functio
 is simple 
implementation covers 
load word
 (lw), store word
 (sw), branch equal
 (beq), and 
the arithmetic-logical instructions 
add, sub, AND, OR, and 
set on less than. We will later enhance the design to include a jump instruction (
j).The ALU Control e MIPS ALU in 
 Appendix B
 nes the 6 following combinations of four 
control inputs:
ALU control linesFunction0000AND0001OR
0010add

0110subtract

0111set on less than

1100NOR
Depending on the instruction class, the ALU will need to perform one of these 
 rst
 ve functions. (NOR is needed for other parts of the MIPS instruction set not 
found in the subset we are implementing.) For load word and store word instructions, 
we use the ALU to compute the memory address by addition. For the R-type 

instructions, the ALU needs to perform one of th
 ve actions (AND, OR, subtract, 
add, or set on less than), depending on the value of the 6-bit funct (or function) 
 eld 
260 Chapter 4 The Processor
in the low-order bits of the instruction (see Chapter 2). For branch equal, the ALU 
must perform a subtraction.
We can generate the 4-bit ALU control input using a small control unit that has 
as inputs the functio
 eld of the instruction and a 2-bit contro
 eld, which we 
call ALUOp. ALUOp indicates whether the 
operation to be performed should be 
add (00) for loads and stores, subtract (01) for 
beq, or determined by the operation 
encoded in the func 
 e output of the ALU control unit is a 4-bit signal 
that directly controls the ALU by generating one of the 4-bit combinations shown 

previously.
In 
Figure 4.12
, we show how to set the ALU control inputs based on the 2-bit 
ALUOp control and the 6-bit function code. Later in this chapter we will see how 

the ALUOp bits are generated from the main control unit.
 is style of using multiple levels of decoding—that is, the main control unit 
generates the ALUOp bits, which then are used as input to the ALU control that 

generates the actual signals to control the ALU unit—is a common implementation 

technique. Using multiple levels of control can reduce the size of the main control 

unit. Using several smaller control units may also potentially increase the speed of 

the control unit. Such optimizations are important, since the speed of the control 

unit is o
 en critical to clock cycle time.
 ere are sev
 erent ways to implement the mapping from the 2-bit 
AL
 eld and the 6-bit func
 eld to the four ALU operation control bits. 
Because only a small number of the 64 possible values of the functio
 eld are of 
interest and the function
 eld is used only when the ALUOp bits equal 10, we can 
use a small piece of logic that recognizes 
the subset of possible values and causes 
the correct setting of the ALU control bits.
As a step in designing this logic, it is useful to create a truth table for the 
interesting combinations of the function co
 eld and the ALUOp bits, as we’ve 
Instruction 
opcodeALUOpInstruction 
operationFunct ÞeldDesired 
ALU actionALU control 
inputLW
00load word
XXXXXXadd
0010SW00store wordXXXXXXadd0010
Branch equal01branch equalXXXXXXsubtract0110

R-type10add100000add0010

R-type10subtract100010subtract0110

R-type10AND100100AND0000

R-type10OR100101OR0001

R-type10set on less than101010set on less than0111
FIGURE 4.12 How the ALU control bits are set depends on the ALUOp control bits and the different function codes for the R-type instruction.
 e opcode, listed in th
 rst column, 
determines the setting of the ALUOp bits. All the encodings are shown in binary. Notice that when the 
ALUOp code is 00 or 01, the desired ALU action does not depend on the function co
 eld; in this case, we 
say that we “don’t care” about the value of the function code, and the func
 eld is shown as XXXXXX. When 
the ALUOp value is 10, then the function code is used to set the ALU control input. See 
 Appendix B
.
 4.4 A Simple Implementation Scheme 
261done in 
Figure 4.13
; this 
truth table
 shows how the 4-bit ALU control is set 
depending on these two inpu
 elds. Since the full truth table is very large (2
8 = 256 entries) and we don’t care about the value of the ALU control for many of these input 
combinations, we show only the truth table entries for which the ALU control must 

have a sp
 c value.
 roughout this chapter, we will use this practice of showing 
only the truth table entries for outputs that must be asserted and not showing those 

that are all deasserted or don’t care
 is practice has a disadvantage, which we 
discuss in Section D.2 of 
 Appendix D
.)Because in many instances we do not care about the values of some of the 
inputs
, and because we wish to keep the tables compact, we also include 
don’t-care terms
. A don’t-care term in this truth table (represented by an X in an input column) 
indicates that the output does not depend on the value of the input corresponding 

to that column. For example, when the ALUOp bits are 00, as in th rst row of 

Figure 4.13
, we always set the ALU control to 0010, independent of the function 

code. In this case, then, the function code inputs will be don’t cares in this line of 

the truth table. Later, we will see examples of another type of don’t-care term. If you 

are unfamiliar with the concept of don’t-care terms, see 
 Appendix B
 for more 
information.
Once the truth table has been constructed, it can be optimized and then turned 
into gat
 is process is completely mechanical
 us, rather than show th
 nal 
steps here, we describe the process and the result in Section D.2 of 
 Appendix D
.Designing the Main Control UnitNow that we have described how to design an ALU that uses the function code and 

a 2-bit signal as its control inputs, we can return to looking at the rest of the control. 

To start this process, let’s identify th
 elds of an instruction and the control lines 
that are needed for the datapath we constructed in 
Figure 4.11
. To understand 

how to connect th
 elds of an instruction to the datapath, it is useful to review 
truth table
 From logic, a 
representation of a logical 
operation by listing all the 

values of the inputs and 

then in each case showing 

what the resulting outputs 

should be.
don’t-care term
 An element of a logical 

function in which the 

output does not depend 

on the values of all the 

inputs. Don’t-care terms 

may be sp
 ed in 
 erent ways.
ALUOpFunct ÞeldOperationALUOp1ALUOp0F5F4F3F2F1F0
00XXXXXX
0010X1XXXXXX
0110 1XXX0000
0010 1XXX0010
0110 1XXX0100
00001XXX0101
0001 1XXX1010
0111 FIGURE 4.13 The truth table for the 4 ALU control bits (called Operation).
 e inputs are the 
ALUOp and function co
 eld. Only the entries for which the ALU control is asserted are shown. Some 
don’t-care entries have been added. For example, the ALUOp does not use the encoding 11, so the truth table 
can contain entries 1X and X1, rather than 10 and 01. Note that when the functio
 eld is used, th
 rst 2 
bits (F5 and F4) of these instructions are always 10, so they are don’t-care terms and are replaced with XX 

in the truth table.

262 Chapter 4 The Processor
the formats of the three instruction classes: the R-type, branch, and load-store 
instructions. 
Figure 4.14
 shows these formats.
 ere are several major observations about this instruction format that we will 
rely on:
 e o
 eld, which as we saw in Chapter 2 is called the 
opcode
, is always 
contained in bits 31:26. We will refer to t
 eld as Op[5:0].
 e two registers to be read are always sp
 ed by the rs and rt
 elds, at 
positions 25:21 an
 is is true for the R-type instructions, branch 
equal, and store.
 e base register for load and store instructions is always in bit positions 
25:21 (rs).
 e 16-bit o
 set for branch equal, load, and store is always in positions 15:0.
 e destination register is in one of two places. For a load it is in bit positions 
20:16 (rt), while for an R-type instruction it is in bit positions 15:11 (rd). 

 us, we will need to add a multiplexor to select whic
 eld of the instruction 
is used to indicate the register number to be written.
 e 
 rst design principle from Chapter 2—
simplicity favors regularity
—pays o
  here in specifying control.
opcode
 e 
 eld that 
denotes the operation and 
format of an instruction.
Field
0rs
rt
rdshamtfunct
Bit positions31:2625:2120:1615:1110:65:0
a. R-type instruction
Field
35 or 43rs
rt
addressBit positions31:2625:2120:16
15:0b. Load or store instruction
Field
4rs
rt
addressBit positions31:2625:2120:16
15:0c. Branch instruction
FIGURE 4.14 The three instruction classes (R-type, load and store, and branch) use two 
different instruction formats.
 e jump instructions use another format, which we will discuss shortly. 
(a) Instruction format for R-format instructions, which all have an opcode o
 ese instructions have three 
register operands: rs, rt, and rd. Fields rs and rt are sources, and rd is the destinatio
 e ALU function is 
in the func
 eld and is decoded by the ALU control design in the previous sectio
 e R-type instructions 
that we implement are 
add, sub, AND, OR, and 
slt e sham
 eld is used only fo
 s; we will ignore it 
in this chapter. (b) Instruction format for load (opcode = 35
ten
) and store (opcode = 43
ten
) instruction
 e register rs is the base register that is added to the 16-bit addr
 eld to form the memory address. For loads, 
rt is the destination register for the loaded value. Fo
r stores, rt is the source register whose value should be 
stored into memory. (c) Instruction format for branch equal (opco
 e registers rs and rt are the 
source registers that are compared for equality
 e 16-bit addr
 eld is sign-extended
 ed, and added 
to the PC + 4 to compute the branch target address.

 4.4 A Simple Implementation Scheme 
263Using this information, we can add the instruction labels and extra multiplexor 
(for the Write register number input of the regist
 le) to the simple datapath. 
Figure 4.15
 shows these additions plus the ALU control block, the write signals for 
state elements, the read signal for the data 
memory, and the control signals for the 
multiplexors. Since all the multiplexors have two inputs, they each require a single 

control line.
Figure 4.15
 shows seven single-bit control lines plus the 2-bit ALUOp control 
signal. We have alread
 ned how the ALUOp contro
l signal works, and it is 
useful t
 ne what the seven other control signals do informally before we 
determine how to set these control signals during instruction execution. 
Figure
 
4.16 describes the function of these seven control lines.
Now that we have looked at the function of each of the control signals, we can 
look at how to set th
 e control unit can set all but one of the control signals 
based solely on the opco
 eld of the instructio
 e PCSrc control line is the 
exceptio
 at control line should be asserted if the instruction is branch on equal 
(a decision that the control unit can make) 
and
 the Zero output of the ALU, which 
is used for equality comparison, is asserted. To generate the PCSrc signal, we will 

need to AND together a signal from the control unit, which we call 
Branch
, with 
the Zero signal out of the ALU.
Readregister 1Write
dataRegisters
ALUAdd
ZeroMemReadMemWrite
RegWrite
PCSrcMemtoRegReaddata 1Readdata 2Sign-extend
1632Instruction
[31:0]ALUresultAdd
ALUresultMuxMuxMuxALUSrcAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PC100
101Mux01ALUcontrolALUOpInstruction [5:0]
Instruction [25:21]
Instruction [15:11]
Instruction [20:16]
Instruction [15:0]
RegDstReadregister 2Write
registerWrite
dataFIGURE 4.15 The datapath of Figure 4.11
 with all necessary multiplexors and all control lines identiﬁ
 ed. e control 
lines are shown in color.
 e ALU control block has also been added
 e PC does not require a write control, since it is written once at the end 
of every clock cycle; the branch control logic determines whether it is written with the incremented PC or the branch target ad
dress.

264 Chapter 4 The Processor
Signal nameEffect when deassertedEffect when asserted
RegDstThe register destination number for the 
Write register comes from the rt Þeld 
(bits 20:16).The register destination number for the Write 
register comes from the rd Þeld (bits 15:11).RegWriteNone.
The register on the Write register input is 
written with the value on the Write data input. ALUSrcThe second ALU operand comes from the 
second register Þle output (Read data 2).The second ALU operand is the sign-
extended, lower 16 bits of the instruction.
PCSrcThe PC is replaced by the output of the 
adder that computes the value of PC + 4.The PC is replaced by the output of the adder 

that computes the branch target.MemReadNone.
Data 
memory contents designated by the 
address input are put on the Read data output. 
MemWriteNone.
Data memory contents designated by the 

address input are replaced by the value on 

the Write data input.MemtoRegThe value fed to the register Write data 
input comes from the ALU.The value fed to the register Write data input 
comes from the data memory.
FIGURE 4.16 The effect of each of the seven control signals.
 When the 1-bit control to a two-
way multiplexor is asserted, the multiplexor selects the input corresponding to 1. Otherwise, if the control 
is deasserted, the multiplexor selects the 0 input. Remember that the state elements all have the clock as an 

implicit input and that the clock is used in controlling writes. Gating the clock externally to a state element 

can create timing problems. (See 
 Appendix B
 for further discussion of this problem.)
 ese nine control signals (seven from 
Figure 4.16
 and two for ALUOp) can 
now be set on the basis of six input signals to the control unit, which are the opcode 
bits 31 to 26. 
Figure 4.17
 shows the datapath with the control unit and the control 

signals.
Before we try to write a set of equations or a truth table for the control unit, it 
will be useful to try to
 ne the control function informally. Because the setting 
of the control lines depends only on the opcode, w
 ne whether each control 
signal should be 0, 1, or don’t care (X) for each of the opcode values. 
Figure 4.18
 
 nes how the control signals should be set for each opcode; this information 
follows directly from 
Figures 4.12, 4.16, and 4.17
.Operation of the DatapathWith the information contained in 
Figures 4.16 and 4.18
, we can design the control 

unit logic, but before we do that, let’s look at how each instruction uses the datapath. 

In the next fe
 gures, we show th
 ow of thre
 erent instruction classes 
through the datapat
 e asserted control signals and active datapath elements 
are highlighted in each of these. Note that a multiplexor whose control is 0 has 

 nite action, even if its control line is not highlighted. Multiple-bit control 
signals are highlighted if any co
nstituent signal is asserted.
Figure 4.19
 shows the operation of the datapath for an R-type instruction, such 
as add $t1,$t2,$t3. Although everything occurs in one clock cycle, we can 

 4.4 A Simple Implementation Scheme 
265think of four steps to execute the instruction; these steps are ordered by th
 ow 
of information:
 e instruction is fetched, and the PC is incremented.
2. Two registers, 
$t2 and 
$t3, are read from the regist
 le; also, the main 
control unit computes the setting of the control lines during this step.
 e ALU operates on the data read from the regist
 le, using the function 
code (bits 5:0, which is the func eld, of the instruction) to generate the 
ALU function.
Readregister 1Write
dataRegisters
ALUAdd
ZeroReaddata 1Readdata 2Sign-extend
1632Instruction
[31Ð0]ALUresultAdd
ALUresultMuxMuxMuxAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PC100
101Mux01ALUcontrol
Instruction [5Ð0]
Instruction [25Ð21]
Instruction [31Ð26]
Instruction [15Ð11]
Instruction [20Ð16]
Instruction [15Ð0]
RegDstBranch

MemRead
MemtoReg
ALUOp
MemWrite

ALUSrc
RegWrite
Control
Readregister 2Write
registerWrite

dataFIGURE 4.17 The simple datapath with the control unit.
 e input to the control unit is the 6-bit opco
 eld from the instruction. 
 e outputs of the control unit consist of three 1-bit signals that are used to control multiplexors (RegDst, ALUSrc, and MemtoR
eg), three 
signals for controlling reads and writes in the regist le and data memory (RegWrite, MemRead, and MemWrite), a 1-bit signal used in 
determining whether to possibly branch (Branch), and a 2-bit control signal for the ALU (ALUOp). An AND gate is used to combine
 the 
branch control signal and the Zero output from the ALU; the AND gate output controls the selection of the next PC. Notice that 
PCSrc is now 
a derived signal, rather than one coming directly from the control uni
 us, we drop the signal name in subsequen
 gures.

266 Chapter 4 The Processor
InstructionRegDstALUSrc
Memto- RegReg- Write
Mem- ReadMem- WriteBranchALUOp1ALUOp0
R-format
1001000
10lw 011110000
swX1X001000
beqX0X000101
FIGURE 4.18 The setting of the control lines is completely determined by the opcode ﬁ elds of the instruction.
 e 
 rst 
row of the table corresponds to the R-format instructions (
add, sub, AND, OR, and 
slt). For all these instructions, the source regist
 elds 
are rs and rt, and the destination regist
 eld is rd; th
 nes how the signals ALUSrc and RegDst are set. Furthermore, an R-type instruction 
writes a register (Reg-Write = 1), but neither reads nor writes data memory. When the Branch control signal is 0, the PC is unc
onditionally 
replaced with PC + 4; otherwise, the PC is replaced by the branch target if the Zero output of the ALU is also hig e AL
 eld for R-type 
instructions is set to 10 to indicate that the ALU control should be generated from the func
 eld. 
 e second and third rows of this table give the 
control signal settings for 
lw and 
sw ese ALUSrc and AL
 elds are set to perform the address calculatio
 e MemRead and MemWrite 
are set to perform the memory access. Finally, RegDst and RegWrite are set for a load to cause the result to be stored into the
 rt register
 e branch instruction is similar to an R-format operation, since it sends the rs and rt registers to the ALU
 e AL
 eld for branch is set for a 
subtract (ALU control = 01), which is used to test for equality. Notice that the Memt
 eld is irrelevant when the RegWrite signal is 0: since 
the register is not being written, the value of the data on the register data write port is not used
 us, the entry MemtoReg in the last two rows 
of the table is replaced with X for don’t care. Don’t cares can also be added to RegDst when RegWrite
 is type of don’t care must be added 
by the designer, since it depends on knowledge of how the datapath works.
Readregister 1Write
dataRegisters
ALUAdd
ZeroReaddata 1Readdata 2Sign-extend
1632Instruction
[31Œ0]ALUresultAdd
ALUresultMuxMuxMuxAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PC100
101Mux01ALUcontrol
Instruction [5Œ0]
Instruction [25Œ21]
Instruction [31Œ26]
Instruction [15Œ11]
Instruction [20Œ16]
Instruction [15Œ0]
RegDstBranch

MemRead
MemtoReg
ALUOp
MemWrite
ALUSrcRegWrite
Control
Readregister 2Write
registerWrite

dataFIGURE 4.19 The datapath in operation for an R-type instruction, such as 
add $t1,$t2,$t3. e control lines, datapath units, 
and connections that are active are highlighted.

 4.4 A Simple Implementation Scheme 
267 e result from the ALU is written into the regist le using bits 15:11 of the 
instruction to select the destination register (
$t1).Similarly, we can illustrate the execution of a load word, such as
lw $t1, offset($t2)in a style similar to 
Figure 4.19
. Figure 4.20
 shows the active functional units and 
asserted control lines for a load. We can think of a load instruction as operating in 

 ve steps (similar to how the R-type executed in four):
1. An instruction is fetched from the instruction memory, and the PC is 
incremented.
2. A register (
$t2) value is read from the regist
 le.
Readregister 1Write

dataRegisters
ALUAdd
ZeroReaddata 1Readdata 2Sign-extend
1632Instruction
[31Œ0]ALUresultAdd
ALUresultMuxMuxMuxAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PC100
101Mux01ALUcontrol
Instruction [5Œ0]
Instruction [25Œ21]
Instruction [31Œ26]
Instruction [15Œ11]
Instruction [20Œ16]
Instruction [15Œ0]
RegDstBranch

MemRead
MemtoReg
ALUOp
MemWrite
ALUSrcRegWrite
Control
Readregister 2Write
registerWrite
dataFIGURE 4.20 The datapath in operation for a load instruction.
 e control lines, datapath units, and connections that are active 
are highlighted. A store instruction would operate very similarly
 e ma
 erence would be that the memory control would indicate a write 
rather than a read, the second register value read would be used for the data to store, and the operation of writing the data memory value to 
the regist
 le would not occur.

268 Chapter 4 The Processor
 e ALU computes the sum of the value read from the regist
 le and the 
sign-extended, lower 16 bits of the instruction (
offset). e sum from the ALU is used as the address for the data memory.
 e data from the memory unit is written into the regist
 le; the register 
destination is given by bits 20:16 of the instruction (
$t1).Finally, we can show the operation of the branch-on-equal instruction, such as 
beq $t1, $t2, offset, in the same fashion. It operates much like an R-format 
instruction, but the ALU output is used to determine whether the PC is written with 
PC + 4 or the branch target address. 
Figure 4.21
 shows the four steps in execution:
1. An instruction is fetched from the instruction memory, and the PC is 
incremented.
Readregister 1Write

dataRegisters
ALUAddZeroReaddata 1Readdata 2Sign-extend
1632Instruction
[31Œ0]ALUresultAdd
ALUresultMuxMuxMuxAddressDatamemoryReaddataShiftleft 24Read
addressInstructionmemory
PC100101Mux01ALUcontrol
Instruction [5Œ0]
Instruction [25Œ21]
Instruction [31Œ26]Instruction [15Œ11]
Instruction [20Œ16]
Instruction [15Œ0]RegDstBranch

MemRead
MemtoReg
ALUOp
MemWrite
ALUSrc
RegWrite
Control
Readregister 2Write
registerWritedataFIGURE 4.21 The datapath in operation for a branch-on-equal instruction. 
 e control lines, datapath units, and connections 
that are active are highlighted
 er using the regist
 le and ALU to perform the compare, the Zero output is used to select the next program 
counter from between the two candidates.

 4.4 A Simple Implementation Scheme 
2692. Two registers, 
$t1 and 
$t2, are read from the regist
 le.
 e ALU performs a subtract on the data values read from the regist
 le. 
 e value of PC + 4 is added to the sign-extended, lower 16 bits of the instruction 
(offset ed 
  by two; the result is the branch target address.
 e Zero result from the ALU is used to decide which adder result to store 
into the PC.
Finalizing Control
Now that we have seen how the instructions operate in steps, let’s continue with 

the control implementatio
 e control function can be precisel
 ned using 
the contents of 
Figure 4.18
 e outputs are the control lines, and the input is the 
6-bit opco
 eld
 us, we can create a truth table for each of the outputs 
based on the binary encoding of the opcodes.
Figure 4.22
 shows the logic in the control unit as one large truth table that 
combines all the outputs and that uses the opcode bits as inputs. It completely 

sp
 es the control function, and we can implement it directly in gates in an 
automated fashion. We show th
 nal step in Section D.2 in 
 Appendix D
.Input or outputSignal nameR-format
lwswbeq
InputsOp50110Op40000Op30010Op20001Op10110Op00110OutputsRegDst10XXALUSrc0110MemtoReg01XXRegWrite1100MemRead0100MemWrite0010Branch0001ALUOp11000ALUOp00001FIGURE 4.22 The control function for the simple single-cycle implementation is completely speciﬁ ed by this truth table.
 e top half of the table gives the combinations of input 
signals that correspond to the four opcodes, one per column, that determine the control output settings. 
(Remember that Op [5:0] corresponds to bits 31:26 of the instruction, which is the o
 eld.) 
 e bottom 
portion of the table gives the outputs for each of the four opco
 us, the output RegWrite is asserted for 
tw
 erent combinations of the inputs. If we consider only the four opcodes shown in this table, then we 
can simplify the truth table by using don’t cares in the input portion. For example, we can detect an R-format 

instruction with the expression Op5
   Op2 , since th
  cient to distinguish the R-format instructions 
from 
lw, sw, and 
beq. We do not take advantage of this simp
 cation, since the rest of the MIPS opcodes 
are used in a full implementation.

270 Chapter 4 The Processor
Now that we have a 
single-cycle implementation
 of most of the MIPS core 
instruction set, let’s add the jump instruction to show how the basic datapath and 
control can be extended to handle other instructions in the instruction set.
Implementing JumpsFigure 4.17
 shows the implementation of many of the instructions we looked at 
in Chapter 2. One class of instructions missing is that of the jump instruction. 

Extend the datapath and control of 
Figure 4.17
 to include the jump instruction. 

Describe how to set any new control lines.
 e jump instruction, shown in 
Figure 4.23
, looks somewhat like a branch 
instruction but computes the targ
 erently and is not conditional. Like 
a branch, the low-order 2 bits of a jump address are always 00
two
 e next 
lower 26 bits of this 32-bit address come from the 26-bit immediat
 eld in the 
instructio
 e upper 4 bits of the address that should replace the PC come 
from the PC of the jump instruction pl
 us, we can implement a jump by 
storing into the PC the concatenation of
 the upper 4 bits of the current PC + 4 (these are bits 31:28 of the 
sequentially following instruction address)
 the 26-bit immediate
 eld of the jump instruction
 the bits 00
two
Figure 4.24
 shows the addition of the control for jump added to 
Figure 4.17
. An additional multiplexor is used to select the source for the new PC value, which 

is either the incremented PC (PC + 4), the branch target PC, or the jump target 

PC. One additional control signal is needed for the additional multiplexor
 is control signal, called 
Jump
, is asserted only when the instruction is a jump—
that is, when the opcode is 2.
EXAMPLEANSWERField
000010addressBit positions31:2625:0FIGURE 4.23 Instruction format for the jump instruction (opcode = 2). 
 e destination 
address for a jump instruction is formed by concatenating the upper 4 bits of the current PC + 4 to the 26-bit 
addr
 eld in the jump instruction and adding 00 as the 2 low-order bits.
single-cycle 
implementation
 Also 
called single clock cycle 
implementation
. An implementation in which 
an instruction is executed 
in one clock cycle. While 

easy to understand, it is 

too slow to be practical.

 4.4 A Simple Implementation Scheme 
271Why a Single-Cycle Implementation Is Not Used Today
Although the single-cycle design will work correctly, it would not be used in 
modern designs because i
  cient. To see why this is so, notice that the clock 
cycle must have the same length for every instruction in this single-cycle design. 

Of course, the longest possible path in the processor determines the clock cycle. 

 is path is almost certainly a load instruction, which us
 ve functional units 
in series: the instruction memory, the regist
 le, the ALU, the data memory, and 
the regist
 le. Although the CPI is 1 (see Chapter 1), the overall performance of 
a single-cycle implementation is likely to be poor, since the clock cycle is too long.
 e penalty for using the single-cycle design wit
 xed clock cyc
 cant, 
but might be considered acceptable for this small instruction set. Historically, early 
Readregister 1Write
dataRegisters
ALUAdd
ZeroReaddata 1Readdata 2Sign-extend
1632Instruction
[31Œ0]ALUresultAdd
ALUresultMuxMuxMuxAddressDatamemory
ReaddataShiftleft 24Read
addressInstructionmemory
PC100101Mux01ALUcontrol
Instruction [5Œ0]
Instruction [25Œ21]
Instruction [31Œ26]
Instruction [15Œ11]
Instruction [20Œ16]Instruction [15Œ0]
RegDstJump

Branch
MemRead
MemtoRegALUOp
MemWrite

ALUSrc
RegWrite
Control
Readregister 2Write
registerWrite

dataMux10Shiftleft 2Instruction [25Œ0]
Jump address [31Œ0]
2628PC + 4 [31Œ28]FIGURE 4.24 The simple control and datapath are extended to handle the jump instruction.
 An additional multiplexor (at 
the upper right) is used to choose between the jump target and either the branch target or the sequential instruction following
 this one.
 is multiplexor is controlled by the jump control signal.
 e jump target address is obtained by
 ing the lower 26 bits of the jump instruction 
  2 bi
 ectively adding 00 as the low-order bits, and then concatenating the upper 4 bits of PC + 4 as the high-order bits, thus yield
ing a 
32-bit address.

272 Chapter 4 The Processor
computers with very simple instruction sets did use this implementation technique. 
However, if we tried to implement th
 oating-point unit or an instruction set with 
more complex instructions, this single-cycle design wouldn’t work well at all.
Because we must assume that the clock cycle is equal to the worst-case delay 
for all instructions, it’s useless to try implementation techniques that reduce the 

delay of the common case but do not improve the worst-case cycle time. A single-

cycle implementation thus violates the great idea from Chapter 1 of making the 

common case fast
. In next section, we’ll look at another implementation technique, called 
pipelining, that uses a datapath very similar to the single-cycle datapath but is 

much more e
  cient by having a much higher throughput. Pipelining improves 
  ciency by executing multiple instructions simultaneously.
Look at the control signals in 
Figure 4.22
. Can you combine any together? Can any 

control signal output in th
 gure be replaced by the inverse of another? (Hint: take 
into account the don’t cares.) If so, can you use one signal for the other without 

adding an inverter?
 4.5 An Overview of Pipelining
Pipelining
 is an implementation technique in which multiple instructions are 
overlapped in execution. Today, 
pipelining
 is nearly universal.
 is section relies heavily on one analogy to give an overview of the pipelining 
terms and issues. If you are interested in just the big picture, you should concentrate 

on this section and then skip to Sections 4.10 and 4.11 to see an introduction to the 

advanced pipelining techniques used in recent processors such as the Intel Core i7 

and ARM Cortex-A8. If you are interested in exploring the anatomy of a pipelined 

computer, this section is a good introduction to Sections 4.6 through 4.9.
Anyone who has done a lot of laundry has intuitively used pipelinin
 e non-
pipelined
 approach to laundry would be as follows:
1. Place one dirty load of clothes in the washer.
2. When th
 nished, place the wet load in the dryer.
3. When the dry
 nished, place the dry load on a table and fold.
4. When foldin
 nished, ask your roommate to put the clothes away.
When your roommate is done, start over with the next dirty load.
 e pipelined
 approach takes much less time, as 
Figure 4.25
 shows. As soon 
as th
 nished with th
 rst load and placed in the dryer, you load the 
washer with the second dirty load. When th
 rst load is dry, you place it on the 
table to start folding, move the wet load to the dryer, and put the next dirty load 
Check Yourself
pipelining
 An implementation 
technique in which 

multiple instructions are 

overlapped in execution, 

much like an assembly 

line.
Never waste time.
American proverb

 4.5 An Overview of Pipelining 
273into the washer. Next you have your roommate put th
 rst load away, you start 
folding the second load, the dryer has the third load, and you put the fourth load 
into the washer. At this point all steps—called 
stages
 in pipelining—are operating 
concurrently. As long as we have separate resources for each stage, we can pipeline 

the tasks.
 e pipelining paradox is that the time from placing a single dirty sock in the 
washer until it is dried, folded, and put away is not shorter for pipelining; the reason 

pipelining is faster for many loads is that everything is working in parallel, so more 

loads ar
 nished per hour. Pipelining improves throughput of our laundry system. 
Hence, pipelining would not decrease the time to complete one load of laundry, 

but when we have many loads of laundry to do, the improvement in throughput 

decreases the total time to complete the work.
If all the stages take about the same amount of time and there is enough work 
to do, then the speed-up due to pipelining is equal to the number of stages in the 
TimeTaskorderAB
C
D6 PM78910111212 AM
TimeTaskorderAB
CD6 PM78910111212 AM
FIGURE 4.25 The laundry analogy for pipelining.
 Ann, Brian, Cathy, and Don each have dirty 
clothes to be washed, dried, folded, and put away
 e washer, dryer, “folder,” and “storer” each take 30 
minutes for their task. Sequential laundry takes 8 hours 
for 4 loads of wash, whi
le pipelined laundry takes 
just 3.5 hours. We show the pipeline stage of
 erent loads over time by showing copies of the four resources 
on this two-dimensional time line, but we really have just one of each resource.

274 Chapter 4 The Processor
pipeline, in this case four: washing, drying, folding, and putting away
 erefore, 
pipelined laundry is potentially four tim
es faster than nonpipelined: 20 loads would 
take about 5 times as long as 1 load, while 20 loads of sequential laundry takes 20 
times as long as 1 load. It’s only 2.3 times faster in 
Figure 4.25
, because we only 

show 4 loads. Notice that at the beginning 
and end of the workload in the pipelined 
version in 
Figure 4.25
, the pipeline is not completely full; this start-up and wind-

down a
 ects performance when the number of tasks is not large compared to the 
number of stages in the pipeline. If the number of loads is much larger than 4, then 

the stages will be full most of the time and the increase in throughput will be very 

close to 4.
 e same principles apply to processors where we pipeline instruction-execution. 
MIPS instructions classically tak
 ve steps:
1. Fetch instruction from memory.
2. Read registers while decoding the instructio
 e regular format of MIPS 
instructions allows reading and decoding to occur simultaneously.
3. Execute the operation or calculate an address.

4. Access an operand in data memory.

5. Write the result into a register.
Hence, the MIPS pipeline we explore in this chapt
 ve stag
 e following 
example shows that pipelining speeds up instruction execution just as it speeds up 
the laundry.
Single-Cycle versus Pipelined Performance
To make this discussion concrete, let’s create a pipeline. In this example, and in 

the rest of this chapter, we limit our attention to eight instructions: load word 

(lw), store word (
sw), add (add), subtract (
sub), AND (and), OR (
or), set 
less than (
slt), and branch on equal (
beq).Compare the average time between instructions of a single-cycle 
implementation, in which all instructions take one clock cycle, to a pipelined 

implementatio
 e operation times for the major functional units in this 
example are 200 ps for memory access, 200 ps for ALU operation, and 100 ps 

for regist
 le read or write. In the single-cycle model, every instruction takes 
exactly one clock cycle, so the clock cycle must be stretched to accommodate 

the slowest instruction.
Figure 4.26
 shows the time required for each of the eight instructions. 
 e single-cycle design must allow for the slowest instruction—in 
Figure 
4.26 it is 
lw—so the time required for every instruction is 800 ps. Similarly 
EXAMPLEANSWER
 4.5 An Overview of Pipelining 
275to 
Figure 4.25
, Figure 4.27
 compares nonpipelined and pipelined execution 
of three load word instruction
 us, the time between th
 rst and fourth 
instructions in the nonpipelined design is 3 × 800 ns or 2400 ps.
All the pipeline stages take a single clock cycle, so the clock cycle must be long 
enough to accommodate the slowest operation. Just as the single-cycle design 

must take the worst-case clock cycle of 800 ps, even though some instructions 

can be as fast as 500 ps, the pipelined execution clock cycle must have the 

worst-case clock cycle of 200 ps, even though some stages take only 100 ps. 

Pipelining still o
 ers a fourfold performance improvement: the time between 
th
 rst and fourth instructions is 3 × 200 ps or 600 ps.
We can turn the pipelining speed-up discussion above into a formula. If the 
stages are perfectly balanced, then the time between instructions on the pipelined 

processor—assuming ideal conditions—is equal to
Time bet
tions
Time between ins
tructio
pipelined
ween instruc
nn
nonpipelinedNumber of pipe stages
Under ideal conditions and with a large number of instructions, the speed-up 
from pipelining is approximately equal to the number of pipe stag
 ve-stage 
pipeline is nearly
 ve times faster.
 e formula suggests tha
 ve-stage pipeline should o
 er nearly
 vefold 
improvement over the 800 ps nonpipelined time, or a 160 ps clock cycle
 e example shows, however, that the stages may be imperfectly balanced. Moreover, 

pipelining involves some overhead, the source of which will be clearer shortly. 

 us, the time per instruction in the pipelined processor will exceed the minimum 
possible, and speed-up will be less than the number of pipeline stages.
Instruction class
Instruction 
fetchRegister read
ALU operationData accessRegister writeTotal 
timeLoad word (lw)200 ps100 ps200 ps200 ps100 ps800 ps
Store word (sw)200 ps100 ps200 ps200 ps
700 psR-format (
add, sub, AND, OR, slt)200 ps100 ps200 ps100 ps600 ps
Branch (beq)200 ps100 ps200 ps
500 psFIGURE 4.26 Total time for each instruction calculated from the time for each component.
  is calculation assumes that the multiplexors, control unit, PC accesses, and sign extension unit have no 
delay.

276 Chapter 4 The Processor
Moreover, even our claim of fourfold improvement for our example is not 
re
 ected in the total execution time for the three instructions: it’s 1400 ps versus 
2400 ps. Of course, this is because the number of instructions is not large. What 
would happen if we increased the number of instructions? We could extend the 

previo
 gures to 1,000,003 instructions. We would add 1,000,000 instructions 
in the pipelined example; each instruction adds 200 ps to the total execution time. 

 e total execution time would be 1,000,000 × 200 ps + 1400 ps, or 200,001,400 
ps. In the nonpipelined example, we would add 1,000,000 instructions, each 

taking 800 ps, so total execution time would be 1,000,000 × 800 ps + 2400 ps, or 

800,002,400 ps. Under these conditions, the ratio of total execution times for real 

programs on nonpipelined to pipelined pr
ocessors is close to the ratio of times 
between instructions:
800002400
200001400
,,,,pspspsps800
2004.00Programexecution
order
(in instructions)lw  $1, 100($0)lw  $2, 200($0)
lw  $3, 300($0)Time100012001400
200400600800
100012001400
200400600800
16001800
InstructionfetchDataaccessRegInstructionfetchDataaccessRegInstructionfetch800 ps800 ps800 psProgramexecution
order
(in instructions)lw  $1, 100($0)lw  $2, 200($0)
lw  $3, 300($0)TimeInstructionfetchDataaccessRegInstructionfetchInstructionfetchDataaccessRegDataaccessReg200 ps200 ps200 ps200 ps200 ps200 ps200 psALURegALURegALUALUALURegRegRegFIGURE 4.27 Single-cycle, nonpipelined execution in top versus pipelined execution in 
bottom. Both use the same hardware components, whose time is listed in 
Figure 4.26
. In this case, we see 
a fourfold speed-up on average time between instructions, from 800 ps down to 200 ps. Compare t
 gure 
to 
Figure 4.25
. For the laundry, we assumed all stages were equal. If the dryer were slowest, then the dryer 

stage would set the stage time
 e pipeline stage times of a computer are also limited by the slowest resource, 
either the ALU operation or the memory access. We assume the write to the regist
 le occurs in th
 rst 
half of the clock cycle and the read from the regist
 le occurs in the second half. We use this assumption 
throughout this chapter.

 4.5 An Overview of Pipelining 
277Pipelining improves performance by 
increasing instruction throughput, as 
opposed to decreasing the execution time of an individual instruction
, but instruction 
throughput is the important metric because real programs execute billions of 
instructions.
Designing Instruction Sets for Pipelining
Even with this simple explanation of pipelining, we can get insight into the design 

of the MIPS instruction set, which was designed for pipelined execution.
First, all MIPS instructions are the same lengt
 is restriction makes it much 
easier to fetch instructions in th
 rst pipeline stage and to decode them in the 
second stage. In an instruction set like the x86, where instructions vary from 1 byte 

to 15 bytes, pipelining is considerably more challenging. Recent implementations 

of the x86 architecture actually translate x86 instructions into simple operations 

that look like MIPS instructions and then pipeline the simple operations rather 

than the native x86 instructions! (See Section 4.10.)
Second, MIPS has only a few instruction formats, with the source regist
 elds being located in the same place in each instructio
 is symmetry means that the 
second stage can begin reading the regist
 le at the same time that the hardware 
is determining what type of instruction was fetched. If MIPS instruction formats 

were not symmetric, we would need to sp
lit stage 2, resulting in six pipeline stages. 
We will shortly see the downside of longer pipelines.
 ird, memory operands only appear in loads or stor
 is restriction 
means we can use the execute stage to calculate the memory address and then 

access memory in the following stage. If we could operate on the operands in 

memory, as in the x86, stages 3 and 4 would expand to an address stage, memory 

stage, and then execute stage.
Fourth, as discussed in Chapter 2, operands must be aligned in memory. Hence, 
we need not worry about a single data transfer instruction requiring two data 

memory accesses; the requested data can be transferred between processor and 

memory in a single pipeline stage.
Pipeline Hazards ere are situations in pipelining when the next instruction cannot execute in the 
following clock cycle.
 ese events are called 
hazards
, and there are thre
 erent 
types.
Hazards e 
 rst hazard is called a 
structural hazard
. It means that the hardware cannot 
support the combination of instructions that we want to execute in the same clock 
cycle. A structural hazard in the laundry room would occur if we used a washer-

dryer combination instead of a separate washer and dryer, or if our roommate was 

busy doing something else and wouldn’t put clothes away. Our carefully scheduled 

pipeline plans would then be foiled.
structural hazard
 When 
a planned instruction 
cannot execute in the 

proper clock cycle because 

the hardware does not 

support the combination 

of instructions that are set 

to execute.

278 Chapter 4 The Processor
As we said above, the MIPS instruction set was designed to be pipelined, 
making it fairly easy for designers to avoid structural hazards when designing a 
pipeline. Suppose, however, that we had a single memory instead of two memories. 

If the pipeline in 
Figure 4.27
 had a fourth instruction, we would see that in the 

same clock cycle th
 rst instruction is accessing data from memory while the 
fourth instruction is fetching an instruction from that same memory. Without two 

memories, our pipeline could have a structural hazard.
Data HazardsData hazards
 occur when the pipeline must be stalled because one step must wait 
for another to complete. Suppose you found a sock at the folding station for which 
no match existed. One possible strategy is to run down to your room and search 

through your clothes bureau to see if you ca
 nd the match. Obviously, while you 
are doing the search, loads must wait that have completed drying and are ready to 

fold as well as those that hav
 nished washing and are ready to dry.
In a computer pipeline, data hazards arise from the dependence of one 
instruction on an earlier one that is still in the pipeline (a relationship that does not 

really exist when doing laundry). For example, suppose we have an add instruction 

followed immediately by a subtract instruction that uses the sum (
$s0):add   $s0, $t0, $t1sub   $t2, $s0, $t3Without intervention, a data hazard could severely stall the pipeline
 e add 
instruction doesn’t write its result until th
 h stage, meaning that we would have 
to waste three clock cycles in the pipeline.
Although we could try to rely on compilers to remove all such hazards, the 
results would not be satisfactory
 ese dependences happen just too o
 en and the 
delay is just too long to expect the compiler to rescue us from this dilemma.
 e primary solution is based on the observation that we don’t need to wait for 
the instruction to complete before trying to resolve the data hazard. For the code 
sequence above, as soon as the ALU creates the sum for the add, we can supply it as 

an input for the subtract. Adding extra hardware to retrieve the missing item early 

from the internal resources is called 
forwarding
 or 
bypassing
.Forwarding with Two Instructions
For the two instructions above, show what pipeline stages would be connected 

by forwarding. Use the drawing in 
Figure 4.28
 to represent the datapath during 

th
 ve stages of the pipeline. Align a copy of the datapath for each instruction, 
similar to the laundry pipeline in 
Figure 4.25
.data hazard
 Also 
called a pipeline data 
hazard. When a planned 
instruction cannot 
execute in the proper 
clock cycle because data 

that is needed to execute 

the instruction is not yet 

available.
forwarding
 Also called
 bypassing
. A method of 
resolving a data hazard 

by retrieving the missing 

data element from 

internal bu
 ers rather 
than waiting for it to 

arrive from programmer-

visible registers or 

memory.
EXAMPLE
 4.5 An Overview of Pipelining 
279Figure 4.29
 shows the connection to forward the value in 
$s0 er the 
execution stage of the add instruction as input to the execution stage of the 
sub instruction.
In this graphical representation of events, forwarding paths are valid only if the 
destination stage is later in time than the source stage. For example, there cannot 

be a valid forwarding path from the output of the memory access stage in th
 rst 
instruction to the input of the execution stage of the following, since that would 

mean going backward in time.
Forwarding works very well and is described in detail in Section 4.7. It cannot 
prevent all pipeline stalls, however. For example, suppose th
 rst instruction was a 
load of 
$s0 instead of an add. As we can imagine from looking at 
Figure 4.29
, the 
ANSWERTimeadd $s0, $t0, $t1IFMEMIDWBEX2004006008001000
FIGURE 4.28 Graphical representation of the instruction pipeline, similar in spirit to 
the laundry pipeline in 
Figure 4.25
. Here we use symbols representing the physical resources with 
the abbreviations for pipeline stages used throughout the chapter.
 e symbols for th
 ve stages: 
IF for 
the instruction fetch stage, with the box representing instruction memory; 
ID for the instruction decode/
regist
 le read stage, with the drawing showing the regist
 le being read; 
EX for the execution stage, 
with the drawing representing the ALU; 
MEM for the memory access stage, with the box representing data 
memory; and 
WB for the write-back stage, with the drawing showing the regist
 le being writt
 e shading indicates the element is used by the instruction. Hence, MEM has a white background because 
add does not access the data memory. Shading on the right half of the regist
 le or memory means the element 
is read in that stage, and shading of th
  half means it is written in that stage. Hence the right half of ID is 
shaded in the second stage because the regist
 le is read, and th
  half of WB is shaded in th
 h stage 
because the regist
 le is written.
Timeadd $s0, $t0, $t1sub $t2, $s0, $t3 IFMEMIDWBEXIFMEMIDWBEXProgram
execution

order
(in instructions)
2004006008001000
FIGURE 4.29 Graphical representation of forwarding. 
 e connection shows the forwarding path 
from the output of the EX stage of 
add to the input of the EX stage for 
sub, replacing the value from register 
$s0 read in the second stage of 
sub.
280 Chapter 4 The Processor
desired data would be available only 
 er the fourth stage of th
 rst instruction 
in the dependence, which is too late for the 
input of the third stage of 
sub. Hence, 
even with forwarding, we would have to stall one stage for a 
load-use data hazard
, as Figure 4.30 sho
 is 
 gure shows an important pipeline concept, o
  cially 
called a pipeline stall
, but o
 en given the nickname 
bubble
. We shall see stalls 
elsewhere in the pipeline. Section 4.7 shows how we can handle hard cases like 
these, using either hardware detection and stalls or so
 ware that reorders code to 
try to avoid load-use pipeline stalls, as this example illustrates.
Reordering Code to Avoid Pipeline Stalls
Consider the following code segment in C:
a = b + e;c = b + f;Here is the generated MIPS code for this segment, assuming all variables are in 
memory and are addressable as o
 sets from 
$t0:lw    $t1, 0($t0)lw    $t2, 4($t0)
add   $t3, $t1,$t2
sw    $t3, 12($t0)
lw    $t4, 8($t0)
add   $t5, $t1,$t4
sw    $t5, 16($t0)load-use data hazard
 A sp
 c form of data 
hazard in which the data 
being loaded by a load 

instruction has not yet 

become available when 

it is needed by another 

instruction.
pipeline stall
 Also called
 bubble
. A stall initiated 
in order to resolve a 

hazard.
EXAMPLE200400600800100012001400
Timelw $s0, 20($t1)sub $t2, $s0, $t3 IFMEMIDWBEXIFMEMIDWBEXProgramexecution
order
(in instructions)bubblebubblebubblebubblebubbleFIGURE 4.30 We need a stall even with forwarding when an R-format instruction following 
a load tries to use the data. Without the stall, the path from memory access stage output to execution 
stage input would be going backward in time, which is impossible
 is 
 gure is actually a simp
 cation, 
since we cannot know until a
 er the subtract instruction is fetched and decoded whether or not a stall will be 
necessary. Section 4.7 shows the details of what really happens in the case of a hazard.

 4.5 An Overview of Pipelining 
281Find the hazards in the preceding code segment and reorder the instructions 
to avoid any pipeline stalls.
Both 
add instructions have a hazard because of their respective dependence 
on the immediately preceding 
lw instruction. Notice that bypassing eliminates 
several other potential hazards, including the dependence of th
 rst 
add on 
th
 rst 
lw and any hazards for store instructions. Moving up the third 
lw instruction to become the third instruction eliminates both hazards:
lw   $t1, 0($t0)lw   $t2, 4($t0)lw   $t4, 8($t0)add  $t3, $t1,$t2sw   $t3, 12($t0)
add  $t5, $t1,$t4
sw   $t5, 16($t0)On a pipelined processor with forwar
ding, the reordered sequence will 
complete in two fewer cycles than the original version.
Forwarding yields another insight into the MIPS architecture, in addition to the 
four mentioned on page 277. Each MIPS instruction writes at most one result and 
does this in the last stage of the pipeline. Forwarding is harder if there are multiple 

results to forward per instruction or if there is a need to write a result early on in 

instruction execution.
Elaboration: The name “forwarding” comes from the idea that the result is passed 
forward from an earlier instruction to a later instruction. “Bypassing” comes from 
 le to the desired unit.
Control Hazards e third type of hazard is called a 
control hazard
, arising from the need to make a 
decision based on the results of one instruction while others are executing.
Suppose our laundry crew was given the happy task of cleaning the uniforms 
of a football team. Given ho
 lthy the laundry is, we need to determine whether 
the detergent and water temperature setting we select is strong enough to get the 
uniforms clean but not so strong that the uniforms wear out sooner. In our laundry 

pipeline, we have to wait until a
 er the second stage to examine the dry uniform to 
see if we need to change the washer setup or not. What to do?
Here is th
 rst of two solutions to control hazards in the laundry room and its 
computer equivalent.
Stall:
 Just operate sequentially until th
 rst batch is dry and then repeat until 
you have the right formula.
 is conservative option certainly works, but it is slow.
ANSWERcontrol hazard
 Also 
called branch hazard
. When the proper 
instruction cannot 

execute in the proper 

pipeline clock cycle 

because the instruction 

that was fetched is not the 

one that is needed; that 

is, th
 ow of instruction 
addresses is not what the 

pipeline expected.

282 Chapter 4 The Processor
 e equivalent decision task in a computer is the branch instruction. Notice that 
we must begin fetching the instruction following the branch on the very next clock 
cycle. Nevertheless, the pipeline cannot po
ssibly know what the next instruction 
should be, since it 
only just received
 the branch instruction from memory! Just as 
with laundry, one possible solution is to stall immediately a er we fetch a branch, 

waiting until the pipeline determines the outcome of the branch and knows what 

instruction address to fetch from.
Let’s assume that we put in enough extra hardware so that we can test registers, 
calculate the branch address, and update the PC during the second stage of the 

pipeline (see Section 4.8 for details). Even with this extra hardware, the pipeline 

involving conditional branches would look like 
Figure 4.31
 e lw instruction, 
executed if the branch fails, is stalled one extra 200 ps clock cycle before starting.
Performance of “Stall on Branch”
Estimate the impact on the 
clock cycles per instruction
 (CPI) of stalling on 
branches. Assume all other instructions have a CPI of 1.
Figure 3.27 in Chapter 3 shows that branches are 17% of the instructions 
executed in SPECint2006. Since the other instructions run have a CPI of 1, 

and branches took one extra clock cycle for the stall, then we would see a CPI 

of 1.17 and hence a slowdown of 1.17 versus the ideal case.
EXAMPLEANSWERadd $4, $5, $6beq $1, $2, 40or $7, $8, $9TimeInstructionfetchDataaccessDataaccessDataaccessRegInstructionfetchInstructionfetchRegReg200 ps400 psbubblebubblebubblebubblebubble200400600800100012001400
Programexecution
order
(in instructions)RegALURegALU
RegALU
FIGURE 4.31 Pipeline showing stalling on every conditional branch as solution to control 
hazards.  is example assumes the conditional branch is taken, and the instruction at the destination of 
the branch is the 
OR instructio
 ere is a one-stage pipeline stall, or bubble, a
 er the branch. In reality, the 
process of creating a stall is slightly more complicated, as we will see in Sectio
 e 
 ect on performance, 
however, is the same as would occur if a bubble were inserted.

 4.5 An Overview of Pipelining 
283If we cannot resolve the branch in the second stage, as is o
 en the case for longer 
pipelines, then we’d see an even larger slowdown if we stall on branch
 e cost of 
this option is too high for most computers to use and motivates a second solution 
to the control hazard using one of our great ideas from Chapter 1:
Predict:
 If you’re pretty sure you have the right formula to wash uniforms, then 
just 
predict
 that it will work and wash the second load while waiting for th
 rst 
load to dry.
 is option does not slow down the pipeline when you are correct. When you are 
wrong, however, you need to redo the load that was washed while guessing the 

decision.
Computers do indeed use 
prediction
 to handle branches. One simple approach 
is to predict always that branches will be untaken. When you’re right, the pipeline 

proceeds at full speed. Only when branches are taken does the pipeline stall. 
Figure
 
4.32 shows such an example.
add $4, $5, $6beq $1, $2, 40
lw  $3, 300($0) TimeInstructionfetchInstructionfetchDataaccessRegInstructionfetchDataaccessDataaccessRegRegRegALU
RegALU
RegALURegALU
RegALU
RegALU
200 ps200 psadd $4, $5, $6
beq $1, $2, 40or $7, $8, $9TimeInstructionfetchDataaccessRegInstructionfetchInstructionfetchDataaccessRegDataaccessReg200 ps400 psbubblebubblebubblebubblebubble200400600800100012001400
Programexecution
order
(in instructions)200400600800100012001400
Program
execution
order
(in instructions)FIGURE 4.32 Predicting that branches are not taken as a solution to control hazard.  e top drawing shows the pipeline when the branch is not tak e bottom drawing shows the pipeline when 
the branch is taken. As we noted in 
Figure 4.31
, the insertion of a bubble in this fashion simp
 es what 
actually happens, at least during th
 rst clock cycle immediately following the branch. Section 4.8 will reveal 
the details.

284 Chapter 4 The Processor
A more sophisticated version of 
branch prediction
 would have some branches 
predicted as taken and some as untaken. In our analogy, the dark or home uniforms 
might take one formula while the light or road uniforms might take another. In the 
case of programming, at the bottom of loops are branches that jump back to the top 

of the loop. Since they are likely to be taken and they branch backward, we could 

always predict taken for branches that jump to an earlier address.
Such rigid approaches to branch prediction rely on stereotypical behavior 
and don’t account for the individuality of a sp c branch instruction. 
Dynamic
 hardware predictors, in stark contrast, make their guesses depending on the 

behavior of each branch and may change predictions for a branch over the life of 

a program. Following our analogy, in dynamic prediction a person would look at 

how dirty the uniform was and guess at the formula, adjusting the next 
prediction 
depending on the success of recent guesses.
One popular approach to dynamic prediction of branches is keeping a history 
for each branch as taken or untaken, and then using the recent past behavior 

to predict the future. As we will see later, the amount and type of history kept 

have become extensive, with the result being that dynamic branch predictors can 

correctly predict branches with more than 90% accuracy (see Section 4.8). When 

the guess is wrong, the pipeline control must ensure that the instructions following 

the wrongly guessed branch hav
 ect and must restart the pipeline from the 
proper branch address. In our laundry analogy, we must stop taking new loads so 

that we can restart the load that we incorrectly predicted.
As in the case of all other solutions to control hazards, longer pipelines exacerbate 
the problem, in this case by raising the cost of misprediction. Solutions to control 

hazards are described in more detail in Section 4.8.
Elaboration: There is a third approach to the control hazard, called 
delayed decision
. In our analogy, whenever you are going to make such a decision about laundry, just place 
a load of nonfootball clothes in the washer while waiting for football uniforms to dry. As 

long as you have enough dirty clothes that are not affected by the test, this solution 

wor ne.
Called the delayed branch
 in computers, and mentioned above, this is the solution 
actually used by the MIPS architecture. The delayed branch always executes the next 

sequential instruction, with the branch taking place 
after that one instruction delay. 

It is hidden from the MIPS assembly language programmer because the assembler 

can automatically arrange the instructions to get the branch behavior desired by the 

programmer. MIPS software will place an instruction immediately after the delayed 

branch instruction that is not affected by the branch, and a taken branch changes 

the address of the instruction that 
follows
 this safe instruction. In our example, the 

add instruction before the branch in 
Figure 4.31
 does not affect the branch and can 

be moved after the branch to fully hide the branch delay. Since delayed branches are 

useful when the branches are short, no processor uses a delayed branch of more 

than one cycle. For longer branch delays, hardware-based branch prediction is usually 

used.branch prediction
 A method of resolving 
a branch hazard that 

assumes a given outcome 

for the branch and 

proceeds from that 

assumption rather than 

waiting to ascertain the 

actual outcome.

 4.5 An Overview of Pipelining 
285Pipeline Overview Summary
Pipelining is a technique that exploits 
parallelism
 among the instructions in 
a sequential instruction stream. It has 
the substantial advantage that, unlike 
programming a multiprocessor, it is fundamentally invisible to the programmer.
In the next few sections of this chapter, we cover the concept of pipelining using 
the MIPS instruction subset from the single-cycle implementation in Section 4.4 
and show a simp
 ed version of its pipeline. We then look at the problems that 
pipelining
 introduces and the performance attainable under typical situations.
If you wish to focus more on the so
 ware and the performance implications of 
pipelining, you now have
  cient background to skip to Section 4.10. Section 
4.10 introduces advanced pipelining concepts, such as superscalar and dynamic 

scheduling, and Section 4.11 examines the pipelines of recent microprocessors.
Alternatively, if you are interested in understanding how pipelining is 
implemented and the challenges of dealing with hazards, you can proceed to 

examine the design of a pipelined datapath
 and the basic control, explained in 

Section 4.6. You can then use this understanding to explore the implementation of 

forwarding and stalls in Section 4.7. You can then read Section 4.8 to learn more 

about solutions to branch hazards, and then see how exceptions are handled in 

Section 4.9.
For each code sequence below, state whether it must stall, can avoid stalls using 
only forwarding, or can execute without stalling or forwarding.
Sequence 1Sequence 2Sequence 3lw   $t0,0($t0)add   $t1,$t0,$t0addi  $t1,$t0,#1add  $t1,$t0,$t0addi  $t2,$t0,#5addi  $t2,$t0,#2
addi  $t4,$t1,#5addi  $t3,$t0,#2
addi  $t3,$t0,#4addi  $t5,$t0,#5Outside the memory system, th
 ective operation of the pipeline is usually 
the most important factor in determining the CPI of the processor and hence its 
performance. As we will see in Section 4.10, understanding the performance of a 

modern multiple-issue pipelined processo
r is complex and requires understanding 
more than just the issues that arise in 
a simple pipelined processor. Nonetheless, 
structural, data, and control hazards remain important in both simple pipelines 

and more sophisticated ones.
For modern pipelines, structural hazards usually revolve around th
 oating-
point unit, which may not be fully pipelined, while control hazards are usually more 

of a problem in integer programs, which tend to have higher branch frequencies 

as well as less predictable branches. Data hazards can be performance bottlenecks 
Check Yourself
Understanding 
Program 

Performance

286 Chapter 4 The Processor
in both integer an
 oating-point program
 en it is easier to deal with data 
hazard
 oating-point programs because the lower branch frequency and more 
regular memory access patterns allow the compiler to try to schedule instructions 
to avoid hazards. It is more
  cult to perform such optimizations in integer 
programs that have less regular memory access, involving more use of pointers. 

As we will see in Section 4.10, there are more ambitious compiler and hardware 

techniques for reducing data dependences through scheduling.
Pipelining
 increases the number of simultaneously executing instructions 

and the rate at which instructions are started and completed. Pipelining 

does not reduce the time it takes to complete an individual instruction, 

also called the 
latency
. For example, th
 ve-stage pipeline still takes 5 
clock cycles for the instruction to complete. In the terms used in Chapter 

1, pipelining improves instruction 
throughput
 rather than individual 
instruction 
execution time
 or 
latency
. The BIGPicturelatency (pipeline)
 e number of stages in a 
pipeline or the number 

of stages between two 

instructions during 

execution.
Instruction sets can either simplify or make life harder for pipeline 
designers, who must already cope with structural, control, and data hazards. 
Branch 
prediction
 and forwarding help make a computer fast while still getting 
the right answers.
 4.6 Pipelined Datapath and Control
Figure 4.33
 shows the single-cycle datapath from Section 4.4 with the pipeline 

stages iden
 ed
 e division of an instruction int
 ve stages mean
 ve-stage 
pipeline, which in turn means that up to
 ve instructions will be in execution 
during any single clock cycle
 us, we must separate the datapath into
 ve pieces, 
with each piece named corresponding to a stage of instruction execution:
1. IF: Instruction fetch
2. ID: Instruction decode and regist
 le read
3. EX: Execution or address calculation

4. MEM: Data memory access

5. WB: Write back

In 
Figure 4.33
, thes
 ve components correspond roughly to the way the data-
path is drawn; instructions and data move generally from
  to right through the 
 ere is less in this 
than meets the eye.
Tallulah 
Bankhead, remark 

to Alexander 

Woollcott, 1922

 4.6 Pipelined Datapath and Control 
287 ve stages as they complete execution. Returning to our laundry analogy, clothes 
get cleaner, drier, and more organized as they move through the line, and they 
never move backward.
 ere are, however, two exceptions to t
 -to-right 
 ow of instructions:
 e write-back stage, which places the result back into the regist
 le in the 
middle of the datapath
 e selection of the next value of the PC, choosing between the incremented 
PC and the branch address from the MEM stage
Data
 owing from right t
  does not a
 ect the current instruction; these 
reverse data movemen uence only later instructions in the pipeline. Note that 
WB: Write backMEM: Memory accessIF: Instruction fetchEX: Execute/address calculation1Mux00Mux1AddressWritedataReaddataDatamemoryRead
register 1Readregister 2Writeregister Write
dataRegistersReaddata 1Readdata 2ALUZeroALUresultADDAddresultShift
left 2AddressInstructionInstructionmemoryAdd4PCSign-extend0Mux132ID: Instruction decode/register file read16FIGURE 4.33 The single-cycle datapath from Section 4.4 (similar to Figure 4.17
). Each step of the instruction can be mapped 
onto the datapath from le
  to righ
 e only exceptions are the update of the PC and the write-back step, shown in color, which sends either 
the ALU result or the data from memory to th
  to be written into the regist
 le. (Normally we use color lines for control, but these are 
data lines.)

288 Chapter 4 The Processor
th
 rst right-t
  
 ow of data can lead to data hazards and the second leads to 
control hazards.
One way to show what happens in pipelined execution is to pretend that each 
instruction has its own datapath, and then to place these datapaths on a timeline to 
show their relationship. 
Figure 4.34
 shows the execution of the instructions in 
Figure
 

4.27 by displaying their private datapaths on a common timeline. We use a stylized 

version of the datapath in 
Figure 4.33 
to show the relationships in 
Figure 4.34.
Figure 4.34
 seems to suggest that three instructions need three datapaths. 
Instead, we add registers to hold data so that portions of a single datapath can be 

shared during instruction execution.
For example, as 
Figure 4.34
 shows, the instruction memory is used during 
only one of the
 ve stages of an instruction, allowing it to be shared by following 
instructions during the other four stages. To retain the value of an individual 

instruction for its other four stages, the value read from instruction memory must 

be saved in a register. Similar arguments apply to every pipeline stage, so we must 

place registers wherever there are dividing lines between stages in 
Figure 4.33
. 
Returning to our laundry analogy, we might have a basket between each pair of 

stages to hold the clothes for the next step.
Programexecution
order
(in instructions)lw $1, 100($0)lw $2, 200($0)lw $3, 300($0)Time (in clock cycles)IMDMRegRegALUIMDMRegRegALUIMDMRegRegALUCC 1CC 2CC 3CC 4CC 5CC 6CC 7
FIGURE 4.34 Instructions being executed using the single-cycle datapath in 
Figure 4.33
, 
assuming pipelined execution. Similar to 
Figures 4.28 through 4.30
, t
 gure pretends that each 
instruction has its own datapath, and shades each portion according to use. Unlike thos
 gures, each stage 
is labeled by the physical resource used in that stage, corresponding to the portions of the datapath in 
Figure
 4.33. IM represents the instruction memory and the PC in the instruction fetch stage, 
Reg
 stands for the 
regist
 le and sign extender in the instruction decode/regist
 le read stage (ID), and so on. To maintain 
proper time order, this stylized datapath breaks the regist
 le into two logical parts: registers read during 
register fetch (ID) and registers written during write bac
 is dual use is represented by drawing 
the un
  half of the regist
 le using dashed lines in the ID stage, when it is not being written, and 
the unshaded right half in dashed lines in the WB stage, when it is not being read. As before, we assume the 

regist
 le is written in th
 rst half of the clock cycle and the regist
 le is read during the second half.

 4.6 Pipelined Datapath and Control 
289Figure 4.35
 shows the pipelined datapath with the pipeline registers high-
lighted. All instructions advance during each clock cycle from one pipeline register 
to th e registers are named for the two stages separated by that register. 

For example, the pipeline register between the IF and ID stages is called IF/ID.
Notice that there is no pipeline register at the end of the write-back stage. All 
instructions must update some state in the processor—the regist
 le, memory, or 
the PC—so a separate pipeline register is redundant to the state that is updated. For 

example, a load instruction will place its result in 1 of the 32 registers, and any later 

instruction that needs that data will simply read the appropriate register.
Of course, every instruction updates the PC, whether by incrementing it or by 
setting it to a branch destination addr
 e PC can be thought of as a pipeline 
register: one that feeds the IF stage of 
the pipeline. Unlike the shaded pipeline 
registers in 
Figure 4.35
, however, the PC is part of the visible architectural state; 

its contents must be saved when an exception occurs, while the contents of the 

pipeline registers can be discarded. In th
e laundry analogy, you could think of the 
PC as corresponding to the basket that holds the load of dirty clothes before the 

wash step.
To show how the pipelining works, throughout this chapter we show sequences 
of
 gures to demonstrate operation over time
 ese extra pages would seem to 
require much more time for you to understand. Fear not; the sequences take much 
AddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.35 The pipelined version of the datapath in
 Figure 4.33
.  e pipeline registers, in color, separate each pipeline stage. 
 ey are labeled by the stages that they separate; for example, th
 rst is labeled 
IF/ID because it separates the instruction fetch and instruction 
decode stag
 e registers must be wide enough to store all the data corresponding to the lines that go through them. For example, the 
IF/ID register must be 64 bits wide, because it must hold both the 32-bit instruction fetched from memory and the incremented 3
2-bit PC 
address. We will expand these registers over the course of this 
chapter, but for now the other three pipeline registers contain
 128, 97, and 64 
bits, respectively.

290 Chapter 4 The Processor
less time than it might appear, because you can compare them to see what changes 
occur in each clock cycle. Section 4.7 describes what happens when there are data 

hazards between pipelined instructions; ignore them for now.
Figures 4.36 through 4.38
, o
 rst sequence, show the active portions of the 
datapath highlighted as a load instruction goes through th
 ve stages of pipelined 
execution. We sho
 rst because it is active
 ve stages. As in 
Figures
 4.28 through 4.30
, we highlight the 
right half
 of registers or memory when they are 
being 
read
 and highlight the 
  half 
when they are being 
written
.We show the instruction abbreviation 
lw with the name of the pipe stage that is 
active in each
 gure. 
 e 
 ve stages are the following:
1. Instruction fetch:
 e top portion of 
Figure 4.36
 shows the instruction being 
read from memory using the address in the PC and then being placed in the 

IF/ID pipeline register
 e PC address is incremented by 4 and then written 
back into the PC to be ready for the next clock cycle
 is incremented 
address is also saved in the IF/ID pipeline 
register in case it is needed later 
for an instruction, such as 
beq e computer cannot know which type of 
instruction is being fetched, so it must prepare for any instruction, passing 

potentially needed information down the pipeline.
2. Instruction decode and regist
 le read:
 e bottom portion of 
Figure 4.36
 shows the instruction portion of the IF/ID pipeline register supplying the 

16-bit immediate
 eld, which is sign-extended to 32 bits, and the register 
numbers to read the two registers. All three values are stored in the ID/EX 

pipeline register, along with the in
cremented PC address. We again transfer 
everything that might be needed by any instruction during a later clock 

cycle.
3. Execute or address calculation:
 Figure 4.37
 shows that the load instruction 
reads the contents of register 1 and the sign-extended immediate from the 

ID/EX pipeline register and adds them using the ALU.
 at sum is placed in 
the EX/MEM pipeline register.
4. Memory access: 
 e top portion of 
Figure 4.38
 shows the load instruction 
reading the data memory using the address from the EX/MEM pipeline 

register and loading the data into the MEM/WB pipeline register.
5. Write-back:
 e bottom portion of 
Figure 4.38
 shows th nal step: reading 
the data from the MEM/WB pipeline register and writing it into the register 

 le in the middle of th
 gure.
 is walk-through of the load instruction shows that any information needed 
in a later pipe stage must be passed to that stage via a pipeline register. Walking 

through a store instruction shows the similarity of instruction execution, as well 

as passing the information for later stages. Here are th
 ve pipe stages of the store 
instruction:

 4.6 Pipelined Datapath and Control 
291Instruction decodelwInstruction fetchlwAddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux10Mux1MEM/WBAddAddressInstructionmemoryRead
register 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.36 IF and ID: First and second pipe stages of an instruction, with the active portions of the datapath in 
Figure 4.35
 highlighted.  e highlighting convention is the same as that used in 
Figure 4.28
. As in Section 4.2, there is no confusion when 
reading and writing registers, because the contents change only on the clock edge. Although the load needs only the top registe
r in stage 2, 
the processor doesn’t know what instruction is being decoded, 
so it sign-extends the 16-bit constant and reads both registers i
nto the ID/EX 
pipeline register. We don’t need all three operands, but it simp
 es control to keep all three.

292 Chapter 4 The Processor
1. Instruction fetch:
 e instruction is read from memory using the address 
in the PC and then is placed in the IF/ID pipeline register
 is stage occurs 
before the instruction is iden
 ed, so the top portion of 
Figure 4.36
 works 
for store as well as load.
2. Instruction decode and regist
 le read:
 e instruction in the IF/ID pipeline 
register supplies the register numbers for reading two registers and extends 
the sign of the 16-bit immediate.
 ese three 32-bit values are all stored 
in the ID/EX pipeline register
 e bottom portion of 
Figure 4.36
 for load 
instructions also shows the operations of the second stage for stor
 ese 
 rst two stages are executed by all instructions, since it is too early to know 
the type of the instruction.
3. Execute and address calculation: 
Figure 4.39
 shows the third step; the 
 ective address is placed in the EX/MEM pipeline register.
4. Memory access: 
 e top portion of 
Figure 4.40
 shows the data being written 
to memory. Note that the register containing the data to be stored was read in 

an earlier stage and stor
 e only way to make the data available 
during the MEM stage is to place the data into the EX/MEM pipeline register 

in the EX stage, just as we stored th
 ective address into EX/MEM.
ExecutionIwAddAddressInstructionmemoryReadregister 1InstructionRead
register 2Write
registerWritedataRead
data 1Readdata 2RegistersAddressWrite
dataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.37 EX: The third pipe stage of a load instruction, highlighting the portions of the datapath in 
Figure 4.35
 used in this pipe stage.  e register is added to the sign-extended immediate, 
and the sum is placed in the EX/MEM pipeline register.

 4.6 Pipelined Datapath and Control 
293MemoryIwWrite-back
IwAddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux10Mux1MEM/WBAddAddressInstructionmemoryRead
register 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.38 MEM and WB: The fourth and ﬁ fth pipe stages of a load instruction, highlighting the portions of the 
datapath in Figure 4.35
 used in this pipe stage. Data memory is read using the address in the EX/MEM pipeline registers, and the 
data is placed in the MEM/WB pipeline register. Next, data is re
ad from the MEM/WB pipeline register and written into the regis
t le in the 
middle of the datapath. Note: there is a bug in this design that is repaired in 
Figure 4.41
.
294 Chapter 4 The Processor
5. Write-back: 
 e bottom portion of 
Figure 4.40
 shows th
 nal step of the 
store. For this instruction, nothing happens in the write-back stage. Since 
every instruction behind the store is already in progress, we have no way 

to accelerate those instructions. Hence, an instruction passes through a 

stage even if there is nothing to do, because later instructions are already 

progressing at the maximum rate.
 e store instruction again illustrates that to pass something from an early pipe 
stage to a later pipe stage, the information must be placed in a pipeline register; 

otherwise, the information is lost when the next instruction enters that pipeline 

stage. For the store instruction we needed to pass one of the registers read in the 

ID stage to the MEM stage, where it is stored in memory
 e dat
 rst placed 
in the ID/EX pipeline register and then pa
ssed to the EX/MEM pipeline register.
Load and store illustrate a second key point: each logical component of the 
datapath—such as instruction memory, register read ports, ALU, data memory, 

and register write port—can be used only within a 
single
 pipeline stage. Otherwise, 
we would have a 
structural hazard
 (see page 277). Hence these components, and 
their control, can be associated with a single pipeline stage.
Now we can uncover a bug in the design of the load instruction. Did you see it? 
Which register is changed in th
 nal stage of the load? More sp
 cally, which 
ExecutionswAddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.39 EX: The third pipe stage of a store instruction.
 Unlike the third stage of the load instruction in 
Figure 4.37,
 the 
second register value is loaded into the EX/MEM pipeline register to 
be used in the next stage. Although it wouldn’t hurt to al
ways write this 
second register into the EX/MEM pipeline register, we write the second register only on a store instruction to make the pipelin
e easier to 
understand.

 4.6 Pipelined Datapath and Control 
295MemoryswWrite-back
swAddAddressInstructionmemoryReadregister 1InstructionReadregister 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShiftleft 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux10Mux1MEM/WBAddAddressInstructionmemoryRead
register 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.40 MEM and WB: The fourth and ﬁ fth pipe stages of a store instruction. 
In the fourth stage, the data is written into 
data memory for the store. Note that the data comes from the EX/MEM pipeline register and that nothing is changed in the MEM/WB
 pipeline 
register. Once the data is written in memory, there is nothin
  for the store instruction to do, so nothing happens in stage 5.

296 Chapter 4 The Processor
instruction supplies the write register numb
 e instruction in the IF/ID pipeline 
register supplies the write register number, yet this instruction occurs considerably 
 er the load instruction!
Hence, we need to preserve the destination register number in the load 
instruction. Just as store passed the register 
contents
 from the ID/EX to the EX/
MEM pipeline registers for use in the MEM stage, load must pass the 
register
 number from the ID/EX through EX/MEM to the MEM/WB pipeline register for 

use in the WB stage. Another way to think about the passing of the register number 

is that to share the pipelined datapath, we need to preserve the instruction read 

during the IF stage, so each pipeline register contains a portion of the instruction 

needed for that stage and later stages.
Figure 4.41
 shows the correct version of the datapath, passing the write register 
numb
 rst to the ID/EX register, then to the EX/MEM register, an
 nally to the 
MEM/WB register
 e register number is used during the WB stage to specify 
the register to be written.
 Figure 4.42
 is a single drawing of the corrected datapath, 

highlighting the hardware us
 ve stages of the load word instruction in 
Figures 4.36 through 4.38
. See Section 4.8 for an explanation of how to make the 

branch instruction work as expected.
Graphically Representing PipelinesPipelining can b
  cult to understand, since many instructions are simultaneously 
executing in a single datapath in every clock cycle. To aid understanding, there are 
AddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.41 The corrected pipelined datapath to handle the load instruction properly. 
 e write register number now 
comes from the MEM/WB pipeline register along with the dat
 e register number is passed from the ID pipe stage until it reaches the MEM/
WB pipeline register, addin
 ve more bits to the last three pipeline register
 is new path is shown in color.

 4.6 Pipelined Datapath and Control 
297two basic styles of pip
 gures: 
multiple-clock-cycle pipeline diagrams
, such as 
Figure 4.34
 on page 288, and 
single-clock-cycle pipeline diagrams
, such as 
Figures
 4.36 through 4.40
 e multiple-clock-cycle diagrams are simpler but do not contain 
all the details. For example, consider the followin
 ve-instruction sequence:
lw     $10, 20($1)sub    $11, $2, $3
add    $12, $3, $4
lw     $13, 24($1)
add    $14, $5, $6Figure 4.43
 shows the multiple-clock-cycle pipeline diagram for these 
instructions. Time advances fro
  to right across the page in these diagrams, 
and instructions advance from the top to the bottom of the page, similar to the 
laundry pipeline in 
Figure 4.25
. A representation of the pipeline stages is placed 

in each portion along the instruction axis, occupying the proper clock cycles. 

 ese stylized datapaths represent th
 ve stages of our pipeline graphically, but 
a rectangle naming each pipe stage works just as well. 
Figure 4.44
 shows the more 

traditional version of the multiple-clock-cycle pipeline diagram. Note that 
Figure
 
4.43 shows the physical resources used at each stage, while 
Figure 4.44
 uses the 

name
 of each stage.
Single-clock-cycle pipeline diagrams show the state of the entire datapath during 
a single clock cycle, and usuall
 ve instructions in the pipeline are iden
 ed by 
labels above their respective pipeline stages. We use this type of
 gure to show the 
details of what is happening within the pipeline during each clock cycle; typically, 
AddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataRead
data 1Readdata 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShift
left 2Sign-extendPC4ID/EXIF/IDEX/MEM16320Mux10Mux11Mux0MEM/WBFIGURE 4.42 The portion of the datapath in 
Figure 4.41
 that is used in all ﬁ ve stages of a load instruction.

298 Chapter 4 The Processor
the drawings appear in groups to show pipeline operation over a sequence of 
clock cycles. We use multiple-clock-cycle diagrams to give overviews of pipelining 

situations. (
 Section 4.13
 gives more illustrations of single-clock diagrams 
if you would like to see more details about 
Figure 4.43
.) A single-clock-cycle 

diagram represents a vertical slice through a set of multiple-clock-cycle diagrams, 

showing the usage of the datapath by each of the instructions in the pipeline at 

the designated clock cycle. For example, 
Figure 4.45
 shows the single-clock-cycle 

diagram corresponding to clock cycle 5 of 
Figures 4.43 and 4.44
. Obviously, the 

single-clock-cycle diagrams have more detail and tak cantly more space 

to show the same number of clock cyc
 e exercises ask you to create such 
diagrams for other code sequences.
A group of students were debating th
  ciency of th
 ve-stage pipeline when 
one student pointed out that not all instructions are active in every stage of the 
pipeline. A
 er deciding to ignore th
 ects of hazards, they made the following 
four statements. Which ones are correct?
Check Yourself
Programexecution
order
(in instructions)lw $10, 20($1)sub $11, $2, $3 add $12, $3, $4lw $13, 24($1)add $14, $5, $6Time (in clock cycles)IMRegRegIMDMRegRegIMRegRegRegRegRegRegALUALUALUALUALUDMDMDMCC 1CC 2CC 3CC 4CC 5CC 6CC 7CC 8CC 9
DMIMIMFIGURE 4.43 Multiple-clock-cycle pipeline diagram of ﬁ ve instructions.
 is style of pipeline representation shows the complete 
execution of instructions in a sing
 gure. Instructions are listed in instruction execution order from top to bottom, and clock cycles move 
fro
  to right. Unlike 
Figure 4.28
, here we show the pipeline registers between each stage. 
Figure 4.44
 shows the traditional way t
o draw 
this diagram.

 4.6 Pipelined Datapath and Control 
299Programexecution
order
(in instructions)lw $10, 20($1)sub $11, $2, $3add $12, $3, $4
lw $13, 24($1)add $14, $5, $6Time (in clock cycles)InstructionfetchInstructiondecodeExecutionDataaccessDataaccessDataaccessDataaccessDataaccessWrite-backCC 9CC 8CC 7CC 6CC 5CC 4CC 3CC 2CC 1InstructionfetchInstructionfetchInstructionfetchInstructionfetchInstructiondecodeInstructiondecodeInstructiondecodeInstructiondecodeExecutionWrite-backExecutionWrite-backExecutionWrite-backExecutionWrite-backFIGURE 4.44 Traditional multiple-clock-cycle pipeline diagram of ﬁ ve instructions in 
Figure 4.43
.AddAddressInstructionmemoryReadregister 1Read
register 2Write
registerWritedataRead
data 1Read
data 2RegistersAddressWritedataReaddataDatamemoryAddAddresultALUALUresultZeroShiftleft 2Sign-extendPC4ID/EXIF/IDEX/MEMMemorysub $11, $2, $3Write-backlw $10, 20($1)Executionadd $12, $3, $4Instruction decodelw $13, 24 ($1)Instruction fetchadd $14, $5, $61632InstructionMEM/WB0Mux10Mux11Mux0FIGURE 4.45 The single-clock-cycle diagram corresponding to clock cycle 5 of the pipeline in 
Figures 4.43 and 4.44
. As you can see, a single-clock-cyc
 gure is a vertical slice through a multiple-clock-cycle diagram.
1. Allowing jumps, branches, and ALU instructions to take fewer stages than 
th ve required by the load instruction will increase pipeline performance 
under all circumstances.

300 Chapter 4 The Processor
2. Trying to allow some instructions to take fewer cycles does not help, since 
the throughput is determined by the clock cycle; the number of pipe stages 
per instruction a ects latency, not throughput.
3. You cannot make ALU instructions take fewer cycles because of the write-
back of the result, but branches and jumps can take fewer cycles, so there is 

some opportunity for improvement.
4. Instead of trying to make instructions take fewer cycles, we should explore 
making the pipeline longer, so that instructions take more cycles, but the 

cycles are shorter
 is could improve performance.
Pipelined ControlJust as we added control to the single-cycle datapath in Section 4.3, we now add 

control to the pipelined datapath. We start with a simple design that views the 

problem through rose-colored glasses.
 e 
 rst step is to label the control lines on the existing datapath. 
Figure 4.46
 shows those lines. We borrow as much as we can from the control for the simple 

datapath in 
Figure 4.17
. In particular, we use the same ALU control logic, branch 

logic, destination-register-number multiplexor, and contro
 ese functions 
ar ned in 
Figures 4.12, 4.16, and 4.18
. We reproduce the key information in 

Figures 4.47 through 4.49
 on a single page to make the following discussion easier 

to follow.
As was the case for the single-cycle implementation, we assume that the PC is 
written on each clock cycle, so there is no separate write signal for the PC. By the 

same argument, there are no separate write signals for the pipeline registers (IF/

ID, ID/EX, EX/MEM, and MEM/WB), since the pipeline registers are also written 

during each clock cycle.
To specify control for the pipeline, we need only set the control values during 
each pipeline stage. Because each control line is associated with a component active 

in only a single pipeline stage, we can divide the control lines int
 ve groups 
according to the pipeline stage.
1. Instruction fetch:
 e control signals to read instruction memory and to 
write the PC are always asserted, so there is nothing special to control in this 

pipeline stage.
2. Instruction decode/regist
 le read: 
As in the previous stage, the same thing 
happens at every clock cycle, so there are no optional control lines to set.
3. Execution/address calculation: 
 e signals to be set are RegDst, ALUOp, 
and ALUSrc (see 
Figures 4.47 and 4.48
 e signals select th
e Result register, 
the ALU operation, and either Read data 2 or a sign-extended immediate 

for the ALU.
In the 6600 Computer, 

perhaps even more 

than in any previous 

computer, the control 

system is the di
 erence.
Ja
 ornton, 
Design 
of a Computer
 e Control Data 6600, 
1970
 4.6 Pipelined Datapath and Control 
301MemWritePCSrcMemtoRegMemReadAddAddressInstructionmemoryReadregister 1InstructionRead
register 2WriteregisterWritedataInstruction(15Œ0)Instruction(20Œ16)Instruction(15Œ11)Readdata 1Read
data 2RegistersAddressWritedataReaddataDatamemoryAddAddresultAddALUresultZeroShiftleft 2Sign-extendPC4ID/EXIF/IDEX/MEM16326ALUcontrolRegDstALUOpALUSrcRegWriteBranchMEM/WB0Mux10Mux10Mux10Mux1FIGURE 4.46 The pipelined datapath of Figure 4.41
 with the control signals identiﬁ ed.  is datapath borrows the control 
logic for PC source, register destination number, and ALU control from Section 4.4. Note that we now need the 6-bit func
 eld (function 
code) of the instruction in the EX stage as input to ALU control, so these bits must also be included in the ID/EX pipeline reg
ister. Recall that 
these 6 bits are also the 6 le
 cant bits of the immediate
 eld in the instruction, so the ID/EX pipeline register can supply them from the 
immediate
 eld since sign extension leaves these bits unchanged.
Instruction 
opcodeALUOp
Instruction 
operationFunction code
Desired 
ALU actionALU control 
inputLW
00load word
XXXXXXadd
0010SW00store word
XXXXXXadd
0010Branch equal01branch equal
XXXXXXsubtract
0110R-type10add
100000add
0010R-type10subtract
100010subtract
0110R-type10AND
100100AND
0000R-type10OR
100101OR
0001R-type10set on less than
101010set on less than
0111FIGURE 4.47 A copy of 
Figure 4.12
. is 
 gure shows how the ALU control bits are set depending on the ALUOp control bits and the 
 erent function codes for the R-type instruction.

302 Chapter 4 The Processor
4. Memory access: 
 e control lines set in this stage are Branch, MemRead, and 
MemWrite
 e branch equal, load, and stor
e instructions set these signals, 
respectively. Recall that PCSrc in 
Figure 4.48
 selects the next sequential 
address unless control asserts Branch and the ALU result was 0.
5. Write-back: 
 e two control lines are MemtoReg, which decides between 
sending the ALU result or the memory value to the regist
 le, and Reg-
Write, which writes the chosen value.
Since pipelining the datapath leaves the meaning of the control lines unchanged, 
we can use the same control values. 
Figure 4.49
 has the same values as in Section 

4.4, but now the nine control lines are grouped by pipeline stage.
Signal nameEffect when deasserted (0) 
Effect when asserted (1)
RegDstThe register destination number for the Write register comes from the rt Þeld (bits 20:16).
The register destination number for the Write register comes 
from the rd Þeld (bits 15:11).RegWriteNone.The register on the Write register input is written with the value 
on the Write data input. ALUSrcThe second ALU operand comes from the second 
register Þle output (Read data 2).The second ALU operand is the sign-extended, lower 16 bits of 

the instruction.
PCSrcThe PC is replaced by the output of the adder that 

computes the value of PC + 4.The PC is replaced by the output of the adder that computes 

the branch target.MemReadNone.
Data memory contents designated by the address input are 

put on the Read data output. MemWriteNone.
Data memory contents designated by the address input are 

replaced by the value on the Write data input.
MemtoRegThe value fed to the register Write data input 
comes from the ALU.The value fed to the register Write data input comes from the 
data memory.
FIGURE 4.48 A copy of 
Figure 4.16
.  e function of each of seven contro
 ned. 
 e ALU control lines (ALUOp) ar
 ned 
in the second column of
 Figure 4.47
. When a 1-bit control to a 2-way multiplexor is asserted, the multiplexor selects the input
 corresponding 
to 1. Otherwise, if the control is deasserted, the multiplexor selects the 0 input. Note that PCSrc is controlled by an AND gat
e in Figure 4.46
. If the Branch signal and the ALU Zero signal are both set, then 
PCSrc is 1; otherwise, it is 0. Control sets the Branch signal 
only during a beq 
instruction; otherwise, PCSrc is set to 0.
Instruction
Execution/address calculation stage 
control lines
Memory access stage 
control lines
Write-back stage 
control lines
RegDstALUOp1ALUOp0ALUSrc Branch
 Mem- ReadMem- Write
Reg- Write
Memto- RegR-format1
10000010lw000101011swX0010010X
beqX0101000X
FIGURE 4.49 The values of the control lines are the same as in Figure 4.18
, but they have been shufﬂ ed into three 
groups corresponding to the last three pipeline stages.

 4.7 Data Hazards: Forwarding versus Stalling 
303Implementing control means setting the nine control lines to these values in 
each stage for each instructio
 e simplest way to do this is to extend the pipeline 
registers to include control information.
Since the control lines start with the EX stage, we can create the control 
information during instruction decode. 
Figure 4.50
 above shows that these control 
signals are then used in the appropriate pipeline stage as the instruction moves 

down the pipeline, just as the destination register number for loads moves down 

the pipeline in 
Figure 4.41
. Figure 4.51
 shows the full datapath with the extended 

pipeline registers and with the control lines connected to the proper stage. 

( Section 4.13
 gives more examples of MIPS code executing on pipelined 
hardware using single-clock diagrams, if you would like to see more details.)
 4.7 Data Hazards: Forwarding versus Stalling
 e examples in the previous section show the power of pipelined execution and 
how the hardware performs the task. It’s now time to take o
  the rose-colored 
glasses and look at what happens with real program
 e instructions in 
Figures
 4.43 through 4.45
 were independent; none of them used the results calculated 

by any of the others. Yet in Section 4.5, we saw that data hazards are obstacles to 

pipelined execution.
WBMEXWBMWBControlIF/IDID/EXEX/MEMMEM/WBInstructionFIGURE 4.50 The control lines for the ﬁ nal three stages. 
Note that four of the nine control lines 
are used in the EX phase, with the remainin
 ve control lines passed on to the EX/MEM pipeline register 
extended to hold the control lines; three are used during the MEM stage, and the last two are passed to MEM/
WB for use in the WB stage.
What do you mean, 
why’s it got to be built? 

It’s a bypass. You’ve got 

to build bypasses.
Douglas Adams, 
 e Hitchhiker’s Guide to the 
Galaxy,
 1979
304 Chapter 4 The Processor
Let’s look at a sequence with many dependences, shown in color:
sub   $2, $1,$3      # Register $2 written by suband   $12,$2,$5      # 1st operand($2) depends on subor    $13,$6,$2      # 2nd operand($2) depends on subadd   $14,$2,$2      # 1st($2) & 2nd($2) depend on subsw    $15,100($2)    # Base ($2) depends on sub e last four instructions are all dependent on the result in register 
$2 of the 
 rst instruction. If register 
$2 had the value 10 before the subtract instruction and 
−20 a
 erwards, the programmer intends that −20 will be used in the following 
instructions that refer to register 
$2.WBMEXWBMWBMemWritePCSrcMemtoRegMemReadAddAddressInstructionmemoryReadregister 1Readregister 2Instruction[15Œ0]Instruction[20Œ16]Instruction[15Œ11]WriteregisterWritedataReaddata 1Readdata 2RegistersAddressWritedataRead
dataDatamemoryAddAddresultALUALUresultZeroShiftleft 2Sign-extendPC4ID/EXIF/IDEX/MEMMEM/WB16632ALUcontrolRegDstALUOpALUSrcRegWriteInstructionBranchControl0Mux10MuxMuxMux11001FIGURE 4.51 The pipelined datapath of Figure 4.46
, with the control signals connected to the control portions of 
the pipeline registers. 
 e control values for the last three stages are created 
during the instruction decode stage and then placed in the 
ID/EX pipeline register e control lines for each pipe stage are used, and remaining control lines are then passed to the next pipeline stage.

 4.7 Data Hazards: Forwarding versus Stalling 
305How would this sequence perform with our pipeline? 
Figure 4.52
 illustrates the 
execution of these instructions using a multiple-clock-cycle pipeline representation. 
To demonstrate the execution of this instruction sequence in our current pipeline, 

the top of 
Figure 4.52
 shows the value of register 
$2, which changes during the 
middle of clock cycle 5, when the 
sub instruction writes its result.
 e last potential hazard can be resolved by the design of the regist
 le hardware: What happens when a register is read and written in the same clock 

cycle? We assume that the write is in th
 rst half of the clock cycle and the read 
is in the second half, so the read delivers what is written. As is the case for many 

implementations of regist les, we have no data hazard in this case.
Figure 4.52
 shows that the values read for register 
$2 would 
not be the result of 
the sub instruction unless the read occurred during clock cycle 5 or later
 us, the 
instructions that would get the correct value of −20 are 
add and 
sw; the 
AND and 
Programexecution
order
(in instructions)sub $2, $1, $3and $12, $2, $5 or $13, $6, $2add $14, $2,$2 
sw $15, 100($2)Time (in clock cycles)CC 1CC 2CC 3CC 4CC 5CC 6CC 7CC 8CC 9
IMDMRegRegIMDMRegRegIMDMRegRegIMDMRegRegIMDMRegReg10 10 10 10 Value ofregister $2:10/Œ20Œ20Œ20Œ20Œ20
FIGURE 4.52 Pipelined dependences in a ﬁ ve-instruction sequence using simpliﬁ ed datapaths to show the 
dependences. All the dependent actions are shown in color, and “CC 1” at the top of th
 gure means clock cyc
 e 
 rst instruction 
writes into 
$2, and all the following instructions read 
$2 is register is written in clock cycle 5, so the proper value is unavailable before clock 
cycle 5. (A read of a register during a clock cycle returns the value written at the end of th
 rst half of the cycle, when such a write occur
 e colored lines from the top datapath to the lower ones show the dep
 ose that must go backward in time are 
pipeline data hazards
.
306 Chapter 4 The Processor
OR instructions would get the incorrect value 10! Using this style of drawing, such 
problems become apparent when a dependence line goes backward in time.
As mentioned in Section 4.5, the desired result is available at the end of the 
EX stage or clock cycle 3. When is the data actually needed by the 
AND and 
OR instructions? At the beginning of the EX stage, or clock cycles 4 and 5, respectively. 

 us, we can execute this segment without stalls if we simply 
forward
 the data as 
soon as it is available to any units that need it before it is available to read from the 

regist
 le.
How does forwarding work? For simplicity in the rest of this section, we consider 
only the challenge of forwarding to an operation in the EX stage, which may be 

either an ALU operation or an
 ective address calculatio
 is means that when 
an instruction tries to use a register in its EX stage that an earlier instruction 

intends to write in its WB stage, we actually need the values as inputs to the ALU.
A notation that names th
 elds of the pipeline registers allows for a more 
precise notation of dependences. For example, “ID/EX.RegisterRs” refers to the 

number of one register whose value is found in the pipeline register ID/EX; that is, 

the one from th
 rst read port of the regist
 le. 
 e 
 rst part of the name, to the 
  of the period, is the name of the pipeline register; the second part is the name of 
th
 eld in that register. Using this notation, the two pairs of hazard conditions are
1a. EX/MEM.RegisterRd = ID/EX.RegisterRs
1b. EX/MEM.RegisterRd = ID/EX.RegisterRt

2a. MEM/WB.RegisterRd = ID/EX.RegisterRs

2b. MEM/WB.RegisterRd = ID/EX.RegisterRt

 e 
 rst hazard in the sequence on page 304 is on register 
$2, between the 
result of 
sub $2,$1,$3 and th
 rst read operand of 
and $12,$2,$5 is hazard can be detected when the 
and instruction is in the EX stage and the prior 
instruction is in the MEM stage, so this is hazard 1a:
EX/MEM.RegisterRd = ID/EX.RegisterRs = 
$2Dependence DetectionClassify the dependences in this
 sequence from page 304:
   sub $2,   $1, $3  # Register $2 set by sub   and $12,  $2, $5  # 1st operand($2) set by sub   or  $13,  $6, $2  # 2nd operand($2) set by sub   add $14,  $2, $2  # 1st($2) & 2nd($2) set by sub   sw  $15,  100($2) # Index($2) set by subEXAMPLE
 4.7 Data Hazards: Forwarding versus Stalling 
307As mentioned above, the 
sub-and is a type 1a hazard
 e remaining hazards 
are as follows:
 e sub-or is a type 2b hazard:
MEM/WB.RegisterRd = ID/EX.RegisterRt = 
$2 e two dependences on 
sub-add are not hazards because the register 
 le supplies the proper data during the ID stage of 
add. ere is no data hazard between 
sub and 
sw because 
sw reads 
$2 the 
clock cycle 
 er sub writes 
$2.Because some instructions do not write registers, this policy is inaccurate; 
sometimes it would forward when it shouldn’t. One solution is simply to check 
to see if the RegWrite signal will be active: examining the WB contro
 eld of the 
pipeline register during the EX and MEM stages determines whether RegWrite 

is asserted. Recall that MIPS requires that every use of 
$0 as an operand must 
yield an operand value of 0. In the event that an instruction in the pipeline has 

$0 as its destination (for example, 
sll $0, $1, 2), we want to avoid forwarding 
its possibly nonzero result value. 
Not forwarding results destined for 
$0 frees the 
assembly programmer and the compiler of any requirement to avoid using 
$0 as a destinatio
 e conditions above thus work properly as long we add EX/MEM.
Registo th
 rst hazard condition and MEM/WB.Registo the 
second.
Now that we can detect hazards, half of the problem is resolved—but we must 
still forward the proper data.
Figure 4.53
 shows the dependences between the pipeline registers and the inputs 
to the ALU for the same code sequence as in 
Figure 4.52
 e change is that the 
dependence begins from a 
pipeline
 register, rather than waiting for the WB stage to 
write the regist
 le. 
 us, the required data exists in time for later instructions, 
with the pipeline registers holding the data to be forwarded.
If we can take the inputs to the ALU from 
any
 pipeline register rather than just 
ID/EX, then we can forward the proper data. By adding multiplexors to the input 

of the ALU, and with the proper controls, we can run the pipeline at full speed in 

the presence of these data dependences.
For now, we will assume the only instructions we need to forward are the four 
R-format instructions: 
add, sub, AND, and 
OR. Figure 4.54
 shows a close-up of 
the ALU and pipeline register before and a
 er adding forwarding. 
Figure 4.55
 shows the values of the control lines for the ALU multiplexors that select either the 

regist
 le values or one of the forwarded values.
 is forwarding control will be in the EX stage, because the ALU forwarding 
multiplexors are found in that stage.
 us, we must pass the operand register 
numbers from the ID stage via the ID/EX pipeline register to determine whether 

to forward values. We already have the r eld (bits 20–16). Before forwarding, the 

ID/EX register had no need to include space to hold the r
 eld. Hence, rs (bits 
25–21) is added to ID/EX.
ANSWER
308 Chapter 4 The Processor
Let’s now write both the conditions for detecting hazards and the control signals 
to resolve them:
1. EX hazard:
if (EX/MEM.RegWrite
and (EX/MEM.RegisterRd = ID/EX.RegisterRs)) ForwardA = 10if (EX/MEM.RegWrite
and (EX/MEM.RegisterRd = ID/EX.RegisterRt)) ForwardB = 10Programexecution
order(in instructions)sub$2, $1, $3and $12, $2, $5or $13, $6, $2add $14, $2,$2sw $15, 100($2)Time (in clock cycles)CC 1 CC 2 CC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9
IMRegRegIMRegRegIMRegRegIMRegRegIMDMDMDMDMDMRegReg10 10 10 10         10/Œ20        Œ20          Œ20          Œ20          Œ20
Value of register $2:
Value of EX/MEM:        X        X        X      Œ20         X        X        X        X        X
Value of MEM/WB:         X         X         X         X       Œ20         X        X         X        X
FIGURE 4.53 The dependences between the pipeline registers move forward in time, so it is possible to supply the 
inputs to the ALU needed by the 
AND instruction and 
OR instruction by forwarding the results found in the pipeline 
registers. 
 e values in the pipeline registers show that the desired value is available before it is written into the regist
 le. We assume that 
the regist
 le forwards values that are read and written during the same clock cycle, so the 
add does not stall, but the values come from the 
regist
 le instead of a pipeline register. Regist
 le “forwarding”—that is, the read gets the value of the write in that clock cycle—is why clock 
cycle 5 shows register 
$2 having the value 10 at the beginning and −20 at the end of the clock cycle. As in the rest of this section, we handle all 
forwarding except for the value to be stored by a store instruction.

 4.7 Data Hazards: Forwarding versus Stalling 
309DatamemoryRegistersMuxALUALUID/EXa. No forwardingb. With forwardingEX/MEMMEM/WBDatamemoryRegistersMuxMuxMuxMuxID/EXEX/MEMMEM/WBForwardingunitEX/MEM.RegisterRdMEM/WB.RegisterRdRsRtRtRdForwardBForwardAFIGURE 4.54 On the top are the ALU and pipeline registers before adding forwarding. 
On the bottom, the multiplexors have been expanded to add the forwarding paths, and we show the forwarding 
uni
 e new hardware is shown in color.
 is 
 gure is a stylized drawing, however, leaving out details 
from the full datapath such as the sign extension hardware. Note that the ID/EX.RegisterR
 eld is shown 
twice, once to connect to the Mux and once to the forwarding unit, but it is a single signal. As in the earlier 

discussion, this ignores forwarding of a store value to 
a store instruction. Also note that this mechanism 
works for 
slt instructions as well.

310 Chapter 4 The Processor
Note that the EX/MEM.Regist
 eld is the register destination for either 
an ALU instruction (which comes from th eld of the instruction) or a load 
(which comes from the R
 eld). is case forwards the result from the previous instruction to either input of the 
ALU. If the previous instruction is going to write to the regist
 le, and the write 
register number matches the read register number of ALU inputs A or B, provided 

it is not register 0, then steer the multiplexor to pick the value instead from the 

pipeline register EX/MEM.
2. MEM hazard:
if (MEM/WB.RegWrite
and ( MEM/WB.RegisterRd = ID/EX.RegisterRs)) ForwardA = 01
if (MEM/WB.RegWrite
and  (MEM/WB.RegisterRd = ID/EX.RegisterRt)) ForwardB = 01
As mentioned above, there is no hazard in the WB stage, because we assume that 
the regist
 le supplies the correct result if the instruction in the ID stage reads 
the same register written by the instruction in the WB stage. Such a regist
 le performs another form of forwarding, but it occurs within the regist
 le.
One complication is potential data hazard
s between the result of the instruction 
in the WB stage, the result of the instruction in the MEM stage, and the source 
operand of the instruction in the ALU stage. For example, when summing a vector 

of numbers in a single register, a sequence of instructions will all read and write to 

the same register:
add $1,$1,$2add $1,$1,$3
add $1,$1,$4
. . .Mux controlSource
ExplanationForwardA = 00ID/EXThe Þrst ALU operand comes from the register Þle.
ForwardA = 10EX/MEMThe Þrst ALU operand is forwarded from the prior ALU result.

ForwardA = 01MEM/WBThe Þrst ALU operand is forwarded from data memory or an earlier 
ALU result.ForwardB = 00ID/EXThe second ALU operand comes from the register Þle.

ForwardB = 10EX/MEMThe second ALU operand is forwarded from the prior ALU result.

ForwardB = 01MEM/WBThe second ALU operand is forwarded from data memory or an 
earlier ALU result.
FIGURE 4.55 The control values for the forwarding multiplexors in 
Figure 4.54
.  e signed 
immediate that is another input to the ALU is described in the 
Elaboration
 at the end of this section.

 4.7 Data Hazards: Forwarding versus Stalling 
311In this case, the result is forwarded from the MEM stage because the result in the 
MEM stage is the more recent resul
 us, the control for the MEM hazard would 
be (with the additions highlighted):
if (MEM/WB.RegWrite       and  (MEM/WB.RegisterRd = ID/EX.RegisterRs)) ForwardA = 01

if (MEM/WB.RegWrite       and  (MEM/WB.RegisterRd = ID/EX.RegisterRt)) ForwardB = 01
Figure 4.56
 shows the hardware necessary to support forwarding for operations 
that use results during the EX stage. Note that the EX/MEM.Regist
 eld is the 
register destination for either an ALU instruction (which comes from th
 eld of the instruction) or a load (which comes from the R
 eld).FIGURE 4.56 The datapath modiﬁ ed to resolve hazards via forwarding.
 Compared with the datapath in 
Figure 4.51
, the additions 
are the multiplexors to the inputs to the ALU
 is 
 gure is a more stylized drawing, however, leaving out details from the full datapath, such 
as the branch hardware and the sign extension hardware.
MWBWBRegistersInstructionmemoryMuxMuxMuxMuxALUID/EXEX/MEMMEM/WBForwardingunitEX/MEM.RegisterRdMEM/WB.RegisterRdRsRtRtRdPCControlEXMWBIF/ID.RegisterRsIF/ID.RegisterRtIF/ID.RegisterRtIF/ID.RegisterRdInstructionIF/IDDatamemory
312 Chapter 4 The Processor
 Section 4.13
 shows two pieces of MIPS code with hazards that cause 
forwarding, if you would like to see more illustrated examples using single-cycle 
pipeline drawings.
Elaboration: Forwarding can also help with hazards when store instructions are 
dependent on other instructions. Since they use just one data value during the MEM 
stage, forwarding is easy. However, consider loads immediately followed by stores, useful 

when performing memory-to-memory copies in the MIPS architecture. Since copies are 

frequent, we need to add more forwarding hardware to make them run faster. If we were 

to redraw 
Figure 4.53
, replacing the 
sub and AND instructions with 
lw and sw, we would 

see that it is possible to avoid a stall, since the data exists in the MEM/WB register of 

a load instruction in time for its use in the MEM stage of a store instruction. We would 

need to add forwarding into the memory access stage for this option. We leave this 

 cation as an exercise to the reader.
In addition, the signed-immediate input to the ALU, needed by loads and stores, is 
missing from the datapath in Figure 4.56
. Since central control decides between register 

and immediate, and since the forwarding unit chooses the pipeline register for a register 
DatamemoryRegistersMuxMuxMuxMuxMuxALUID/EXEX/MEM
MEM/WBForwardingunitALUSrcFIGURE 4.57 A close-up of the datapath in
 Figure 4.5
4 shows a 2:1 multiplexor, which has been added to select the 
signed immediate as an ALU input.
 4.7 Data Hazards: Forwarding versus Stalling 
313input to the ALU, the easiest solution is to add a 2:1 multiplexor that chooses between 
the ForwardB multiplexor output and the signed immediate. 
Figure 4.57
 shows this 
addition.Data Hazards and StallsAs we said in Section 4.5, one case where forwarding cannot save the day is when 
an instruction tries to read a register following a load instruction that writes 

the same register. 
Figure 4.58
 illustrates the prob
 e data is still being read 
from memory in clock cycle 4 while the ALU is performing the operation for the 

following instruction. Something must stall the pipeline for the combination of 

load followed by an instruction that reads its result.
Hence, in addition to a forwarding unit, we need a 
hazard detection unit
. It 
operates during the ID stage so that it can insert the stall between the load and its 
Programexecution
order
(in instructions)lw $2, 20($1)and $4, $2, $5or $8, $2, $6 add $9, $4, $2 
slt $1, $6, $7Time (in clock cycles)CC 1 CC 2 CC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9
IMDMRegRegIMDMRegRegIMDMRegRegIMDMRegRegIMDMRegRegFIGURE 4.58 A pipelined sequence of instructions. 
Since the dependence between the load and the following instruction (
and) goes backward in time, this hazard cannot be solved by forwarding. Hence, this combination must result in a stall by the hazard
 detection unit.
If a
 rst you don’t 
succeed, re
 ne 
success.
Anonymous

314 Chapter 4 The Processor
use. Checking for load instructions, the control for the hazard detection unit is this 
single condition:
if (ID/EX.MemRead and   ((ID/EX.RegisterRt = IF/ID.RegisterRs) or
     (ID/EX.RegisterRt = IF/ID.RegisterRt)))
     stall the pipeline e 
 rst line tests to see if the instruction is a load: the only instruction that reads 
data memory is a load
 e next two lines check to see if the destination register 
 eld of the load in the EX stage matches either source register of the instruction 
in the ID stage. If the condition holds, the instruction stalls one clock cycle.
 er this 1-cycle stall, the forwarding logic can handle the dependence and execution 
proceeds. (If there were no forwarding, then the instructions in 
Figure 4.58
 would 

need another stall cycle.)
If the instruction in the ID stage is stalled, then the instruction in the IF stage 
must also be stalled; otherwise, we would lose the fetched instruction. Preventing 

these two instructions from making progress is accomplished simply by preventing 

the PC register and the IF/ID pipeline register from changing. Provided these 

registers are preserved, the instruction in the IF stage will continue to be read 

using the same PC, and the registers in the ID stage will continue to be read using 

the same instructio
 elds in the IF/ID pipeline register. Returning to our favorite 
analogy, it’s as if you restart the washer with the same clothes and let the dryer 

continue tumbling empty. Of course, like the dryer, the back half of the pipeline 

starting with the EX stage must be doing something; what it is doing is executing 

instructions that have
 ect: 
nops
.How can we insert these nops, which act like bubbles, into the pipeline? In 
Figure
 4.49, we see that deasserting all nine control signals (setting them to 0) in the EX, 

MEM, and WB stages will create a “do nothing” or nop instruction. By identifying 

the hazard in the ID stage, we can insert a bubble into the pipeline by changing the 

EX, MEM, and WB contro
 elds of the ID/EX pipeline register to
 ese benign 
control values are percolated forward at each clock cycle with the prop
 ect: no 
registers or memories are written if the control values are all 0.
Figure 4.59
 shows what really happens in the hardware: the pipeline execution 
slot associated with the 
AND instruction is turned into a nop and all instructions 
beginning with the 
AND instruction are delayed one cycle. Like an air bubble in 
a water pipe, a stall bubble delays everything behind it and proceeds down the 

instruction pipe one stage each cycle until it exits at the end. In this example, the 

hazard forces the 
AND and 
OR instructions to repeat in clock cycle 4 what they 
did in clock cycle 3: 
AND reads registers and decodes, and 
OR is refetched from 
instruction memory. Such repeated work is what a stall looks like, but i
 ect is 
to stretch the time of the 
AND and 
OR instructions and delay the fetch of the 
add instruction.
Figure 4.60
 highlights the pipeline connections for both the hazard detection 
unit and the forwarding unit. As before, the forwarding unit controls the ALU 
nop
 An instruction that 
does no operation to 
change state.

 4.7 Data Hazards: Forwarding versus Stalling 
315multiplexors to replace the value from a general-purpose register with the value 
from the proper pipeline register
 e hazard detection unit controls the writing 
of the PC and IF/ID registers plus the multiplexor that chooses between the real 

control values an
 e hazard detection unit stalls and deasserts the control 
 elds if the load-use hazard test above is true. 
 Section 4.13
 gives an example of 
MIPS code with hazards that causes stalling,
 illustrated using single-clock pipeline 
diagrams, if you would like to see more details.
Although the compiler generally relies upon the hardware to resolve hazards 

and thereby ensure correct execution, the compiler must understand the 

pipeline to achieve the best performance. Otherwise, unexpected stalls 

will reduce the performance of the compiled code.
The BIGPicturebubbleProgramexecution
order
(in instructions)lw $2, 20($1)and becomes nopand $4, $2, $5or $8, $2, $6 add $9, $4, $2Time (in clock cycles)CC 1 CC 2 CC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9CC 10
IMDMRegRegIMDMRegRegIMDMRegRegIMDMRegRegIMDMRegRegFIGURE 4.59 The way stalls are really inserted into the pipeline. 
A bubble is inserted beginning in clock cycle 4, by changing the 
and instruction to a nop. Note that the 
and instruction is really fetched and decoded in clock cycles 2 and 3, but its EX stage is delayed until 
clock cycle 5 (versus the unstalled position in clock cycle 4). Likewise the 
OR instruction is fetched in clock cycle 3, but its ID stage is delayed 
until clock cycle 5 (versus the unstalled clock cycle 4 positio
 er insertion of the bubble, all the dependences go forward in time and no 
further hazards occur.

316 Chapter 4 The Processor
Elaboration: Regarding the remark earlier about setting control lines to 0 to avoid 
writing registers or memory: only the signals RegWrite and MemWrite need be 0, while 
the other control signals can be don’t cares.
 4.8 Control Hazards
 us far, we have limited our concern to hazards involving arithmetic operations 
and data transfers. However, as we saw in Section 4.5, there are also pipeline hazards 
involving branches. 
Figure 4.61 
shows a sequence of instructions and indicates when 

the branch would occur in this pipeline. An instruction must be fetched at every 

clock cycle to sustain the pipeline, yet in our design the decision about whether to 

branch doesn’t occur until the MEM pipeline stage. As mentioned in Section 4.5, 
0MWBWBDatamemoryInstructionmemoryALUID/EXEX/MEMMEM/WBForwardingunitPCControlEXMWBIF/IDMuxMuxMuxMuxMuxHazarddetectionunitID/EX.MemReadIF/ID.RegisterRsInstructionIF/ID.RegisterRtIF/ID.RegisterRtIF/ID.RegisterRdID/EX.RegisterRtPCWriteIF/DWriteRegistersRtRdRsRtFIGURE 4.60 Pipelined control overview, showing the two multiplexors for forwarding, the hazard detection unit, and 
the forwarding unit. 
Although the ID and EX stages have been simp
 ed—the sign-extended immediate and branch logic are missing—
this drawing gives the essence of the forwarding hardware requirements.
 ere are a thousand 
hacking at the 
branches of evil to one 

who is striking at the 

root.
Henry Da
 oreau, 
Walden
, 1854
 4.8 Control Hazards 
317this delay in determining the proper instruction to fetch is called a 
control hazard
 or 
branch hazard
, in contrast to the 
data hazards 
we have just examined.
 is section on control hazards is shorter than the previous sections on data 
hazard
 e reasons are that control hazards are relatively simple to understand, 
they occur less frequently than data hazards, and there is nothin
 ective 
against control hazards as forwarding is against data hazards. Hence, we use 
simpler schemes. We look at two schemes for resolving control hazards and one 

optimization to improve these schemes.
RegProgramexecution
order
(in instructions)40 beq $1, $3, 2844 and $12, $2, $548 or $13, $6, $252 add $14, $2, $272 lw $4, 50($7)Time (in clock cycles)CC 1 CC 2 CC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9
IMDMRegRegIMDMRegRegIMDMRegIMDMRegRegIMDMRegRegFIGURE 4.61 The impact of the pipeline on the branch instruction. 
 e numbers to th  of the instruction (40, 44, …) 
are the addresses of the instructions. Since the branch instruction decides whether to branch in the MEM stage—clock cycle 4 fo
r the 
beq instruction above—the three sequential instructions that follow the branch will be fetched and begin execution. Without interve
ntion, those 
three following instructions will begin execution before 
beq branches to 
lw at location 72. (
Figure 4.31
 assumed extra hardware to reduce the 
control hazard to one clock cycle; t
 gure uses the nonoptimized datapath.)

318 Chapter 4 The Processor
Assume Branch Not Taken
As we saw in Section 4.5, stalling until the branch is complete is too slow. One 
improvement over branch stalling is to 
predict
 that the branch will not be taken 
and thus continue execution down the sequential instruction stream. If the branch 

is taken, the instructions that are being fetched and decoded must be discarded. 

Execution continues at the branch target. If branches are untaken half the time, 

and if it costs little to discard the instructions, this optimization halves the cost of 

control hazards. 
To discard instructions, we merely change the original control values to 0s, much 
as we did to stall for a load-use data hazard
 e 
 erence is that we must also 
change the three instructions in the IF, ID, and EX stages when the branch reaches 

the MEM stage; for load-use stalls, we just change control to 0 in the ID stage and 

let them percolate through the pipeline. Discarding instructions, then, means we 

must be able to 
 ush instructions in the IF, ID, and EX stages of the pipeline.
Reducing the Delay of Branches
One way to improve branch performance is to reduce the cost of the taken branch. 

 us far, we have assumed the next PC for a branch is selected in the MEM 
stage, but if we move the branch execution earlier in the pipeline, then fewer 

instructions need be
 ushed. 
 e MIPS architecture was designed to support fast 
single-cycle branches that could be pipelined with a small branch penalty.
 e designers observed that many branches rely only on simple tests (equality or sign, 

for example) and that such tests do not require a full ALU operation but can be 

done with at most a few gates. When a more complex branch decision is required, 

a separate instruction that uses an ALU to perform a comparison is required—a 

situation that is similar to the use of condition codes for branches (see Chapter 2).
Moving the branch decision up requires two actions to occur earlier: computing 
the branch target address and evaluating the branch decisio
 e easy part of 
this change is to move up the branch address calculation. We already have the PC 

value and the immediate
 eld in the IF/ID pipeline register, so we just move the 
branch adder from the EX stage to the ID stage; of course, the branch target address 

calculation will be performed for all instructions, but only used when needed.
 e harder part is the branch decision itself. For branch equal, we would compare 
the two registers read during the ID stage to see if they are equal. Equality can be 

tested by
 rst exclusive ORing their respective bits and then ORing all the results. 
Moving the branch test to the ID stage implies additional forwarding and hazard 

detection hardware, since a branch dependent on a result still in the pipeline must 

still work properly with this optimization. For example, to implement branch on 

equal (and its inverse), we will need to forward results to the equality test logic that 

operates during ID
 ere are two complicating factors:
1. During ID, we must decode the instruction, decide whether a bypass to the 
equality unit is needed, and complete the equality comparison so that if 

the instruction is a branch, we can set the PC to the branch target address. 
 ush To discard 
instructions in a pipeline, 
usually due to an 

unexpected event.

 4.8 Control Hazards 
319Forwarding for the operands of branches was formerly handled by the ALU 
forwarding logic, but the introduction of the equality test unit in ID will 

require new forwarding logic. Note that the bypassed source operands of a 

branch can come from either the ALU/MEM or MEM/WB pipeline latches.
2. Because the values in a branch comparison are needed during ID but may be 
produced later in time, it is possible that a data hazard can occur and a stall 

will be needed. For example, if an ALU instruction immediately preceding 

a branch produces one of the operands for the comparison in the branch, 

a stall will be required, since the EX stage for the ALU instruction will 

occur a
 er the ID cycle of the branch. By extension, if a load is immediately 
followed by a conditional branch that is on the load result, two stall cycles 

will be needed, as the result from the load appears at the end of the MEM 

cycle but is needed at the beginning of ID for the branch.
Despite thes
  culties, moving the branch execution to the ID stage is an 
improvement, because it reduces the penalty of a branch to only one instruction if 

the branch is taken, namely, the one currently being fetched e exercises explore 

the details of implementing the forwarding path and detecting the hazard.
To
 ush instructions in the IF stage, we add a control line, called IF.Flush, 
that zeros the instructio
 eld of the IF/ID pipeline register. Clearing the register 
transforms the fetched instruction into a 
nop, an instruction that has no action 
and changes no state.
Pipelined BranchShow what happens when the branch is taken in this instruction sequence, 

assuming the pipeline is optimized for branches that are not taken and that we 

moved the branch execution to the ID stage:
36 sub $10, $4, $840 beq $1, $3, 7 # PC-relative branch to 40  + 4 + 7 * 4 = 72

44 and $12, $2, $5
48 or  $13, $2, $6
52 add $14, $4, $2
56 slt $15, $6, $7
. . .
72 lw $4, 50($7)Figure 4.62
 shows what happens when a branch is taken. Unlike 
Figure 4.61
, there is only one pipeline bubble on a taken branch.
EXAMPLEANSWER
320 Chapter 4 The Processor
MWBWBDatamemoryRegistersInstructionmemoryALUID/EXEX/MEMMEM/WBForwardingunitPCControlEXMWBIF/ID0Hazarddetectionunit++Sign-extendShiftleft 2=IF.Flush47248442844$1$3$8$4710and $12, $2, $5beq $1, $3, 7sub $10, $4, $8before<1>before<2>MWBWBDatamemoryRegistersInstructionmemoryMuxALUID/EXEX/MEMMEM/WBForwardingunitPCControlEXMWBIF/ID0Hazarddetectionunit++Sign-extendShiftleft 2=IF.Flush47672767272$310$1lw $4, 50($7)Clock 3Clock 4Bubble (nop)beq $1, $3, 7sub $10, . . .before<1>MuxMuxMuxMuxMuxMuxMuxMuxMuxFIGURE 4.62 The ID stage of clock cycle 3 determines that a branch must be taken, so it selects 72 as the next PC 
address and zeros the instruction fetched for the next clock cycle. 
Clock cycle 4 shows the instruction at location 72 being 
fetched and the single bubble or 
nop instruction in the pipeline as a result of the taken branch. (Since the 
nop is really sll 
$0, $0, 0, it’s 
arguable whether or not the ID stage in clock 4 should be highlighted.)

 4.8 Control Hazards 
321Dynamic Branch PredictionAssuming a branch is not taken is one simple form of 
branch prediction
. In that case, 
we predict that branches are untak
 ushing the pipeline when we are wrong. For 
the simp
 ve-stage pipeline, such an approach, possibly coupled with compiler-
based prediction, is probably adequate. 
With deeper pipelines, the branch penalty 
increases when measured in clock cycles. Similarly, with multiple issue (see Section 
4.10), the branch penalty increases in terms of instruction
 is combination 
means that in an aggressive pipeline, a simple static prediction scheme will probably 

waste too much performance. As we mentioned in Section 4.5, with more hardware 

it is possible to try to 
predict
 branch behavior during program execution. 
One approach is to look up the address of the instruction to see if a branch was 
taken the last time this instruction was executed, and, if so, to begin fetching new 

instructions from the same place as the last time
 is technique is called 
dynamic 
branch prediction
. One implementation of that approach is a 
branch prediction bu
 er or 
branch 
history table
. A branch prediction bu
 er is a small memory indexed by the lower 
portion of the address of the branch instructio
 e memory contains a bit that 
says whether the branch was recently taken or not.
 is is the simplest sort of bu
 er; we don’t know, in fact, if the prediction is 
the right one—it may have been put there by another branch that has the same 

low-order address bits. However, this doesn’t a
 ect correctness. Prediction is just 
a hint that we hope is correct, so fetching begins in the predicted direction. If the 

hint turns out to be wrong, the incorrectly predicted instructions are deleted, the 

prediction bit is inverted and stored back, and the proper sequence is fetched and 

executed.
 is simple 1-bit prediction scheme has a performance shortcoming: even if a 
branch is almost always taken, we can predict incorrectly twice, rather than once, 

when it is not tak
 e following example shows this dilemma.
Loops and PredictionConsider a loop branch that branches nine times in a row, then is not taken 

once. What is the prediction accuracy for this branch, assuming the prediction 

bit for this branch remains in the prediction b
 er? e steady-state prediction behavior will mispredict on th
 rst and last loop 
iterations. Mispredicting the last iteration is inevitable since the prediction 

bit will indicate taken, as the branch has been taken nine times in a row at 

that poin
 e misprediction on th
 rst iteration happens because the bit is 
 ipped on prior execution of the last iteration of the loop, since the branch 
was not taken on that exiting iteratio
 us, the prediction accuracy for this 
dynamic branch 
prediction
 Prediction of 
branches at runtime using 
runtime information.
branch prediction 
bu
 er Also called
 branch history table
. A small memory that 
is indexed by the lower 

portion of the address of 

the branch instruction 

and that contains one 

or more bits indicating 

whether the branch was 

recently taken or not.
EXAMPLEANSWER
322 Chapter 4 The Processor
branch that is taken 90% of the time is only 80% (two incorrect predictions and 
eight correct ones).
Ideally, the accuracy of the predictor would match the taken branch frequency for 
these highly regular branches. To remedy this weakness, 2-bit prediction schemes 

are
 en used. In a 2-bit scheme, a prediction must be wrong twice before it is 
changed. 
Figure 4.63
 shows th
 nite-state machine for a 2-bit prediction scheme.
A branch prediction bu
 er can be implemented as a small, special b
 er accessed 
with the instruction address during the IF pipe stage. If the instruction is predicted 

as taken, fetching begins from the target as soon as the PC is known; as mentioned 

on page 318, it can be as early as the ID stage. Otherwise, sequential fetching and 

executing continue. If the prediction turns out to be wrong, the prediction bits are 

changed as shown in 
Figure 4.63
.Elaboration: As we described in Section 4.5, ve-stage pipeline we can make the 
control hazard a feature b ning the branch. A delayed branch always executes the 
following instruction, but the second instruction following the branch will be affected by 

the branch.Compilers and assemblers try to place an instruction that always executes after the 
branch in the branch delay slot
. The job of the software is to make the successor 
instructions valid and useful. 
Figure 4.64
 shows the three ways in which the branch 

delay slot can be scheduled.
branch delay slot
 e slot directly a
 er a delayed branch 
instruction, which in the 

MIPS architecture
 lled by an instruction that 

does not a
 ect the branch.
Predict taken
Not taken
Not taken
Not taken
Not taken
Taken
Taken
Taken
Taken
Predict not taken
Predict not taken
Predict taken
FIGURE 4.63 The states in a 2-bit prediction scheme. 
By using 2 bits rather than 1, a branch that 
strongly favors taken or not taken—as many branches do—will be mispredicted only once
 e 2 bits are used 
to encode the four states in the syst
 e 2-bit scheme is a general instance of a counter-based predictor, 
which is incremented when the prediction is accurate and decremented otherwise, and uses the mid-point of 
its range as the division between taken and not taken.

 4.8 Control Hazards 
323The limitations on delayed branch scheduling arise from (1) the restrictions on the 
instructions that are scheduled into the delay slots and (2) our ability to predict at 
compile time whether a branch is likely to be taken or not.
Delayed branching w ve-stage pipeline 
issuing one instruction each clock cycle. As processors go to both longer pipelines 

and issuing multiple instructions per clock cycle (see Section 4.10), the branch delay 

becomes longer, and a single dela cient. Hence, delayed branching has 

 exible dynamic approaches. 

Simultaneously, the growth in available transistors per chip has due to 
Moore’s Law 

made dynamic prediction relatively cheaper.
add $s1, $s2, $s3if $s2 = 0 thenDelay slotif $s2 = 0 thenadd $s1, $s2, $s3Becomesa.  From beforesub $t4, $t5, $t6. . .add $s1, $s2, $s3
if $s1 = 0 thenDelay slotadd $s1, $s2, $s3if $s1 = 0 thensub $t4, $t5, $t6Becomesb.  From targetadd $s1, $s2, $s3
if $s1 = 0 thenDelay slotadd $s1, $s2, $s3if $s1 = 0 thensub $t4, $t5, $t6Becomesc.  From fall-throughsub $t4, $t5, $t6FIGURE 4.64 Scheduling the branch delay slot. 
 e top box in each pair shows the code before 
scheduling; the bottom box shows the scheduled code. In 
(a), the delay slot is scheduled with an independent 
instruction from before the branc
 is is the best choice. Strategies (b) and (c) are used when (a) is not 
possible. In the code sequences for (b) and (c), the use of 
$s1 in the branch condition prevents the 
add instruction (whose destination is 
$s1) from being moved into the branch delay slot. In (b) the branch delay 
slot is scheduled from the target of the branch; usually the target instruction will need to be copied because 
it can be reached by another path. Strategy (b) is preferred when the branch is taken with high probability, 

such as a loop branch. Finally, the branch may be scheduled from the not-taken fall-through as in (c). To 

make this optimization legal for (b) or (c), it must be OK to execute the 
sub instruction when the branch 
goes in the unexpected direction. By “OK” we mean that th
e work is wasted, but the program will still execute 
correctly.
 is is the case, for example, if 
$t4 were an unused temporary register when the branch goes in 
the unexpected direction.

324 Chapter 4 The Processor
Elaboration: A branch predictor tells us whether or not a branch is taken, but still 
 ve-stage pipeline, this calculation 
takes one cycle, meaning that taken branches will have a 1-cycle penalty. Delayed 

branches are one approach to eliminate that penalty. Another approach is to use a 
cache to hold the destination program counter or destination instruction using a 
branch target buffer.The 2-bit dynamic prediction scheme uses only information about a particular branch. 
Researchers noticed that using information about both a local branch, and the global 
behavior of recently executed branches together yields greater prediction accuracy for 

the same number of prediction bits. Such predictors are called 
correlating predictors
. A typical correlating predictor might have two 2-bit predictors for each branch, with the 
choice between predictors made based on whether the last executed branch was taken 

or not taken. Thus, the global branch behavior can be thought of as adding additional 

index bits for the prediction lookup.A more recent innovation in branch prediction is the use of tournament predictors. A 
tournament predictor
 uses multiple predictors, tracking, for each branch, which predictor 
yields the best results. A typical tournament predictor might contain two predictions for each branch index: one based on local information and one based on global branch 

behavior. A selector would choose which predictor to use for any given prediction. The 

selector can operate similarly to a 1- or 2-bit predictor, favoring whichever of the two 

predictors has been more accurate. Some recent microprocessors use such elaborate 

predictors.
Elaboration: One way to reduce the number of conditional branches is to add 
conditional move
 instructions. Instead of changing the PC with a conditional branch, the 

instruction conditionally changes the destination register of the move. If the condition 

fails, the move acts as a 
nop. For example, one version of the MIPS instruction set 

architecture has two new instructions called 
movn (move if not zero) and 
movz (move 

if zero). Thus, 
movn $8, $11, $4 copies the contents of register 11 into register 8, 
provided that the value in register 4 is nonzero; otherwise, it does nothing.
The ARMv7 instr eld in most instructions. Hence, ARM 
programs could have fewer conditional branches than in MIPS programs.
Pipeline Summary
We started in the laundry room, showing principles of pipelining in an everyday 
setting. Using that analogy as a guide, we explained instruction pipelining 

step-by-step, starting with the single-cycle datapath and then adding pipeline 

registers, forwarding paths, data hazard detection, branch prediction, an
 ushing 
instructions on exceptions.
 Figure 4.65
 shows th
 nal evolved datapath and control. 
We now are ready for yet another control hazard: the sticky issue of exceptions.
Consider three branch prediction schemes: predict not taken, predict taken, and 
dynamic prediction. Assume that they all have zero penalty when they predict 

correctly and two cycles when they are wrong. Assume that the average predict 
branch target bu
 er A structure that caches 
the destination PC or 

destination instruction 

for a branch. It is usually 

organized as a cache with 

tags, making it more 

costly than a simple 

prediction bu
 er.
correlating predictor
 A branch predictor that 

combines local behavior 

of a particular branch 

and global information 

about the behavior of 

some recent number of 

executed branches.
tournament branch 
predictor
 A branch 
predictor with multiple 
predictions for each 

branch and a selection 

mechanism that chooses 

which predictor to enable 

for a given branch.
Check Yourself

 4.9 Exceptions 325accuracy of the dynamic predictor is 90%. Which predictor is the best choice for 
the following branches? 
1. A branch that is taken with 5% frequency
2. A branch that is taken with 95% frequency

3. A branch that is taken with 70% frequency
 4.9 ExceptionsControl is the most challenging aspect of processor design: it is both the hardest 
part to get right and the hardest part to
 make fast. One of the hardest parts of 
ControlHazarddetectionunit+4PCInstructionmemorySign-extendRegisters=+FowardingunitALUID/EXMEM/WBEX/MEMWBMEXShiftleft 2IF.FlushIF/IDMuxMuxDatamemoryWBWBM0MuxMuxMuxMuxFIGURE 4.65 The ﬁ nal datapath and control for this chapter.
 Note that this is a sty
 gure rather than a detailed datapath, so 
it’s missing the ALUsrc Mux from 
Figure 4.57
 and the multiplexor controls from 
Figure 4.51
.To make a computer 

with automatic 

program-interruption 

facilities behave 

[sequentially] was 

not an easy matter, 

because the number of 

instructions in various 

stages of processing 

when an interrupt 

signal occurs may be 

large.
Fred Brooks, Jr., 
Planning a Computer 

System: Project Stretch, 

1962
326 Chapter 4 The Processor
control is implementing 
exceptions
 and 
interrupts
—events other than branches 
or jumps that change the normal
 ow of instruction executio
 ey were initially 
created to handle unexpected events from within the processor, like arithmetic 
over
 ow. 
 e same basic mechanism was extended for I/O devices to communicate 
with the processor, as we will see in Chapter 5.
Many architectures and authors do not distinguish between interrupts and 
exceptions, o
 en using the older name 
interrupt
 to refer to both types of events. 
For example, the Intel x86 uses interrupt. We follow the MIPS convention, using 

the term 
exception
 to refer to 
any
 unexpected change in contro
 ow without 
distinguishing whether the cause is internal or external; we use the term 
interrupt
 only when the event is externally caused. Here ar
 ve examples showing whether 
the situation is internally generated by the processor or externally generated:
Type of event
From where?MIPS terminology
I/O device request
External
Interrupt
Invoke the operating system from user programInternal
ExceptionArithmetic o ow
Internal
Exception ned instruction
Internal
ExceptionHardware malfunctions
EitherException or interrupt
Many of the requirements to support exceptions come from the sp
 c situation that causes an exception to occur. Accordingly, we will return to this 

topic in Chapter 5, when we will better understand the motivation for additional 

capabilities in the exception mechanism. In this section, we deal with the control 

implementation for detecting two types of exceptions that arise from the portions 

of the instruction set and implementation that we have already discussed.
Detecting exceptional conditions and taking the appropriate action is o
 en on the critical timing path of a processor, which determines the clock cycle time 

and thus performance. Without proper attention to exceptions during design of 

the control unit, attempts to add exceptions to a complicated implementation 

ca
 cantly reduce performance, as well as complicate the task of getting the 
design correct.
How Exceptions Are Handled in the MIPS Architecture e two types of exceptions that our current implementation can generate are 
execution of an
 ned instruction and an arithmetic over
 ow. We’ll use 
arithmetic over
 ow in the instruction 
add $1, $2, $1 as the example exception 
in the next few pag
 e basic action that the processor must perform when an 
exception occurs is to save the address of the o
 ending instruction in the 
exception 
program counter
 (EPC) and then transfer control to the operating system at some 

sp
 ed address.
 e operating system can then take the appropriate action, which may involve 
providing some service to the user program, taking some pr
 ned action in 
exception
 Also 
called interrupt
. An unscheduled event 
that disrupts program 
execution; used to detect 

over
 ow.
interrupt
 An exception 
that comes from outside 

of the processor. (Some 

architectures use the 

term 
interrupt
 for all 
exceptions.)

 4.9 Exceptions 327response to an over
 ow, or stopping the execution of the program and reporting an 
error.
 er performing whatever action is required because of the exception, the 
operating system can terminate the program or may continue its execution, using 
the EPC to determine where to restart the execution of the program. In Chapter 5, 

we will look more closely at the issue of restarting the execution.
For the operating system to handle the exception, it must know the reason for 
the exception, in addition to the instruction that caused i
 ere are two main 
methods used to communicate the reason for an exceptio
 e method used in 
the MIPS architecture is to include a status register (called the 
Cause register
), which ho
 eld that indicates the reason for the exception.
A second method, is to use 
vectored interrupts
. In a vectored interrupt, the 
address to which control is transferred is determined by the cause of the exception. 
For example, to accommodate the two exception types listed above, we might 

 ne the following two exception vector addresses:
Exception typeException vector address (in hex) ned instruction
8000 0000hexArithmetic o ow
8000 0180hex e operating system knows the reason for the exception by the address at which 
it is initiated
 e addresses are separated by 32 bytes or eight instructions, and the 
operating system must record the reason for the exception and may perform some 

limited processing in this sequence. When the exception is not vectored, a single 

entry point for all exceptions can be used, and the operating system decodes the 

status register to
 nd the cause.
We can perform the processing required for exceptions by adding a few extra 
registers and control signals to our basic implementation and by slightly extending 

control. Let’s assume that we are implementing the exception system used in the 

MIPS architecture, with the single entry point being the address 8000 0180
hex. (Implementing vectored exceptions is no mor
  cult.) We will need to add two 
additional registers to our current MIPS implementation:
 EPC: A 32-bit register used to hold the address of the a
 ected instruction. 
(Such a register is needed even when exceptions are vectored.)
 Cause:
 A register used to record the cause of the exception. In the MIPS 
architecture, this register is 32 bits, although some bits are currently unused. 

Assume ther
 ve-bit 
 eld that encodes the two possible exception 
sources mentioned above, with 10 representing a ned instruction and 

12 representing arithmetic over
 ow.
Exceptions in a Pipelined ImplementationA pipelined implementation treats exceptions as another form of control hazard. 

For example, suppose there is an arithmetic over
 ow in an add instruction. Just as 
vectored interrupt
 An interrupt for which 
the address to which 

control is transferred is 

determined by the cause 

of the exception.

328 Chapter 4 The Processor
we did for the taken branch in the previous section, we mu
 ush the instructions 
that follow the 
add instruction from the pipeline and begin fetching instructions 
from the new address. We will use the same mechanism we used for taken branches, 
but this time the exception causes the deasserting of control lines.
When we dealt with branch mispredict, we saw how t
 ush the instruction 
in the IF stage by turning it into a 
nop. T
 ush instructions in the ID stage, we 
use the multiplexor already in the ID stage that zeros control signals for stalls. A 

new control signal, called ID.Flush, is ORed with the stall signal from the hazard 

detection unit to
 ush during ID. To
 ush the instruction in the EX phase, we use 
a new signal called EX.Flush to cause new multiplexors to zero the control lines. To 

start fetching instructions from location 8000 0180
hex, which is the MIPS exception 
address, we simply add an additional input to the PC multiplexor that sends 8000 

0180hex to the PC. 
Figure 4.66
 shows these changes.
 is example points out a problem with exceptions: if we do not stop execution 
in the middle of the instruction, the programm
er will not be able to see the original 
value of register 
$1 that helped cause the over
 ow because it will be clobbered as 
the Destination register of the 
add instruction. Because of careful planning, the 
over
 ow exception is detected during the EX stage; hence, we can use the EX.Flush 
signal to prevent the instruction in the EX 
stage from writing its result in the WB 

stage. Many exceptions require that we eventually complete the instruction that 

caused the exception as if it executed normally.
 e easiest way to do this is to
 ush the instruction and restart it from the beginning a
 er the exception is handled.
 e 
 nal step is to save the address of the o
 ending instruction in the 
exception 
program counter
 (EPC). In reality, we save the address +4, so the exception handling 

the so
 ware routine mu
 rst subtract 4 from the saved value. 
Figure 4.66
 shows 
a stylized version of the datapath, including the branch hardware and necessary 

accommodations to handle exceptions.
Exception in a Pipelined ComputerGiven this instruction sequence,
40hex  sub  $11, $2, $444hex  and  $12, $2, $548hex  or   $13, $2, $64Chex  add   $1, $2, $150hex  slt  $15, $6, $754hex  lw   $16, 50($7). . .
EXAMPLE
 4.9 Exceptions 329assume the instructions to be invoked on an exception begin like this:
80000180hex  sw     $26, 1000($0)80000184hex  sw     $27, 1004($0). . .
Show what happens in the pipeline if an over
 ow exception occurs in the 
add instruction.
Figure 4.67
 shows the events, starting with the add instruction in the EX stage. 
 e over
 ow is detected during that phase, and 8000 0180
hex is forced into the 
PC. Clock cycle 7 shows that the 
add and following instructions ar
 ushed, 
and th
 rst instruction of the exception code is fetched. Note that the address 
of the instruction 
following
 the 
add is saved: 4C
hex + 4 = 50hex.ANSWER000MWBWBDatamemoryInstructionmemoryMuxMuxMuxMuxMuxALUID/EXEX/MEMCauseEPCMEM/WBForwardingunitPCControlEXMWBIF/IDMuxMuxHazarddetectionunitShiftleft 2IF.FlushID.FlushEX.Flush4Sign-extend80000180RegistersMuxFIGURE 4.66 The datapath with controls to handle exceptions.
 e key additions include a new input with the value 8000 0180
hex in the multiplexor that supplies the new PC value; a Cause register to record the cause of the exception; and an Exception PC r
egister to save 
the address of the instruction that caused the exceptio
 e 8000 0180
hex input to the multiplexor is the initial address to begin fetching 
instructions in the event of an exception. Although not shown, the ALU over
 ow signal is an input to the control unit.

330 Chapter 4 The Processor
lw $16, 50($7)slt $15, $6, $7add $1, $2, $1or $13, . . . and $12, . . .sw $26, 1000($0)Clock 6Clock 7bubble (nop)bubblebubbleor $13, . . .
0000050010101000000000000000MWBWBDatamemoryInstructionmemoryMuxID/EXEX/MEMMEM/WBForwardingunitPCControlEXMWBIF/IDMuxHazarddetectionunit++Shiftleft 2=IF.FlushID.FlushEX.Flush4585454$115Sign-extend80000180RegistersMuxMuxCauseEPC12$6$2$1$7131200MWBWBDatamemoryInstructionmemoryMuxMuxMuxID/EXEX/MEMMEM/WBForwardingunitPCControlEXMWBIF/IDMuxMuxMuxHazarddetectionunit++Shiftleft 2=IF.FlushID.FlushEX.Flush458Sign-extend80000180800001808000018080000184RegistersMuxCauseEPC1313ALUMuxMuxMuxMuxMuxFIGURE 4.67 The result of an exception due to arithmetic overﬂ
 ow in the add instruction.
 e over
 ow is detected during 
the EX stage of clock 6, saving the address following the add in the EPC register (4C + 4 = 50
hex). Over
 ow causes all the Flush signals to be set 
near the end of this clock cycle, deasserting control values (setting them to 0) for the 
add. Clock cycle 7 shows the instructions converted to 
bubbles in the pipeline plus the fetching of th
 rst instruction of the exception routine—
sw $25,1000($0)—from instruction location 
8000 0180hex. Note that the 
AND and 
OR instructions, which are prior to the 
add, still complete. Although not shown, the ALU ov
 ow signal 
is an input to the control unit.

 4.9 Exceptions 331We mentio
 ve examples of exceptions on page 326, and we will see others 
in Chapter 5. Wit
 ve instructions active in any clock cycle, the challenge is 
to associate an exception with the appropriate instruction. Moreover, multiple 
exceptions can occur simultaneously in a single clock cycle
 e solution is to 
prioritize the exceptions so that it is easy to determine which is serv
 rst. In 
most MIPS implementations, the hardware sorts exceptions so that the earliest 

instruction is interrupted.
I/O device requests and hardware malfunctions are not associated with a sp
 c instruction, so the implementation has so
 exibility as to when to interrupt the 
pipeline. Hence, the mechanism used for other exceptions works ju
 ne. e EPC captures the address of the interrupted instructions, and the MIPS 
Cause register records all possible exceptions in a clock cycle, so the exception 

so
 ware must match the exception to the instruction. An important clue is knowing 
in which pipeline stage a type of exception can occur. For example, a
 ned 
instruction is discovered in the ID stage, and invoking the operating system 

occurs in the EX stage. Exceptions are collected in the Cause register in a pending 

exception
 eld so that the hardware can interrupt based on later exceptions, once 
the earliest one has been serviced.
 e hardware and the operating system must work in conjunction so that 
exceptions behave as you would expec
 e hardware contract is normally to 
stop the o
 ending instruction in midstream, let all prior instructions complete, 
 ush all following instructions, set a register to show the cause of the exception, 
save the address of the o
 ending instruction, and then jump to a prearranged 
addr
 e operating system contract is to look at the cause of the exception and 
act appropriately. For an
 ned instruction, hardware failure, or arithmetic 
over
 ow exception, the operating system normally kills the program and returns 
an indicator of the reason. For an I/O device request or an operating system service 
call, the operating system saves the state of the program, performs the desired task, 

and, at some point in the future, restores the program to continue execution. In 

the case of I/O device requests, we may o
 en choose to run another task before 
resuming the task that requested the I/O, since that task may o en not be able to 

proceed until the I/O is complete. Exceptions are why the ability to save and restore 

the state of any task is critical. One of the most important and frequent uses of 

exceptions is handling page faults and TLB exceptions; Chapter 5 describes these 

exceptions and their handling in more detail.
Elaboration: culty of always associating the correct exception with the correct 
instruction in pipelined computers has led some computer designers to relax this 
requirement in noncritical cases. Such processors are said to have 
imprecise interrupts
 or imprecise exceptions. In the example above, PC would normally have 58
hex at the start 
of the clock cycle after the exception is detected, even though the offending instruction 
Hardware/ 
Software 

Interfaceimprecise 
interrupt
 Also called
 imprecise exception
. Interrupts or exceptions 
in pipelined computers 

that are not associated 

with the exact instruction 

that was the cause of the 

interrupt or exception.

332 Chapter 4 The Processor
is at address 4Chex. A processor with imprecise exceptions might put 58hex into EPC and leave it up to the operating system to determine which instruction caused the problem. 
MIPS and the vast majority of computers today support 
precise interrupts
 or precise exceptions. (One reason is to support virtual memory, which we shall see in Chapter 5.)
Elaboration: Although MIPS uses the exception entry address 8000 0180
hex for almost all exceptions, it uses the address 8000 0000
hex to improve performance of the 
exception handler for TLB-miss exceptions (see Chapter 5).Which exception should be re
 rst in this sequence?
1. add $1, $2, $1  # arithmetic over
 ow2. XXX $1, $2, $1   ned instruction
3. sub $1, $2, $1  # hardware error
 4.10 Parallelism via Instructions
Be forewarned: this section is a brief overview of fascinating but advanced 
topics. If you want to learn more details, you should consult our more advanced 

book, 
Computer Architecture: A Quantitative Approach
 h edition, where the 
material covered in these 13 pages is expanded to almost 200 pages (including 

appendices)!
Pipelining
 exploits the potential 
parallelism
 among instruction
 is parallelism is called 
instruction-level parallelism (ILP)
 ere are two primary 
methods for increasing the potential amount of instruction-level pa
 e  rst is increasing the depth of the pipeline to overlap more instructions. Using our 
laundry analogy and assuming that the washer cycle was longer than the others 

were, we could divide our washer into three machines that perform the wash, rinse, 

and spin steps of a traditional washer. We would then move from a four-stage to a 

six-stage pipeline. To get the full speed-up, we need to rebalance the remaining steps 

so they are the same length, in processors or in laundry.
 e amount of parallelism 
being exploited is higher, since there are more operations being overlapped. 

Performance is potentially greater since the clock cycle can be shorter.
Another approach is to replicate the internal components of the computer so 
that it can launch multiple instructions in every pipeline stage.
 e general name 
for this technique is 
multiple issue
. A multiple-issue laundry would replace our 
household washer and dryer with, say, three washers and three dryers. You would 
also have to recruit more assistants to fold and put away three times as much 

laundry in the same amount of time
 e downside is the extra work to keep all the 
machines busy and transferring the loads to the next pipeline stage.
Check Yourself
instruction-level 
parallelism
 e parallelism among 
instructions.
multiple issue
 A scheme 
whereby multiple 

instructions are launched 

in one clock cycle.
precise interrupt
 Also 
called precise exception
. An interrupt or exception 

that is always associated 

with the correct 

instruction in pipelined 

computers.

 4.10 Parallelism via Instructions 
333Launching multiple instructions per stage allows the instruction execution rate to 
exceed the clock rate or, stated alternatively, the CPI to be less than 1. As mentioned 
in Chapter 1, it is sometimes useful t
 ip the metric and use 
IPC, or 
instructions 
per clock cycle
. Hence, a 4 GHz four-way multiple-issue microprocessor can execute 
a peak rate of 16 billion instructions per second and have a best-case CPI of 0.25, 

or an IPC of 4. Assumin
 ve-stage pipeline, such a processor would have 20 
instructions in execution at any given time. Today’s high-end microprocessors 

attempt to issue from three to six instructions in every clock cycle. Even moderate 

designs will aim at a peak IPC of
 ere are typically, however, many constraints 
on what types of instructions may be executed simultaneously, and what happens 

when dependences arise.
 ere are two major ways to implement a multiple-issue processor, with the 
majo
 erence being the division of work between the compiler and the hardware. 
Because the division of work dictates whether decisions are being made statically 

(that is, at compile time) or dynamically (that is, during execution), the approaches 

are sometimes called 
static multiple issue
 and 
dynamic multiple issue
. As we will 
see, both approaches have other, more commonly used names, which may be less 
precise or more restrictive.
 ere are two primary and distinct responsibilities that must be dealt with in a 
multiple-issue pipeline:
1. Packaging instructions into 
issue slots
: how does the processor determine 
how many instructions and which instructions can be issued in a given 
clock cycle? In most static issue processors, this process is at least partially 

handled by the compiler; in dynamic issue designs, it is normally dealt with 

at runtime by the processor, although the compiler will o
 en have already 
tried to help improve the issue rate by placing the instructions in a b
 cial order.
2. Dealing with data and control hazards: in static issue processors, the compiler 
handles some or all of the consequences of data and control hazards statically. 

In contrast, most dynamic issue processors
 attempt to alleviate at least some 
classes of hazards using hardware techniques operating at execution time.
Although we describe these as distinct approaches, in reality one approach o
 en borrows techniques from the other, and neither approach can claim to be perfectly 

pure.
The Concept of SpeculationOne of the most important methods fo
 nding and exploiting more ILP is 
speculation. Based on the great idea of 
prediction
, speculation
 is an approach 
that allows the compiler or the processor to “guess” about the properties of an 
instruction, so as to enable execution to begin for other instructions that may 

depend on the speculated instruction. For example, we might speculate on the 

outcome of a branch, so that instructions a
 er the branch could be executed earlier. 
static multiple issue
 An approach to implementing 
a multiple-issue processor 

where many decisions 

are made by the compiler 

before execution.
dynamic multiple 
issue
 An approach to 
implementing a multiple-
issue processor where 

many decisions are made 

during execution by the 

processor.
issue slots
 e positions 
from which instructions 

could issue in a given 

clock cycle; by analogy, 

these correspond to 

positions at the starting 

blocks for a sprint.
speculation
 An approach whereby the 

compiler or processor 

guesses the outcome of an 

instruction to remove it as 

a dependence in executing 

other instructions.

334 Chapter 4 The Processor
Another example is that we might speculate that a store that precedes a load does 
not refer to the same address, which would allow the load to be executed before the 

stor
 e 
  culty with speculation is that it may be wrong. So, any speculation 
mechanism must include both a method to check if the guess was right and a 

method to unroll or back out th
 ects of the instructions that were executed 
speculatively.
 e implementation of this back-out capability adds complexity.
Speculation may be done in the compiler or by the hardware. For example, the 
compiler can use speculation to reorder instructions, moving an instruction across 

a branch or a load across a store
 e processor hardware can perform the same 
transformation at runtime using techniques we discuss later in this section.
 e recovery mechanisms used for incorrect speculation are rath
 erent. 
In the case of speculation in so
 ware, the compiler usually inserts additional 
instructions that check the accuracy of the speculation and prov
 x-up routine 
to use when the speculation is incorrect. In hardware speculation, the processor 

usually bu
 ers the speculative results until it knows they are no longer speculative. 
If the speculation is correct, the instructions are completed by allowing the 

contents of the b
 ers to be written to the registers or memory. If the speculation is 
incorrect, the hardwar
 ushes the bu
 ers and re-executes the correct instruction 
sequence.
Speculation introduces one other possible problem: speculating on certain 
instructions may introduce exceptions that were formerly not present. For 

example, suppose a load instruction is moved in a speculative manner, but the 

address it uses is not legal when the speculation is incorrec
 e result would be 
an exception that should not have occurred e problem is complicated by the 

fact that if the load instruction were not speculative, then the exception must 

occur! In compiler-based speculation, such problems are avoided by adding 

special speculation support that allows such exceptions to be ignored until it is 

clear that they really should occur. In hardware-based speculation, exceptions 

are simply b
 ered until it is clear that the instruction causing them is no longer 
speculative and is ready to complete; at that point the exception is raised, and 

nor-mal exception handling proceeds.
Since speculation can improve performance when done properly and decrease 
performance when done carelessly,
 cant 
 ort goes into deciding when it 
is appropriate to speculate. Later in this section, we will examine both static and 

dynamic techniques for speculation.
Static Multiple IssueStatic multiple-issue processors all use the compiler to assist with packaging 

instructions and handling hazards. In a static issue processor, you can think of the 

set of instructions issued in a given clock cycle, which is called an 
issue packet
, as one large instruction with multiple operation
 is view is more than an analogy. 
Since a static multiple-issue processor usuall
y restricts what mix of instructions can 
be initiated in a given clock cycle, it is useful to think of the issue packet as a single 
issue packet
 e set 
of instructions that 
issues together in one 

clock cycle; the packet 

may be determined 

statically by the compiler 

or dynamically by the 

processor.

 4.10 Parallelism via Instructions 
335instruction allowing several operations in certain pr
 ned 
 elds. 
 is view led to 
the original name for this approach: 
Very Long Instruction Word
 (VLIW)
.Most static issue processors also rely on the compiler to take on some 
responsibility for handling data and control hazard
 e compiler’s responsibilities 
may include static branch prediction and code scheduling to reduce or prevent all 
hazards. Let’s look at a simple static issue version of a MIPS processor, before we 

describe the use of these techniques in more aggressive processors.
An Example: Static Multiple Issue with the MIPS ISATo give
 avor of static multiple issue, we consider a simple two-issue MIPS 
processor, where one of the instructions can be an integer ALU operation or 

branch and the other can be a load or store. Such a design is like that used in some 

embedded MIPS processors. Issuing two instructions per cycle will require fetching 

and decoding 64 bits of instructions. In many static multiple-issue processors, and 

essentially all VLIW processors, the layout of simultaneously issuing instructions 

is restricted to simplify the decoding and instruction issue. Hence, we will require 

that the instructions be paired and aligned on a 64-bit boundary, with the ALU 

or branch portion appearin
 rst. Furthermore, if one instruction of the pair 
cannot be used, we require that it be replaced with a 
nop us, the instructions 
always issue in pairs, possibly with a 
nop in one slot. 
Figure 4.68
 shows how the 
instructions look as they go into the pipeline in pairs.
Static multiple-issue processors vary in how they deal with potential data and 
control hazards. In some designs, the compiler takes full responsibility for removing 

all
 hazards, scheduling the code and inserting no-ops so that the code executes 
without any need for hazard detection or hardware-generated stalls. In others, 

the hardware detects data hazards and generates stalls between two issue packets, 

while requiring that the compiler avoid all dependences within an instruction pair. 

Even so, a hazard generally forces the entire issue packet containing the dependent 
Instruction type
Pipe stagesALU or branch instructionIFIDEXMEMWB
Load or store instructionIFIDEXMEMWB
ALU or branch instructionIFIDEXMEMWB
Load or store instructionIFIDEXMEMWB

ALU or branch instructionIFIDEXMEMWB
Load or store instructionIFIDEXMEMWB
ALU or branch instructionIFIDEXMEMWB

Load or store instructionIFIDEXMEMWB
FIGURE 4.68 Static two-issue pipeline in operation. 
 e ALU and data transfer instructions 
are issued at the same time. Here we have assumed the sa
 ve-stage structure as used for the single-issue 
pipeline. Although this is not strictly necessary, it does have some advantages. In particular, keeping the 
register writes at the end of the pipeline simp es the handling of exceptions and the maintenance of a 

precise exception model, which become more
  cult in multiple-issue processors.
Very Long Instruction 
Word (VLIW)
 A style of instruction set 
architecture that launches 

many operations that are 

 ned to be independent 
in a single wide 

instruction, typically with 

many separate opcode 

 elds.
336 Chapter 4 The Processor
instruction to stall. Whether the so
 ware must handle all hazards or only try to 
reduce the fraction of hazards between separate issue packets, the appearance of 
having a large single instruction with multiple operations is reinforced. We will 

assume the second approach for this example.
To issue an ALU and a data transfer operation in parallel, th
 rst need for 
additional hardware—beyond the usual hazard detection and stall logic—is extra 

ports in the regist
 le (see 
Figure 4.69
). In one clock cycle we may need to read 
two registers for the ALU operation and two more for a store, and also one write 

port for an ALU operation and one write port for a load. Since the ALU is tied 

up for the ALU operation, we also need a separate adder to calculate th
 ective 
address for data transfers. Without th
ese extra resources, our two-issue pipeline 
would be hindered by structural hazards.
Clearly, this two-issue processor can improve performance by up to a factor of 
two. Doing so, however, requires that twice as many instructions be overlapped 

in execution, and this additional overlap increases the relative performance loss 

from data and control hazards. For example, in our simp
 ve-stage pipeline, 
DatamemoryInstructionmemoryMuxMuxALUALUPCSign-extendRegisters4Mux80000180WritedataAddressSign-extendFIGURE 4.69 A static two-issue datapath. e additions needed for double issue are highlighted: another 32 bits from instruction 
memory, two more read ports and one more write port on the regist
 le, and another ALU. Assume the bottom ALU handles address 
calculations for data transfers and the top ALU handles everything else.

 4.10 Parallelism via Instructions 
337loads have a 
use latency
 of one clock cycle, which prevents one instruction from 
using the result without stalling. In the two-issue
 ve-stage pipeline the result of 
a load instruction cannot be used on the next 
clock cycle.
 is means that the next 
two instructions cannot use the load result without stalling. Furthermore, ALU 
instructions that had no use latency in the simp
 ve-stage pipeline now have a 
one-instruction use latency, since the results cannot be used in the paired load or 
store. To
 ectively exploit the parallelism av
ailable in a multiple-issue processor, 
more ambitious compiler or hardware scheduling techniques are needed, and static 

multiple issue requires that the compiler take on this role.
Simple Multiple-Issue Code SchedulingHow would this loop be scheduled on a static two-issue pipeline for MIPS?
Loop: lw    $t0, 0($s1)    # $t0=array element      addu  $t0,$t0,$s2# add scalar in $s2      sw    $t0, 0($s1)# store result      addi  $s1,$s1,–4# decrement pointer      bne   $s1,$zero,Loop# branch $s1!=0Reorder the instructions to avoid as many pipeline stalls as possible. Assume 

branches are predicted, so that control hazards are handled by the hardware.
 e 
 rst three instructions have data dependences, and so do the last two. 
Figure 4.70
 shows the best schedule for these instructions. Notice that just 
one pair of instructions has both issue slots used. It takes four clocks per loop 

iteration; at four clocks to execut
 ve instructions, we get the disappointing 
CPI of 0.8 versus the best case of 0.5., or an IPC of 1.25 versus 2.0. Notice 

that in computing CPI or IPC, we do not count any nops executed as useful 

instructions. Doing so would improve CPI, but not performance!
use latency
 Number 
of clock cycles between 
a load instruction and 

an instruction that can 

use the result of the 

load without stalling the 

pipeline.
EXAMPLEANSWERFIGURE 4.70 The scheduled code as it would look on a two-issue MIPS pipeline.  e empty 
slots are no-ops.
ALU or branch instructionData transfer instructionClock cycle
Loop:lw $t0, 0($s1)
1addi $s1,$s1,Œ42addu $t0,$t0,$s23bne $s1,$zero,Loopsw $t0, 4($s1)
4
338 Chapter 4 The Processor
An important compiler technique to get more performance from loops 
is loop unrolling
, where multiple copies of the loop body are made. After 
unrolling, there is more ILP available by overlapping instructions from different 
iterations.
loop unrolling
 A technique to get more 
performance from loops 

that access arrays, in 

which multiple copies of 

the loop body are made 

and instructions from 

 erent iterations are 
scheduled together
FIGURE 4.71 The unrolled and scheduled code of 
Figure 4.70
 as it would look on a static two-issue MIPS pipeline. e empty slots are no-ops. Since th
 rst instruction in the loop decrements 
$s1 by 16, the addresses loaded are the original value of $s1, then that address minus 4, minus 8, and minus 12.
Loop Unrolling for Multiple-Issue PipelinesSee how well loop unrolling and scheduling work in the example above. For 
simplicity assume that the loop index is a multiple of four.
To schedule the loop without any delays, it turns out that we need to make 
four copies of the loop body
 er unrolling and eliminating the unnecessary 
loop overhead instructions, the loop will contain four copies each of 
lw, add, and 
sw, plus one 
addi and one
 bne. Figure 4.71
 shows the unrolled and 
scheduled code.
During the unrolling process, the compiler introduced additional registers 
($t1, $t2, $t3 e goal of this process, called 
register renaming
, is to 
eliminate dependences that are not tr
ue data dependences, but could either 
lead to potential hazards or prevent the compiler fro
 exibly scheduling 
the code. Consider how the unrolled code would look using only $
t0 ere 
would be repeated instances of 
lw $t0,0($$s1), addu 
$t0, $t0, $s2 followed by 
sw t0,4($s1), but these sequences, despite using 
$t0, are 
actually completely independent—no data val
 ow between one set of these 
instructions and the next s
 is case is what is called an 
antidependence
 or 
name dependence
, which is an ordering forced purely by the reuse of a name, 
rather than a real data dependence that
 is also called a true dependence.
Renaming the registers during the unrolling process allows the compiler 
to move these independent instructions subsequently so as to better schedule 
EXAMPLEANSWERregister renaming
 e renaming of registers 
by the compiler or 

hardware to remove 

antidependences.
antidependence
 Also 
called name dependence
. An ordering forced by the 

reuse of a name, typically 

a register, rather than by 

a true dependence that 

carries a value between 

two instructions.
ALU or branch instructionData transfer instructionClock cycle
Loop:addi $s1,$s1,Œ16
lw $t0, 0($s1)
1lw $t1,12($s1)2addu $t0,$t0,$s2lw $t2, 8($s1)
3addu $t1,$t1,$s2lw $t3, 4($s1)
4addu $t2,$t2,$s2sw $t0, 16($s1)
5addu $t3,$t3,$s2sw $t1,12($s1)
6sw $t2, 8($s1)
7bne $s1,$zero,Loopsw $t3, 4($s1)
8
 4.10 Parallelism via Instructions 
339the code
 e renaming process eliminates the name dependences, while 
preserving the true dependences.
Notice now that 12 of the 14 instructions in the loop execute as pairs. It takes 
8 clocks for 4 loop iterations, or 2 clocks per iteration, which yields a CPI of 8/14 
= 0.57. Loop unrolling and scheduling with dual issue gave us an improvement 

factor of almost 2, partly from reducing the loop control instructions and partly 

from dual issue executio
 e cost of this performance improvement is using four 
temporary registers rather than one, as we
 cant increase in code size.
Dynamic Multiple-Issue Processors
Dynamic multiple-issue processors are also known as 
superscalar
 processors, or 
simply superscalars. In the simplest superscalar processors, instructions issue in 
order, and the processor decides whether zero, one, or more instructions can issue 

in a given clock cycle. Obviously, achieving good performance on such a processor 

still requires the compiler to try to schedule instructions to move dependences 

apart and thereby improve the instruction issue rate. Even with such compiler 

scheduling, there is an importan
 erence between this simple superscalar 
and a VLIW processor: the code, whether scheduled or not, is guaranteed by 

the hardware to execute correctly. Furthermore, compiled code will always run 

correctly independent of the issue rate or 
pipeline structure of the processor. In 
some VLIW designs, this has not been the case, and recompilation was required 

when moving acr
 erent processor models; in ot
her static issue processors, 
code would run correctly acr
 erent implementations, but o
 en so poorly as 
to make compilatio
 ectively required.
Many superscalars extend the basic framework of dynamic issue decisions to 
include 
dynamic pipeline scheduling
. Dynamic pipeline scheduling chooses 
which instructions to execute in a given clock cycle while trying to avoid hazards 
and stalls. Let’s start with a simple example of avoiding a data hazard. Consider the 

following code sequence:
lw     $t0, 20($s2)addu   $t1, $t0, $t2sub    $s4, $s4, $t3slti   $t5, $s4, 20Even though the 
sub instruction is ready to execute, it must wait for the 
lw and 
addu to complet
 rst, which might take many clock cycles if memory is slow. 
(Chapter 5 explains cache misses, the reason that memory accesses are sometimes 
very slow.) Dynamic 
pipeline
 scheduling allows such hazards to be avoided either 
fully or partially. 
Dynamic Pipeline SchedulingDynamic pipeline scheduling chooses which instructions to execute next, possibly 

reordering them to avoid stalls. In such processors, the pipeline is divided into 

three major units: an instruction fetch and issue unit, multiple functional units 
superscalar
 An advanced pipelining 
technique that enables the 

processor to execute more 

than one instruction per 

clock cycle by selecting 

them during execution.
dynamic pipeline 
scheduling
 Hardware 
support for reordering 
the order of instruction 

execution so as to avoid 

stalls.

340 Chapter 4 The Processor
(a dozen or more in high-end designs in 2013), and a 
commit unit
. Figure 4.72
 shows the model.
 e 
 rst unit fetches instructions, decodes them, and sends 
each instruction to a corresponding functional unit for execution. Each functional 
unit has bu
 ers, called 
reservation stations
, which hold the operands and the 
operatio
 e Elaboration discusses an alternative to reservation stations used 
by many recent processors.) As soon as the bu
 er contains all its operands and 
the functional unit is ready to execute, the result is calculated. When the result is 

completed, it is sent to any reservation stations waiting for this particular result 

as well as to the commit unit, which b
 ers the result until it is safe to put the 
result into the regist
 le or, for a store, into memory
 e b
 er in the commit 
unit, o
 en called the 
reorder bu
 er, is also used to supply operands, in much the 
same way as forwarding logic does in a stat
ically scheduled pipeline. Once a result 
is committed to the regist
 le, it can be fetched directly from there, just as in a 
normal pipeline.
 e combination of b
 ering operands in the reservation stations and results 
in the reorder b
 er provides a form of register renaming, just like that used by 
the compiler in our earlier loop-unrolling example on page 338. To see how this 

conceptually works, consider the following steps:
commit unit
 e unit in 
a dynamic or out-of-order 
execution pipeline that 

decides when it is safe to 

release the result of an 

operation to programmer-

visible registers and 

memory.
reservation station
 A b
 er within a 
functional unit that holds 

the operands and the 

operation.
reorder bu
 er e bu
 er that holds results in 
a dynamically scheduled 

processor until it is safe 

to store the results to 

memory or a register.
Instruction fetchand decode unitReservationstationReservationstationReservationstationReservationstationIntegerIntegerFloatingpointLoad-storeCommitunitIn-order issueOut-of-order executeFunctionalunitsIn-order commit. . .. . .FIGURE 4.72 The three primary units of a dynamically scheduled pipeline. 
 e 
 nal step of 
updating the state is also called retirement or graduation.

 4.10 Parallelism via Instructions 
3411. When an instruction issues, it is copied to a reservation station for the 
appropriate functional unit. Any operands that are available in the register 
 le or reorder b
 er are also immediately copied into the reservation station. 
 e instruction is b
 ered in the reservation station until all the operands 
and the functional unit are available. For the issuing instruction, the register 

copy of the operand is no longer required, and if a write to that register 

occurred, the value could be overwritten.
2. If an operand is not in the regist
 le or reorder b
 er, it must be waiting to 
be produced by a functional uni
 e name of the functional unit that will 
produce the result is tracked. When that unit eventually produces the result, 

it is copied directly into the waiting reservation station from the functional 

unit bypassing the registers.
 ese st
 ectively use the reorder bu
 er and the reservation stations to 
implement register renaming.
Conceptually, you can think of a dynamically scheduled pipeline as analyzing 
the data
 ow structure of a progra
 e processor then executes the instructions 
in some order that preserves the dat
 ow order of the progra
 is style of 
execution is called an 
out-of-order execution
, since the instructions can be 
execut
 erent order than they were fetched.
To make programs behave as if they were running on a simple in-order pipeline, 
the instruction fetch and decode unit is required to issue instructions in order, 

which allows dependences to be tracked, and the commit unit is required to write 

results to registers and memory in program fetch order
 is conservative mode is 
called in-order commit
. Hence, if an exception occurs, the computer can point to 
the last instruction executed, and the only registers updated will be those written 
by instructions before the instruction causing the exception. Although the front 

end (fetch and issue) and the back en
d (commit) of the pipeline run in order, 
the functional units are free to initiate execution whenever the data they need is 

available. Today, all dynamically scheduled pipelines use in-order commit.
Dynamic scheduling is o
 en extended by including hardware-based speculation, 
especially for branch outcomes. By predicting the direction of a branch, a 

dynamically scheduled processor can continue to fetch and execute instructions 

along the predicted path. Because the instructions are committed in order, we know 

whether or not the branch was correctly predicted before any instructions from the 

predicted path are committed. A speculative, dynamically scheduled pipeline can 

also support speculation on load addresses, allowing load-store reordering, and 

using the commit unit to avoid incorrect speculation. In the next section, we will 

look at the use of dynamic scheduling with speculation in the Intel Core i7 design.
out-of-order 
execution
 A situation in 
pipelined execution when 
an instruction blocked 

from executing does 

not cause the following 

instructions to wait.
in-order commit
 A commit in which 

the results of pipelined 

execution are written to 

the programmer visible 

state in the same order 

that instructions are 

fetched.

342 Chapter 4 The Processor
Given that compilers can also schedule code around data dependences, you might 
ask why a superscalar processor would use dynamic schedulin
 ere are three 
major reasons. First, not all stalls are predictable. In particular, cache misses 

(see Chapter 5) in the 
memory hierarchy
 cause unpredictable stalls. Dynamic 
scheduling allows the processor to hide some of those stalls by continuing to 

execute instructions while waiting for the stall to end. 
Second, if the processor speculates on branch outcomes using dynamic branch 
prediction
, it cannot know the exact order of instructions at compile time, since it 
depends on the predicted and actual behavior of branches. Incorporating dynamic 

speculation to exploit more 
instruction-level parallelism
 (ILP) without incorporating 
dynamic scheduling wo
 cantly restrict the bene
 ts of speculation.
 ird, as the pipeline latency and issue width change from one implementation 
to another, the best way to compile a code sequence also changes. For example, how 

to schedule a sequence of dependent instructions is a
 ected by both issue width and 
latency.
 e pipeline structure a
 ects both the number of times a loop must be unrolled 
to avoid stalls as well as the process of compiler-based register renaming. Dynamic 

scheduling allows the hardware to hide most of these detai
 us, users and so
 ware 
distributors do not need to worry about having multiple versions of a program for 

 erent implementations of the same instruction set. Similarly, old legacy code will 
get much of the b
 t of a new implementation without the need for recompilation.
Both 
pipelining
 and multiple-issue execution increase peak instruction 
throughput and attempt to exploit instruction-level 
parallelism
 (ILP). Data and control dependences in programs, however, o
 er an upper limit 
on sustained performance because the processor must sometimes wait for 

a dependence to be resolved. So
 ware-centric approaches to exploiting 
ILP rely on the ability of the compiler to
 nd and reduce th
 ects of such 
dependences, while hardware-centric approaches rely on extensions to the 

pipeline and issue mechanisms. Speculation, performed by the compiler 

or the hardware, can increase the amount of ILP that can be exploited via 

prediction
, although care must be taken since speculating incorrectly is 
likely to reduce performance.
The BIGPictureUnderstanding 
Program 
Performance

 4.10 Parallelism via Instructions 
343Modern, high-performance microprocessors are capable of issuing several instructions 
per clock; unfortunately, sustaining that issue rate is very
  cult. For example, despite 
the existence of processors with four to six issues per clock, very few applications can 

sustain more than two instructions per cloc
 ere are two primary reasons for this.
First, within the pipeline, the major performance bottlenecks arise from 
dependences that cannot be alleviated, 
thus reducing the parallelism among 
instructions and the sustained issue rate. Although little can be done about true data 

dependences, o
 en the compiler or hardware does not know precisely whether a 
dependence exists or not, and so must cons
ervatively assume the dependence exists. 
For example, code that makes use of pointers, particularly in ways that may lead to 

aliasing, will lead to more implied pote
ntial dependences. In contrast, the greater 
regularity of array accesses o
 en allows a compiler to deduce that no dependences 
exist. Similarly, branches that cannot be accurately predicted whether at runtime or 

compile time will limit the ability to exploit ILP
 en, additional ILP is available, but 
the ability of the compiler or the hardware t
 nd ILP that may be widely separated 
(sometimes by the execution of thousands of instructions) is limited.
Second, losses in the 
memory hierarchy
 (the topic of Chapter 5) also limit the 
ability to keep the pipeline full. Some memory system stalls can be hidden, but 

limited amounts of ILP also limit the extent to which such stalls can be hidden. 
Energy Efﬁ ciency and Advanced Pipelining
 e downside to the increasing exploitation of instruction-level parallelism via 
dynamic multiple issue and speculation is potential energ
  ciency. Each 
innovation was able to turn more transistors into performance, but they o
 en did 
so very
  ciently. Now that we have hit the power wall, we are seeing designs 
with multiple processors per chip where the processors are not as deeply pipelined 

or as aggressively speculative as its predecessors.
 e belief is that while the simpler processors are not as fast as their sophisticated 
brethren, they deliver better performance per joule, so that they can deliver more 

performance per chip when designs are constrained more by energy than they are 

by number of transistors.
Figure 4.73
 shows the number of pipeline stages, the issue width, speculation level, 
clock rate, cores per chip, and power of several past and recent microprocessors. Note 

the drop in pipeline stages and power as companies switch to multicore designs.
Elaboration: le and memory. Some 
dynamically scheduled processor le immediately during execution, 
using extra registers to implement the renaming function and preserving the older copy of a 

register until the instruction updating the register is no longer speculative. Other processors 

buffer the result, typically in a structure called a reorder buffer, and the actual update to the 

 le occurs later as part of the commit. Stores to memory must be buffered until 

commit time either in a store buffer
 (see Chapter 5) or in the reorder buffer. The commit unit 

allows the store to write to memory from the buffer when the buffer has a valid address and 

valid data, and when the store is no longer dependent on predicted branches.
Hardware/ 
Software 

Interface
344 Chapter 4 The Processor
Elaboration: Memor t from 
nonblocking caches, which continue servicing cache accesses during a cache miss (see Chapter 5). Out-of-order execution 
processors need the cache design to allow instructions to execute during a miss.
State whether the following techniques or components are associated primarily 
with a so
 ware- or hardware-based approach to exploiting ILP. In some cases, the 
answer may be both.
1. Branch prediction
2. Multiple issue

3. VLIW
4. Superscalar

5. Dynamic scheduling

6. Out-of-order execution

7. Speculation

8. Reorder bu
 er9. Register renaming
 4.11  Real Stuff: The ARM Cortex-A8 and Intel 
Core i7 PipelinesFigure 4.74
 describes the two microprocessors we examine in this section, whose 
targets are the two bookends of the PostPC Era. 
Check Yourself
MicroprocessorYearClock Rate
Pipeline StagesIssue Width
Out-of-Order/ 
SpeculationCores/ 
ChipPower
Intel 486198925 MHz5
1No1 5 W
Intel Pentium
199366 MHz5
2No1 10 W
Intel Pentium Pro
1997200 MHz103
Yes
1 29 W
Intel Pentium 4 Willamette20012000 MHz223
Yes
1 75 W
Intel Pentium 4 Prescott
20043600 MHz313
Yes
1 103 W
Intel Core20062930 MHz144Yes
Yes
Yes
2 75 W
Intel Core i5 Nehalem2010 3300 MHz144187W
Intel Core i5 Ivy Bridge20123400 MHz144877W
FIGURE 4.73 Record of Intel Microprocessors in terms of pipeline complexity, number of cores, and power. 
 e Pentium 
4 pipeline stages do not include the commit stages. If we included them, the Pentium 4 pipelines would be even deeper.

 4.11 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Pipelines 
345ProcessorIntel Core i7 920ARM A8MarketThermal design power
Clock rateCores/ChipFloating point?Multiple Issue?Peak instructions/clock cyclePipeline StagesPipeline scheduleBranch prediction1st level caches / core2nd level cache / core3rd level cache (shared)Personal Mobile Device2 Watts1 GHz1NoDynamic214Static In-order2-level32 KiB I, 32 KiB D128 - 1024 KiB--Server, Cloud130 Watts2.66 GHz4YesDynamic414Dynamic Out-of-order with Speculation2-level32 KiB I, 32 KiB D256 KiB2 - 8 MiBFIGURE 4.74 Speciﬁ cation of the ARM Cortex-A8 and the Intel Core i7 920.
The ARM Cortex-A8
 e ARM Corxtex-A8 runs at 1 GHz with a 14-stage pipeline. It uses dynamic 
multiple issue, with two instructions per clock cycle. It is a static in-order pipeline, 
in that instructions issue, execute, and commit in order
 e pipeline consists of 
three sections for instruction fetch, instruction decode, and execute.  
Figure 4.75
 
shows the overall pipeline.
 e 
 rst three stages fetch two instructions at a time and try to keep a 
12-instruction entry prefetch bu er full. It uses a two-level branch predictor using 

both a 512-entry branch target bu
 er, a 4096-entry global history b
 er, and an 
8-entry return stack to predict future returns. When the branch prediction is 

wrong, it empties the pipeline, resulting in a 13-clock cycle misprediction penalty.
 e 
 ve stages of the decode pipeline determine if there are dependences 
between a pair of instructions, which would force sequential execution,  and in 

which pipeline of the execution stages to send the instructions. 
 e six stages of the instruction execution section o
 er one pipeline for load 
and store instructions and two pipelines for arithmetic operations, although only 

th
 rst of the pair can handle multiplies. Either instruction from the pair can be 
issued to the load-store pipeline.
 e execution stages have full bypassing between 
the three pipelines.
Figure 4.76
 shows the CPI of the A8 using small versions of programs derived 
from the SPEC2000 benchmarks. While the ideal CPI is 0.5, the best case here is 

1.4, the median case is 2.0, and the worst case is 5.2. For the median case, 80% of 

the stalls are due to the pipelining hazards and 20% are stalls due to the memory 

346 Chapter 4 The Processor
hierarchy. Pipeline stalls are caused by branch mispredictions, structural hazards, 
and data dependencies between pairs of instructions. Given the static pipeline of the 

A8, it is up to the compiler to try to avoid structural hazards and data dependences.
Elaboration: The Cor gurable core that supports the ARMv7 instruction 
set architecture. It is delivered as an IP (Intellectual Property
) core
. IP cores are the dominant form of technology delivery in the embedded, personal mobile device, and 
related markets; billions of ARM and MIPS processors have been created from these 

IP cores. Note that IP cores are different than the cores in the Intel i7 multicore computers. An 
IP core (which may itself be a multicore) is designed to be incorporated with other logic 

(hence it is the “core” of a chip), c processors (such as an 

encoder or decoder for video), I/O interfaces, and memory interfaces, and then fabricated 

to yield a processor optimized for a particular application. Although the processor core is 

almost identical, the resultant chips have many differences. One parameter is the size 

of the L2 cache, which can vary by a factor of eight. 
The Intel Core i7 920x86 microprocessors employ sophisticated pipelining approaches, using both 
dynamic multiple issue and dynamic pipeline scheduling with out-of-order 

execution and speculation for its 14-stage pipeline
 ese processors, however, 
are still faced with the challenge of implementing the complex x86 instruction 

set, described in Chapter 2. Intel fetches x86 instructions and translates them into 

internal MIPS-like instructions, which Intel calls 
micro-operations
 e micro-
operations are then executed by a sophisticated, dynamically scheduled, speculative 

pipeline capable of sustaining an execution rate of up to six micro-operations per 

clock cycle.
 is section focuses on that micro-operation pipeline. 
FIGURE 4.75 The A8 pipeline. 
 e 
 rst three stages fetch instructions into a 12-entry instruction fetch 
bu
 er. 
 e Address Generation Unit
 (AGU) uses a 
Branch Target
 er (BTB), 
Global History Bu
 er (GHB), and a 
Return Stack
 (RS) to predict branches to try to keep the fetch queue full. Instruction deco
 ve 
stages and instruction execution is six stages.
F0F1F2D0D1
Branch mispredict
penalty=13 cyclesInstruction execute and load/storeALU pipe 1LS pipe 0 or 1D2D3
Instruction decodeArchitectural register fileInstructionfetchAGU
RAM+TLB12-entry
fetch
queueBTBGHBRSD4E0E1E2E3E4E5
BPupdateALU/MUL pipe 0BPupdateBPupdate
 4.11 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Pipelines 
347When we consider the design of sophisticated, dynamically scheduled processors, the 
design of the functional units, the cache and regist le, instruction issue, and overall 
pipeline control become intermingled, making i
  cult to separate the datapath from 
the pipeline. Because of this, many engineers and researchers have adopted the term 
microarchitecture
 to refer to the detailed internal architecture of a processor. 
 e Intel Core i7 uses a scheme for resolving antidependences and incorrect 
speculation that uses a reorder b
 er together with register renaming. Register 
renaming explicitly renames the 
architectural registers
 in a processor (16 in the case 
of the 64-bit version of the x86 architecture) to a larger set of physical register
 e Core i7 uses register renaming to remove antidependences. Register renaming requires 

the processor to maintain a map between the architectural registers and the physical 

registers, indicating which physical register is the most current copy of an architectural 

register. By keeping track of the renamings that have occurred, register renaming o
 ers 
another approach to recovery in the event of incorrect speculation: simply undo the 

mappings that have occurred since th
 rst incorrectly speculated instructio
 is 
will cause the state of the processor to return to the last correctly executed instruction, 

keeping the correct mapping between the architectural and physical registers.
Figure 4.77
 shows the overall organization and pipeline of the Core i7. Below are 
the eight steps an x86 instruction goes through for execution.
1. Instruction fetc
 e processor uses a multilevel branch target b
 er to 
achieve a balance between speed and prediction accuracy.
 ere is also a 
return address stack to speed up function return. Mispredictions cause a 

penalty of about 15 cycles. Using the predicted address, the instruction fetch 

unit fetches 16 bytes from the instruction cache.
 e 16 bytes are placed in the predecode instruction b
 er— 
 e predecode 
stage transforms the 16 bytes into individual x86 instruction
 is predecode 
microarchitecture
 e organization of the 
processor, including the 

major functional units, 

their interconnection, and 

control.
architectural 
registers
 e instruction 
set of visible registers of 
a processor; for example, 

in MIPS, these are the 32 

integer an
 oating-
point registers.
1.00twolfbzip2gzipparsergapperlbmkgcccraftyvprvortexeonmcf
2.003.00
4.00
5.006.00Memory hierarchy stalls
Pipeline stalls
Ideal CPI1.411.63 1.69 1.70 1.851.95 2.01 2.07 2.11 2.413.20 5.17FIGURE 4.76 CPI on ARM Cortex A8 for the Minnespec benchmarks, which are small versions of the SPEC2000 
benchmarks. 
 ese benchmarks use the much smaller inputs to reduce running time by several orders of magnitude
 e smaller size 
 cantly 
underestimates
 the CPI impact of the memory hierarchy (See Chapter 5). 

348 Chapter 4 The Processor
is nontrivial since the length of an x86 instruction can be from 1 to 15 bytes 
and the predecoder must look through a number of bytes before it knows the 

instruction length. Individual x86 instructions are placed into the 18-entry 

instruction queue.
3. Micro-op decode—Individual x86 instructions are translated into micro-
operations (micro-o
 ree of the decoders handle x86 instructions that 
translate directly into one micro-op. For x86 instructions that have more complex 

semantics, there is a microcode engine that is used to produce the micro-op 

sequence; it can produce up to four micro-ops every cycle and continues until 

the necessary micro-op sequence has been generated.
 e micro-ops are placed 
according to the order of the x86 instructions in the 28-entry micro-op b
 er.
 e micro-op b
 er performs 
loop stream detection
—If there is a small 
sequence of instructions (less than 28 instructions or 256 bytes in length) 

that comprises a loop, the loop stream detecto
 nd the loop and directly 
FIGURE 4.77 The Core i7 pipeline with memory components
 e total pipeline depth is 14 
stages, with branch mispredictions costing 17 clock cyc
 is design can bu
 er 48 loads and 32 stor
 e six independent units can begin execution of a ready RISC operation each clock cycle.
256 KB unified l2cache (eight-way)
Register alias table and allocator
128-Entry reorder buffer
36-Entry reservation station
Retirementregister fileALUshiftSSEshuffleALU128-bitFMULFDIV128-bitFMULFDIV128-bitFMULFDIVSSEshuffleALUSSEshuffleALUMemory order buffer
ALUshiftALUshiftLoadaddressStoreaddressStoredataStore
& loadMicro-codeComplex
macro-opdecoder28-Entry micro-op loop stream detect buffer
Simplemacro-opdecoderSimplemacro-opdecoderSimplemacro-opdecoder128-Entry
inst. TLB
(four-way)
Instruction
fetch
hardware
18-Entry instruction queue
32 KB Inst. cache (four-way associative)
16-Byte pre-decode+macro-opfusion, fetch buffer
64-Entry data TLB
(4-way associative)
32-KB dual-ported data
cache (8-way associative)
512-Entry unified
L2 TLB (4-way)
8 MB all core shared and inclusive L3
cache (16-way associative)
Uncore arbiter (handles scheduling andclock/power state differences)

 4.11 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Pipelines 
349issue the micro-ops from the bu
 er, eliminating the need for the instruction 
fetch and instruction decode stages to be activated.
5. Perform the basic instruction issue—Looking up the register location in the 
register tables, renaming the registers, allocating a reorder b
 er entry, and 
fetching any results from the registers or reorder b er before sending the 
micro-ops to the reservation stations.
 e i7 uses a 36-entry centralized reservation station shared by six functional 
units. Up to six micro-ops may be dispatched to the functional units every 

clock cycle.
 e individual function units execute micro-ops and then results are sent 
back to any waiting reservation station as well as to the register retirement 

unit, where they will update the register state, once it is known that the 

instruction is no longer speculative
 e entry corresponding to the 
instruction in the reorder b
 er is marked as complete.
8. When one or more instructions at the head of the reorder bu
 er have been 
marked as complete, the pending writes in the register retirement unit are 

executed, and the instructions are removed from the reorder b
 er.
Elaboration: Hardware in the second and fourth steps can combine or 
fuse operations together to reduce the number of operations that must be performed. 
Macro-op fusion in the second step takes x86 instruction combinations, such as compare followed by a 
branch, and fuses them into a single operation. 
Microfusion in the fourth step combines 
micro-operation pairs such as load/ALU operation and ALU operation/store and issues 

them to a single reservation station (where they can still issue independently), thus 

increasing the usage of the buffer. In a study of the Intel Core architecture, which also 

incorporated microfusion and macrofusion, Bird et al. [2007] discovered that microfusion 

had little impact on performance, while macrofusion appears to have a modest positive 

impact on integer perfor
 oating-point performance.
Performance of the Intel Core i7 920
Figure 4.78
 shows the CPI of the Intel Core i7 for each of the SPEC2006 benchmarks. 
While the ideal CPI is 0.25, the best case here is 0.44, the median case is 0.79, and 

the worst case is 2.67. 
While i
  cult to 
 erentiate between pipeline stalls and memory stalls 
in a dynamic out-of-order execution pipeline, we can show th
 ectiveness of 
branch prediction and speculation. 
Figure 4.79
 shows the percentage of branches 

mispredicted and the percentage of the work (measured by the numbers of micro-

ops dispatched into the pipeline) that does not retire (that is, their results are 

annulled) relative to all micro-op dispatch
 e min, median, and max of branch 
mispredictions are 0%, 2%, and 10%. For wasted work, they are 1%, 18%, and 39%.
 e wasted work in some cases closely matches the branch misprediction rates, 
such as for gobmk and astar. In several instances, such as mcf, the wasted work 

seems relatively larger than the misprediction rate.
 is divergence is likely due 

350 Chapter 4 The Processor
32.521.5CPI10.50.44 0.59 0.61 0.650.74 0.770.821.02 1.06 1.23 2.12 2.670libquantumh264refhmmerperlbench
bzip2xalancbmksjenggobmkastargccomnetppmcfStalls, misspeculation
Ideal CPIFIGURE 4.78 CPI of Intel Core i7 920 running SPEC2006 integer benchmarks.
FIGURE 4.79 Percentage of branch mispredictions and wasted work due to unfruitful 
speculation of Intel Core i7 920 running SPEC2006 integer benchmarks. 
40%35%
30%25%20%15%
10%5%
0%libquantumh264refhmmerperlbench
bzip2xalancbmksjenggobmkastargccomnetppmcfBranch misprediction %Wasted work %
0% 2% 2% 2% 5% 1% 5% 10%9% 2% 2% 6% 1%  5%  6%  11%  24%  7%  25%  32%  38%  15%  22%  39%  
 4.12 Going Faster:  Instruction-Level Parallelism and Matrix Multiply 
351to the memory behavior. With very high data cache miss rates, mcf will dispatch 
many instructions during an incorrect speculation as lon
  cient reservation 
stations are available for the stalled memory references. When a branch among the 

many speculated instructions is
 nally mispredicted, the micro-ops corresponding 
to all these instructions will be
 ushed.
 e Intel Core i7 combines a 14-stage pipe
line and aggressive multiple issue to 
achieve high performance. By keeping the latencies for back-to-back operations 

low, the impact of data dependences is reduced. What are the most serious potential 

performance bottlenecks for programs running on this processo
 e following 
list includes some potential performance problems, the last three of which can 

apply in some form to any high-performance pipelined processor.
 e use of x86 instructions that do not map to a few simple micro-operations
 Branches that ar
  cult to predict, causing misprediction stalls and restarts 
when speculation fails
 Long dependences—typically caused by long-running instructions or the 
memory hierarchy
—that lead to stalls
 Performance delays arising in accessing memory (see Chapter 5) that cause 
the processor to stall
 4.12
  Going Faster:  Instruction-Level 
Parallelism and Matrix Multiply
Returning to the DGEMM example from Chapter 3, we can see the impact of 
instruction level parallelism by unrolling the loop so that the multiple issue, out-of-

order execution processor has more instructions to work with. 
Figure 4.80
 shows 

the unrolled version of Figure 3.23, which contains the C intrinsics to produce the 

AVX instructions. 
Like the unrolling example in 
Figure 4.71
 above, we are going to unroll the loop 
4 times. (We use the constant 
UNROLL in the C code to control the amount of 
unrolling in case we want to try other values.) Rather than manually unrolling the 

loop in C by making 4 copies of each of the intrinsics in Figure 3.23, we can rely 

on the gcc compiler to do the unrolling at –O3 optimization. We surround each 

intrinsic with a simple 
for loop that 4 iterations (lines 9, 14, and 20) and replace the 
scalar 
C0 in Figure 3.23 with a 4-element array 
c[] (lines 8, 10, 16, and 21).
Figure 4.81
 shows the assembly language output of the unrolled code. As 
expected, in 
Figure 4.81
 there are 4 versions of each of the AVX instructions in 

Figure 3.24, with one exception. We only need 1 copy of the 
vbroadcastsd Understanding 

Program 

Performance

352 Chapter 4 The Processor
instruction, since we can use the four copies of the B element in register 
%ymm0 repeatedly throughout the loop
 us, the 5 AVX instructions in Figure 3.24 
become 17 in 
Figure 4.81
, and the 7 integer instructions appear in both, although 
the constants and addressing changes to account for the unrolling. Hence, despite 

unrolling 4 times, the number of instructions in the body of the loop only doubles: 

from 12 to 24. 
Figure 4.82
 shows the performance increase DGEMM for 32x32 matrices in 
going from unoptimized to AVX and then to AVX with unrolling. Unrolling more 

than doubles performance, going from 6.4 GFLOPS to 14.6 GFLOPS. Optimizations 

for 
subword parallelism
 and 
instruction level parallelism
 result in an overall 
speedup of 8.8 versus the unoptimized DGEMM in Figure 3.21.
Elaboration: As mentioned in the Elaboration in Section 3.8, these results are with 
Turbo mode turned off. If we turn it on, like in Chapter 3 we improve all the results by the 
temporary increase in the clock rate of 3.3/2.6 = 1.27 to 2.1 GFLOPS for unoptimized 

DGEMM, 8.1 GFLOPS with AVX, and 18.6 GFLOPS with unrolling and AVX. As mentioned 

in Section 3.8, Turbo mode works particularly well in this case because it is using only 

a single core of an eight-core chip.1 #include <x86intrin.h>
2 #define UNROLL (
4)3
4 void dge
mm (int n, double* A, double* B, double* C)
5 {
6  for ( int i = 0; i < n; i+=UNROLL*
4 )7   for ( int j = 0; j < n; j++ ) {

8    __
m256d c[4];
9    for ( int x = 0; x < UNROLL; x++ )

10     c[x] = _
mm256_load_pd(C+i+x*4+j*n);11
12    for( int k = 0; k < n; k++ )

13    {
14     __
m256d b = _mm256_broadcast_sd(B+k+j*n);15     for (int x = 0; x < UNROLL; x++)

16     c[x] = _
mm256_add_pd(c[x],17      _
mm256_mul_pd(_mm256_load_pd(A+n*k+x*4+i), b));18    }
19
20    for ( int x = 0; x < UNROLL; x++ )

21     _
mm256_store_pd(C+i+x*4+j*n, c[x]);22    }
23  }FIGURE 4.80 Optimized C version of DGEMM using C intrinsics to generate the AVX subword-
parallel instructions for the x86 (Figure 3.23) and loop unrolling to create more opportunities for 

instruction-level parallelism. 
Figure 4.81
 shows the assembly language produced by the compiler for the inner 

loop, which unrolls the three for-loop bodies to expose instruction level parallelism.

 4.12 Going Faster:  Instruction-Level Parallelism and Matrix Multiply 
353Elaboration: There are no pipeline stalls despite the reuse of register %ymm5 in lines 9 to 17 Figure 4.81
 because the Intel Core i7 pipeline renames the registers. 
Are the following statements true or false?
 e Intel Core i7 uses a multiple-issue pipeline to directly execute x86 
instructions.
2. Both the A8 and the Core i7 use dynamic multiple issue.
 e Core i7 microarchitecture has many more registers than x86 requires.
 e Intel Core i7 uses less than half the pipeline stages of the earlier Intel 
Pentium 4 Prescott (see 
Figure 4.73
).Check Yourself
vmovapd (%r11),%ymm4# Load 4 elements of C into %ymm41mov    %rbx,%rax# register %rax = %rbx2xor    %ecx,%ecx# register %ecx = 03vmovapd 0x20(%r11),%ymm3# Load 4 elements of C into %ymm3
4vmovapd 0x40(%r11),%ymm2# Load 4 elements of C into %ymm2
5vmovapd 0x60(%r11),%ymm1# Load 4 elements of C into %ymm1
6vbroadcastsd (%rcx,%r9,1),%ymm0# Make 4 copies of B element
7add    $0x8,%rcx
# register %rcx = %rcx + 88vmulpd (%rax),%ymm0,%ymm5# Parallel mul %ymm1,4 A elements
9mm4vaddpd %ymm5,%ymm4,%ymm4# Parallel add %ymm5, %y
10vmulpd 0x20(%rax),%ymm0,%ymm5# Parallel mul %ymm1,4 A elements
11vaddpd %ymm5,%ymm3,%ymm3# Parallel add %ymm5, %ymm3
12vmulpd 0x40(%rax),%ymm0,%ymm5# Parallel mul %ymm1,4 A elements
13vmulpd 0x60(%rax),%ymm0,%ymm0# Parallel mul %ymm1,4 A elements
14add   %r8,%rax# register %rax = %rax + %r8
15cmp    %r10,%rcx# compare %r8 to %rax
16vaddpd %ymm5,%ymm2,%ymm2# Parallel add %ymm5, %ymm2
17vaddpd %ymm0,%ymm1,%ymm1# Parallel add %ymm0, %ymm1
18jne    68 <dgemm+0x68># jump if not %r8 != %rax
19add    $0x1,%esi# register %esi= %esi+ 1
20vmovapd %ymm4,(%r11)# Store %ymm4 into 4 C elements
21vmovapd %ymm3,0x20(%r11)# Store %ymm3 into 4 C elements
22vmovapd %ymm2,0x40(%r11)# Store %ymm2 into 4 C elements
23vmovapd %ymm1,0x60(%r11)# Store %ymm1 into 4 C elements
24FIGURE 4.81 The x86 assembly language for the body of the nested loops generated by compiling 
the unrolled C code in Figure 4.80
.
354 Chapter 4 The Processor
 4.13  Advanced Topic: An Introduction to 
Digital Design Using a Hardware Design 

Language to Describe and Model a 
Pipeline and More Pipelining IllustrationsModern digital design is done using hardware description languages and modern 
computer-aided synthesis tools that can create detailed hardware designs from the 

descriptions using both libraries and logic synthesis. Entire books are written on 

such languages and their use in digit
 is section, which appears online, 
gives a brief introduction and shows how a hardware design language, Verilog in 

this case, can be used to describe the MIPS control both behaviorally and in a 

form suitable for hardware synthesis. It then provides a series of behavioral models 

in Verilog of th
 ve-stage pipeline.
 e initial model ignores hazards, and 
additions to the model highlight the changes for forwarding, data hazards, and 

branch hazards.
We then provide about a dozen illustrations using the single-cycle graphical 
pipeline representation for readers who want to see more detail on how pipelines 

work for a few sequences of MIPS instructions.
4.13FIGURE 4.82 Performance of three versions of DGEMM for 32x32 matrices. 
Subword 
parallelism and instruction level parallelism has led to speedup of almost a factor of 9 over the unoptimized 
code in Figure 3.21.
Ð4.0unoptimized
1.76.414.6AVX
AVX+unroll
8.0GFLOPS12.016.0
4.13-2 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
  An Introduction to Digital Design Using a 
Hardware Design Language to Describe 
and Model a Pipeline and More Pipelining 
Illustrations is CD section covers hardware decription languages and then gives a dozen 
examples of pipeline diagrams, starting on page 4.13-18.
As mentioned in Appendix C, Verilog can describe processors for simulation 
or with the intention that the Verilog sp
 cation be synthesized. To achieve 
acceptable synthesis results in size and speed, and a behavioral sp
 cation 
intended for synthesis must carefully de
lineate the highly combinational portions 
of the design, such as a datapath, from the control.
 e datapath can then be 
synthesized using available libraries. A Verilog sp
 cation intended for synthesis 
is usually longer and more complex.
We start with a behavioral model of the 5-stage pipeline. To illustrate the 
dichotomy between behavioral and synthesizeable designs, we then give two 
Verilog descriptions of a multiple-cycle-per-instruction MIPS processor: one 

intended solely for simulations and one suitable for synthesis.
Using Verilog for Behavioral Speciﬁ
 cation with Simulation 
for the 5-Stage PipelineFigure 4.13.1 shows a Verilog behavioral description of the pipeline that handles 

ALU instructions as well as loads and stores. It does not accommodate branches 

(even incorrectly!), which we postpone including until later in the chapter.
Because Verilog lacks the ability t
 ne registers with na
 elds such as 
structures in C, we use several independent registers for each pipeline register. We 

name these registers with a pr
 x using the same convention; hence, IFIDIR is the 
IR portion of the IFID pipeline register.
 is version is a behavioral description not intended for synthesis. Instructions 
take the same number of clock cycles as our hardware design, but the control 

is done in a simpler fashion by repeatedly decodin
 elds of the instruction in 
each pipe stage. Because of th
 erence, the instruction register (IR) is needed 
throughout the pipeline, and the entire IR is passed from pipe stage to pipe stage. 

As you read the Verilog descriptions in this chapter, remember that the actions 

in the 
always block all occur in parallel on every clock cycle. Since there are 
no blocking assignments, the order of the events within the 
always block is 
arbitrary.
4.13
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-3FIGURE 4.13.1 A Verilog behavorial model for the MIPS ﬁ
 ve-stage pipeline, ignoring branch and data hazards.
 As in the 
design earlier in Chapter 4, we use separate instruction and data memories, which would be implemented using separate caches as
 we describe 
in Chapter 5. (
continues on next page
)module CPU (clock);  // Instruction opcodes   parameter LW = 6™b100011, SW = 6™b101011, BEQ = 6™b000100, no-op = 32™b00000_100000, ALUop = 6™b0;   input clock;
   reg[31:0] PC, Regs[0:31], IMemory[0:1023], DMemory[0:1023], // separate memories
            IFIDIR, IDEXA, IDEXB, IDEXIR, EXMEMIR, EXMEMB, // pipeline registers
            EXMEMALUOut, MEMWBValue, MEMWBIR; // pipeline registers
   wire [4:0] IDEXrs, IDEXrt, EXMEMrd, MEMWBrd, MEMWBrt; // Access register ˚ elds   wire [5:0] EXMEMop, MEMWBop, IDEXop; // Access opcodeswire [31:0] Ain, Bin; // the ALU inputs// These assignments de˚ ne ˚ elds from the pipeline registers   assign IDEXrs = IDEXIR[25:21];   // rs ˚ eld   assign IDEXrt = IDEXIR[20:16];   // rt ˚ eld   assign EXMEMrd = EXMEMIR[15:11]; // rd ˚ eld   assign MEMWBrd = MEMWBIR[15:11]; //rd ˚ eld   assign MEMWBrt = MEMWBIR[20:16]; //rt ˚ eld--used for loads    assign EXMEMop = EXMEMIR[31:26]; // the opcode    assign MEMWBop = MEMWBIR[31:26]; // the opcode 
   assign IDEXop = IDEXIR[31:26];   // the opcode    // Inputs to the ALU come directly from the ID/EX pipeline registers   assign Ain = IDEXA; 
   assign Bin = IDEXB;   reg [5:0] i; //used to initialize registers 
   initial begin         PC = 0;       IFIDIR = no-op; IDEXIR = no-op; EXMEMIR = no-op; MEMWBIR = no-op; // put no-ops in pipeline registers

       for (i=0;i<=31;i=i+1) Regs[i] = i; //initialize registers--just so they aren™t cares
   end   always @ (posedge clock) begin    // Remember that ALL these actions happen every pipe stage and with the use of <= they happen in parallel!

   // ˚ rst instruction  in the pipeline is being fetched          IFIDIR <= IMemory[PC>>2];           PC <= PC + 4;
      end // Fetch & increment PC      // second instruction in pipeline is fetching registers          IDEXA <= Regs[IFIDIR[25:21]]; IDEXB <= Regs[IFIDIR[20:16]]; // get two registers
         IDEXIR <= IFIDIR;  //pass along IR--can happen anywhere, since this affects next stage only!

      // third instruction is doing address calculation or ALU operation      if ((IDEXop==LW) |(IDEXop==SW))  // address calculation            EXMEMALUOut <= IDEXA +{{16{IDEXIR[15]}}, IDEXIR[15:0]}; 
         else if (IDEXop==ALUop) case (IDEXIR[5:0]) //case for the various R-type instructions
              32: EXMEMALUOut <= Ain + Bin;  //add operation              default: ; //other R-type operations: subtract, SLT, etc.         endcase
4.13-4 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
FIGURE 4.13.1 A Verilog behavorial model for the MIPS ﬁ
 ve-stage pipeline, ignoring branch and data hazards.
 (Continued
)Implementing Forwarding in Verilog
To further extend the Verilog model, Figure 4.13.2 shows the addition of forwarding 
logic for the case when the source and destination are ALU instructions. Neither 

load stalls nor branches are handled; we will add these shortly
 e changes from 
the earlier Verilog description are highlighted.
Someone has proposed moving the write for a result from an ALU instruction 
from the WB to the MEM stage, pointing out that this would reduce the maximum 

length of forwards from an ALU instruction by one cycle. Which of the following 

are accurate reasons 
not to
 consider such a change?
1. It would not actually change the forwarding logic, so it has no advantage.
2. It is impossible to implement this change under any circumstance since the 
write for the ALU result must stay in the same pipe stage as the write for a 
load result.
3. Moving the write for ALU instructions would create the possibility of writes 
occurring from tw
 erent instructions during the same clock cycle. Either 
an extra write port would be required on the regist
 le or a structural 
hazard would be created.
 e result of an ALU instruction is not available in time to do the write 
during MEM.
Check Yourself
      EXMEMIR <= IDEXIR; EXMEMB <= IDEXB; //pass along the IR & B register     //Mem stage of pipeline
     if (EXMEMop==ALUop) MEMWBValue <= EXMEMALUOut; //pass along ALU result
         else if (EXMEMop == LW) MEMWBValue <= DMemory[EXMEMALUOut>>2]; 
         else if (EXMEMop == SW) DMemory[EXMEMALUOut>>2] <=EXMEMB; //store 
        MEMWBIR <= EXMEMIR; //pass along IR

     // the WB stage
     if ((MEMWBop==ALUop) & (MEMWBrd != 0)) // update registers if ALU operation and destination not 0
         Regs[MEMWBrd] <= MEMWBValue; // ALU operation         else if ((EXMEMop == LW)& (MEMWBrt != 0)) // Update registers if load and destination not 0            Regs[MEMWBrt] <= MEMWBValue;   endendmodule
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-5module CPU (clock);parameter LW = 6™b100011, SW = 6™b101011, BEQ = 6™b000100, no-op = 32™b00000_100000, ALUop = 6™b0;
input clock;
   reg[31:0] PC, Regs[0:31], IMemory[0:1023], DMemory[0:1023], // separate memories
            IFIDIR, IDEXA, IDEXB, IDEXIR, EXMEMIR, EXMEMB, // pipeline registers
            EXMEMALUOut, MEMWBValue, MEMWBIR; // pipeline registers
   wire [4:0] IDEXrs, IDEXrt, EXMEMrd, MEMWBrd, MEMWBrt; //hold register ˚ elds   wire [5:0] EXMEMop, MEMWBop, IDEXop; Hold opcodes
   wire [31:0] Ain, Bin;// declare the bypass signals    wire bypassAfromMEM, bypassAfromALUinWB,bypassBfromMEM, bypassBfromALUinWB,
        bypassAfromLWinWB, bypassBfromLWinWB;    assign IDEXrs = IDEXIR[25:21];    assign IDEXrt = IDEXIR[15:11];    assign EXMEMrd = EXMEMIR[15:11]; 
   assign MEMWBrd = MEMWBIR[20:16]; assign EXMEMop = EXMEMIR[31:26]; 
   assign MEMWBrt = MEMWBIR[25:20]; 
   assign MEMWBop = MEMWBIR[31:26];  assign IDEXop = IDEXIR[31:26];   // The bypass to input A from the MEM stage for an ALU operation   assign bypassAfromMEM = (IDEXrs == EXMEMrd) & (IDEXrs!=0) & (EXMEMop==ALUop); // yes, bypass   // The bypass to input B from the MEM stage for an ALU operation   assign bypassBfromMEM = (IDEXrt == EXMEMrd)&(IDEXrt!=0) & (EXMEMop==ALUop); // yes, bypass   // The bypass to input A from the WB stage for an ALU operation   assign bypassAfromALUinWB =( IDEXrs == MEMWBrd) & (IDEXrs!=0) & (MEMWBop==ALUop);    // The bypass to input B from the WB stage for an ALU operation   assign bypassBfromALUinWB = (IDEXrt == MEMWBrd) & (IDEXrt!=0) & (MEMWBop==ALUop); /   // The bypass to input A from the WB stage for an LW operation   assign bypassAfromLWinWB =( IDEXrs == MEMWBIR[20:16]) & (IDEXrs!=0) & (MEMWBop==LW);    // The bypass to input B from the WB stage for an LW operation   assign bypassBfromLWinWB = (IDEXrt == MEMWBIR[20:16]) & (IDEXrt!=0) & (MEMWBop==LW);    // The A input to the ALU is bypassed from MEM if there is a bypass there,    // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Ain = bypassAfromMEM? EXMEMALUOut :
                (bypassAfromALUinWB | bypassAfromLWinWB)? MEMWBValue : IDEXA;   // The B input to the ALU is bypassed from MEM if there is a bypass there,    // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Bin = bypassBfromMEM? EXMEMALUOut :
                (bypassBfromALUinWB | bypassBfromLWinWB)? MEMWBValue: IDEXB;   reg [5:0] i; //used to initialize registers 
   initial begin         PC = 0; 
      IFIDIR = no-op; IDEXIR = no-op; EXMEMIR = no-op; MEMWBIR = no-op; // put no-ops in pipeline registers
       for (i = 0;i<=31;i = i+1) Regs[i] = i; //initialize registers--just so they aren™t cares
   end   always @ (posedge clock) begin       // ˚ rst instruction in the pipeline is being fetched          IFIDIR <= IMemory[PC>>2];           PC <= PC + 4;
      end // Fetch & increment PCFIGURE 4.13.2 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with bypassing to ALU operations and address 
calculations. e code added to Figure 4.13.1 to handle bypassing is highlighted. Because these bypasses only require changing where the 
ALU inputs come from, the only changes required are in the combinational logic responsible for selecting the ALU inputs. (
continues on next 
page
)
4.13-6 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
The Behavioral Verilog with Stall Detection
If we ignore branches, stalls for data hazards in the MIPS pipeline are co
 ned 
to one simple case: loads whose results are currently in the WB clock stage
 us, extending the Verilog to handle a load with a destination that is either an ALU 
instruction or a
 ective address calculation is reasonably straightforward, and 
Figure 4.13.3 shows the few additions needed.
Someone has asked about the possibility of data hazards occurring through 
memory, as opposed to through a register. Which of the following statements about 

such hazards are true?
1. Since memory accesses only occur in the MEM stage, all memory operations 
are done in the same order as instruction execution, making such hazards 

impossible in this pipeline.
2. Such hazards 
are possible in this pipeline; we just have not discussed them 
yet.
3. No pipeline can ever have a hazard involving memory, since it is the 
programmer’s job to keep the order of memory references accurate.
Check Yourself
      // second instruction is in register fetch          IDEXA <= Regs[IFIDIR[25:21]]; IDEXB <= Regs[IFIDIR[20:16]]; // get two registers
         IDEXIR <= IFIDIR;  //pass along IR--can happen anywhere, since this affects next stage only!

      // third instruction is doing address calculation or ALU operation
      if ((IDEXop==LW) |(IDEXop==SW))  // address calculation & copy B

EXMEMALUOut <= IDEXA +{{16{IDEXIR[15]}}, IDEXIR[15:0]}; 
else if (IDEXop==ALUop) case (IDEXIR[5:0]) //case for the various R-type instructions           32: EXMEMALUOut <= Ain + Bin;  //add operation
           default: ; //other R-type operations: subtract, SLT, etc.
          endcase      EXMEMIR <= IDEXIR; EXMEMB <= IDEXB; //pass along the IR & B register
      //Mem stage of pipeline       if (EXMEMop==ALUop) MEMWBValue <= EXMEMALUOut; //pass along ALU result
          else if (EXMEMop == LW) MEMWBValue <= DMemory[EXMEMALUOut>>2]; 
          else if (EXMEMop == SW) DMemory[EXMEMALUOut>>2] <=EXMEMB; //store        MEMWBIR <= EXMEMIR; //pass along IR      // the WB stage
      if ((MEMWBop==ALUop) & (MEMWBrd != 0)) Regs[MEMWBrd] <= MEMWBValue; // ALU operation
      else if ((EXMEMop == LW)& (MEMWBrt != 0)) Regs[MEMWBrt] <= MEMWBValue;   endendmoduleFIGURE 4.13.2 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with bypassing to ALU operations and address 
calculations. (Continued
)
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-7FIGURE 4.13.3 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with stalls for loads when the destination is an 
ALU instruction or effective address calculation.
 e changes from Figure 4.13.2 are highlighted. (
continues on next page
)module CPU (clock);parameter LW = 6™b100011, SW = 6™b101011, BEQ = 6™b000100, no-op = 32™b00000_100000, ALUop = 6™b0;
input clock;
   reg[31:0] PC, Regs[0:31], IMemory[0:1023], DMemory[0:1023], // separate memories
             IFIDIR, IDEXA, IDEXB, IDEXIR, EXMEMIR, EXMEMB, // pipeline registers
             EXMEMALUOut, MEMWBValue, MEMWBIR; // pipeline registers
   wire [4:0] IDEXrs, IDEXrt, EXMEMrd, MEMWBrd, MEMWBrt; //hold register ˚ elds   wire [5:0] EXMEMop, MEMWBop, IDEXop; Hold opcodes
   wire [31:0] Ain, Bin;// declare the bypass signals    wire stall, bypassAfromMEM, bypassAfromALUinWB,bypassBfromMEM, bypassBfromALUinWB,        bypassAfromLWinWB, bypassBfromLWinWB;    assign IDEXrs = IDEXIR[25:21];    assign IDEXrt = IDEXIR[15:11];    assign EXMEMrd = EXMEMIR[15:11]; 
   assign MEMWBrd = MEMWBIR[20:16]; assign EXMEMop = EXMEMIR[31:26];    
      assign MEMWBrt = MEMWBIR[25:20]; 
   assign MEMWBop = MEMWBIR[31:26];  assign IDEXop = IDEXIR[31:26];
   // The bypass to input A from the MEM stage for an ALU operation
   assign bypassAfromMEM = (IDEXrs == EXMEMrd) & (IDEXrs!=0) & (EXMEMop==ALUop); // yes, bypass
   // The bypass to input B from the MEM stage for an ALU operation
   assign bypassBfromMEM = (IDEXrt== EXMEMrd)&(IDEXrt!=0) & (EXMEMop==ALUop); // yes, bypass
   // The bypass to input A from the WB stage for an ALU operation
   assign bypassAfromALUinWB =( IDEXrs == MEMWBrd) & (IDEXrs!=0) & (MEMWBop==ALUop); 
   // The bypass to input B from the WB stage for an ALU operation
   assign bypassBfromALUinWB = (IDEXrt==MEMWBrd) & (IDEXrt!=0) & (MEMWBop==ALUop); /
   // The bypass to input A from the WB stage for an LW operation
   assign bypassAfromLWinWB =( IDEXrs ==MEMWBIR[20:16]) & (IDEXrs!=0) & (MEMWBop==LW); 
   // The bypass to input B from the WB stage for an LW operation
   assign bypassBfromLWinWB = (IDEXrt==MEMWBIR[20:16]) & (IDEXrt!=0) & (MEMWBop==LW);
   // The A input to the ALU is bypassed from MEM if there is a bypass there, 
   // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Ain = bypassAfromMEM? EXMEMALUOut :
                (bypassAfromALUinWB | bypassAfromLWinWB)? MEMWBValue : IDEXA;
   // The B input to the ALU is bypassed from MEM if there is a bypass there, 
   // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Bin = bypassBfromMEM? EXMEMALUOut :
                (bypassBfromALUinWB | bypassBfromLWinWB)? MEMWBValue: IDEXB;   // The signal for detecting a stall based on the use of a result from LW   assign stall = (MEMWBIR[31:26]==LW) && // source instruction is a load
        ((((IDEXop==LW)|(IDEXop==SW)) && (IDEXrs==MEMWBrd)) | // stall for address calc
   ((IDEXop==ALUop) && ((IDEXrs==MEMWBrd)|(IDEXrt==MEMWBrd)))); // ALU use   reg [5:0] i; //used to initialize registers    initial begin       PC = 0; 
    IFIDIR = no-op; IDEXIR = no-op; EXMEMIR = no-op; MEMWBIR = no-op; // put no-ops in pipeline registers
     for (i = 0;i<=31;i = i+1) Regs[i] = i; //initialize registers--just so they aren™t cares
   end   always @ (posedge clock) begin 
     if (~stall) begin // the ˚ rst three pipeline stages stall if there is a load hazard
4.13-8 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
FIGURE 4.13.3 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with stalls for loads when the destination is an 
ALU instruction or effective address calculation.
 (Continued
)4. Memory hazards may be possible in some pipelines, but they cannot occur 
in this particular pipeline.
5. Although the pipeline control would be obligated to maintain ordering 
among memory references to avoid hazards, it is impossible to design a 
pipeline where the references could be out of order.
Implementing the Branch Hazard Logic in Verilog
We can extend our Verilog behavioral model to implement the control for branches. 

We add the code to model branch equal using a “predict not taken” strategy. 

 e Verilog code is shown in Figure 4.13.4. It implements the branch hazard by 
detecting a taken branch in ID and using that signal to squash the instruction in 

IF (by setting the IR to 0, which is a
 ective 
no-op in MIPS-32); in addition, 
the PC is assigned to the branch target. Note that to prevent an unexpected latch, 

it is important that the PC is clearly assigned on every path through the always 

block; hence, we assign the PC in a single 
if statement. Lastly, note that although 
Figure 4.13.4 incorporates the basic logic for branches and control hazards, the 

incorporation of branches requires additional bypassing and data hazard detection, 

which we have not included.
      // ˚ rst instruction  in the pipeline is being fetched          IFIDIR <= IMemory[PC>>2];           PC <= PC + 4;         IDEXIR <= IFIDIR;  //pass along IR--can happen anywhere, since this affects next stage only!

      // second instruction is in register fetch        IDEXA <= Regs[IFIDIR[25:21]]; IDEXB <= Regs[IFIDIR[20:16]]; // get two registers      // third instruction is doing address calculation or ALU operation          if ((IDEXop==LW) |(IDEXop==SW))  // address calculation & copy B
                 EXMEMALUOut <= IDEXA +{{16{IDEXIR[15]}}, IDEXIR[15:0]}; 
         else if (IDEXop==ALUop) case (IDEXIR[5:0]) //case for the various R-type instructions
              32: EXMEMALUOut <= Ain + Bin;  //add operation
              default: ; //other R-type operations: subtract, SLT, etc.
            endcase
       EXMEMIR <= IDEXIR; EXMEMB <= IDEXB; //pass along the IR & B register
     end   else EXMEMIR <= no-op; /Freeze ˚ rst three stages of pipeline; inject a nop into the EX output      //Mem stage of pipeline       if (EXMEMop==ALUop) MEMWBValue <= EXMEMALUOut; //pass along ALU result
          else if (EXMEMop == LW) MEMWBValue <= DMemory[EXMEMALUOut>>2]; 
            else if (EXMEMop == SW) DMemory[EXMEMALUOut>>2] <=EXMEMB; //store        MEMWBIR <= EXMEMIR; //pass along IR
      // the WB stage
      if ((MEMWBop==ALUop) & (MEMWBrd != 0)) Regs[MEMWBrd] <= MEMWBValue; // ALU operation
      else if ((EXMEMop == LW)& (MEMWBrt != 0)) Regs[MEMWBrt] <= MEMWBValue;   endendmodule
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-9module CPU (clock);parameter LW = 6™b100011, SW = 6™b101011, BEQ = 6™b000100, no-op = 32™b0000000_0000000_0000000_0000000, ALUop = 
6™b0;input clock;   reg[31:0] PC, Regs[0:31], IMemory[0:1023], DMemory[0:1023], // separate memories
             IFIDIR, IDEXA, IDEXB, IDEXIR, EXMEMIR, EXMEMB, // pipeline registers
             EXMEMALUOut, MEMWBValue, MEMWBIR; // pipeline registers
   wire [4:0] IDEXrs, IDEXrt, EXMEMrd, MEMWBrd; //hold register ˚ elds   wire [5:0] EXMEMop, MEMWBop, IDEXop; Hold opcodes   wire [31:0] Ain, Bin;
   // declare the bypass signals 
   wire takebranch, stall, bypassAfromMEM, bypassAfromALUinWB,bypassBfromMEM, bypassBfromALUinWB,      bypassAfromLWinWB, bypassBfromLWinWB; 
   assign IDEXrs = IDEXIR[25:21];  assign IDEXrt = IDEXIR[15:11];  assign EXMEMrd = EXMEMIR[15:11]; 
   assign MEMWBrd = MEMWBIR[20:16]; assign EXMEMop = EXMEMIR[31:26];    
   assign MEMWBop = MEMWBIR[31:26];  assign IDEXop = IDEXIR[31:26];
   // The bypass to input A from the MEM stage for an ALU operation
   assign bypassAfromMEM = (IDEXrs == EXMEMrd) & (IDEXrs!=0) & (EXMEMop==ALUop); // yes, bypass
   // The bypass to input B from the MEM stage for an ALU operation
   assign bypassBfromMEM = (IDEXrt == EXMEMrd)&(IDEXrt!=0) & (EXMEMop==ALUop); // yes, bypass
   // The bypass to input A from the WB stage for an ALU operation
   assign bypassAfromALUinWB =( IDEXrs == MEMWBrd) & (IDEXrs!=0) & (MEMWBop==ALUop); 
   // The bypass to input B from the WB stage for an ALU operation
   assign bypassBfromALUinWB = (IDEXrt == MEMWBrd) & (IDEXrt!=0) & (MEMWBop==ALUop); /
   // The bypass to input A from the WB stage for an LW operation
   assign bypassAfromLWinWB =( IDEXrs == MEMWBIR[20:16]) & (IDEXrs!=0) & (MEMWBop==LW); 
   // The bypass to input B from the WB stage for an LW operation
   assign bypassBfromLWinWB = (IDEXrt == MEMWBIR[20:16]) & (IDEXrt!=0) & (MEMWBop==LW); 
   // The A input to the ALU is bypassed from MEM if there is a bypass there, 
   // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Ain = bypassAfromMEM? EXMEMALUOut :
               (bypassAfromALUinWB | bypassAfromLWinWB)? MEMWBValue : IDEXA;
   // The B input to the ALU is bypassed from MEM if there is a bypass there, 
   // Otherwise from WB if there is a bypass there, and otherwise comes from the IDEX register
   assign Bin = bypassBfromMEM? EXMEMALUOut :
               (bypassBfromALUinWB | bypassBfromLWinWB)? MEMWBValue: IDEXB;
   // The signal for detecting a stall based on the use of a result from LW
   assign stall = (MEMWBIR[31:26]==LW) && // source instruction is a load
         ((((IDEXop==LW)|(IDEXop==SW)) && (IDEXrs==MEMWBrd)) | // stall for address calc
((IDEXop==ALUop) && ((IDEXrs==MEMWBrd)|(IDEXrt==MEMWBrd)))); // ALU useFIGURE 4.13.4 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with stalls for loads when the destination is an 
ALU instruction or effective address calculation.
 e changes from Figure 4.13.2 are highlighted. (
continues on next page
)
4.13-10 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
FIGURE 4.13.4 A behavioral deﬁ
 nition of the ﬁ
 ve-stage MIPS pipeline with stalls for loads when the destination is an 
ALU instruction or effective address calculation.
 (Continued
)// Signal for a taken branch: instruction is BEQ and registers are equalassign takebranch = (IFIDIR[31:26]==BEQ) && (Regs[IFIDIR[25:21]]== Regs[IFIDIR[20:16]]); 
   reg [5:0] i; //used to initialize registers    initial begin  
      PC = 0; 
     IFIDIR = no-op; IDEXIR = no-op; EXMEMIR = no-op; MEMWBIR = no-op; // put no-ops in pipeline registers

      for (i = 0;i<=31;i = i+1) Regs[i] = i; //initialize registers--just so they aren™t don™t cares
   end   always @ (posedge clock) begin    if (~stall) begin // the ˚ rst three pipeline stages stall if there is a load hazard      if (~takebranch) begin     // ˚ rst instruction in the pipeline is being fetched normally          IFIDIR <= IMemory[PC>>2]; 
          PC <= PC + 4;      end else begin // a taken branch is in ID; instruction in IF is wrong; insert a no-op and reset the PC         IFDIR <= no-op; 
         PC <= PC + 4 + ({{16{IFIDIR[15]}}, IFIDIR[15:0]}<<2); 
         end       // second instruction is in register fetch        IDEXA <= Regs[IFIDIR[25:21]]; IDEXB <= Regs[IFIDIR[20:16]]; // get two registers      // third instruction is doing address calculation or ALU operation         IDEXIR <= IFIDIR;  //pass along IR
if ((IDEXop==LW) |(IDEXop==SW))  // address calculation & copy B
           EXMEMALUOut <= IDEXA +{{16{IDEXIR[15]}}, IDEXIR[15:0]}; 
       else if (IDEXop==ALUop) case (IDEXIR[5:0]) //case for the various R-type instructions
             32: EXMEMALUOut <= Ain + Bin;  //add operation
             default: ; //other R-type operations: subtract, SLT, etc.
            endcase
       EXMEMIR <= IDEXIR; EXMEMB <= IDEXB; //pass along the IR & B register
     end
   else EXMEMIR <= no-op; /Freeze ˚ rst three stages of pipeline; inject a nop into the EX output      //Mem stage of pipeline
       if (EXMEMop==ALUop) MEMWBValue <= EXMEMALUOut; //pass along ALU result
          else if (EXMEMop == LW) MEMWBValue <= DMemory[EXMEMALUOut>>2]; 
            else if (EXMEMop == SW) DMemory[EXMEMALUOut>>2] <=EXMEMB; //store       // the WB stageMEMWBIR <= EXMEMIR; //pass along IR
      if ((MEMWBop==ALUop) & (MEMWBrd != 0)) Regs[MEMWBrd] <= MEMWBValue; // ALU operation      else if ((EXMEMop == LW)& (MEMWBIR[20:16] != 0)) Regs[MEMWBIR[20:16]] <= MEMWBValue;   endendmodule
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-11Using Verilog for Behavioral Speciﬁ
 cation with Synthesis
To demonstate the contrasting types of Verilog, we show two descriptions of a 
 erent, nonpipelined implementation style of MIPS that uses multiple clock cycles 
per instruction. (Since some instructors make a synthesizable description of the MIPS 

pipe line project for a class, we chose not to include it here. It would also be long.)
Figure 4.13.5 gives a behavioral sp
 cation of a multicycle implementation 
of the MIPS processor. Because of the use of behavioral operations, it would be 

  cult to synthesize a separate datapath and control unit with any reasonable 
  ciency. 
 is version demonstrates another approach to the control by using a 
Meal
 nite-state machine (see discussion in Section C.10 of App
 e use of a Mealy machine, which allows the output to depend both on inputs and the 

current state, allows us to decrease the total number of states.
Since a version of the MIPS design intended for synthesis is considerably more 
complex, we have relied on a number of Verilog modules that were sp
 ed in 
Appendix B, including the following:
 e 4-to-1 multiplexor shown in Figure B.4.2, and the 3-to-1 multiplexor that 
can be trivially derived based on the 4-to-1 multiplexor.
 e MIPS ALU shown in Figure B.5.15.
 e MIPS ALU contro
 ned in Figure B.5.16.
 e MIPS regist
 le 
 ned in Figure B.8.11.
Now, let’s look at a Verilog version of the MIPS processor intended for synthesis. 
Figure 4.13.6 shows the structural version of the MIPS datapath. Figure 4.13.7 uses 

the datapath module to specify the MIPS CPU
 is version also demonstrates 
another approach to implementing the control unit, as well as some optimizations 

that rely on relationships between various control signals. Observe that the state 

machine sp
 cation only provides the sequencing actions.
 e setting of the control lines is done with a series of 
assign statements that 
depend on the state as well as the opco
 eld of the instruction register. If one 
were to fold the setting of the control into the state sp
 cation, this would look 
like a Mealy-sty
 nite-state control unit. Because the setting of the control lines 
is sp
 ed using 
assign statements outside of the 
always block, most logic 
synthesis systems will generate 
a small implementation o
 nite-state machine 
that determines the setting of the state register and then uses external logic to 

derive the control inputs to the datapath.
In writing this version of the control, we have also taken advantage of a number 
of insights about the relationship between various control signals as well as 

situations where we don’t care about the control signal value; some examples of 

these are given in the following elaboration.

4.13-12 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
module CPU (clock);parameter LW = 6™b100011, SW = 6™b101011, BEQ=6™b000100, J=6™d2;
input clock; //the clock is an external input
// The architecturally visible registers and scratch registers for implementationreg [31:0] PC, Regs[0:31], Memory [0:1023], IR, ALUOut, MDR, A, B;reg [2:0] state; // processor state 
wire [5:0] opcode; //use to get opcode easily
wire [31:0] SignExtend,PCOffset; //used to get sign-extended offset ˚ eldassign opcode = IR[31:26]; //opcode is upper 6 bits
assign SignExtend = {{16{IR[15]}},IR[15:0]}; //sign extension of lower 16 bits of instruction
assign PCOffset = SignExtend << 2; //PC offset is shifted
// set the PC to 0 and start the control in state 0initial begin PC = 0; state = 1; end//The state machine--triggered on a rising clockalways @(posedge clock) begin     Regs[0] = 0; //make R0 0 //shortcut way to make sure R0 is always 0
    case (state) //action depends on the state
      1: begin // ˚ rst step: fetch the instruction, increment PC, go to next state         IR <= Memory[PC>>2];            PC <= PC + 4;          state = 2; //next state      end      2: begin // second step: Instruction decode, register fetch, also compute branch address         A <= Regs[IR[25:21]];             B <= Regs[IR[20:16]]; 
         state = 3;
         ALUOut <= PC + PCOffset; // compute PC-relative branch target      end      3: begin // third step: Load-store execution, ALU execution, Branch completion           state = 4; // default next state           if ((opcode==LW) |(opcode==SW)) ALUOut <= A + SignExtend; //compute effective address
           else if (opcode==6™b0) case (IR[5:0]) //case for the various R-type instructions
             32: ALUOut = A + B; //add operation
             default: ALUOut = A; //other R-type operations: subtract, SLT, etc.
           endcaseFIGURE 4.13.5 A behavioral speciﬁ
 cation of the multicycle MIPS design.
 is has the same cycle behavior as the multicycle 
design, but is purely for simulation and sp
 cation. It cannot be used for synthesis. (
continues on next page
)
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-13          else if (opcode == BEQ) begin                      if (A==B) PC <= ALUOut; // branch taken--update PC 
                     state = 1; 
           end           else if (opocde=J) begin                 PC = {PC[31:28], IR[25:0],2™b00}; // the jump target PC
                state = 1; 
           end  //Jumps                   else ; // other opcodes or exception for unde˚ ned instruction would go here   end   4: begin        if (opcode==6™b0) begin //ALU Operation
             Regs[IR[15:11]] <= ALUOut; // write the result
             state = 1; 
       end //R-type ˚ nishes          else if (opcode == LW) begin // load instruction
              MDR <= Memory[ALUOut>>2]; // read the memory
              state = 5; // next state
           end                  else if (opcode == LW) begin                       Memory[ALUOut>>2] <= B; // write the memory
                      state = 1; // return to state 1 
                  end //store ˚ nishes                       else ; // other instructions go here          end   5: begin // LW is the only instruction still in execution         Regs[IR[20:16]] = MDR; // write the MDR to the register
         state = 1;
      end //complete an LW instruction
   endcase
end
endmoduleFIGURE 4.13.5 A behavioral speciﬁ
 cation of the multicycle MIPS design.
 (Continued
)
4.13-14 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
module Datapath (ALUOp, RegDst, MemtoReg, MemRead, MemWrite, IorD, RegWrite, IRWrite,PCWrite,   PCWriteCond, ALUSrcA, ALUSrcB, PCSource, opcode, clock); // the control inputs + clock
input [1:0] ALUOp, ALUSrcB, PCSource; // 2-bit control signals 
input RegDst, MemtoReg, MemRead, MemWrite, IorD, RegWrite, IRWrite, PCWrite, PCWriteCond,
ALUSrcA,    clock; // 1-bit control signals
output [5:0] opcode ;// opcode is needed as an output by control
reg [31:0] PC, Memory [0:1023], MDR,IR, ALUOut; // CPU state + some temporaries
wire [31:0] A,B,SignExtendOffset, PCOffset, ALUResultOut, PCValue, JumpAddr, Writedata, ALUAin, 
    ALUBin,MemOut; / these are signals derived from registers 
wire [3:0] ALUCtl; //. the ALU control lines
wire Zero; the Zero out signal from the ALU
wire[4:0] Writereg;// the signal used to communicate the destination register 
 initial PC = 0; //start the PC at 0//Combinational signals used in the datapath// Read using word address with either ALUOut or PC as the address sourceassign MemOut = MemRead ? Memory[(IorD ? ALUOut : PC)>>2]:0; 
assign opcode = IR[31:26];// opcode shortcut// Get the write register address from one of two ˚ elds depending on RegDstassign Writereg = RegDst ? IR[15:11]: IR[20:16];
// Get the write register data either from the ALUOut or from the MDRassign Writedata = MemtoReg ? MDR : ALUOut;// Sign-extend the lower half of the IR from load/store/branch offsetsassign SignExtendOffset = {{16{IR[15]}},IR[15:0]}; //sign-extend lower 16 bits;// The branch offset is also shifted to make it a word offsetassign PCOffset = SignExtendOffset << 2; // The A input to the ALU is either the rs register or the PCassign ALUAin = ALUSrcA ? A : PC; //ALU input is PC or A // Compose the Jump addressassign JumpAddr = {PC[31:28], IR[25:0],2™b00}; //The jump addressFIGURE 4.13.6 A Verilog version of the multicycle MIPS datapath that is appropriate for synthesis.
 is datapath relies 
on several units from Appendix B. Initial statements do not synth
esize, and a version used for synthesis would have to incorpor
ate a reset signal 
that had th
 ect. Also note that resetting 
R0 to 0 on every clock is not the best way to ensure that 
R0 stays 0; instead, modifying the register 
 le module to produce 0 whenever 
R0 is read and to ignore writes to 
R0 would be a mor
  cient solution. (
continues on next page
)
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-15// Creates an instance of the ALU control unit (see the module de˚ ned in Figure C.5.16 on page C-38   // Input ALUOp is control-unit set and used to describe the instruction class as in Chapter 4   // Input IR[5:0] is the function code ˚ eld for an ALU instruction   // Output ALUCtl are the actual ALU control bits as in Chapter 4ALUControl alucontroller (ALUOp,IR[5:0],ALUCtl); //ALU control unit// Creates a 3-to-1 multiplexor used to select the source of the next PC    // Inputs are ALUResultOut (the incremented PC) , ALUOut (the branch address), the jump target address   // PCSource is the selector input and PCValue is the multiplexor outputMult3to1 PCdatasrc (ALUResultOut,ALUOut,JumpAddr, PCSource , PCValue);  // Creates a 4-to-1 multiplexor used to select the B input of the ALU   //  Inputs are register B,constant 4, sign-extended lower half of IR, sign-extended lower half of IR << 2   // ALUSrcB is the selector input   // ALUBin is the multiplexor outputMult4to1 ALUBinput (B,32™d4,SignExtendOffset,PCOffset,ALUSrcB,ALUBin);  // Creates a MIPS ALU   // Inputs are ALUCtl (the ALU control), ALU value inputs (ALUAin, ALUBin)   // Outputs are ALUResultOut (the 32-bit output) and Zero (zero detection output)MIPSALU ALU (ALUCtl, ALUAin, ALUBin, ALUResultOut,Zero); //the ALU // Creates a MIPS register ˚ le   // Inputs are    // the rs and rt ˚ elds of the IR used to specify which registers to read,    // Writereg (the write register number), Writedata (the data to be written), RegWrite (indicates a 
write), the clock
// Outputs are A and B, the registers read
register˚ le regs (IR[25:21],IR[20:16],Writereg,Writedata,RegWrite,A,B,clock); //Register ˚ le// The clock-triggered actions of the datapathalways @(posedge clock) begin   if (MemWrite) Memory[ALUOut>>2] <= B; // Write memory--must be a store   ALUOut <= ALUResultOut; //Save the ALU result for use on a later clock cycle   if (IRWrite) IR <= MemOut; // Write the IR if an instruction fetch 
   MDR <= MemOut; // Always save the memory read value
   // The PC is written both conditionally (controlled by PCWrite) and unconditionally      if (PCWrite || (PCWriteCond & Zero)) PC <=PCValue; end endmoduleFIGURE 4.13.6 A Verilog version of the multicycle MIPS datapath that is appropriate for synthesis.
 
4.13-16 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
module CPU (clock);   parameter LW = 6™b100011, SW = 6™b101011, BEQ = 6™b000100, J = 6™d2; //constants 
   input clock; reg [2:0] state;
   wire [1:0] ALUOp, ALUSrcB, PCSource; wire [5:0] opcode; 
   wire RegDst, MemRead, MemWrite, IorD, RegWrite, IRWrite, PCWrite, PCWriteCond, 
        ALUSrcA, MemoryOp, IRWwrite, Mem2Reg; // Create an instance of the MIPS datapath, the inputs are the control signals; opcode is only outputDatapath MIPSDP (ALUOp,RegDst,Mem2Reg, MemRead, MemWrite, IorD, RegWrite,    IRWrite, PCWrite, PCWriteCond, ALUSrcA, ALUSrcB, PCSource, opcode, clock); initial begin state = 1; end // start the state machine in state 1// These are the de˚ nitions of the control signalsassign IRWrite = (state==1);assign Mem2Reg = ~ RegDst;
assign MemoryOp = (opcode==LW)|(opcode==SW); // a memory operation
assign ALUOp = ((state==1)|(state==2)|((state==3)&MemoryOp)) ? 2™b00 : // add
       ((state==3)&(opcode==BEQ)) ? 2™b01 : 2™b10; // subtract or use function code   assign RegDst = ((state==4)&(opcode==0)) ? 1 : 0;   assign MemRead = (state==1) | ((state==4)&(opcode==LW));
   assign MemWrite = (state==4)&(opcode==SW);   assign IorD = (state==1) ? 0 : (state==4) ? 1 : X;   assign RegWrite = (state==5) | ((state==4) &(opcode==0));   assign PCWrite = (state==1) | ((state==3)&(opcode==J));    assign PCWriteCond = (state==3)&(opcode==BEQ);
   assign ALUSrcA = ((state==1)|(state==2)) ? 0 :1;   assign ALUSrcB = ((state==1) | ((state==3)&(opcode==BEQ))) ? 2™b01 : (state==2) ? 2™b11 :          ((state==3)&MemoryOp) ? 2™b10 : 2™b00; // memory operation or other   assign PCSource = (state==1) ? 2™b00 : ((opcode==BEQ) ? 2™b01 : 2™b10); // Here is the state machine, which only has to sequence states   always @(posedge clock) begin // all state updates on a positive clock edge      case (state)
      1: state = 2;  //unconditional next state      2: state = 3;  //unconditional next state      3: // third step: jumps and branches complete
         state = ((opcode==BEQ) | (opcode==J)) ? 1 : 4;// branch or jump go back else next state
      4: state = (opcode==LW) ? 5 : 1; //R-type and SW ˚ nish      5: state = 1; // go back
       endcase end  endmoduleFIGURE 4.13.7 The MIPS CPU using the datapath from Figure 4.13.6.

 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-17Elaboration: When specifying control, designers often take advantage of knowledge 
of the control so as to simplify or shor
 cation. Here are a few 
 cation in Figures 4.13.6 and 4.13.7.
1. MemtoReg is set only in two cases, and then it is always the inverse of 
RegDst, so we just use the inverse of 
RegDst.2. IRWrite is set only in state 1.
 e ALU does not operate in every state and, when unused, can safely do 
anything.
4. RegDst is 1 in only one case and can otherwise be set to 0. In practice it 
might be better to set it explicitly when needed and otherwise set it to X, as 
we do for 
IorD. First, it allows additional logic optimization possibilities 
through the exploitation of don’t-care terms (see Appendix B for further 

discussion and examples). Second, it is a more precise sp
 cation, and 
this allows the simulation to more closely model the hardware, possibly 

uncovering additional errors in the sp
 cation.
More Illustrations of Instruction Execution on the 
Hardware
To reduce the cost of this book, in the third edition we moved sections an
 gures 
that were used by a minority of instructors online
 is subsection recaptures 
thos
 gures for readers who would like more supplemental material to better 
understand pipelinin
 ese are all single-clock-cycle pipeline diagrams, which 
take many
 gures to illustrate the execution of a sequence of instructions.
 e three examples are respectively for code with no hazards, an example of 
forwarding on the pipelined implementation, and an example of bypassing on the 
pipelined implementation.
No Hazard IllustrationsOn page 297, we gave the example code sequence
lw $10, 20($1)
sub $11, $2, $3

add $12, $3, $4

lw $13, 24($1)

add $14, $5, $6
Figures 4.43 and 4.44 showed the multiple-clock-cycle pipeline diagrams for this 
two-instruction sequence executing across six clock cycles. Figures 4.13.8 through 

4.13.10 show the corresponding single-clock-cycle pipeline diagrams for these two 

instructions. Note that the order of the instruction
 ers between these two types 
of diagrams: the newest instruction is at the 
bottom and to the right
 of the multiple-
clock-cycle pipeline diagram, and it is on the 
  in the single-clock-cycle pipeline 
diagram.

4.13-18 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
Instruction memory Address 4 32 Instruction IF/ID EX/MEM MEM/WB Add Add PC Registers Read data 1 Read data 2 Read register 1 Read register 2 16 Sign- extend Write 
register Write data ID/EX Instruction decode lw $10,20($1)Instruction fetch sub $11,$2,$3Instruction memory Address 4 32 Add Add result Shift left 2 Shift left 2 Instruction IF/ID EX/MEM PC Write 
data Registers Read data 1 Read data 2 Read register 1 Read register 2 16 Write register Write data Read data ALU result ALU Zero Add Add result ALU result ALU Zero ID/EX Instruction fetch lw $10,20($1)Address Data memory Write data Read data Address Data memory Clock 1 Clock 2 M u x 0 1 M u x 0 1 M u x 0 1 M u x 1 
0 M u x 1 0 M u x 0 1 Sign- extend MEM/WB FIGURE 4.13.8  Single-cycle pipeline diagrams for clock cycles 1 (top diagram) and 2 (bottom diagram). is style of 
pipeline representation is a snapshot of every instruction executing during one clock cycle. Our example has but two instructio
ns, so at most 
two stages are iden
 ed in each clock cycle; normally,
 ve stages are occupied
 e highlighted portions of the datapath are active in that 
clock cycle.
 e load is fetched in clock cycle 1 and decoded in clock cycle 2, with the subtract fetched in the second clock cycle. To make 
the 
 gures easier to understand, the other pipeline stages are empty, but normally there is an instruction in every pipeline stage.

 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-19InstructionmemoryAddress432InstructionIF/IDEX/MEMMEM/WB
AddAddRegistersReaddata 1Readdata 2Readregister 1Readregister 216Sign-extendSign-extendWrite
registerWritedataID/EXMemorylw $10,20($1)Executionsub $11,$2,$3InstructionmemoryAddress432AddAddresultShiftleft 2AddAddresultShiftleft 2InstructionIF/IDEX/MEMMEM/WBPCPCWritedataRegistersReaddata 1Readdata 2Readregister 1Readregister 216WriteregisterWritedataReaddataALUresultALUZeroALUresultALUZeroID/EXAddressDatamemoryWritedataReaddataAddressDatamemoryClock 3Clock 4Mux01Mux01Mux01Mux10Mux10Mux01Instruction decodesub $11,$2,$3Executionlw $10,20($1)FIGURE 4.13.9  Single-cycle pipeline diagrams for clock cycles 3 (top diagram) and 4 (bottom diagram). In the third 
clock cycle in the top diagram, 
lw enters the EX stage. At the same time, sub enters ID. In the fourth clock cycle (bottom datapath), 
lw moves 
into MEM stage, reading memory using the address found in EX/MEM at the beginning of clock cycle 4. At the same time, the ALU s
ubtracts 
and then places th
 erence into EX/MEM at the end of the clock cycle.

4.13-20 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
FIGURE 4.13.10 Single-cycle pipeline diagrams for clock cycles 5 (top diagram) and 6 (bottom diagram). In clock cycle 
5, lw completes by writing the data in MEM/WB into register 10, and sub sends th
 erence in EX/MEM to 
MEM/WB. In the next clock cycle, 
sub writes the value in MEM/WB to register 11.
InstructionmemoryAddress432InstructionIF/IDEX/MEMMEM/WBAddAddRegistersReaddata 1Readdata 2Readregister 1Readregister 216Sign-extendWriteregisterWritedataID/EXInstructionmemoryAddress432AddAddresultShiftleft 2Shiftleft 2InstructionIF/IDEX/MEMMEM/WB
WritedataRegistersReaddata 1Readdata 2Readregister 1Readregister 216WriteregisterWritedataReaddataALUresultALUZeroALUresultALUZeroID/EXAddressDatamemoryWritedataReaddataAddressDatamemoryClock 5Clock 6Mux01Mux0
1Mux01Mux1
0Mux0
1Mux01Memorysub $11, $2, $3Write backlw $10, 20($1)Write backsub $11, $2, $3Sign-extendAddAddresultPCPC
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-21FIGURE 4.13.11 Clock cycles 1 and 2. e phrase “
before <i>
” means the 
ith instruction before 
lw.  e lw instruction in the top datapath is in the IF stage. At the end of the clock cycle, the 
lw instruction is 
in the IF/ID pipeline registers. In the second clock cycle, seen in the bottom datapath, the 
lw moves to the 
ID stage, and 
sub enters in the IF stage. Note that the values of the instructio
 elds and the selected source 
registers are shown in the ID stage. Hence register 
$1 and the constant 20, the operands of 
lw, are written 
into the ID/EX pipeline register
 e number 10, representing the destination register number of 
lw, is also 
placed in ID/EX. Bits 15–11 are 0, but we use 
X to show that
 eld plays no role in a given instructio
 e top of the ID/EX pipeline register shows the control values for 
lw to be used in the remaining stag
 ese 
control values can be read from the 
lw row of the table in Figure 4.18.
Instruction[20Œ16]MemtoRegALUOpBranchRegDstALUSrc4Instruction
[15Œ0]ALUcontrolRegWriteMemReadControlInstruction[15Œ11]EXMWBMWBWBInstructionIF/IDEX/MEMID/EXID:before<1>EX:
before<2>MEM:
before<3>WB:
before<4>MEM/WBIF: lw $10,20($1)00000000000000000000000001PCWBEXMMemtoRegALUOpBranchRegDstALUSrc4ALUcontrolRegWriteMWBWBInstructionIF/IDID:lw $10,20($1)EX:
before<1>MEM:
before<2>WB:
before<3>MEM/WBIF:
sub $11,$2,$30101100010000000000000000PClwControlX1Instruction[20Œ16]Instruction[15Œ0]Instruction[15Œ11]20$X$110XMemWriteMemReadMemWriteClock 2Clock 1Mux01Mux01Mux10Mux01Mux01Mux01Mux0
1Mux0AddAddInstructionmemoryAddressInstructionmemoryAddressRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterWritedataWritedataWritedataReaddataALUresultALUZeroAddressDatamemoryWrite
dataReaddataAddressDatamemorySign-extendSign-extendX1020EX/MEMID/EXALUresultALUZeroShiftleft 2AddAddresultShiftleft 2AddAddresult
FIGURE 4.13.12  Clock cycles 3 and 4. In the top diagram, 
lw enters the EX stage in the third 
clock cycle, adding 
$1 and 20 to form the address in the EX/MEM pipeline register
 e lw instruction is 
written 
lw $10,. upon reaching EX, because the identity of instruction operands is not needed by EX 
or the subsequent stages. In this version of the pipeline, the actions of EX, MEM, and WB depend only on 
the instruction and its destination register or its target address.) At the same time, 
sub enters ID, reading 
registers 
$2 and 
$3, and the 
and instruction starts IF. In the fourth clock cycle (bottom datapath), 
lw moves 
into MEM stage, reading memory using the value in EX/MEM as the address. In the same clock cycle, the 

ALU subtracts 
$3 from 
$2 and places th
 erence into EX/MEM, reads registers 
$4 and 
$5 during ID, 
and the or instruction enters IF
 e two diagrams show the control signals being created in the ID stage and 
peeled o
  as they are used in subsequent pipe stages.
Instruction[20Œ16]MemtoRegALUOpBranchRegDstALUSrc4Instruction[15Œ0]Shiftleft 2RegWriteMemReadControlInstruction[15Œ11]EXMWBMWBWBInstructionIF/IDEX/MEMID/EXID:sub $11,$2,$3EX:
lw $10,...MEM:
before<1>WB:
before<2>MEM/WBIF:
and $12,$4,$500010110001011000100000001PCWBEXMMemtoRegALUOpBranchRegDstALUSrc4ALUcontrolALUcontrolShiftleft 2RegWriteWBWBInstructionIF/IDID: and $12,$4,$5EX: sub $11,...MEM: lw $10,...WB: before<1>MEM/WBIF: or $13,$6,$70001011000001010101110000PCandControl54Instruction[20Œ16]Instruction[15Œ0]Instruction
[15Œ11]X$5$4$3
$2X2010121011MemWriteMemReadMemWriteClock 4Clock 3Mux01Mux0
1Mux10Mux01Mux0
1Mux0
1Mux01Mux0AddAddInstructionmemoryAddressInstructionmemoryAddressRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterWritedataWritedataAddAddresultWritedataReaddataALUresultALUZeroAddAddresultAddressDatamemoryWrite
dataReaddataAddressDatamemorySign-extendSign-extend12XXEX/MEMID/EXALUresultALUZero23$2$1
$3XX11XX11M
Instruction[20Œ16]MemtoRegALUOpBranchRegDstALUSrc4Instruction
[15Œ0]Shiftleft 2RegWriteMemReadControlInstruction
[15Œ11]EXMWBMWBWBInstructionIF/IDorEX/MEMID/EXID:
or $13,$6,$7EX:
and $12,...MEM:
sub $11,...WB:
lw $10,..MEM/WBIF:
add $14,$8,$900010110000010101010000111PCWBEXMMemtoRegALUOpBranchRegDstALUSrc4ALUcontrolALUcontrolShiftleft 2RegWriteMWBWBInstructionIF/IDID:add $14,$8,$9EX:
or $13,...MEM:
and $12,...WB:
sub $11,.MEM/WBIF:
after<1>0001011000001010101000010PCaddControl9118Instruction[20Œ16]Instruction[15Œ0]Instruction[15Œ11]X$9$8$7
$6X111014112113MemWriteMemReadMemWriteClock 6Clock 5Mux01Mux01Mux1
0Mux0
1Mux01Mux01Mux0
1Mux0AddAddInstructionmemoryAddressInstructionmemoryAddressRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterWritedataWritedataAddAddresultWritedataReaddataALUresultALUZeroAddAddresultAddressDatamemoryWrite
dataReaddataAddressDatamemorySign-extendSign-extend12XXEX/MEMID/EXALUresultALUZero6710$6$4
$5$7XX13XX1312
FIGURE 4.13.13 Clock cycles 5 and 6. With 
add, th
 nal instruction in this example, entering 
IF in the top datapath, all instructions are engaged. By writing the data in MEM/WB into register 10, 
lw completes; both the data and the register number are in MEM/WB. In the same clock cycle, 
sub sends the 
 erence in EX/MEM to MEM/WB, and the rest of the instructions move forward. In the next clock cycle, 
sub selects the value in MEM/WB to write to register number 11, again found in MEM/WB
 e remaining 
instructions play follow-the-leader: the ALU calculates the OR of 
$6 and 
$7 for the 
or instruction in the 
EX stage, and registers 
$8 and 
$9 are read in the ID stage for the 
add instructio
 e instructions a
 er add are shown as inactive just to emphasize what occurs for th
 ve instructions in the example
 e phrase 
“a
 eri” means the 
ith instruction a
 er add.
4.13-24 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
Instruction[20Œ16]MemtoRegALUOpBranchRegDstALUSrc4Instruction
[15Œ0]ALUcontrolShiftleft 2RegWriteMemReadControlInstruction[15Œ11]EXMWBMWBWBInstructionIF/IDEX/MEMID/EXID:after<1>EX:
add $14,...MEM:
or $13,...WB:
and $12,.MEM/WBIF:
after<2>00000000000010101010000101PCWBEXMMemtoRegALUOpBranchRegDstALUSrc4ALUcontrolShiftleft 2RegWriteMWBWBInstructionIF/IDID:after<2>EX:
after<1>MEM:
add $14,...WB:
or $13,..MEM/WBIF:after<3>0000000000000000001000010PCControl13Instruction[20Œ16]Instruction
[15Œ0]Instruction
[15Œ11]3141MemWriteMemReadMemWriteClock 8Clock 7Mux01Mux01Mux10Mux01Mux01Mux01Mux01Mux0AddAddInstructionmemoryAddressInstructionmemoryAddressRegistersReaddata 1Readdata 2Readregister 1Readregister 2WriteregisterRegistersReaddata 1Readdata 2Read
register 1Readregister 2WriteregisterWritedataAddAddresultWritedataReaddataALUresultALUZeroAddAddresultAddressDatamemoryWritedataReaddataAddressDatamemorySign-extendSign-extendEX/MEMID/EXALUresultALUZero12$8$9142131Write
dataFIGURE 4.13.14  Clock cycles 7 and 8. In the top datapath, the 
add instruction brings up the rear, 
adding the values corresponding to registers 
$8 and 
$9 during the EX stage.
 e result of the 
or instruction 
is passed from EX/MEM to MEM/WB in the MEM stage, and the WB stage writes the result of the 
and instruction in MEM/WB to register 
$12. Note that the control signals are deasserted (set to 0) in the ID 
stage, since no instruction is being executed. In the following clock cycle (lower drawing), the WB stage 
writes the result to register 
$13, thereby completing 
or, and the MEM stage passes the sum from the 
add in EX/MEM to MEM/WB
 e instructions a
 er add are shown as inactive for pedagogical reasons.

 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-25WBEXMMemtoRegALUOpBranch
RegDstALUSrc4ALUcontrol
Shiftleft 2RegWrite
MWBWBInstruction
IF/IDID:after<3>EX:
after<2>MEM:
after<1>WB:
add $14,.MEM/WBIF:after<4>0000000000000000000000010PCControl
14Instruction
[20Œ16]Instruction
[15Œ0]Instruction
[15Œ11]14MemReadMemWrite
Clock 9
Mux01Mux01Mux10Mux01Add
Instructionmemory
AddressRegisters
Readdata 1Readdata 2Readregister 1Readregister 2Write
registerWrite
dataAdd
AddresultWrite
dataReaddataAddressDatamemory
Sign-extend
EX/MEMID/EXALUresultALUZeroFIGURE 4.13.15  Clock cycle 9. e WB stage writes the 
sum in MEM/WB into register 
$14, completing 
add and th
 ve-instruction 
sequence
 e instructions a
 er add are shown as inactive for pedagogical reasons.
More ExamplesTo understand how pipeline control works, let’s consider thes
 ve instructions 
going through the pipeline:
lw $10, 20($1)
sub $11, $2, $3

and $12, $4, $5

or $13, $6, $7

add $14, $8, $9
Figures 4.13.11 through 4.13.15 show these instructions proceeding through the 
nine clock cycles it takes them to complete execution, highlighting what is active 

in a stage and identifying the instruction associated with each stage during a clock 

cycle. If you examine them carefully, you may notice:
 In Figure 4.13.13 you can see the sequence of the destination register numbers 
fro
  to right at the bottom of the pipeline register
 e numbers advance 

4.13-26 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
to the right during each clock cycle, with the MEM/WB pipeline register 
supplying the number of the register written during the WB stage.
 When a stage is inactive, the values of control lines that are deasserted are 
shown as 0 or X (for don’t care).
 Sequencing of control is embedded in the pipeline structure itself. 
First, all instructions take the same number of clock cycles, so there is no special 

control for instruction duration. Second, all control information is computed 

during instruction decode and then passed along by the pipeline registers.
Forwarding Illustrations
We can use the single-clock-cycle pipeline diagrams to show how forwarding 

operates, as well as how the control activates the forwarding paths. Consider the 

following code sequence in which the dependences have been highlighted:
sub $2, $1, $3
and $4, $2, $5

or $4, $4, $2

add $9, $4, $2
Figures 4.13.16 and 4.13.17 show the events in clock cycles 3–6 in the execution of 
these instructions.
In clock cycle 4, the forwarding unit sees the writing by the 
sub instruction of 
register 
$2 in the MEM stage, while the and instruction in the EX stage is reading 
register 
$2 e forwarding unit selects the EX/MEM pipeline register instead of 
the ID/EX pipeline register as the upper input to the ALU to get the proper value 

for register 
$2 e following or instruction reads register $4, which is written by 
the 
and instruction, and register 
$2, which is written by the 
sub instruction.
 us, in clock cycle 5, the forwarding unit selects the EX/MEM pipeline register 
for the upper input to the ALU and the MEM/WB pipeline register for the lower 

input to the ALU
 e following 
add instruction reads both register 
$4, the target of 
the 
and instruction, and register 
$2, which the 
sub instruction has already written. 
Notice that the prior two instructions both write register 
$4, so the forwarding unit 
must pick the immediately preceding one (MEM stage).
In clock cycle 6, the forwarding unit thus selects the EX/MEM pipeline register, 
containing the result of the 
or instruction, for the upper ALU input but uses the 
nonforwarding register value for the lower input to the ALU.
Illustrating Pipelines with Stalls and Forwarding
We can use the single-clock-cycle pipeline diagrams to show how the control for 

stalls works. Figures 4.13.18 through 4.13.20 show the single-cycle diagram for 

clocks 2 through 7 for the following code sequence (dependences highlighted):
1w $2, 20($1)
and $4, $2,$5

or $4, $4,$2

add $9, $4,$2

PCInstructionmemory
Registers
MuxMuxMuxEXMWBWBDatamemory
MuxForwarding
unitInstruction
IF/IDand $4,$2,$5sub $2, $1, $3ID/EXbefore<1>
EX/MEMbefore<2>
MEM/WBor $4,$4,$2Clock 3
251010
$2$5524$1$3312Control
ALUMWBPCInstructionmemory
Registers
MuxMuxMuxEXMWBDatamemory
MuxForwarding
unitInstruction
IF/IDor $4,$4,$2and $4,$2,$5ID/EXsub $2,...EX/MEMbefore<1>
MEM/WBadd $9,$4,$2Clock 4
421010
10$4$2244$2$552 2 4Control
ALUMWBWBFIGURE 4.13.16 Clock cycles 3 and 4 of the instruction sequence on page 4.13-26.
 e bold lines are those active in a clock 
cycle, and the italicized register numbers in color indicate a hazard
 e forwarding unit is highlighted by shading it when it is forwarding data 
to the ALU
 e instructions before 
sub are shown as inactive just to emphasize what occurs for the four instructions in the example. Operand 
names are used in EX for control of forwarding; thus they are included in the instruction label for EX. Operand names are not n
eeded in MEM or WB, s. is used. Compare this with Figures 4.13.12 through 4.13.15, which show the datapath without forwarding where ID
 is the last 
stage to need operand information.
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-27
4.13-28 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
PCInstruction
memory
RegistersControlMuxDatamemory
MuxMuxMuxALUInstruction
IF/IDadd $9,$4,$2or $4,$4,$2ID/EXand $4,...EX/MEMsub $2,..MEM/WBafter<1>Clock 5
422424
2 94$2$4$2
$41010
101 2 4after<1>after<2>add $9,$4,$2or $4,...EX/MEMand $4,..MEM/WBID/EXEXWBMWBWBMForwarding
unitPCInstruction
memory
RegistersControlMuxMu
xMu
xDatamemory
MuALUxInstruction
IF/IDClock 6
44 29$2$4101014 4EXWBMWBWBMForwarding
unitFIGURE 4.13.17 Clock cycles 5 and 6 of the instruction sequence on page 4.13-26.
 e forwarding unit is highlighted when 
it is forwarding data to the ALU
 e two instructions a
 er add are shown as inactive just to emphasize what occurs for the four instructions 
in the example
 e bold lines are those active in a clock cycle, and the italicized register numbers in color indicate a hazard.

Registers
Instruction
ID/EX25Control
PCInstructionmemory
PCInstructionmemory
Hazard
detectionunit0MuxIF/IDWrite
PCWrite
IF/IDWrite
PCWrite
ID/EX.RegisterRtbefore<3>
Registers
MuxMuxEXMWBMWBDatamemory
MuxInstruction
IF/IDlw $2,20($1)ID/EXbefore<2>
EX/MEMMEM/WBClock 2
11XX11$1$XX21Control
ALUWBlw $2,20($1)before<1>before<2>
or $4,$4,$2and $4,$2,$5
and $4,$2,$5Clock 3
MuxMuxMuxEXMWBMWBDatamemory
MuxForwarding
unitForwarding
unitEX/MEMMEM/WB0011
$1$XX12$5$22 5542 ALUWBHazard
detectionunit0MuxID/EX.RegisterRtbefore<1>
ID/EX.MemReadID/EX.MemReadMuxIF/IDFIGURE 4.13.18  Clock cycles 2 and 3 of the instruction sequence on page 4.13-26 with a load replacing 
sub e bold 
lines are those active in a clock cycle, the italicized register numbers in color indicate a hazard, and th. in the place
 of operands means that 
their identity is information not needed by that stage
 e values of th
 cant control lines, registers, and register numbers are labeled in 
th
 gures. 
 e and instruction wants to read the value created by the 
lw instruction in clock cycle 3, so the hazard detection unit stalls the 
and and 
or instructions. Hence, the hazard detection unit is highlighted.
 4.13 An Introduction to Digital Design Using a Hardware Design Language 
4.13-29
4.13-30 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe
RegistersInstructionID/EX422ControlPCInstructionmemoryPCInstructionmemoryHazarddetectionunit0MuxIF/IDWritePCWriteIF/IDWritePCWriteID/EX.RegisterRtbefore<1>RegistersMuxMuxEXMWBMWBDatamemoryMuxInstructionIF/IDand $4,$2,$5ID/EXlw $2,...   EX/MEMMEM/WBClock 422551000
11$2$5542$2$554
22ControlALUWBand $4,$2,$5Bubble lw $2,...add $9,$4,$2or $4,$4,$2
or $4,$4,$2Clock 5MuxMuxMuxEXMWBMWBDatamemoryMuxForwardingunitForwardingunitEX/MEMMEM/WB1010
112 0$2$5544$2$42 5242 ALUWBHazarddetectionunit0MuxID/EX.RegisterRtBubble ID/EX.MemReadID/EX.MemReadMuxIF/IDFIGURE 4.13.19  Clock cycles 4 and 5 of the instruction sequence on page 4.13-26 with a load replacing 
sub e bubble is inserted in the pipeline in clock cycle 4, and then the 
and instruction is allowed to proceed in clock cyc
 e forwarding unit 
is highlighted in clock cycle 5 because it is forwarding data from 
lw to the ALU. Note that in clock cycle 4, the forwarding unit forwards the 
address of the 
lw as if it were the contents of register 
$2; this is rendered harmless by the insertion of the bubble
 e bold lines are those active 
in a clock cycle, and the italicized register numbers in color indicate a hazard.

 4.13 An Introduction to Digital Design Using a Hardware Design Language to Describe 4.13-
31Registers
Instruction
ID/EX4Control
PCInstructionmemory
PCInstructionmemory
Hazard
detectionunit0MuxIF/IDWrite
PCWrite
IF/IDWrite
PCWrite
ID/EX.RegisterRtBubble Registers
MuxMuxEXMWBMWBDatamemory
MuxInstruction
IF/IDadd $9,$4,$2ID/EXand $4,...EX/MEMMEM/WBClock 6
44221010
100$4$2294$4$224
44 Control
ALUWBadd $9,$4,$2or $4,...and $4,...
after<2>after<1>
after<1>Clock 7
MuxMuxMuxEXMWBMWBDatamemory
MuxForwarding
unitForwarding
unitEX/MEMMEM/WB1010144 10$4$2294 ALUWBHazard
detectionunit0MuxID/EX.RegisterRtor $4,$4,$2ID/EX.MemReadID/EX.MemReadMuxIF/IDFIGURE 4.13.20 Clock cycles 6 and 7 of the instruction sequence on page 4.13-26 with a load replacing 
sub. Note that 
unlike in Figure 4.13.17, the stall allows the 
lw to complete, and so there is no forwarding from MEM/WB in clock cycle 6. Register 
$4 for the 
add in the EX stage still depends on the result from or in EX/MEM, so the forwarding unit passes the result to the ALU.
 e bold lines show 
ALU input lines active in a clock cycle, and the italicized register numbers indicate a hazard
 e instructions a
 er add are shown as inactive 
for pedagogical reasons.

 4.14 Fallacies and Pitfalls 
355 4.14 Fallacies and Pitfalls
Fallacy: Pipelining is easy.
Our books testify to the subtlety of correct pipeline execution. Our advanced book 
had a pipeline bug in i
 rst edition, despite its being reviewed by more than 100 
people and being class-tested at 18 universit e bug was uncovered only when 

someone tried to build the computer in that boo
 e fact that the Verilog to 
describe a pipeline like that in the Intel Core i7 will be many thousands of lines is 

an indication of the complexity. Beware!
Fallacy: Pipelining ideas can be implemented independent of technology.
When the number of transistors on-chip and the speed of transistors made a 

 ve-stage pipeline the best solution, then the delayed branch (see the 
Elaboration
 on page 255) was a simple solution to control hazards. With longer pipelines, 

superscalar execution, and dynamic branch prediction, it is now redundant. In 

the early 1990s, dynamic pipeline scheduling took too many resources and was 

not required for high performance, but as transistor budgets continued to double 

due to 
Moore’s Law
 and logic became much faster than memory, then multiple 

functional units and dynamic pipelining made more sense. Today, concerns about 

power are leading to less aggressive designs.
Pitfall: Failure to consider instruction set design can adversely impact pipelining.
Many of th
  culties of pipelining arise because of instruction set complications. 
Here are some examples:
 Widely variable instruction lengths and running times can lead to imbalance 
among pipeline stages and severely complicate hazard detection in a design 

pipelined at the instruction set level
 is problem was overcome, initially 
in the DEC VAX 8500 in the late 1980s, using the micro-operations and 

micropipelined scheme that the Intel Core i7 employs today. Of course, the 

overhead of translation and maintaining correspondence between the micro-

operations and the actual instructions remains.
 Sophisticated addressing modes can lead t
 erent sorts of problems. 
Addressing modes that update registers complicate hazard detection. Other 

addressing modes that require multiple memory accesses substantially 

complicate pipeline control and make i
  cult to keep the pip
 owing 
smoothly.
 Perhaps the best example is the DEC Alpha and the DEC NVAX. In 
comparable technology, the newer instruction set architecture of the Alpha 

allowed an implementation whose performance is more than twice as fast 

as NVAX. In another example, Bhandarkar and Clark [1991] compared the 

MIPS M/2000 and the DEC VAX 8700 by counting clock cycles of the SPEC 

benchmarks; they concluded that although the MIPS M/2000 executes more 

356 Chapter 4 The Processor
instructions, the VAX on average executes 2.7 times as many clock cycles, so 
the MIPS is faster.
 4.15 Concluding Remarks
As we have seen in this chapter, both the datapath and control for a processor can be 

designed starting with the instruction set architecture and an understanding of the 

basic characteristics of the technology. In Section 4.3, we saw how the datapath for 

a MIPS processor could be constructed based on the architecture and the decision 

to build a single-cycle implementation. Of course, the underlying technology also 

 ects many design decisions by dictating what components can be used in the 
datapath, as well as whether a single-cycle implementation even makes sense.
Pipelining
 improves throughput but not the inherent execution time, or 
instruction latency
, of instructions; for some instructions, the latency is similar 
in length to the single-cycle approach. Multiple instruction issue adds additional 
datapath hardware to allow multiple instructions to begin every clock cycle, but at 

an increas
 ective latency. Pipelining was presented as reducing the clock cycle 
time of the simple single-cycle datapath. Multiple instruction issue, in comparison, 

clearly focuses on reducing 
clock cycles per instruction
 (CPI).
Pipelining and multiple issue both attempt to exploit instruction-level 
pa
 e presence of data and control dependences, which can become 
hazards, are the primary limitations on how much parallelism can be exploited. 

Scheduling and speculation via 
prediction
, both in hardware and in so
 ware, are 
the primary techniques used to reduce the performance impact of dependences. 
We showed that unrolling the DGEMM loop four times exposed more 
instructions that could take advantage of the out-of-order execution engine of the 

Core i7 to more than double performance.
 e switch to longer pipelines, multiple instruction issue, and dynamic 
scheduling in the mid-1990s has helped sustain the 60% per year processor 

performance increase that started in the early 1980s. As mentioned in Chapter 

1, these microprocessors preserved the sequential programming model, but 

they eventually ran into the power wall
 us, the industry has been forced to 
switch to multiprocessors, which exploit parallelism at much coarser levels (the 

subject of Chapt
 is trend has also caused designers to reassess the energy-
performance implications of some of the inventions since the mid-1990s, resulting 

in a simp
 cation of pipelines in the more recent versions of microarchitectures.
To sustain the advances in processing performance via parallel processors, 
Amdahl’s law suggests that another part 
of the system will become the bottleneck. 
 at bottleneck is the topic of the next chapter: the 
memory hierarchy
.instruction latency
 e inherent execution time 
for an instruction.
Nine-tenths of wisdom 
consists of being wise 

in time.
American proverb

 4.17 Exercises 357 4.16  Historical Perspective and Further 
Reading is section, which appears online, discusses the history of th
 rst pipelined 
processors, the earliest superscalars, and the development of out-of-order and 
speculative techniques, as well as important developments in the accompanying 

compiler technology.
 4.17 Exercises4.1 Consider the following instruction:
Instruction:  
AND Rd,Rs,RtInterpretation:  
Reg[Rd] = Reg[Rs] AND Reg[Rt]4.1.1 [5] <§4.1> What are the values of control signals generated by the control in 

Figure 4.2
 for the above instruction?
4.1.2 [5] <§4.1> Which resources (blocks) perform a useful function for this 
instruction?
4.1.3 [10] <§4.1> Which resources (blocks) produce outputs, but their outputs 
are not used for this instruction? Which resources produce no outputs for this 

instruction?
4.2  e basic single-cycle MIPS implementation in 
Figure 4.
2 can only implement 
some instructions. New instructions can 
be added to an existing Instruction Set 
Architecture (ISA), but the decision whether or not to do that depends, among 
other things, on the cost and complexity the proposed addition introduces into the 

processor datapath and control.
 e 
 rst three problems in this exercise refer to the 
new instruction:
Instruction:  
LWI Rt,Rd(Rs)Interpretation:  
Reg[Rt] = Mem[Reg[Rd]+Reg[Rs]]4.2.1 [10] <§4.1> Which existing blocks (if any) can be used for this instruction?

4.2.2 [10] <§4.1> Which new functional blocks (if any) do we need for this 
instruction?
4.2.3 [10] <§4.1> What new signals do we need (if any) from the control unit to 
support this instruction?
4.16
358 Chapter 4 The Processor
4.3 When processor designers consider a possible improvement to the processor 
datapath, the decision usually depends on the cost/performance trade-o
 . In 
the following three problems, assume that we are starting with a datapath from 
Figure 4.2
, where I-Mem, Add, Mux, ALU, Regs, D-Mem, and Control blocks have 

latencies of 400 ps, 100 ps, 30 ps, 120 ps, 200 ps, 350 ps, and 100 ps, respectively, 

and costs of 1000, 30, 10, 100, 200, 2000, and 500, respectively.
Consider the addition of a multiplier to the ALU
 is addition will add 300 ps to the 
latency of the ALU and will add a cost of 600 to the ALU
 e result will be 5% fewer 
instructions executed since we will no longer need to emulate the MUL instruction.

4.3.1 [10] <§4.1> What is the clock cycle time with and without this improvement?

4.3.2 [10] <§4.1> What is the speedup achieved by adding this improvement?

4.3.3 [10] <§4.1> Compare the cost/performance ratio with and without this 
improvement.
4.4 Problems in this exercise assume that logic blocks needed to implement a 
processor’s datapath have the following latencies:
I-Mem  AddMuxALURegsD-MemSign-ExtendShift-Left-2
200ps70ps20ps90ps90ps250ps15ps
10ps4.4.1 [10] <§4.3> If the only thing we need to do in a processor is fetch consecutive 
instructions (
Figure 4.6
), what would the cycle time be?
4.4.2 [10] <§4.3> Consider a datapath similar to the one in 
Figure 4.11
, but for a 
processor that only has one type of instruction: unconditional PC-relative branch. 

What would the cycle time be for this datapath?
4.4.3 [10] <§4.3> Repeat 4.4.2, but this time we need to support only conditional 
PC-relative branches.
 e remaining three problems in this exercise refer to the datapath element S
 - -2:4.4.4 [10] <§4.3> Which kinds of instructions require this resource?

4.4.5 [20] <§4.3> For which kinds of instructions (if any) is this resource on the 
critical path?
4.4.6 [10] <§4.3> Assuming that we only support 
beq and 
add instructions, 
discuss how changes in the given latency of this resource a
 ect the cycle time of the 
processor. Assume that the latencies of other resources do not change.

 4.17 Exercises 3594.5 For the problems in this exercise, assume that there are no pipeline stalls and 
that the breakdown of executed instructions is as follows:
add  addinotbeqlwsw
20%20%0%25%25%10%
4.5.1 [10] <§4.3> In what fraction of all cycles is the data memory used?
4.5.2 [10] <§4.3> In what fraction of all cycles is the input of the sign-extend 
circuit needed? What is this circuit doing in cycles in which its input is not needed?
4.6 When silicon chips are fabricated, defects in materials (e.g., silicon) and 
manufacturing errors can result in defective circuits. A very common defect is for 
one wire to
 ect the signal in another.
 is is called a cross-talk fault. A special 
class of cross-talk faults is when a signal is connected to a wire that has a constant 

logical value (e.g., a power supply wire). In this case we have a stuck-at-0 or a stuck-

at-1 fault, and the a
 ected signal always has a logical value of 0 or 1, respectively.  
 e following problems refer to bit 0 of the Write Register input on the regist
 le in Figure 4.24
.4.6.1 [10] <§§4.3, 4.4> Let us assume that processor testing is done by
 lling the 
PC, registers, and data and instruction memories with some values (you can choose 
which values), letting a single instruction execute, then reading the PC, memories, 

and register
 ese values are then examined to determine if a particular fault is 
present. Can you design a test (values for PC, memories, and registers) that would 

determine if there is a stuck-at-0 fault on this signal?
4.6.2 [10] <§§4.3, 4.4> Repeat 4.6.1 for a stuck-at-1 fault. Can you use a single 
test for both stuck-at-0 and stuck-at-1? If yes, explain how; if no, explain why not.
4.6.3 [60] <§§4.3, 4.4> If we know that the processor has a stuck-at-1 fault on 
this signal, is the processor still usable? To be usable, we must be able to convert 

any program that executes on a normal MIPS processor into a program that works 

on this processor. You can assume that there is enough free instruction memory 

and data memory to let you make the program longer and store additional 

data. Hint: the processor is usable if every instruction “broken” by this fault can 

be replaced with a sequence of “working” instructions that achieve the same 

 ect.4.6.4 [10] <§§4.3, 4.4> Repeat 4.6.1, but now the fault to test for is whether 

the “MemRead” control signal becomes 0 if 
RegDst control signal is 0, no fault 
otherwise.
4.6.5 [10] <§§4.3, 4.4> Repeat 4.6.4, but now the fault to test for is whether the 
“Jump” control signal becomes 0 if RegDst control signal is 0, no fault otherwise.

360 Chapter 4 The Processor
4.7 In this exercise we examine in detail how an instruction is executed in a 
single-cycle datapath. Problems in this exercise refer to a clock cycle in which the 
processor fetches the following instruction word:
10101100011000100000000000010100.Assume that data memory is all zeros and that the processor’s registers have the 

following values at the beginning of the cycle in which the above instruction word 

is fetched:
r0  r1r2r3r4r5r6r8r12r31
0–12–3–410682–16
4.7.1 [5] <§4.4> What are the outputs of the sign-extend and the jump “S
  
  2” unit (near the top of
 Figure 4.24
) for this instruction word?
4.7.2 [10] <§4.4> What are the values of the ALU control unit’s inputs for this 
instruction?
4.7.3 [10] <§4.4> What is the new PC address a
 er this instruction is executed? 
Highlight the path through which this value is determined.

4.7.4 [10] <§4.4> For each Mux, show the values of its data output during the 
execution of this instruction and these register values.
4.7.5 [10] <§4.4> For the ALU and the two add units, what are their data input 
values?
4.7.6 [10] <§4.4> What are the values of all inputs for the “Registers” unit?
4.8 In this exercise, we examine how pipelining a
 ects the clock cycle time of the 
processor. Problems in this exercise assume that individual stages of the datapath 
have the following latencies:
IFIDEXMEMWB250ps350ps150ps300ps200ps
Also, assume that instructions executed by the processor are broken down as 

follows:
alu  beqlw
sw45%20%20%15%
4.8.1 [5] <§4.5> What is the clock cycle time in a pipelined and non-pipelined 

processor?
4.8.2 [10] <§4.5> What is the total latency of an LW instruction in a pipelined 
and non-pipelined processor?

 4.17 Exercises 3614.8.3 [10] <§4.5> If we can split one stage of the pipelined datapath into two new 
stages, each with half the latency of the original stage, which stage would you split 

and what is the new clock cycle time of the processor?
4.8.4 [10] <§4.5> Assuming there are no stalls or hazards, what is the utilization 
of the data memory?
4.8.5 [10] <§4.5> Assuming there are no stalls or hazards, what is the utilization 
of the write-register port of the “Registers” unit?
4.8.6 [30] <§4.5> Instead of a single-cycle organization, we can use a multi-cycle 
organization where each instruction takes multiple cycles but one instruction 

 nishes before another is fetched. In this organization, an instruction only goes 
through stages it actually needs (e.g., ST only takes 4 cycles because it does not 

need the WB stage). Compare clock cycle times and execution times with single-

cycle, multi-cycle, and pipelined organization.
4.9 In this exercise, we examine how data dependences a
 ect execution in the 
basic 5-stage pipeline described in Section 4.5. Problems in this exercise refer to the 
following sequence of instructions:
or r1,r2,r3or r2,r1,r4
or r1,r1,r2Also, assume the following cycle times for each of the options related to forwarding:
Without Forwarding With Full ForwardingWith ALU-ALU Forwarding Only
250ps300ps290ps4.9.1 [10] <§4.5> Indicate dependences and their type.
4.9.2 [10] <§4.5> Assume there is no forwarding in this pipelined processor. 
Indicate hazards and add 
nop instructions to eliminate them.
4.9.3 [10] <§4.5> Assume there is full forwarding. Indicate hazards and add 
NOP instructions to eliminate them.
4.9.4 [10] <§4.5> What is the total execution time of this instruction sequence 
without forwarding and with full forwarding? What is the speedup achieved by 

adding full forwarding to a pipeline that had no forwarding?
4.9.5 [10] <§4.5> Add 
nop instructions to this code to eliminate hazards if there 
is ALU-ALU forwarding only (no forwarding from the MEM to the EX stage).

4.9.6 [10] <§4.5> What is the total execution time of this instruction sequence 
with only ALU-ALU forwarding? What is the speedup over a no-forwarding 

pipeline?

362 Chapter 4 The Processor
4.10 In this exercise, we examine how resource hazards, control hazards, and 
Instruction Set Architecture (ISA) design can a
 ect pipelined execution. Problems 
in this exercise refer to the following fragment of MIPS code:
    sw  r16,12(r6)    lw  r16,8(r6)
    beq r5,r4,Label # Assume r5!=r4
    add r5,r1,r4
    slt r5,r15,r4Assume that individual pipeline stages have the following latencies:
IF  IDEXMEMWB
200ps120ps150ps190ps100ps
4.10.1 [10] <§4.5> For this problem, assume that all branches are perfectly 
predicted (this eliminates all control hazards) and that no delay slots are used. If we 

only have one memory (for both instructions and data), there is a structural hazard 

every time we need to fetch an instruction in the same cycle in which another 

instruction accesses data. To guarantee forward progress, this hazard must always 

be resolved in favor of the instruction that accesses data. What is the total execution 

time of this instruction sequence in the 5-stage pipeline that only has one memory? 

We have seen that data hazards can be eliminated by adding 
nops to the code. Can 
you do the same with this structural hazard? Why?
4.10.2 [20] <§4.5> For this problem, assume that all branches are perfectly 
predicted (this eliminates all control hazards) and that no delay slots are used. 

If we change load/store instructions to use a register (without an o
 set) as the 
address, these instructions no longer need to use the ALU. As a result, MEM and 

EX stages can be overlapped and the pipeline has only 4 stages. Change this code to 

accommodate this changed ISA. Assuming this change does not a
 ect clock cycle 
time, what speedup is achieved in this instruction sequence?
4.10.3 [10] <§4.5> Assuming stall-on-branch and no delay slots, what speedup is 
achieved on this code if branch outcomes are determined in the ID stage, relative to 

the execution where branch outcomes are determined in the EX stage?
4.10.4 [10] <§4.5> Given these pipeline stage latencies, repeat the speedup 
calculation from 4.10.2, but take into account the (possible) change in clock cycle 

time. When EX and MEM are done in a single stage, most of their work can be 

done in parallel. As a result, the resulting EX/MEM stage has a latency that is the 

larger of the original two, plus 20 ps needed for the work that could not be done 

in parallel.
4.10.5 [10] <§4.5> Given these pipeline stage latencies, repeat the speedup 
calculation from 4.10.3, taking into account the (possible) change in clock cycle 

time. Assume that the latency ID stage increases by 50% and the latency of the EX 

stage decreases by 10ps when branch outcome resolution is moved from EX to ID.

 4.17 Exercises 3634.10.6 [10] <§4.5> Assuming stall-on-branch and no delay slots, what is the new 
clock cycle time and execution time of this instruction sequence if 
beq address 
computation is moved to the MEM stage? What is the speedup from this change? 

Assume that the latency of the EX stage is reduced by 20 ps and the latency of the 

MEM stage is unchanged when branch outcome resolution is moved from EX to 

MEM.4.11 Consider the following loop.
loop:lw  r1,0(r1)     and r1,r1,r2
     lw  r1,0(r1)
     lw  r1,0(r1)
     beq r1,r0,loop Assume that perfect branch prediction is used (no stalls due to control hazards), 
that there are no delay slots, and that the pipeline has full forwarding support. Also 

assume that many iterations of this loop are executed before the loop exits.
4.11.1 [10] <§4.6> Show a pipeline execution diagram for the third iteration of 
this loop, from the cycle in which we fetch th
 rst instruction of that iteration up 
to (but not including) the cycle in which we can fetch th
 rst instruction of the 
next iteration. Show all instructions that are in the pipeline during these cycles (not 

just those from the third iteration).
4.11.2 [10] <§4.6> How o en (as a percentage of all cycles) do we have a cycle in 
whic
 ve pipeline stages are doing useful work?
4.12  is exercise is intended to help you understand the cost/complexity/
performance trade-o
 s of forwarding in a pipelined processor. Problems in this 
exercise refer to pipelined datapaths from 
Figure 4.45
 ese problems assume 
that, of all the instructions executed in a processor, the following fraction of these 

instructions have a particular type of RAW data dependence
 e type of RAW 
data dependence is iden
 ed by the stage that produces the result (EX or MEM) 
and the instruction that consumes the result (1st instruction that follows the one 

that produces the result, 2nd instruction that follows, or both). We assume that the 

register write is done in th
 rst half of the clock cycle and that register reads are 
done in the second half of the cycle, so “EX to 3rd” and “MEM to 3rd” dependences 

are not counted because they cannot result in data hazards. Also, assume that the 

CPI of the processor is 1 if there are no data hazards.
EX to 1st OnlyMEM to 1st OnlyEX to 2nd OnlyMEM to 2nd  OnlyEX to 1st and MEM to 2ndOther RAW 
Dependences5%20%5%10%10%10%
364 Chapter 4 The Processor
Assume the following latencies for individual pipeline stages. For the EX stage, 
latencies are given separately for a processor without forwarding and for a processor 

with
 erent kinds of forwarding.
IF IDEX (no FW)EX (full FW)EX (FW from EX/MEM only)EX (FW from MEM/WB only)MEMWB
150 ps100 ps120 ps150 ps140 ps
130 ps120 ps100 ps
4.12.1 [10] <§4.7> If we use no forwarding, what fraction of cycles are we stalling 

due to data hazards?
4.12.2 [5] <§4.7> If we use full forwarding (forward all results that can be 
forwarded), what fraction of cycles are we staling due to data hazards?
4.12.3 [10] <§4.7> Let us assume that we cannot a
 ord to have three-input Muxes 
that are needed for full forwarding. We have to decide if it is better to forward 
only from the EX/MEM pipeline register (next-cycle forwarding) or only from 

the MEM/WB pipeline register (two-cycle forwarding). Which of the two options 

results in fewer data stall cycles?
4.12.4 [10] <§4.7> For the given hazard probabilities and pipeline stage latencies, 
what is the speedup achieved by adding full forwarding to a pipeline that had no 

forwarding?
4.12.5 [10] <§4.7> What would be the additional speedup (relative to a processor 
with forwarding) if we added time-travel forwarding that eliminates all data 

hazards? Assume that the yet-to-be-invented time-travel circuitry adds 100 ps to 

the latency of the full-forwarding EX stage.
4.12.6 [20] <§4.7> Repeat 4.12.3 but this time determine which of the two 
options results in shorter time per instruction.
4.13  is exercise is intended to help you understand the relationship between 
forwarding, hazard detection, and ISA design. Problems in this exercise refer to 
the following sequence of instructions, and assume that it is executed on a 5-stage 

pipelined datapath:
add r5,r2,r1lw  r3,4(r5)
lw  r2,0(r2)
or  r3,r5,r3
sw  r3,0(r5)4.13.1 [5] <§4.7> If there is no forwarding or hazard detection, insert 
nops to 
ensure correct execution.

 4.17 Exercises 3654.13.2 [10] <§4.7> Repeat 4.13.1 but now use 
nops only when a hazard cannot be 
avoided by changing or rearranging these instructions. You can assume register R7 
can be used to hold temporary values in your mo
 ed code.
4.13.3 [10] <§4.7> If the processor has forwarding, but we forgot to implement 

the hazard detection unit, what happens when this code executes?
4.13.4 [20] <§4.7> If there is forwarding, for th
 rst 
 ve cycles during the 
execution of this code, specify which signals are asserted in each cycle by hazard 
detection and forwarding units in 
Figure 4.60
.4.13.5 [10] <§4.7> If there is no forwarding, what new inputs and output signals 
do we need for the hazard detection unit in 
Figure 4.60
? Using this instruction 

sequence as an example, explain why each signal is needed.
4.13.6 [20] <§4.7> For the new hazard detection unit from 4.13.5, specify which 
output signals it asserts in each of th
 rs
 ve cycles during the execution of this 
code.
4.14  is exercise is intended to help you understand the relationship between 
delay slots, control hazards, and branch execution in a pipelined processor. In 
this exercise, we assume that the following MIPS code is executed on a pipelined 

processor with a 5-stage pipeline, full
 forwarding, and a predict-taken branch 
predictor:
                   lw r2,0(r1)label1: beq r2,r0,label2 # not taken once, then taken        lw r3,0(r2)
        beq r3,r0,label1 # taken
        add r1,r3,r1label2: sw r1,0(r2)4.14.1 [10] <§4.8> Draw the pipeline execution diagram for this code, assuming 
there are no delay slots and that branches execute in the EX stage.
4.14.2 [10] <§4.8> Repeat 4.14.1, but assume that delay slots are used. In the 
given code, the instruction that follows the branch is now the delay slot instruction 

for that branch.
4.14.3 [20] <§4.8> One way to move the branch resolution one stage earlier is to 
not need an ALU operation in conditional branch
 e branch instructions would 
be “
bez rd,label” and “
bnez rd,label”, and it would branch if the register has 
and does not have a zero value, respectively. Change this code to use these branch 

instructions instead of 
beq. You can assume that register R8 is available for you 
to use as a temporary register, and that an 
seq (set if equal) R-type instruction can 
be used.

366 Chapter 4 The Processor
Section 4.8 describes how the severity of control hazards can be reduced by moving 
branch execution into the ID stage
 is approach involves a dedicated comparator 
in the ID stage, as shown in 
Figure 4.62
. However, this approach potentially adds 

to the latency of the ID stage, and requires additional forwarding logic and hazard 

detection.
4.14.4 [10] <§4.8> Using th
 rst branch instruction in the given code as an 
example, describe the hazard detection logic needed to support branch execution 
in the ID stage as in 
Figure 4.62
. Which type of hazard is this new logic supposed 

to detect?
4.14.5 [10] <§4.8> For the given code, what is the speedup achieved by moving 
branch execution into the ID stage? Explain your answer. In your speedup 

calculation, assume that the additional comparison in the ID stage does not a
 ect clock cycle time.
4.14.6 [10] <§4.8> Using th
 rst branch instruction in the given code as an 
example, describe the forwarding support that must be added to support branch 
execution in the ID stage. Compare the complexity of this new forwarding unit to 

the complexity of the existing forwarding unit in 
Figure 4.62
.4.15  e importance of having a good branch predictor depends on how o
 en conditional branches are executed. Together with branch predictor accuracy, this 
will determine how much time is spent stalling due to mispredicted branches. In 

this exercise, assume that the breakdown of dynamic instructions into various 

instruction categories is as follows:
R-TypeBEQJMPLWSW
40%25%5%25%5%
Also, assume the following branch predictor accuracies:
Always-Taken  Always-Not-Taken2-Bit
45%55%85%4.15.1 [10] <§4.8> Stall cycles due to mispredicted branches increase the 

CPI. What is the extra CPI due to mispredicted branches with the always-taken 

predictor? Assume that branch outcomes are determined in the EX stage, that there 

are no data hazards, and that no delay slots are used.
4.15.2 [10] <§4.8> Repeat 4.15.1 for the “always-not-taken” predictor.

4.15.3 [10] <§4.8> Repeat 4.15.1 for for the 2-bit predictor.

4.15.4 [10] <§4.8> With the 2-bit predictor, what speedup would be achieved if 
we could convert half of the branch instructions in a way that replaces a branch 

instruction with an ALU instruction? Assume that correctly and incorrectly 

predicted instructions have the same chance of being replaced.

 4.17 Exercises 3674.15.5 [10] <§4.8> With the 2-bit predictor, what speedup would be achieved if 
we could convert half of the branch instructions in a way that replaced each branch 

instruction with two ALU instructions? Assume that correctly and incorrectly 

predicted instructions have the same chance of being replaced.
4.15.6 [10] <§4.8> Some branch instructions are much more predictable than 
others. If we know that 80% of all executed branch instructions are easy-to-predict 

loop-back branches that are always predicted correctly, what is the accuracy of the 

2-bit predictor on the remaining 20% of the branch instructions?
4.16  is exercise examines the accuracy of various branch predictors for the 
following repeating pattern (e.g., in a loop) of branch outcomes: 
T, NT, T, T, NT4.16.1 [5] <§4.8> What is the accuracy of always-taken and always-not-taken 
predictors for this sequence of branch outcomes?
4.16.2 [5] <§4.8> What is the accuracy of the two-bit predictor for th
 rst 4 
branches in this pattern, assuming that the predictor starts o
  in the botto
  state from 
Figure 4.63
 (predict not taken)?

4.16.3 [10] <§4.8> What is the accuracy of the two-bit predictor if this pattern is 
repeated forever?
4.16.4 [30] <§4.8> Design a predictor that would achieve a perfect accuracy if 
this pattern is repeated forever. You predictor should be a sequential circuit with 

one output that provides a prediction (1 for taken, 0 for not taken) and no inputs 

other than the clock and the control signal that indicates that the instruction is a 

conditional branch.
4.16.5 [10] <§4.8> What is the accuracy of your predictor from 4.16.4 if it is 
given a repeating pattern that is the exact opposite of this one?
4.16.6 [20] <§4.8> Repeat 4.16.4, but now your predictor should be able to 
eventually (a
 er a warm-up period during which it can make wrong predictions) 
start perfectly predicting both this pattern and its opposite. Your predictor should 

have an input that tells it what the real outcome was. Hint: this input lets your 

predictor determine which of the two repeating patterns it is given.
4.17  is exercise explores how exception handling a
 ects pip
 e  rst three problems in this exercise refer to the following two instructions:
Instruction 1 Instruction 2
BNE R1, R2, Label
LW R1, 0(R1)
4.17.1 [5] <§4.9> Which exceptions can each of these instructions trigger? For 
each of these exceptions, specify the pipeline stage in which it is detected.

368 Chapter 4 The Processor
4.17.2 [10] <§4.9> If there is a separate handler address for each exception, show 
how the pipeline organization must be changed to be able to handle this exception. 

You can assume that the addresses of these handlers are known when the processor 

is designed.
4.17.3 [10] <§4.9> If the second instruction is fetched right a
 er th
 rst 
instruction, describe what happens in the pipeline when th
 rst instruction causes 
th rst exception you listed in 4.17.1. Show the pipeline execution diagram from 
the time th
 rst instruction is fetched until the time th
 rst instruction of the 
exception handler is completed.
4.17.4 [20] <§4.9> In vectored exception handling, the table of exception handler 
addresses is in data memory at a know xed) address. Change the pipeline to 

implement this exception handling mechanism. Repeat 4.17.3 using this mo
 ed pipeline and vectored exception handling.
4.17.5 [15] <§4.9> We want to emulate vectored exception handling (described 
in 4.17.4) on a machine that has only on
 xed handler address. Write the code 
that should be at that
 xed address. Hint: this code should identify the exception, 
get the right address from the exception vector table, and transfer execution to that 

handler.
4.18 In this exercise we compare the performance of 1-issue and 2-issue 
processors, taking into account program transformations that can be made to 
optimize for 2-issue execution. Problems in this exercise refer to the following loop 

(written in C):
for(i=0;i!=j;i+=2)  b[i]=a[i]–a[i+1];When writing MIPS code, assume that variables are kept in registers as follows, and 
that all registers except those indicated as Free are used to keep various variables, 

so they cannot be used for anything else.
i  jabc
Free
R5R6R1R2R3R10, R11, R12
4.18.1 [10] <§4.10> Translate this C code into MIPS instructions. Your translation 

should be direct, without rearranging instructions to achieve better performance.
4.18.2 [10] <§4.10> If the loop exits a
 er executing only two iterations, draw a 
pipeline diagram for your MIPS code from 
4.18.1 executed on a 2-issue processor 
shown in 
Figure 4.69
. Assume the processor has perfect branch prediction and can 
fetch any two instructions (not just consecutive instructions) in the same cycle.
4.18.3 [10] <§4.10> Rearrange your code from 4.18.1 to achieve better 
performance on a 2-issue statically scheduled processor from 
Figure 4.69
.
 4.17 Exercises 3694.18.4 [10] <§4.10> Repeat 4.18.2, but this time use your MIPS code from 4.18.3.
4.18.5 [10] <§4.10> What is the speedup of going from a 1-issue processor to 
a 2-issue processor from 
Figure 4.69
? Use your code from 4.18.1 for both 1-issue 

and 2-issue, and assume that 1,000,000 iterations of the loop are executed. As in 

4.18.2, assume that the processor has perfect branch predictions, and that a 2-issue 

processor can fetch any two instructions in the same cycle.
4.18.6 [10] <§4.10> Repeat 4.18.5, but this time assume that in the 2-issue 
processor one of the instructions to be executed in a cycle can be of any kind, and 

the other must be a non-memory instruction.
4.19  is exercise explores energ
  ciency and its relationship with performance. 
Problems in this exercise assume the following energy consumption for activity in 
Instruction memory, Registers, and Data memory. You can assume that the other 

components of the datapath spend a negligible amount of energy.
I-Mem 1 Register ReadRegister WriteD-Mem ReadD-Mem Write
140pJ70pJ60pJ140pJ120pJAssume that components in the datapath have the following latencies. You can 

assume that the other components of the datapath have negligible latencies.
I-MemControlRegister Read or WriteALUD-Mem Read or Write
200ps150ps
90ps90ps250ps4.19.1 [10] <§§4.3, 4.6, 4.14> How much energy is spent to execute an ADD 

instruction in a single-cycle design and in the 5-stage pipelined design?
4.19.2 [10] <§§4.6, 4.14> What is the worst-case MIPS instruction in terms of 
energy consumption, and what is the energy spent to execute it?
4.19.3 [10] <§§4.6, 4.14> If energy reduction is paramount, how would you 
change the pipelined design? What is the percentage reduction in the energy spent 

by an LW instruction a
 er this change?
4.19.4 [10] <§§4.6, 4.14> What is the performance impact of your changes from 

4.19.3?4.19.5 [10] <§§4.6, 4.14> We can eliminate the MemRead control signal and have 
the data memory be read in every cycle, i.e., we can permanently have MemRead=1. 

Explain why the processor still functions correctly a
 er this change. What is the 
 ect of this change on clock frequency and energy consumption?
4.19.6 [10] <§§4.6, 4.14> If an idle unit spends 10% of the power it would spend 

if it were active, what is the energy spent by the instruction memory in each cycle? 

What percentage of the overall energy spent by the instruction memory does this 

idle energy represent?

370 Chapter 4 The Processor
§4.1, page 248: 3 of 5: Control, Datapath, Memory. Input and Output are missing.
§4.2, page 251: false. Edge-triggered state elements make simultaneous reading and 

writing both possible and unambiguous.

§4.3, page 257: I. a. II. c.

§4.4, page 272: Yes, Branch and ALUOp0 are identical. In addition, MemtoReg and 

RegDst are inverses of one another. You don’t need an inverter; simply use the other 

signal an
 ip the order of the inputs to the multiplexor!
§4.5, page 285
: I. Stall on the 
lw result. 2. Bypass th
 rst 
add result written into 
$t1. 3. No stall or bypass required.

§4.6, page 298
: Statements 2 and 4 are correct; the rest are incorrect.

§4.8, page 324
: 1. Predict not taken. 2. Predict taken. 3. Dynamic prediction.

§4.9, page 332
: e 
 rst instruction, since it is logically executed before the others.
§4.10, page 344
: 1. Both. 2. Both. 3. So ware. 4. Hardware. 5. Hardware. 6. 

Hardware. 7. Both. 8. Hardware. 9. Both.

§4.11, page 353
: First two are false and the last two are true.
Answers to 
Check Yourself


5Ideally one would desire an 
inde
 nitely large memory 
capacity such that any 

particular 
… word would be 
immediately available. 
… We 
are 
… forced to recognize the 
possibility of constructing a 

hierarchy of memories, each 

of which has greater capacity 

than the preceding but which 

is less quickly accessible.
A. W. Burks, H. H. Goldstine, and 
J. von Neumann Preliminary Discussion of the Logical Design of an 
Electronic Computing Instrument,
 1946Large and Fast: 
Exploiting Memory 

Hierarchy
5.1 Introduction 3745.2 Memory Technologies 
3785.3 The Basics of Caches 
3835.4 Measuring and Improving Cache 
Performance 
3985.5 Dependable Memory Hierarchy 
4185.6 Virtual Machines 
4245.7 Virtual Memory 
427Computer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
5.8 A Common Framework for Memory Hierarchy 
4545.9 Using a Finite-State Machine to Control a Simple Cache 
4615.10 Parallelism and Memory Hierarchies: Cache Coherence 
4665.11 Parallelism and Memory Hierarchy: Redundant Arrays of 
Inexpensive Disks 4705.12 Advanced Material: Implementing Cache Controllers 
4705.13 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Memory 
Hierarchies 4715.14 Going Faster: Cache Blocking and Matrix Multiply 
4755.15 Fallacies and Pitfalls 
4785.16 Concluding Remarks 
4825.17 Historical Perspective and Further Reading 
4835.18 Exercises 483The Five Classic Components of a Computer

374 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 5.1 IntroductionFrom the earliest days of computing, programmers have wanted unlimited 
amounts of fast memory
 e topics in this chapter aid programmers by creating 
that illusion. Before we look at creating the illusion, let’s consider a simple analogy 

that illustrates the key principles and mechanisms that we use.
Suppose you were a student writing a term paper on important historical 
developments in computer hardware. You are sitting at a desk in a library with 

a collection of books that you have pulled from the shelves and are examining. 

Yo
 nd that several of the important computers that you need to write about are 
described in the books you have, but there is nothing about the EDSA
 erefore, 
you go back to the shelves and look for an additional book. Yo
 nd a book on 
early British computers that covers the EDSAC. Once you have a good selection of 

books on the desk in front of you, there is a good probability that many of the topics 

you need can be found in them, and you may spend most of your time just using 

the books on the desk without going back to the shelves. Having several books on 

the desk in front of you saves time compared to having only one book there and 

constantly having to go back to the shelves to return it and take out another.
 e same principle allows us to create the illusion of a large memory that we 
can access as fast as a very small memory. Just as you did not need to access all the 

books in the library at once with equal probability, a program does not access all of 

its code or data at once with equal probability. Otherwise, it would be impossible 

to make most memory accesses fast and still have large memory in computers, just 

as it would be impossible for you to
 t all the library books 
on your desk and still 
 nd what you wanted quickly.
 is principle of locality
 underlies both the way in which you did your work in 
the library and the way that programs operate
 e principle of locality states that 
programs access a relatively small portion of their address space at any instant of 

time, just as you accessed a very small portion of the library’s collectio
 ere are 
tw erent types of locality:
 Temporal locality
 (locality in time): if an item is referenced, it will tend to be 
referenced again soon. If you recently brought a book to your desk to look at, 
you will probably need to look at it again soon.
 Spatial locality
 (locality in space): if an item is referenced, items whose 
addresses are close by will tend to be referenced soon. For example, when 

you brought out the book on early English computers to
 nd out about the 
EDSAC, you also noticed that there was another book shelved next to it about 

early mechanical computers, so you also brought back that book and, later 

on, found something useful in that book. Libraries put books on the same 

topic together on the same shelves to increase spatial locality. We’ll see how 

memory hierarchies use spatial locality a little later in this chapter.
temporal locality
  e principle stating that if a 
data location is referenced 

then it will tend to be 

referenced again soon.
spatial locality
 e locality principle stating 

that if a data location is 

referenced, data locations 

with nearby addresses 

will tend to be referenced 

soon.

 5.1 Introduction 375Just as accesses to books on the desk naturally exhibit locality, locality in 
programs arises from simple and natural program structures. For example, 
most programs contain loops, so instructions and data are likely to be accessed 

repeatedly, showing high amounts of temporal locality. Since instructions are 

normally accessed sequentially, programs also show high spatial locality. Accesses 

to data also exhibit a natural spatial locality. For example, sequential accesses to 

elements of an array or a record will nat
urally have high degrees of spatial locality.
We take advantage of the principle of locality by implementing the memory 
of a computer as a 
memory hierarchy
. A memory hierarchy consists of multiple 
levels of memory with
 erent speeds an
 e faster memories are more 
expensive per bit than the slower memories and thus are smaller.
Figure 5.1
 shows the faster memory is close to the processor and the slower, 
less expensive memory is below i
 e goal is to present the user with as much 
memory as is available in the cheapest technology, while providing access at the 

speed o
 ered by the fastest memory.
 e data is similarly hierarchical: a level closer to the processor is generally a 
subset of any level further away, and all the data is stored at the lowest level. By 

analogy, the books on your desk form a subset of the library you are working in, 

which is in turn a subset of all the libraries on campus. Furthermore, as we move 

away from the processor, the levels take progressively longer to access, just as we 

might encounter in a hierarchy of campus libraries.
A memory hierarchy can consist of multiple levels, but data is copied between 
only two adjacent levels at a time, so we can focus our attention on just two levels. 
memory hierarchy
 A structure that uses 
multiple levels of 

memories; as the distance 

from the processor 

increases, the size of the 

memories and the access 

time both increase.
SpeedFastestSlowestSmallestBiggestSizeCost ($/bit)
Currenttechnology
HighestLowestSRAMDRAMMagnetic diskProcessorMemoryMemoryMemoryFIGURE 5.1 The basic structure of a memory hierarchy.
 By implementing the memory system as 
a hierarchy, the user has the illusion of a memory that is as large as the largest level of the hierarchy, but can 
be accessed as if it were all built from the fastest memory. Flash memory has replaced disks in many personal 

mobile devices, and may lead to a new level in the storage hierarchy for desktop and server computers; see 

Section 5.2.

376 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 e upper level—the one closer to the processor—is smaller and faster than the lower 
level, since the upper level uses technology that is more expensive. 
Figure 5.2
 shows 
that the minimum unit of information that can be either present or not present in 

the two-level hierarchy is called a 
block
 or a 
line; in our library analogy, a block of 
information is one book.
If the data requested by the processor appears in some block in the upper level, 
this is called a 
hit (analogous to yo
 nding the information in one of the books 
on your desk). If the data is not found in the upper level, the request is called a 
miss
.  e lower level in the hierarchy is then accessed to retrieve the block containing the 
requested data. (Continuing our analogy, 
you go from your desk to the shelves to 
 nd the desired boo
 e hit rate
, or 
hit ratio
, is the fraction of memory accesses 
found in the upper level; it is o
 en used as a measure of the performance of the 
memory hierarchy
 e miss rate
 (1−hit rate) is the fraction of memory accesses 
not found in the upper level.
Since performance is the major reason for having a memory hierarchy, the time 
to service hits and misses is important. 
Hit time
 is the time to access the upper level 
of the memory hierarchy, which includes the time needed to determine whether 
the access is a hit or a miss (that is, the time needed to look through the books on 

th
 e miss penalty
 is the time to replace a block in the upper level with 
the corresponding block from the lower level, plus the time to deliver this block to 
the processor (or the time to get another book from the shelves and place it on the 

desk). Because the upper level is smaller and built using faster memory parts, the 

hit time will be much smaller than the time to access the next level in the hierarchy, 

which is the major component of the miss penalty e time to examine the books 

on the desk is much smaller than the time to get up and get a new book from the 

shelves.)
block (or line)
 e minimum unit of 
information that can 

be either present or not 

present in a cache.
hit rate
 e fraction of 
memory accesses found 

in a level of the memory 

hierarchy.
miss rate
 e fraction 
of memory accesses not 

found in a level of the 

memory hierarchy.
hit time e time 
required to access a level 

of the memory hierarchy, 

including the time needed 

to determine whether the 

access is a hit or a miss.
miss penalty
 e time 
required to fetch a block 

into a level of the memory 

hierarchy from the lower 

level, including the time 

to access the block, 

transmit it from one level 

to the other, insert it in 

the level that experienced 

the miss, and then pass 

the block to the requestor.
ProcessorData is transferredFIGURE 5.2 Every pair of levels in the memory hierarchy can be thought of as having an 
upper and lower level.
 Within each level, the unit of information that is present or not is called a 
block
 or 
a line
. Usually we transfer an entire block when we copy something between levels.

 5.1 Introduction 377As we will see in this chapter, the concepts used to build memory systems a
 ect many other aspects of a computer, including how the operating system manages 
memory and I/O, how compilers generate code, and even how applications use 

the computer. Of course, because all programs spend much of their time accessing 

memory, the memory system is necessarily a major factor in determining 

performance.
 e reliance on memory hierarchies to achieve performance 
has meant that programmers, who used to be able to think of memory
 at, 
random access storage device, now need to understand that memory is a hierarchy 

to get good performance. We show how important this understanding is in later 

examples, such as 
Figure 5.18
 on page 408, and Section 5.14, which shows how to 

double matrix multiply performance.
Since memory systems are critical to performance, computer designers devote a 
great deal of attention to these systems and develop sophisticated mechanisms for 

improving the performance of the memory system. In this chapter, we discuss the 

major conceptual ideas, although we use many simp
 cations and abstractions to 
keep the material manageable in length and complexity.
Programs exhibit both temporal locality, the tendency to reuse recently 

accessed data items, and spatial locality, the tendency to reference data 

items that are close to other recently accessed items. Memory hierarchies 

take advantage of temporal locality by keeping more recently accessed 

data items closer to the processor. Memory hierarchies take advantage of 

spatial locality by moving blocks consisting of multiple contiguous words 

in memory to upper levels of the hierarchy.
Figure 5.3
 shows that a memory hierarchy uses smaller and faster 
memory technologies close to the processor.
 us, accesses that hit in the 
highest level of the hierarchy can be processed quickly. Accesses that miss 

go to lower levels of the hierarchy, which are larger but slower. If the hit 

rate is high enough, the memory hierarchy has an
 ective access time 
close to that of the highest (and fastest) level and a size equal to that of the 

lowest (and largest) level.
In most systems, the memory is a true hierarchy, meaning that data 
cannot be present in level 
i unless it is also present in level 
i  1.The BIGPictureWhich of the following statements are generally true?
1. Memory hierarchies take advantage of temporal locality.
2. On a read, the value returned depends on which blocks are in the cache.

3. Most of the cost of the memory hierarchy is at the highest level.

4. Most of the capacity of the memory hierarchy is at the lowest level.
Check Yourself

378 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 5.2 Memory Technologies
 ere are four primary technologies used today in memory hierarchies. Main 
memory is implemented from DRAM (dynamic random access memory), while 
levels closer to the processor (caches) use SRAM (static random access memory). 

DRAM is less costly per bit than SRAM, although it is substantially slower
 e pr
 erence arises because DRAM us
 cantly less area per bit of memory, 
and DRAMs thus have larger capacity for the same amount of silicon; the speed 

 erence arises from several factors described in 
Section B.9
 of 
 Appendix B
.  e third technolog
 ash memory
 is nonvolatile memory is the secondary 
memory in Personal Mobile De
 e fourth technology, used to implement 
the largest and slowest level in the hierarchy in server
 e access 
time and price per bit vary widely among these technologies, as the table below 

shows, using typical values for 2012:
Memory technology
Typical access 
time$ per 
GiB in 2012SRAM semiconductor memory
0.5–2.5 ns$500–$1000DRAM semiconductor memory
50–70 ns$10–$20Flash semiconductor memory
5,000–50,000 ns$0.75–$1.00Magnetic disk5,000,000–20,000,000 ns$0.05–$0.10We describe each memory technology in the remainder of this section.
CPULevel 1Level 2Level nIncreasing distancefrom the CPU inaccess timeLevels in thememory hierarchySize of the memory at each levelFIGURE 5.3 This diagram shows the structure of a memory hierarchy: as the distance 
from the processor increases, so does the size.
 is structure, with the appropriate operating 
mechanisms, allows the processor to have an access time that is determined primarily by level 1 of the 
hierarchy and yet have a memory as large as level 
n. Maintaining this illusion is the subject of this chapter. 
Although the local disk is normally the bottom of the hierarchy, some systems use tape o
 le server over a 
local area network as the next levels of the hierarchy.

 5.2 Memory 
Technologies 
379SRAM Technology
SRAMs are simply integrated circuits that are memory arrays with (usually) a 
single access port that can provide either a read or a write. SRAMs hav
 xed 
access time to any datum, though the read and write access times ma
 er. 
SRAMs don’t need to refresh and so the access time is very close to the cycle 
time. SRAMs typically use six to eight transistors per bit to prevent the information 

from being disturbed when read. SRAM needs only minimal power to retain the 

charge in standby mode.
In the past, most PCs and server systems used separate SRAM chips for either 
their primary, secondary, or even tertiary caches. Today, thanks to 
Moore’s Law
, all levels of caches are integrated onto the processor chip, so the market for separate 

SRAM chips has nearly evaporated.
DRAM Technology
In a SRAM, as long as power is applied, the value can be kep
 nitely. In a 
dynamic RAM (DRAM), the value kept in a cell is stored as a charge in a capacitor. 

A single transistor is then used to access this stored charge, either to read the 

value or to overwrite the charge stored there. Because DRAMs use only a single 

transistor per bit of storage, they are much denser and cheaper per bit than SRAM. 

As DRAMs store the charge on a capacitor, it cannot be kep
 nitely and must 
periodically be refreshed.
 at is why this memory structure is called dynamic, as 
opposed to the static storage in an SRAM cell.
To refresh the cell, we merely read its contents and write it bac
 e charge 
can be kept for several milliseconds. If every bit had to be read out of the DRAM 

and then written back individually, we would constantly be refreshing the DRAM, 

leaving no time for accessing it. Fortunately, DRAMs use a two-level decoding 

structure, and this allows us to refresh an entire 
row
 (which shares a word line) 
with a read cycle followed immediately by a write cycle. 
Figure 5.4
 shows the internal organization of a DRAM, and 
Figure 5.5
 shows 
how the density, cost, and access time of DRAMs have changed over the years.
 e row organization that helps with refresh also helps with performance. To 
improve performance, DRAMs bu
 er rows for repeat
 e b
 er acts 
like an SRAM; by changing the address, random bits can be accessed in the b
 er until the next ro
 is capability improves th
 cantly, 
since the access time to bits in the row is much lower. Making the chip wider also 

improves the memory bandwidth of the chip. When the row is in the b
 er, it 
can be transferred by successive addresses at whatever the width of the DRAM is 

(typically 4, 8, or 16 bits), or by specifying a block transfer and the starting address 

within the bu
 er. 
To further improve the interface to processors, DRAMs added clocks and are 
properly called Synchronous DRAMs or SD
 e advantage of SDRAMs 
is that the use of a clock eliminates the time for the memory and processor to 

synchronize e speed advantage of synchronous DRAMs comes from the ability 

to transfer the bits in the burst without having to specify additional address bits. 

380 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Instead, the clock transfers the successive bits in a burs
 e fastest version is called 
Double Data Rate 
(DDR) SD
 e name means data transfers on both the 
rising 
and
 falling edge of the clock, thereby getting twice as much bandwidth as you 
might expect based on the clock rate and the data widt
 e latest version of this 
technology is called DDR4. A DDR4-3200 DRAM can do 3200 million transfers 
per second, which means it has a 1600 MHz clock.
Sustaining that much bandwidth requires clever organization 
inside
 the DRAM. 
Instead of just a faster row bu
 er, the DRAM can be internally organized to read or 
FIGURE 5.5 DRAM size increased by multiples of four approximately once every three 
years until 1996, and thereafter considerably slower.
 e improvements in access time have been 
slower but continuous, and cost roughly tracks density improvements, although cost is o
 en a
 ected by other 
issues, such as availability and demand. e cost per gibibyte is not adjusted fo
 ation.
Year introducedChip size$ per GiB
Total access time to 
a new row/column
Average columnaccess time to existing row  198064 Kibibit$1,500,000
250 ns150 ns1983256 Kibibit$500,000185 ns100 ns
19851 Mebibit$200,000135 ns40 ns

19894 Mebibit$50,000110 ns40 ns

199216 Mebibit$15,00090 ns30 ns

199664 Mebibit$10,00060 ns12 ns

1998128 Mebibit$4,00060 ns10 ns

2000256 Mebibit$1,00055 ns7 ns

2004512 Mebibit$25050 ns5 ns

20071 Gibibit$5045 ns1.25 ns
20102 Gibibit
4 Gibibit$3040 ns1 ns
2012$135 ns0.8 nsFIGURE 5.4 Internal organization of a DRAM. 
Modern DRAMs are organized in banks, typically 
four for DDR3. Each bank consists of a series of rows. Sending a PRE (precharge) command opens or closes a 
bank. A row address is sent with an Act (activate), which causes the row to transfer to a bu
 er. When the row 
is in the b
 er, it can be transferred by successive column addresses at whatever the width of the DRAM is 
(typically 4, 8, or 16 bits in DDR3) or by specifying a block transfer and the starting address. Each command, 

as well as block transfers, is synchronized with a clock.
ColumnRd/Wr
PreActRow
Bank
 5.2 Memory 
Technologies 
381write from multiple 
banks
, with each having its own row b
 er. Sending an address 
to several banks permits them all to read or write simultaneously. For example, 
with four banks, there is just one access time and then accesses rotate between 

the four banks to supply four times the bandwidt
 is rotating access scheme is 
called address
 interleaving
. Although Personal Mobile Devices like the iPad (see Chapter 1) use individual 
DRAMs, memory for servers are commonly sold on small boards called 
dual inline 
memory modules 
(DIMMs). DIMMs typically contain 4–16 DRAMs, and they are 
normally organized to be 8 bytes wide for server systems. A DIMM using DDR4-

3200 SDRAMs could transfer at 8 
 3200  25,600 megabytes per second. Such 
DIMMs are named a
 er their bandwidth: PC25600. Since a DIMM can have so 
many DRAM chips that only a portion of them are used for a particular transfer, we 

need a term to refer to the subset of chips in a DIMM that share common address 

lines. To avoid confusion with the internal DRAM names of row and banks, we use 

the term 
memory rank
 for such a subset of chips in a DIMM.
Elaboration: One way to measure the performance of the memory system behind the 
caches is the Stream benchmark [McCalpin, 1995]. It measures the performance of 
long vector operations. They have no temporal locality and they access arrays that are 

larger than the cache of the computer being tested.Flash Memory
Flash memory is a type of 
electrically erasable programmable read-only memory
 (EEPROM). 
Unlike disks and DRAM, but like other EEPROM technologies, writes can wear out 
 ash memory bits. To cope with such limi
 ash products include a controller 
to spread the writes by remapping blocks that have been written many times to less 
trodden bloc
 is technique is called 
wear leveling
. With wear leveling, personal 
mobile devices are very unlikely to exceed the write limits in th
 ash. Such wear 
leveling lowers the potential performance o ash, but it is needed unless higher-

level so
 ware monitors block wear. Flash controllers that perform wear leveling can 
also improve yield by mapping out memory cells that were manufactured incorrectly.
Disk Memory
As 
Figure 5.6
 shows, a magnetic hard disk consists of a collection of platters, which 

rotate on a spindle at 5400 to 15,000 revolutions per minute
 e metal platters are 
covered with magnetic recording material on both sides, similar to the material found 

on a cassette or videotape. To read and write information on a hard disk, a movable 
arm
 containing a small electromagnetic coil called a 
read-write head
 is located just above 
each surface
 e entire drive is permanently sealed to control the environment inside 
the drive, which, in turn, allows the disk heads to be much closer to the drive surface.
Each disk surface is divided into concentric circles, called 
tracks
 ere are 
typically tens of thousands of tracks per surface. Each track is in turn divided into 
track
 One of thousands 
of concentric circles that 
makes up the surface of a 

magnetic disk.
382 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
sectors
 that contain the information; each track may have thousands of sectors. 
Sectors are typically 512 to 4096 bytes in size
 e sequence recorded on the 
magnetic media is a sector number, a gap, the information for that sector including 
error correction code (see Section 5.5), a gap, the sector number of the next sector, 

and so on.
 e disk heads for each surface are connected together and move in conjunction, 
so that every head is over the same track of every surface
 e term 
cylinder
 is used 
to refer to all the tracks under the heads at a given point on all surfaces.
FIGURE 5.6 A disk showing 10 disk platters and the read/write heads.
  e diameter of 
today’s disks is 2.5 or 3.5 inches, and there are typically one or two platters per drive today.
To access data, the operating system must direct the disk through a three-stage 
pro
 e 
 rst step is to position the head over the proper trac
 is operation is 
called a seek
, and the time to move the head to the desired track is called the 
seek time
.Disk manufacturers report minimum seek time, maximum seek time, and average 
seek time in their manu
 e 
 rst two are easy to measure, but the average is open to 
wide interpretation because it depends on the seek distance
 e industry calculates 
average seek time as the sum of the time for all possible seeks divided by the number 

of possible seeks. Average seek times are usually advertised as 3 ms to 13 ms, but, 

depending on the application and scheduling of disk requests, the actual average seek 

time may be only 25% to 33% of the advertised number because of locality of disk 
sector
 One of the 
segments that make up a 
track on a magnetic disk; 

a sector is the smallest 

amount of information 

that is read or written on 

a disk.seek
 e process of 
positioning a read/write 

head over the proper 

track on a disk.

 5.3 The Basics of Caches 
383refere
 is locality arises both because of successive accesses to the sa
 le and 
because the operating system tries to schedule such accesses together.
Once the head has reached the correct track, we must wait for the desired sector 
to rotate under the read/write head
 is time is called the 
rotational latency
 or 
rotational delay
 e average latency to the desired information is halfway around 
the disk. Disks rotate at 5400 RPM t
 e average rotational latency 
at 5400 RPM is
Average rotational 
latency0.5 rotation RPM0.5 rotati5400o
on RPM/seconds
minute
0.0056 seconds5.6 m
540060
ss
 e last component of a disk access, 
transfer time,
 is the time to transfer a block 
of bi
 e transfer time is a function of the sector size, the rotation speed, and the 
recording density of a track. Transfer rates in 2012 were between 100 and 200 MB/sec. 
One complication is that most disk controllers have a built-in cache that stores 
sectors as they are passed over; transfer rates from the cache are typically higher, 
and were up to 750 MB/sec (6 Gbit/sec) in 2012. 
Alas, where block numbers are located is no longer intuitive
 e assumptions of 
the sector-track-cylinder model above are that nearby blocks are on the same track, 

blocks in the same cylinder take less time to access since there is no seek time, 

and some tracks are closer than other
 e reason for the change was the raising 
of the level of the disk interfaces. To speed-up sequential transfers, these higher-

level interfaces organize disks more like tapes than like random access devices. 

 e logical blocks are ordered in serpentine fashion across a single surface, trying 
to capture all the sectors that are recorded at the same bit density to try to get best 

performance. Hence, sequential blocks may be o
 erent tracks.
In summary, the two primar
 erences between magnetic disks and 
semiconductor memory technologies are that disks have a slower access time because 

they are mechanical de
 ash is 1000 times as fast and DRAM is 100,000 times 
as fast—yet they are cheaper per bit because they have very high storage capacity at a 

modest cost—disk is 10 to 100 time cheaper.
 Magnetic disks are nonvolatile lik
 ash, 
but unlike
 ash there is no write wear-out problem. However,
 ash is much more 
rugged and hence a better match to the jostling inherent in personal mobile devices.
 5.3 The Basics of Caches
In our library example, the desk acted as a cache—a safe place to store things 

(books) that we needed to examine. 
Cache
 was the name chosen to represent the 
level of the memory hierarchy between the processor and main memory in th
 rst 
commercial computer to have this extra level
 e memories in the datapath in 
Chapter 4 are simply replaced by caches. Today, although this remains the dominant 
rotational latency
 Also 
called rotational delay
.  e time required for 
the desired sector of a 
disk to rotate under the 

read/write head; usually 

assumed to be half the 

rotation time.
Cache: a safe place 
for hiding or storing 

things.
Webster’s New World 
Dictionary of the 

American Language, 

 ird College Edition,
 1988
384 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
use of the word 
cache, the term is also used to refer to any storage managed to take 
advantage of locality of access. Cach
 rst appeared in research computers in the 
early 1960s and in production computers later in that same decade; every general-
purpose computer built today, from servers to low-power embedded processors, 

includes caches.
In this section, we begin by looking at a very simple cache in which the processor 
requests are each one word and the blocks also consist of a single word. (Readers 

already familiar with cache basics may want to skip to Section 5.4.) 
Figure 5.7
 shows 

such a simple cache, before and a
 er requesting a data item that is not initially in 
the cache. Before the request, the cache contains a collection of recent references 

X1, X2, …, Xn1, and the processor requests a word X
n that is not in the cache.
 is request results in a miss, and the word X
n is brought from memory into the cache.
In looking at the scenario in 
Figure 5.7
, there are two questions to answer: How 
do we know if a data item is in the cache? Moreover, if it is, how do w
 nd i
 e answers are related. If each word can go in exactly one place in the cache, then it 

is straightforward t nd the word if it is in the cache e simplest way to assign 

a location in the cache for each word in memory is to assign the cache location 

based on the 
address
 of the word in memory
 is cache structure is called 
direct 
mapped
, since each memory location is mapped directly to exactly one location in 
the cache.
 e typical mapping between addresses and cache locations for a direct-
mapped cache is usually simple. For example, almost all direct-mapped caches use 

this mapping to
 nd a block:
(Block address) modulo (Number of blocks in the cache)
If the number of entries in the cache is a power of 2, then modulo can be 
computed simply by using the low-order log
2 (cache size in blocks) bits of the 
addr
 us, an 8-block cache uses the three lowest bits (8 
 23) of the block 
address. For example, 
Figure 5.8
 shows how the memory addresses between 1
ten
 (00001two
) and 29
ten
 (11101two
) map to locations 1
ten
 (001two
) and 5
ten
 (101two
) in a direct-mapped cache of eight words.
Because each cache location can contain the contents of a number of
 erent 
memory locations, how do we know whether the data in the cache corresponds 

to a requested wor
 at is, how do we know whether a requested word is in the 
cache or not? We answer this question by adding a set of 
tags to the cache.
 e tags contain the address information required to identify whether a word in the 
cache corresponds to the requested word
 e tag needs only to contain the upper 
portion of the address, corresponding to the bits that are not used as an index into 

the cache. For example, in 
Figure 5.8
 we need only have the upper 2 of the 5 address 

bits in the tag, since the lower 3-bi eld of the address selects the block. 

Architects omit the index bits because they are redundant, since b
 nition the 
 eld of any address of a cache block must be that block number.
We also need a way to recognize that a cache block does not have valid 
information. For instance, when a processor starts up, the cache does not have good 

data, and the ta
 elds will be meaningless. Even a
 er executing many instructions, 
direct-mapped cache
 A cache structure in 
which each memory 

location is mapped to 

exactly one location in the 

cache.
tag A 
 eld in a table used 
for a memory hierarchy 

that contains the address 

information required 

to identify whether the 

associated block in the 

hierarchy corresponds to 

a requested word.

 5.3 The Basics of Caches 
385X4X1Xn Œ 2Xn Œ 1X2X3a. Before the reference to X
nX4X1Xn Œ 2Xn Œ 1X2X3b. After the reference to X
nXnFIGURE 5.7 The cache just before and just after a reference to a word Xn that is not initially in the cache. is reference causes a miss that forces the cache to fetch X
n from memory and 
insert it into the cache.
CacheMemory0000110001010100101111110000001011001010100101101
101011100111101
FIGURE 5.8 A direct-mapped cache with eight entries showing the addresses of memory 
words between 0 and 31 that map to the same cache locations.
 Because there are eight 
words in the cache, an address X maps to the direct-mapped cache word X modu
 at is, the low-order 
log2(8)  3 bits are used as the cach
 us, addresses 00001
two
, 01001two
, 10001two
, and 11001
two
 all map 
to entry 001
two
 of the cache, while addresses 00101
two
, 01101two
, 10101two
, and 11101
two
 all map to entry 101
two
 of the cache.

386 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
some of the cache entries may still be empty, as in 
Figure 5.7.
 us, we need to 
know that the tag should be ignored for such entr
 e most common method is 
to add a 
valid bit
 to indicate whether an entry contains a valid address. If the bit is 
not set, there cannot be a match for this block.
For the rest of this section, we will focus on explaining how a cache deals with 
reads. In general, handling reads is a little simpler than handling writes, since reads 
do not have to change the contents of the cache.
 er seeing the basics of how 
reads work and how cache misses can be handled, we’ll examine the cache designs 

for real computers and detail how these caches handle writes.
valid bit
 A 
 eld in 
the tables of a memory 
hierarchy that indicates 

that the associated block 

in the hierarchy contains 

valid data.
Caching is perhaps the most important example of the big idea of 
prediction
. It relies on the principle of locality to try to
 nd the 
desired data in the higher levels of the memory hierarchy, and provides 

mechanisms to ensure that when the prediction is wrong i
 nds and 
uses the proper data from the lower levels of the memory hierarchy
 e hit rates of the cache prediction on modern computers are o
 en higher 
than 95% (see 
Figure 5.47
).The BIGPictureAccessing a CacheBelow is a sequence of nine memory references to an empty eight-block cache, 

including the action for each reference. 
Figure 5.9
 shows how the contents of the 

cache change on each miss. Since there are eight blocks in the cache, the low-order 

three bits of an address give the block number:
Decimal addressof referenceBinary address
of referenceHit or missin cacheAssigned cache block(where found or placed)2210110twomiss (5.6b)(10110two mod 8) = 110two2611010twomiss (5.6c)(11010two mod 8) = 010two2210110twohit(10110two mod 8) = 110two2611010twohit(11010two mod 8) = 010two1610000twomiss (5.6d)(10000two mod 8) = 000two300011twomiss (5.6e)(00011two mod 8) = 011two1610000twohit(10000two mod 8) = 000two1810010twomiss (5.6f)(10010two mod 8) = 010two1610000twohit(10000two mod 8) = 000twoSince the cache is empty, several of th
 rst references are misses; the caption of 
Figure 5.9
 describes the actions for each memory reference. On the eighth reference 

 5.3 The Basics of Caches 
387IndexVTag
DataIndexVTag
Data000N
000N
001N001N
010N010N

011N011N

100N100N

101N101N

110N110Y
10twoMemory (10110
two)111N
111N
a. The initial state of the cache after power-onb. After handling a miss of address (10110two)IndexVTag
DataIndexVTag
Data000N
000Y
10twoMemory (10000
two)001N
001N
010Y11twoMemory (11010
two)010Y11
twoMemory (11010
two)011N011N
100N100N
101N101N
110Y
10twoMemory (10110
two)110Y10
twoMemory (10110
two)111N111N
c. After handling a miss of address (11010two)d. After handling a miss of address (10000two)IndexVTag
DataIndexVTag
Data000Y10
twoMemory (10000
two)000Y10
twoMemory (10000
two)001N001N

010Y11
twoMemory (11010
two)010Y
10twoMemory (10010
two)011Y
00twoMemory (00011
two)011Y00
twoMemory (00011
two)100N100N

101N101N
110Y
10twoMemory (10110
two)110Y10
twoMemory (10110
two)111N111N
e. After handling a miss of address (00011two)f. After handling a miss of address (10010two)FIGURE 5.9 The cache contents are shown after each reference request that misses, with the index and tag ﬁ
 elds shown in binary for the sequence of addresses on page 386.
 e cache is initially empty, with all valid bits (V entry in cache) 
turned
 
 e processor requests the following addresses: 10110
two
 (miss), 11010two
 (miss), 10110two
 (hit), 11010
two
 (hit), 10000
two
 (miss), 00011two
 (miss), 10000two
 (hit), 10010
two
 (miss), and 10000
two
 (hi
 e 
 gures show the cache contents a
 er each miss in the sequence has been 
handled. When address 10010
two
 (18) is referenced, the entry for address 11010
two
 (26) must be replaced, and a reference to 11010
two
 will cause a 
subsequen
 e ta
 eld will contain only the upper portion of the addr
 e full address of a word contained in cache block 
i with tag 
 eld j for this cache is 
j  8  i, or equivalently the concatenation of the ta
 eld j and the index 
i. For example, in cache 
f above, index 010
two
 has tag 10
two
 and corresponds to address 10010
two
.
388 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
we have con
 icting demands for a bloc e word at address 18 (10010
two
) should 
be brought into cache block 2 (010
two
). Hence, it must replace the word at address 
26 (11010two
), which is already in cache block 2 (010
two
 is behavior allows a 
cache to take advantage of temporal locality: recently referenced words replace less 
recently referenced words.
 is situation is directly analogous 
to needing a book from the shelves and 
having no more space on your desk—some book already on your desk must be 

returned to the shelves. In a direct-mapped cache, there is only one place to put the 

newly requested item and hence only one choice of what to replace.
We know where to look in the cache for each possible address: the low-order bits 
of an address can be used t
 nd the unique cache entry to which the address could 
map. 
Figure 5.10
 shows how a referenced address is divided into
 A t
ag
 eld
, which is used to compare with the value of the ta
 eld of the 
cache
 A cache index
, which is used to select the block
 e index of a cache block, together with the tag contents of that block, uniquely 
sp
 es the memory address of the word contained in the cache block. Because 
th eld is used as an address to reference the cache, and because an 
n-bit 
 eld has 2
n values, the total number of entries in a direct-mapped cache must be a 
power of 2. In the MIPS architecture, since words are aligned to multiples of four 

bytes, the le
 cant two bits of every address specify a byte within a word. 
Hence, the le
 cant two bits are ignored when selecting a word in the block.
 e total number of bits needed for a cache is a function of the cache size and 
the address size, because the cache includes both the storage for the data and the 

ta
 e size of the block above was one word, but normally it is several. For the 
following situation:
 32-bit addresses
 A direct-mapped cache
 e cache size is 2
n blocks, so 
n bits are used for the index
 e block size is 2
m words (2
m+2 bytes), so 
m bits are used for the word within 
the block, and two bits are used for the byte part of the address
the size of the ta
 eld is
32  (n  m  2). e total number of bits in a direct-mapped cache is
2n  (block size 
 tag size 
 eld size).

 5.3 The Basics of Caches 
389Since the block size is 2
m words (2
m5 bits), and we need 1 bit for th
 eld, the 
number of bits in such a cache is
2n  (2m  32  (32  n  m  2)  1)  2n  (2m  32  31  n  m).Although this is the actual size in bits, the naming convention is to exclude the size 
of the tag an
 eld and to count only the size of the dat
 us, the cache in 
Figure 5.10
 is called a 4 KiB cache.
Address (showing bit positions)DataHitDataTagValidTag
3220Index012102310221021=Index2010Byteoffset31 3013 12 112   1 0
FIGURE 5.10 For this cache, the lower portion of the address is used to select a cache 
entry consisting of a data word and a tag.
 is cache holds 1024 words or 4 KiB. We assume 32-bit 
addresses in this chapter.
 e tag from the cache is compared against the upper portion of the address to 
determine whether the entry in the cache corresponds 
to the requested address. Because the cache has 2
10 (or 
1024) words and a block size of one word, 10 bits are used to index the cache, leaving 32 −10 − 2 = 20 bits to 
be compared against the tag. If the tag and upper 20 bits of the address are equal and the valid bit is on, then 

the request hits in the cache, and the word is supplied to the processor. Otherwise, a miss occurs.

390 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Bits in a CacheHow many total bits are required for a direct-mapped cache with 16 KiB of 
data and 4-word blocks, assuming a 32-bit address?
We know that 16 KiB is 4096 (2
12) words. With a block size of 4 words (2
2), there are 1024 (2
10) blocks. Each block has 4 
 32 or 128 bits of data plus a 
tag, which is 32 
 10  2  2 bits, plus a valid bi
 us, the total cache size is
210  (4  32  (32  10  2  2)  1)  210  147  147 Kibibits
or 18.4 KiB for a 16 KiB cache. For this cache, the total number of bits in the 
cache is about 1.15 times as many as needed just for the storage of the data.
Mapping an Address to a Multiword Cache BlockConsider a cache with 64 blocks and a block size of 16 bytes. To what block 

number does byte address 1200 map?
We saw the formula on pag
 e block is given by
(Block address) modulo (Number of blocks in the cache)
where the address of the block is
Byte address
Bytes per block
Notice that this block address is the block containing all addresses between
Byte address
Bytes per block
Bytes per block
EXAMPLEANSWEREXAMPLEANSWER
 5.3 The Basics of Caches 
391andByte address
Bytes per block
Bytes per block(Bytes
  per block 1)
 us, with 16 bytes per block, byte address 1200 is block address
1200675which maps to cache block number (75 modulo 64) 
 11. In fact, this block 
maps all addresses between 1200 and 1215.
Larger blocks exploit spatial locality to lower miss rates. As 
Figure 5.11
 shows, 
increasing the block size usually decreases the miss rate
 e miss rate may go up 
eventually if the block size beco
 cant fraction of the cache size, because 
the number of blocks that can be held in the cache will become small, and there will 

be a great deal of competition for those blocks. As a result, a block will be bumped 

out of the cache before many of its words are accessed. Stated alternatively, spatial 

locality among the words in a block decreases with a very large block; consequently, 

the be
 ts in the miss rate become smaller.
A more serious problem associated with just increasing the block size is that the 
cost of a miss increas
 e miss penalty is determined by the time required to fetch 
4K1610%16K64K256K5%0%3264128256MissrateBlock sizeFIGURE 5.11 Miss rate versus block size. 
Note that the miss rate actually goes up if the block size 
is too large relative to the cache size. Each line represents a cache of
 erent size
 is 
 gure is independent 
of associativity, discussed soon.) Unfortunately, SPEC CPU2000 traces would take too long if block size were 
included, so this data is based on SPEC92.

392 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
the block from the next lower level of the hierarchy and load it into the cache
 e time to fetch the block has two parts: the latency to th rst word and the transfer 
time for the rest of the block. Clearly, unless we change the memory system, the 

transfer time—and hence the miss penalty—will likely increase as the block size 

increases. Furthermore, the improvement in the miss rate starts to decrease as the 

blocks become larger
 e result is that the increase in the miss penalty overwhelms 
the decrease in the miss rate for blocks that are too large, and cache performance 

thus decreases. Of course, if we design the memory to transfer larger blocks more 

  ciently, we can increase the block size and obtain further improvements in cache 
performance. We discuss this topic in the next section.
Elaboration: Although it is hard to do anything about the longer latency component of 
the miss penalty for large blocks, we may be able to hide some of the transfer time so 
that the miss penalty is effectively smaller. The simplest method for doing this, called 

early restart
, is simply to resume execution as soon as the requested word of the block 

is returned, rather than wait for the entire block. Many processors use this technique 

for instruction access, where it works best. Instruction accesses are largely sequential, 

so if the memory system can deliver a word every clock cycle, the processor may be 

able to restart operation when the requested word is returned, with the memory system 

delivering new instruction words just in time. This technique is usually less effective for 

data caches because it is likely that the words will be requested from the block in a 

less predictable way, and the probability that the processor will need another word from 

a different cache block before the transfer completes is high. If the processor cannot 
access the data cache because a transfer is ongoing, then it must stall.
An even more sophisticated scheme is to organize the memory so that the requested 
word is transferred from the memor rst. The remainder of the block 

is then transferred, starting with the address after the requested word and wrapping 

around to the beginning of the block. This technique, called 
requested word ﬁ rst
 or critical word ﬁ rst
, can be slightly faster than early restart, but it is limited by the same 

properties that limit early restart.
Handling Cache MissesBefore we look at the cache of a real system, let’s see how the control unit deals with 
cache misses
. (We describe a cache controller in detail in Sectio
 e control 
unit must detect a miss and process the miss by fetching the requested data from 
memory (or, as we shall see, a lower-level cache). If the cache reports a hit, the 

computer continues using the data as if nothing happened.
Modifying the control of a processor to handle a hit is trivial; misses, however, 
require some extra wo
 e cache miss handling is done in collaboration with 
the processor control unit and with a separate controller that initiates the memory 

access and r
 lls the cache.
 e processing of a cache miss creates a pipeline stall 
(Chapter 4) as opposed to an interrupt, which would require saving the state of all 

registers. For a cache miss, we can stall th
e entire processor, essentially freezing 
the contents of the temporary and programmer-visible registers, while we wait 
cache miss
 A request for 
data from the cache that 
cannot b
 lled because 
the data is not present in 

the cache.

 5.3 The Basics of Caches 
393for memory. More sophisticated out-of-order processors can allow execution of 
instructions while waiting for a cache miss, but we’ll assume in-order processors 

that stall on cache misses in this section.
Let’s look a little more closely at how instruction misses are handled; the same 
approach can be easily extended to handle data misses. If an instruction access 

results in a miss, then the content of the Instruction register is invalid. To get the 

proper instruction into the cache, we must be able to instruct the lower level in the 

memory hierarchy to perform a read. Since the program counter is incremented in 

th rst clock cycle of execution, the address of the instruction that generates an 

instruction cache miss is equal to the value of the program counter minus 4. Once 

we have the address, we need to instruct the main memory to perform a read. We 

wait for the memory to respond (since the access will take multiple clock cycles), 

and then write the words containing the desired instruction into the cache.
We can no
 ne the steps to be taken on an instruction cache miss:
1. Send the original PC value (current PC – 4) to the memory.
2. Instruct main memory to perform a read and wait for the memory to 
complete its access.
3. Write the cache entry, putting the data from memory in the data portion of 
the entry, writing the upper bits of the address (from the ALU) into the tag 
 eld, and turning the valid bit on.
4. Restart the instruction execution at th
 rst step, which will refetch the 
instruction, t nding it in the cache.
 e control of the cache on a data access 
is essentially identical: on a miss, we 
simply stall the processor until the memory responds with the data.
Handling WritesWrites work somewhat
 erently. Suppose on a store instruction, we wrote the 
data into only the data cache (without changing main memory); then, a
 er the 
write into the cache, memory would hav
 erent value from that in the cache. 
In such a case, the cache and memory are said to be 
inconsistent
 e simplest way 
to keep the main memory and the cache consistent is always to write the data into 

both the memory and the cache.
 is scheme is called 
write-through
. e other key aspect of writes is what occurs on a write miss. W
 rst fetch the 
words of the block from memory
 er the block is fetched and placed into the 
cache, we can overwrite the word that caused the miss into the cache block. We also 

write the word to main memory using the full address.
Although this design handles writes very simply, it would not provide very 
good performance. With a write-through scheme, every write causes the data 

to be written to main memory
 ese writes will take a long time, likely at least 
100 processor clock cycles, and could slow down the processor considerably. For 

example, suppose 10% of the instructions are stores. If the CPI without cache 
write-through
 A scheme in which writes 
always update both the 

cache and the next lower 

level of the memory 

hierarchy, ensuring that 

data is always consistent 

between the two.

394 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
misses was 1.0, spending 100 extra cycles on every write would lead to a CPI of 
1.0  100  10%  11, reducing performance by more than a factor of 10.
One solution to this problem is to use a 
write bu
 er. A write bu
 er stores the 
data while it is waiting to be written to memory
 er writing the data into the 
cache and into the write bu
 er, the processor can continue execution. When a write 
to main memory completes, the entry in the write b
 er is freed. If the write bu
 er is full when the processor reaches a write, the processor must stall until there is an 

empty position in the write bu
 er. Of course, if the rate at which the memory can 
complete writes is less than the rate at which the processor is generating writes, no 

amount of bu
 ering can help, because writes are being generated faster than the 
memory system can accept them.
 e rate at which writes are generated may also be 
less
 than the rate at which the 
memory can accept them, and yet stalls may still occur.
 is can happen when the 
writes occur in bursts. To reduce the occurrence of such stalls, processors usually 

increase the depth of the write b
 er beyond a single entry.
 e alternative to a write-through scheme is a scheme called 
write-back
. In a 
write-back scheme, when a write occurs, the new value is written only to the block 
in the cache
 e mo
 ed block is written to the lower level of the hierarchy when 
it is replaced. Write-back schemes can improve performance, especially when 

processors can generate writes as fast or faster than the writes can be handled by 

main memory; a write-back scheme is, however, more complex to implement than 

write-through.
In the rest of this section, we describe caches from real processors, and we 
examine how they handle both reads and writes. In Section 5.8, we will describe 

the handling of writes in more detail.
Elaboration: Writes introduce several complications into caches that are not present 
for reads. Here w cient 
implementation of writes in write-back caches.Consider a miss in a write-through cache. The most common strategy is to allocate a 
block in the cache, called 
write allocate. The block is fetched from memory and then the 

appropriate portion of the block is overwritten. An alternative strategy is to update the portion 

of the block in memory but not put it in the cache, called 
no write allocate. The motivation is 
that sometimes programs write entire blocks of data, such as when the operating system 

zeros a page of memory. In such cases, the fetch associated with the initial write miss may 

be unnecessary. Some computers allow the write allocation policy to be changed on a per 

page basis. ciently in a cache that uses a write-back strategy is 
more complex than in a write-through cache. A write-through cache can write the data 
into the cache and read the tag; if the tag mismatches, then a miss occurs. Because the 

cache is write-through, the overwriting of the block in the cache is not catastrophic, since 

memory has the correct value. In a write-back cache, w rst write the block back 

to memor ed and we have a cache miss. If we simply 

overwrote the block on a store instruction before we knew whether the store had hit in 

the cache (as we could for a write-through cache), we would destroy the contents of the 

block, which is not backed up in the next lower level of the memory hierarchy.
write bu
 er A queue 
that holds data while 
the data is waiting to be 

written to memory.
write-back
 A scheme 
that handles writes by 

updating values only to 

the block in the cache, 

then writing the mo
 ed block to the lower level 

of the hierarchy when the 

block is replaced.

 5.3 The Basics of Caches 
395In a write-back cache, because we cannot overwrite the block, stores either require 
two cycles (a cycle to check for a hit followed by a cycle to actually perform the write) or 
require a write buffer to hold that data—effectively allowing the store to take only one 
cycle by pipelining it. When a store buffer is used, the processor does the cache lookup 

and places the data in the store buffer during the normal cache access cycle. Assuming 

a cache hit, the new data is written from the store buffer into the cache on the next 

unused cache access cycle.By comparison, in a write-through cache, writes can always be done in one cycle. 
We read the tag and write the data portion of the selected block. If the tag matches 

the address of the block being written, the processor can continue normally, since the 

correct block has been updated. If the tag does not match, the processor generates a 

write miss to fetch the rest of the block corresponding to that address.
Many write-back caches also include write buffers that are used to reduce the miss 
 ed block. In such a case, ed block is 

moved to a write-back buffer associated with the cache while the requested block is read 

from memory. The write-back buffer is later written back to memory. Assuming another 

miss does not occur immediately, this technique halves the miss penalty when a dirty 

block must be replaced.An Example Cache: The Intrinsity FastMATH Processor
 e Intrinsity FastMATH is an embedded microprocessor that uses the MIPS 
architecture and a simple cache implementation. Near the end of the chapter, we 
will examine the more complex cache designs of ARM and Intel microprocessors, 

but we start with this simple, yet real, example for pedagogical reasons. 
Figure 5.12
 
shows the organization of the Intrinsity FastMATH data cache.
 is processor has a 12-stage pipeline. When operating at peak speed, the 
processor can request both an instruction word and a data word on every clock. 

To satisfy the demands of the pipeline without stalling, separate instruction 

and data caches are used. Each cache is 16 KiB, or 4096 words, with 16-word 

blocks.
Read requests for the cache are straightforward. Because there are separate 
data and instruction caches, we need separate control signals to read and write 

each cache. (Remember that we need to update the instruction cache when a miss 

occur
 us, the steps for a read request to either cache are as follows:
1. Send the address to the appropriate cache
 e address comes either from 
the PC (for an instruction) or from the ALU (for data).
2. If the cache signals hit, the requested word is available on the data lines. 
Since there are 16 words in the desired block, we need to select the right one. 

A bloc
 eld is used to control the multiplexor (shown at the bottom 
of th gure), which selects the requested word from the 16 words in the 

indexed block.

396 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
3. If the cache signals miss, we send the address to the main memory. When 
the memory returns with the data, we write it into the cache and then read it 
to fu ll the request.
For writes, the Intrinsity FastMATH o
 ers both write-through and write-back, 
leaving it up to the operating system to decide which strategy to use for an 

application. It has a one-entry write bu
 er.
What cache miss rates are attained with a cache structure like that used by the 
Intrinsity FastMATH? 
Figure 5.13
 shows the miss rates for the instruction and 

data cach
 e combined miss rate is th
 ective miss rate per reference for 
each program a
 er accounting for th
 ering frequency of instruction and data 
accesses.
Address (showing bit positions)DataHitDataTagVTag3218=Index188Byteoffset3114 132 1 0
6 54Block offset256entries512 bits18 bitsMux323232FIGURE 5.12 The 16 KiB caches in the Intrinsity FastMATH each contain 256 blocks with 16 words per block.
 e tag 
 eld is 18 bits wide and th
 eld is 8 bits wide, while a 4-bi
 eld (bits 5–2) is used to index the block and select the word from the block 
using a 16-to-1 multiplexor. In practice, to eliminate the multiplexor, caches use a separate large RAM for the data and a smal
ler RAM for the 
tags, with the block o
 set supplying the extra address bits for the large data RAM. In this case, the large RAM is 32 bits wide and must have 16 
times as many words as blocks in the cache.

 5.3 The Basics of Caches 
397Although miss rate is an important characteristic of cache designs, the ultimate 
measure will be th
 ect of the memory system on program execution time; we’ll 
see how miss rate and execution time are related shortly.
Elaboration: A combined cache with a total size equal to the sum of the two 
split caches will usually have a better hit rate. This higher rate occurs because the combined 
cache does not rigidly divide the number of entries that may be used by instructions 
from those that may be used by data. Nonetheless, almost all processors today use 
split instruction and data caches to increase cache 
bandwidth 
to match what modern 
pipelines expect. (There may also be few ict misses; see Section 5.8.)
Here are miss rates for caches the size of those found in the Intrinsity FastMATH 
processor, and for a combined cache whose size is equal to the sum of the two caches:
 Total cache size: 32 KiB
 Split cache effective miss rate: 3.24% Combined cache miss rate: 3.18%The miss rate of the split cache is only slightly worse.
The advantage of doubling the cache bandwidth, by supporting both an instruction 
and data access simultaneously, easily overcomes the disadvantage of a slightly 

increased miss rate. This observation cautions us that we cannot use miss rate as the 

sole measure of cache performance, as Section 5.4 shows.
Summary
We began the previous section by examining the simplest of caches: a direct-mapped 
cache with a one-word block. In such a cache, both hits and misses are simple, since 

a word can go in exactly one location and there is a separate tag for every word. To 

keep the cache and memory consistent, a write-through scheme can be used, so 

that every write into the cache also causes memory to be updated
 e alternative 
to write-through is a write-back scheme that copies a block back to memory when 

it is replaced; we’ll discuss this scheme further in upcoming sections.
split cache
 A scheme 
in which a level of the 
memory hierarchy 

is composed of two 

independent caches that 

operate in parallel with 

each other, with one 

handling instructions and 

one handling data.
Instruction miss rateData miss rateEffective combined miss rate
0.4%11.4%3.2%FIGURE 5.13 Approximate instruction and data miss rates for the Intrinsity FastMATH 
processor for SPEC CPU2000 benchmarks.
 e combined miss rate is th
 ective miss rate seen 
for the combination of the 16 KiB instruction cache and 16 KiB data cache. It is obtained by weighting the 
instruction and data individual miss rates by the frequency of instruction and data references.

398 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
To take advantage of spatial locality, a cache must have a block size larger than 
one word
 e use of a larger block decreases the miss rate and improves the 
  ciency of the cache by reducing the amount of tag storage relative to the amount 
of data storage in the cache. Although a larger block size decreases the miss rate, it 
can also increase the miss penalty. If the miss penalty increased linearly with the 

block size, larger blocks could easily lead to lower performance.
To avoid performance loss, the bandwidth of main memory is increased to 
transfer cache blocks mor
  ciently. Common methods for increasing bandwidth 
external to the DRAM are making the memory wider and interleaving. DRAM 

designers have steadily improved the interface between the processor and memory 

to increase the bandwidth of burst mode transfers to reduce the cost of larger cache 

block sizes.
 e speed of the memory system a
 ects the designer’s decision on the size of 
the cache block. Which of the following cache designer guidelines are generally 
valid? e shorter the memory latency, the smaller the cache block
 e shorter the memory latency, the larger the cache block
 e higher the memory bandwidth, the smaller the cache block
 e higher the memory bandwidth, the larger the cache block
 5.4  Measuring and Improving Cache 
Performance
In this section, we begin by examining ways to measure and analyze cache 
performance. We then explore tw
 erent techniques for improving cache 
performance. One focuses on reducing the miss rate by reducing the probability 

that tw
 erent memory blocks will contend for the same cache locatio
 e second technique reduces the miss penalty by adding an additional level to the 

hierarchy
 is technique, called 
multilevel caching
 rst appeared in high-end 
computers selling for more than $100,000 in 1990; since then it has become 

common on personal mobile devices selling for a few hundred dollars!
Check Yourself

 5.4 Measuring and Improving Cache Performance 
399CPU time can be divided into the clock cycles that the CPU spends executing 
the program and the clock cycles that the CPU spends waiting for the memory 
system. Normally, we assume that the costs of cache accesses that are hits are part 

of the normal CPU execution cyc
 us,CPU time   (CPU execution clock cycles 
 Memory-stall clock cycles) 
 Clock cycle time
 e memory-stall clock cycles come primarily from cache misses, and we make 
that assumption here. We also restrict the discussion to a simp ed model of the 

memory system. In real processors, the stalls generated by reads and writes can be 

quite complex, and accurate performance prediction usually requires very detailed 

simulations of the processor and memory system.
Memory-stall clock cycles can b
 ned as the sum of the stall cycles coming 
from reads plus those coming from writes:
Memory-stall clock cycles 
 (Read-stall cycles 
 Write-stall cycles)
 e read-stall cycles can be
 ned in terms of the number of read accesses per 
program, the miss penalty in clock cycles for a read, and the read miss rate:
Read-stall cycles
ReadsProgram
Read miss rateRead miss pe
nnaltyWrites are more complicated. For a write-through scheme, we have two sources of 
stalls: write misses, which usually require that we fetch the block before continuing 

the write (see the 
Elaboration
 on page 394 for more details on dealing with writes), 
and write bu
 er stalls, which occur when the write b
 er is full when a write 
occur
 us, the cycles stalled for writes equals the sum of these two:
Write-stall cycles
Writes
Program
Write miss rateWrite mis
ss penalty
stalls
Because the write bu
 er stalls depend on the proximity of writes, and not just 
the frequency, it is not possible to give a simple equation to compute such stalls. 
Fortunately, in systems with a reasonable write b
 er depth (e.g., four or more 
words) and a memory capable of accepting writes at a rate that
 cantly exceeds 
the average write frequency in programs (e.g., by a factor of 2), the write bu
 er stalls will be small, and we can safely ignore them. If a system did not meet these 

criteria, it would not be well designed; in
stead, the designer should have used either 
a deeper write bu
 er or a write-back organization.

400 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Write-back schemes also have potential additional stalls arising from the need 
to write a cache block back to memory when the block is replaced. We will discuss 
this more in Section 5.8.
In most write-through cache organizations, the read and write miss penalties are 
the same (the time to fetch the block from memory). If we assume that the write 

bu
 er stalls are negligible, we can combine the reads and writes by using a single 
miss rate and the miss penalty:
Memory-stall clock cyclesMemory accessesProgram
Miss rateMiss penaltyWe can also factor this as
Memory-stall clock cyclesInstructions
Program
Misses
Instru
cction
Miss penaltyLet’s consider a simple example to help us understand the impact of cache 
performance on processor performance.
Calculating Cache Performance
Assume the miss rate of an instruction cache is 2% and the miss rate of the data 

cache is 4%. If a processor has a CPI of 2 without any memory stalls and the 

miss penalty is 100 cycles for all misses, determine how much faster a processor 

would run with a perfect cache that never missed. Assume the frequency of all 

loads and stores is 36%.
 e number of memory miss cycles for instructions in terms of the Instruction 
count (I) is
Instruction miss cycles 
 I  2%  100  2.00  IAs the frequency of all loads and stores is 36%, we ca
 nd the number of 
memory miss cycles for data references:
Data miss cycles 
 I  36%  4%  100  1.44  IEXAMPLEANSWER
 5.4 Measuring and Improving Cache Performance 
401What happens if the processor is made faster, but the memory system is no
 e amount of time spent on memory stalls will take up an increasing fraction of the 
execution time; Amdahl’s Law, which we
 examined in Chapter 1, reminds us of 
this fact. A few simple examples show how serious this problem can be. Suppose 

we speed-up the computer in the previous example by reducing its CPI from 2 to 1 

without changing the clock rate, which might be done with an improved pipeline. 

 e system with cache misses would then have a CPI of 1 
 3.44  4.44, and the 
system with the perfect cache would be
444
1.4.44 times as fast
. e amount of execution time spent on memory stalls would have risen from
344
544
.
.63%to344
444
..77%Similarly, increasing the clock rate without changing the memory system also 
increases the performance lost due to cache misses.
 e previous examples and equations assume that the hit time is not a factor in 
determining cache performance. Clearly, if the hit time increases, the total time to 

access a word from the memory system will in
crease, possibly causing an increase in 
the processor cycle time. Although we will see additional examples of what can increase 
 e total number of memory-stall cycles is 2.00 I 
 1.44 I  is is 
more than three cycles of memory stall per instruction. Accordingly, the total 

CPI including memory stalls is 2 
 3.44  5.44. Since there is no change in 
instruction count or clock rate, the ratio of the CPU execution times is
CPU time with stalls
CPU time with perfect cache
ICPI
stall
Clock cycle
ICPIClock cycle
CPI
CPI
5perfect
stall
perfect
..44
2 e performance with the perfect cache is better by 
544
2.2.72.
402 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
hit time shortly, one example is increasing the cache size. A larger cache could clearly 
have a longer access time, just as, if your desk in the library was very large (say, 3 square 

meters), it would take longer to locate a book on the desk. An increase in hit time 

likely adds another stage to the pipeline, since it may take multiple cycles for a cache 

hit. Although it is more complex to calculate the performance impact of a deeper 

pipeline, at some point the increase in hit time for a larger cache could dominate the 

improvement in hit rate, leading to a decrease in processor performance.
To capture the fact that the time to access data for both hits and misses a
 ects 
performance, designers sometime use 
average memory access time
 (AMAT) as 
a way to examine alternative cache designs. Average memory access time is the 

average time to access memory considering both hits and misses and the frequency 

o
 erent accesses; it is equal to the following:
AMAT 
 Time for a hit 
 Miss rate 
 Miss penalty
Calculating Average Memory Access Time
Find the AMAT for a processor with a 1 ns clock cycle time, a miss penalty of 

20 clock cycles, a miss rate of 0.05 misses per instruction, and a cache access 

time (including hit detection) of 1 clock cycle. Assume that the read and write 

miss penalties are the same and ignore other write stalls.
 e average memory access time per instruction is
AMATTime 
for a hitMiss rateMiss penalt
y10.0520
2 clocck cyclesor 2 ns.
 e next subsection discusses alternative cache organizations that decrease 
miss rate but may sometimes increase hit time; additional examples appear in 
Section 5.15, Fallacies and Pitfalls.
Reducing Cache Misses by More Flexible Placement 
of BlocksSo far, when we place a block in the cache, we have used a simple placement scheme: 
A block can go in exactly one place in the cache. As mentioned earlier, it is called 

direct mapped
 because there is a direct mapping from any block address in memory 
to a single location in the upper level of the hierarchy. However, there is actually a 

whole range of schemes for placing blocks. Direct mapped, where a block can be 

placed in exactly one location, is at one extreme.
EXAMPLEANSWER
 5.4 Measuring and Improving Cache Performance 
403At the other extreme is a scheme where a block can be placed in 
any
 location 
in the cache. Such a scheme is called 
fully associative
, because a block in memory 
may be associated with any entry in the cache. To
 nd a given block in a fully 
associative cache, all the entries in the cache must be searched because a block 
can be placed in any one. To make the search practical, it is done in parallel with 

a comparator associated with each cache entry.
 ese comparator
 cantly 
increase the hardware
 ectively making fully associative placement practical 
only for caches with small numbers of blocks.
 e middle range of designs between direct mapped and fully associative 
is called 
set associative
. In a set-associative cache, there ar
 xed number of 
locations where each block can be placed. A set-associative cache with 
n locations 
for a block is called an 
n-way set-associative cache. An 
n-way set-associative cache 
consists of a number of sets, each of which consists of 
n blocks. Each block in the 
memory maps to a unique 
set
 in the cache given by th
 eld, and a block can 
be placed in 
any
 element of that s
 us, a set-associative placement combines 
direct-mapped placement and fully associative placement: a block is directly 

mapped into a set, and then all the blocks in the set are searched for a match. For 

example, 
Figure 5.14
 shows where block 12 may be placed in a cache with eight 

blocks total, according to the three block placement policies.
Remember that in a direct-mapped cache, the position of a memory block is 
given by
(Block number) modulo (Number of 
blocks
 in the cache)
fully associative 
cache
 A cache structure 
in which a block can be 
placed in any location in 

the cache.
set-associative cache
 A cache tha
 xed 
number of locations (at 

least two) where each 

block can be placed.
Direct mapped2457
6013
Block #
DataTag
Search12Set associative2013
Set #DataTag
Search1
2Fully associative
DataTag
Search1
2FIGURE 5.14 The location of a memory block whose address is 12 in a cache with eight 
blocks varies for direct-mapped, set-associative, and fully associative placement.
 In direct-
mapped placement, there is only one cache block where memory block 12 can be found, and that block is 
given by (12 modulo 8) 
 4. In a two-way set-associative cache, there would be four sets, and memory block 
12 must be in set (12 mod 4) 
 0; the memory block could be in either element of the set. In a fully associative 
placement, the memory block for block address 12 can appear in any of the eight cache blocks.

404 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
In a set-associative cache, the set containing a memory block is given by
(Block number) modulo (Number of 
sets
 in the cache)
Since the block may be placed in any element of the set, 
all the tags of all the elements 
of the set
 must be searched. In a fully associative cache, the block can go anywhere, 
and 
all tags of all the blocks in the cache
 must be searched.
We can also think of all block placement strategies as a variation on set 
associativity. 
Figure 5.15
 shows the possible associativity structures for an eight-
block cache. A direct-mapped cache is simply a one-way set-associative cache: 

each cache entry holds one block and each set has one element. A fully associative 

cache with 
m entries is simply an 
m-way set-associative cache; it has one set with 
m blocks, and an entry can reside in any block within that set.
 e advantage of increasing the degree of associativity is that it usually decreases 
the miss rate, as the next example sho e main disadvantage, which we discuss 

in more detail shortly, is a potential increase in the hit time.
Eight-way set associative (fully associative)TagTagDataDataTag
TagDataData
TagTagDataDataTag
TagDataData
TagTagDataDataTag
TagDataData
SetFour-way set associativeTagTagDataData
Set01012301
2
3
4567Two-way set associativeTagData
BlockOne-way set associative(direct mapped)FIGURE 5.15 An eight-block cache conﬁ gured as direct mapped, two-way set associative, 
four-way set associative, and fully associative.
 e total size of the cache in blocks is equal to the 
number of sets times the associativity
 us, fo
 xed cache size, increasing the associativity decreases 
the number of sets while increasing the number of elements per set. With eight blocks, an eight-way set-
associative cache is the same as a fully associative cache.

 5.4 Measuring and Improving Cache Performance 
405Misses and Associativity in CachesAssume there are three small caches, each consisting of four one-word blocks. 
One cache is fully associative, a second is two-way set-associative, and the 

third is direct-mapped. Find the number of misses for each cache organization 

given the following sequence of block addresses: 0, 8, 0, 6, and 8.
 e direct-mapped case is easiest. First, let’s determine to which cache block 
each block address maps:
Block addressCache block
0(0 modulo 4)  06(6 modulo 4)  28(8 modulo 4)  0Now we ca
 ll in the cache contents a
 er each reference, using a blank entry to 
mean that the block is invalid, colored text to show a new entry added to the cache 
for the associated reference, and plain text to show an old entry in the cache:
Address of memory
block accessedHitor missContents of cache blocks after reference01230missMemory[0]
8missMemory[8]
0missMemory[0]
6missMemory[0]
Memory[6]
8missMemory[8]
Memory[6]
 e direct-mapped cache generat
 ve misses for th
 ve accesses.
 e set-associative cache has two sets (with indices 0 and 1) with two 
elements per set. Let’ rst determine to which set each block address maps:
Block addressCache set0(0 modulo 2)  06(6 modulo 2)  08(8 modulo 2)  0Because we have a choice of which entry in a set to replace on a miss, we need 

a replacement rule. Set-associative caches usually replace the least recently 

used block within a set; that is, the block that was used furthest in the past 
EXAMPLEANSWER
406 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
is replaced. (We will discuss other replacement rules in more detail shortly.) 
Using this replacement rule, the contents of the set-associative cache a
 er each 
reference looks like this:
Address of memory
block accessedHitor missContents of cache blocks after referenceSet 0Set 0Set 1Set 1
0missMemory[0]
8missMemory[0]
Memory[8]
0hitMemory[0]Memory[8]
6missMemory[0]
Memory[6]
8missMemory[8]
Memory[6]
Notice that when block 6 is referenced, it replaces block 8, since block 8 has 
been less recently referenced than bloc
 e two-way set-associative cache 
has four misses, one less than the direct-mapped cache.
 e fully associative cache has four cache blocks (in a single set); any 
memory block can be stored in any cache bloc
 e fully associative cache has 
the best performance, with only three misses:
Address of memory
block accessedHitor missContents of cache blocks after referenceBlock 0Block 1Block 2Block 3
0missMemory[0]
8missMemory[0]
Memory[8]
0hitMemory[0]Memory[8]
6missMemory[0]Memory[8]
Memory[6]
8hitMemory[0]Memory[8]Memory[6]
For this series of references, three misses is the best we can do, because three 
unique block addresses are accessed. Notice that if we had eight blocks in the 
cache, there would be no replacements in the two-way set-associative cache 

(check this for yourself), and it would have the same number of misses as the 

fully associative cache. Similarly, if we had 16 blocks, all 3 caches would have 

the same number of misses. Even this trivial example shows that cache size and 

associativity are not independent in determining cache performance.
How much of a reduction in the miss rate is achieved by associativity? 
Figure 5.16
 shows the improvement for a 64 KiB data cache with a 16-word block, 

and associativity ranging from direct mapped to eight-way. Going from one-way 

to two-way associativity decreases the miss rate by about 15%, but there is little 

further improvement in going to higher associativity.

 5.4 Measuring and Improving Cache Performance 
407Locating a Block in the CacheNow, let’s consider the task o nding a block in a cache that is set associative. 
Just as in a direct-mapped cache, each block in a set-associative cache includes 

an address tag that gives the block addr
 e tag of every cache block within 
the appropriate set is checked to see if it matches the block address from the 

processor. 
Figure 5.17
 decomposes the addr
 e index value is used to select 
the set containing the address of interest, and the tags of all the blocks in the set 

must be searched. Because speed is of the essence, all the tags in the selected set are 

searched in parallel. As in a fully associative cache, a sequential search would make 

the hit time of a set-associative cache too slow.
If the total cache size is kept the same, increasing the associativity increases the 
number of blocks per set, which is the number of simultaneous compares needed 

to perform the search in parallel: each increase by a factor of 2 in associativity 

doubles the number of blocks per set and halves the number of sets. Accordingly, 

each factor-of-2 increase in associativity decreases the size of the index by 1 bit and 

increases the size of the tag by 1 bit. In a fully associative cache, ther
 ectively 
only one set, and all the blocks must be checked in parallel
 us, there is no index, 
and the entire address, excluding the block o
 set, is compared against the tag of 
every block. In other words, we search the entire cache without any indexing.
In a direct-mapped cache, only a single comparator is needed, because the entry can 
be in only one block, and we access the cache simply by indexing. 
Figure 5.18
 shows 

that in a four-way set-associative cache, four comparators are needed, together with 

a 4-to-1 multiplexor to choose among the four potential members of the selected set. 

 e cache access consists of indexing the appropriate set and then searching the tags 
of the s e costs of an associative cache are the extra comparators and any delay 

imposed by having to do the compare and select from among the elements of the set.
AssociativityData miss rate110.3%28.6%48.3%88.1%FIGURE 5.16 The data cache miss rates for an organization like the Intrinsity FastMATH 
processor for SPEC CPU2000 benchmarks with associativity varying from one-way to 
eight-way.
 ese results for 10 SPEC CPU2000 programs are from Hennessy and Patterson (2003).
Block offsetTagIndexFIGURE 5.17 The three portions of an address in a set-associative or direct-mapped 
cache.  e index is used to select the set, then the tag is used to choose the block by comparison with the 
blocks in the selected s
 e block o
 set is the address of the desired data within the block.

408 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 e choice among direct-mapped, set-associative, or fully associative mapping 
in any memory hierarchy will depend on the cost of a miss versus the cost of 
implementing associativity, both in time and in extra hardware.
Elaboration: A Content Addressable Memory
 (CAM) is a circuit that combines comparison and storage in a single device. Instead of supplying an address and reading 
a word like a RAM, you supply the data and the CAM looks to see if it has a copy and 

returns the index of the matching row. CAMs mean that cache designers can afford to 

implement much higher set associativity than if they needed to build the hardware out 

of SRAMs and comparators. In 2013, the greater size and power of CAM generally leads 

to 2-way and 4-way set associativity being built from standard SRAMs and comparators, 

with 8-way and above built using CAMs.
AddressDataTagVTag
=Index22831 3012 11 10 9 83 2 1 0
4-to-1 multiplexorIndex01
2253254255DataVTag
=DataVTag
=DataVTag
22=32DataHitFIGURE 5.18 The implementation of a four-way set-associative cache requires four 
comparators and a 4-to-1 multiplexor.
 e comparators determine which element of the selected set 
(if any) matches the ta
 e output of the comparators is used to select the data from one of the four blocks 
of the indexed set, using a multiplexor with a decoded 
select signal. In some implementations, the Output 
enable signals on the data portions of the cache RAMs can be used to select the entry in the set that drives the 
outpu e Output enable signal comes from the comparators, causing the element that matches to drive the 

data outpu
 is organization eliminates the need for the multiplexor.

 5.4 Measuring and Improving Cache Performance 
409Choosing Which Block to ReplaceWhen a miss occurs in a direct-mapped cache, the requested block can go in 
exactly one position, and the block occupying that position must be replaced. In 

an associative cache, we have a choice of where to place the requested block, and 

hence a choice of which block to replace. In a fully associative cache, all blocks are 

candidates for replacement. In a set-associative cache, we must choose among the 

blocks in the selected set.
 e most commonly used scheme is 
least recently used (LRU)
, which we used 
in the previous example. In an LRU scheme, the block replaced is the one that has 
been unused for the longest time e set associative example on page 405 uses 

LRU, which is why we replaced Memory(0) instead of Memory(6).
LRU replacement is implemented by keeping track of when each element in a 
set was used relative to the other elements in the set. For a two-way set-associative 

cache, tracking when the two elements were used can be implemented by keeping 

a single bit in each set and setting the bit to indicate an element whenever that 

element is referenced. As associativity increases, implementing LRU gets harder; in 

Section 5.8, we will see an alternative scheme for replacement.
Size of Tags versus Set Associativity
Increasing associativity requires more comparators and more tag bits per 

cache block. Assuming a cache of 4096 blocks, a 4-word block size, and a 

32-bit addr
 nd the total number of sets and the total number of tag bits 
for caches that are direct mapped, two-way and four-way set associative, and 

fully associative.
Since there are 16 (
 24) bytes per block, a 32-bit address yields 32
4  28 bits 
to be used for index and ta
 e direct-mapped cache has the same number 
of sets as blocks, and hence 12 bits of index, since log
2(4096)  12; hence, the 
total number is (28
12)  4096  16  4096  66 K tag bits.
Each degree of associativity decreases the number of sets by a factor of 2 and 
thus decreases the number of bits used to index the cache by 1 and increases 
the number of bits in the tag b
 us, for a two-way set-associative cache, 
there are 2048 sets, and the total number of tag bits is (28
11)  2  2048  34  2048  70 Kbits. For a four-way set-associative cache, the total number 
of sets is 1024, and the total number is (28
10)  4  1024  72  1024  74 K tag bits.
For a fully associative cache, there is only one set with 4096 blocks, and the 
tag is 28 bits, leading to 28 
 4096  1  115 K tag bits.
least recently used 
(LRU)
 A replacement 
scheme in which the 
block replaced is the one 

that has been unused for 

the longest time.
EXAMPLEANSWER
410 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Reducing the Miss Penalty Using Multilevel Caches
All modern computers make use of caches. To close the gap further between the 
fast clock rates of modern processors and the increasingly long time required to 

access DRAMs, most microprocessors support an additional level of cachin
 is second-level cache is normally on the same chip and is accessed whenever a miss 

occurs in the primary cache. If the second-level cache contains the desired data, 

the miss penalty for th
 rst-level cache will be essentially the access time of the 
second-level cache, which will be much less than the access time of main memory. 

If neither the primary nor the secondary cache contains the data, a main memory 

access is required, and a larger miss penalty is incurred.
Ho
 cant is the performance improvement from the use of a secondary 
cach
 e next example shows us.
Performance of Multilevel Caches
Suppose we have a processor with a 
base CPI of 1.0, assuming all references 
hit in the primary cache, and a clock rate of 4 GHz. Assume a main memory 

access time of 100 ns, including all the miss handling. Suppose the miss rate 

per instruction at the primary cache is 2%. How much faster will the processor 

be if we add a secondary cache that has a 5 ns access time for either a hit or 

a miss and is large enough to reduce the miss rate to main memory to 0.5%?
 e miss penalty to main memory is
100025
 ns nsclock cycle
400 clock cycles. e 
 ective CPI with one level of caching is given by
Total CPI 
 Base CPI 
 Memory-stall cycles per instruction
For the processor with one level of caching,
Total CPI 
 1.0  Memory-stall cycles per instruction 
 1.0  2%  400  9With two levels of caching, a miss in the primary (o
 rst-level) cache can be 
satis
 ed either by the secondary cache or by main memory
 e miss penalty 
for an access to the second-level cache is
5025
 ns nsclock cycle
20 clock cycles.EXAMPLEANSWER
 5.4 Measuring and Improving Cache Performance 
411If the miss is sa
 ed in the secondary cache, then this is the entire miss 
penalty. If the miss needs to go to main memory, then the total miss penalty is 
the sum of the secondary cache access time and the main memory access time.
 us, for a two-level cache, total CPI is the sum of the stall cycles from both 
levels of cache and the base CPI:
Total CPI1Primary stalls per instructionSecondary stall
ss per instruction
12%200.5%40010.42.03.4
 us, the processor with the secondary cache is faster by
9034..2.6Alternatively, we could have computed the stall cycles by summing the stall 
cycles of those references that hit in the secondary cache ((2%
0.5%)  20  ose references that go to main memory, which must include the 
cost to access the secondary cache as well as the main memory access time, are 

(0.5%  (20  400)  e sum, 1.0 
 0.3  2.1, is again 3.4.
 e design considerations for a primary and secondary cache ar
 cantly 
 erent, because the presence of the other cache changes the best choice versus 
a single-level cache. In particular, a two-level cache structure allows the primary 

cache to focus on minimizing hit time to yield a shorter clock cycle or fewer 

pipeline stages, while allowing the secondary cache to focus on miss rate to reduce 

the penalty of long memory access times.
 e 
 ect of these changes on the two caches can be seen by comparing each 
cache to the optimal design for a single level of cache. In comparison to a single-

level cache, the primary cache of a 
multilevel cache
 is o
 en smaller. Furthermore, 
the primary cache may use a smaller block size, to go with the smaller cache size and 

also to reduce the miss penalty. In comparison, the secondary cache will be much 

larger than in a single-level cache, since the access time of the secondary cache is 

less critical. With a larger total size, the secondary cache may use a larger block size 

than appropriate with a single-level cache. It o
 en uses higher associativity than 
the primary cache given the focus of reducing miss rates.
Sorting has been exhaustively analyzed t
 nd better algorithms: Bubble Sort, 
Quicksort, Radix Sort, and so on. 
Figure 5.19(a)
 shows instructions executed by 
item searched for Radix Sort versus Quicksort. As expected, for large arrays, Radix 

Sort has an algorithmic advantage over Quicksort in terms of number of operations. 

Figure 5.19(b)
 shows time per key instead of instructions executed. We see that the 

lines start on the same trajectory as in 
Figure 5.19(a)
, but then the Radix Sort line 
multilevel cache
 A memory hierarchy with 
multiple levels of caches, 

rather than just a cache 

and main memory.
Understanding 
Program 

Performance

412 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
FIGURE 5.19 Comparing Quicksort and Radix Sort by (a) instructions executed per item 
sorted, (b) time per item sorted, and (c) cache misses per item sorted.
 is data is from a 
paper by LaMarca and Ladner [1996]. Due to such results, new versions of Radix Sort have been invented 
that take memory hierarchy into account, to regain its algorithmic advantages (see Sectio
 e basic 
idea of cache optimizations is to use all the data in a block repeatedly before it is replaced on a miss.
Radix Sort
Quicksort
Size (K items to sort)
Instructions/item
0481632
200400
600
8001000
120064128256512
102420484096
a.Radix Sort
Quicksort
Size (K items to sort)
Clock cycles/item
0481632
4008001200
1600200064128256512
102420484096
b.Radix Sort
Quicksort
Size (K items to sort)
Cache misses/item
0481632
1234564128256512
102420484096
c.
 5.4 Measuring and Improving Cache Performance 
413diverges as the data to sort increases. What is going on? 
Figure 5.19(c)
 answers by 
looking at the cache misses per item sorted: Quicksort consistently has many fewer 

misses per item to be sorted.
Alas, standard algorithmic analysis o
 en ignores the impact of the memory 
hierarchy. As faster clock rates and 
Moore’s Law
 allow architects to squeeze all of 
the performance out of a stream of instructions, using the memory hierarchy well 

is critical to high performance. As we said in the introduction, understanding the 

behavior of the memory hierarchy is critical to understanding the performance of 

programs on today’s computers.
Software Optimization via Blocking
Given the importance of the memory hierarchy to program performance, not 

surprisingly many so
 ware optimizations were invented that can dramatically 
improve performance by reusing data within the cache and hence lower miss rates 

due to improved temporal locality.
When dealing with arrays, we can get good performance from the memory 
system if we store the array in memory so that accesses to the array are sequential 

in memory. Suppose that we are dealing with multiple arrays, however, with some 

arrays accessed by rows and some by columns. Storing the arrays row-by-row 

(called row major order
) or column-by-column (
column major order
) does not 
solve the problem because both rows and columns are used in every loop iteration. 
Instead of operating on entire rows or columns of an array, 
blocked
 algorithms 
operate on submatrices or 
blocks
 e goal is to maximize accesses to the data 
loaded into the cache before the data are replaced; that is, improve temporal locality 

to reduce cache misses. 
For example, the inner loops of DGEMM (lines 4 through 9 of Figure 3.21 in 
Chapter 3) are
for (int j = 0; j < n; ++j)     {
     double cij = C[i+j*n]; /* cij = C[i][j] */
     for( int k = 0; k < n; k++ )
       cij += A[i+k*n] * B[k+j*n]; /* cij += A[i][k]*B[k][j] */

     C[i+j*n] = cij; /* C[i][j] = cij */
     }
}It reads all 
N-by-
N elements of 
B, reads the same 
N elements in what corresponds to 
one row of 
A repeatedly, and writes what corresponds to one row of 
N elements of 
C e comments make the rows and columns of the matrices easier to identify.) 
Figure 5.20
 gives a snapshot of the accesses to the three arrays. A dark shade 
indicates a recent access, a light shade indicates an older access, and white means 

not yet accessed.

414 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 e number of capacity misses clearly depends on 
N and the size of the cache. If 
it can hold all three 
N-by-
N matrices, then all is well, provided there are no cache 
co
 icts. We purposely picked the matrix size to be 32 by 32 in DGEMM for 
Chapters 3 and 4 so that this would be the case. Each matrix is 32 
 32  1024 elements and each element is 8 bytes, so the three matrices occupy 24 KiB, which 
comfortabl t in the 32 KiB data cache of the Intel Core i7 (Sandy Bridge).
If the cache can hold one 
N-by-
N matrix and one row of 
N, then at least the 
ith 
row of 
A and the array 
B may stay in the cache. Less than that and misses may 

occur for both 
B and 
C. In the worst case, there would be 2 
N3  N2 memory words 
accessed for 
N3 operations.
To ensure that the elements being accessed ca
 t in the cache, the original code 
is changed to compute on a submatrix. Hence, we essentially invoke the version of 

DGEMM from Figure 4.80 in Chapter 4 repeatedly on matrices of size 
BLOCKSIZE by 
BLOCKSIZE. BLOCKSIZE is called the 
blocking factor
. Figure 5.21
 shows the blocked version of D
 e function 
do_block is DGEMM from Figure 3.21 with three new parameters 
si, sj, and 
sk to specify 
the starting position of each submatrix of of 
A, B, and 
C e two inner loops of the 
do_block now compute in steps of size 
BLOCKSIZE rather than the full length 

of 
B and 
C e gcc optimizer removes any function call overhead by “inlining” the 
function; that is, it inserts the code directly to avoid the conventional parameter 

passing and return address bookkeeping instructions.
Figure 5.22
 illustrates the accesses to the three arrays using blocking. Looking 
only at capacity misses, the total number of memory words accessed is 2 
N3/ BLOCKSIZE  N2 is total is an improvement by about a factor of 
BLOCKSIZE. Hence, blocking exploits a combination of spatial and temporal locality, since 
A bene
 ts from spatial locality and 
B bene
 ts from temporal locality.
FIGURE 5.20 A snapshot of the three arrays 
C, A, and
 B when N  6 and i  1. e age of 
accesses to the array elements is indicated by shade: wh
ite means not yet touched, light means older accesses, 
and dark means newer accesses. Compared to 
Figure 5.21
, elements of 
A and 
B are read repeatedly to calculate 
new elements of 
x e variables 
i, j, and 
k are shown along the rows or columns used to access the arrays.
01
2
345102345
xji0
1
2
345102345
yki0
1
2
345102345
zjk
 5.4 Measuring and Improving Cache Performance 
415FIGURE 5.21 Cache blocked version of DGEMM in Figure 3.21.
 Assume 
C is initialized to zero.
 e do_block function is basically DGEMM from Chapter 3 with new parameters to specify the starting positions of the submatrices of 
BLOCKSIZE e gcc optimizer can remove the function overhead instructions by inlining the 
do_block function.
FIGURE 5.22 The age of accesses to the arrays
 C, A, and
 B when BLOCKSIZE  3. Note that, 
in contrast to 
Figure 5.20,
 fewer elements are accessed.
01
2
3
4
5102345
xji0
1
2
3
4
5102345
yki0
1
2
3
4
5102345
zjk1 #define BLOCKSIZE 32
2 void do_block (int n, int si, int sj, int sk, double *A, double

3 *B, double *C)

4 {
5  for (int i = si
; i < si+BLOCKSIZE; ++i)
6   for (int j = sj
; j < sj+BLOCKSIZE; ++j)
7     {

8     double cij = C[i+j*n]
;/* cij = C[i][j] */9     for( int k = sk
; k < sk+BLOCKSIZE; k++ )10      cij += A[i+k*n] * B[k+j*n]
;/* cij+=A[i][k]*B[k][j] */11     C[i+j*n] = cij
;/* C[i][j] = cij */
12     }

13 }
14 void dgemm (int n, double* A, double* B, double* C)
15 {
16   for ( int sj = 0
; sj < n; sj += BLOCKSIZE )
17    for ( int si = 0
; si < n; si += BLOCKSIZE )
18    for ( int sk = 0
; sk < n; sk += BLOCKSIZE )
19     do_block(n, si, sj, sk, A, B, C)
;20 }Although we have aimed at reducing cache misses, blocking can also be used to 
help register allocation. By taking a small blocking size such that the block can be 
held in registers, we can minimize the number of loads and stores in the program, 

which also improves performance.

416 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Figure 5.23
 shows the impact of cache blocking on the performance of the 
unoptimized DGEMM as we increase the matrix size beyond where all three 
matr
 t in the cache
 e unoptimized performance is halved for the largest 
matr
 e cache-blocked version is less than 10% slower even at matrices that are 
960x960, or 900 times larger than th
×trices in Chapters 3 and 4.
Elaboration: Multilevel caches create several complications. First, there are now 
several different types of misses and corresponding miss rates. In the example on 
pages 410–411, we saw the primary cache miss rate and the 
global miss rate—the fraction of references that missed in all cache levels. There is also a miss rate for the secondary cache, which is the ratio of all misses in the secondary cache divided by the 

number of accesses to it. This miss rate is called the local miss rate of the secondary 
cache. Because the primar lters accesses, especially those with good spatial 
and temporal locality, the local miss rate of the secondary cache is much higher than the 
global miss rate. For the example on pages 410–411, we can compute the local miss 

rate of the secondary cache as 0.5%/2% 
 25%! Luckily, the global miss rate dictates 

how often we must access the main memory.
Elaboration: With out-of-order processors (see Chapter 4), performance is more 
complex, since they execute instructions during the miss penalty. Instead of instruction 

miss rates and data miss rates, we use misses per instruction, and this formula:
Memorystall cycles
Instruction
Misses
Instruction
(Total mi
sss latencyOverlapped miss latency)
global miss rate
 e fraction of references 
that miss in all levels of a 

multilevel cache.
local miss rate
 e fraction of references to 

one level of a cache that 

miss; used in multilevel 

hierarchies.
1.81.5
1.2
0.9
0.6GFLOPS0.3ŒUnoptimized
1.71.51.30.81.71.61.6
1.5Blocked
32x32160x160480x480960x960
FIGURE 5.23 Performance of unoptimized DGEMM (Figure 3.21) versus cache blocked 
DGEMM (Figure 5.21
) as the matrix dimension varies from 32x32 (where all three matrices ﬁ t in the cache) to 960x960.

 5.4 Measuring and Improving Cache Performance 
417There is no general way to calculate overlapped miss latency, so evaluations of 
memory hierarchies for out-of-order processors inevitably require simulation of the 
processor and the memory hierarchy. Only by seeing the execution of the processor 

during each miss can we see if the processor stalls w nds other 

work to do. A guideline is that the processor often hides the miss penalty for an L1 

cache miss that hits in the L2 cache, but it rarely hides a miss to the L2 cache.
Elaboration: The performance challenge for algorithms is that the memory hierarchy 
varies between different implementations of the same architecture in cache size, 

associativity, block size, and number of caches. To cope with such variability, some 

recent numerical libraries parameterize their algorithms and then search the parameter 
space at r nd the best combination for a particular computer. This approach 

is called autotuning.Which of the following is generally true about a design with multiple levels of 
caches?
1. First-level caches are more concerned about hit time, and second-level 
caches are more concerned about miss rate.
2. First-level caches are more concerned about miss rate, and second-level 
caches are more concerned about hit time.
Summary
In this section, we focused on four topics: cache performance, using associativity to 

reduce miss rates, the use of multilevel cache hierarchies to reduce miss penalties, 

and so
 ware optimizations to improv
 ectiveness of caches.
 e memory syst
 cant 
 ect on program execution time
 e number of memory-stall cycles depends on both the miss rate and the miss penalty. 

 e challenge, as we will see in Section 5.8, is to reduce one of these factors without 
 cantly a
 ecting other critical factors in the memory hierarchy.
To reduce the miss rate, we examined the use of associative placement schemes. 
Such schemes can reduce the miss rate of a cache by allowing mor
 exible 
placement of blocks within the cache. Fully associative schemes allow blocks to be 

placed anywhere, but also require that every block in the cache be searched to satisfy 

a req
 e higher costs make large fully associative caches impractical. Set-
associative caches are a practical alternative, since we need only search among the 

elements of a unique set that is chosen by indexing. Set-associative caches have higher 

miss rates but are faster to
 e amount of associativity that yields the best 
performance depends on both the technology and the details of the implementation.
We looked at multilevel caches as a technique to reduce the miss penalty by 
allowing a larger secondary cache to handle misses to the primary cache. Second-

level caches have become commonplace as designer
 nd that limited silicon and 
the goals of high clock rates prevent primary caches from becoming large
 e secondary cache, which is o
 en ten or more times larger than the primary cache, 
handles many accesses that miss in the primary cache. In such cases, the miss 

penalty is that of the access time to the secondary cache (typically < 10 processor 
Check Yourself

418 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
cycles) versus the access time to memory (typically > 100 processor cycles). As with 
associativity, the design tradeo
 s between the size of the secondary cache and its 
access time depend on a number of aspects of the implementation.
Finally, given the importance of the memory hierarchy in performance, we 
looked at how to change algorithms to improve cache behavior, with blocking 

being an important technique when dealing with large arrays.
 5.5 Dependable Memory Hierarchy
Implicit in all the prior discussion is that the memory hierarchy doesn’t forget. Fast 

but undependable is not very attractive. As we learned in Chapter 1, the one great 

idea for 
dependability
 is redundancy. In this section we’l
 rst go over the terms to 
 ne terms and measures associated with failure, and then show how redundancy 
can make nearly unforgettable memories.
Deﬁ ning Failure
We start with an assumption that you have a sp
 cation of proper service. Users 
can then see a system alternating between two states of delivered service with 

respect to the service sp
 cation:
1. Service accomplishment,
 where the service is delivered as sp
 ed2. Service interruption,
 where the delivered serv
 erent from the 
sp
 ed service
Transitions from state 1 to state 2 are caused by 
failures
, and transitions from state 
2 to state 1 are called 
restorations
. Failures can be permanent or intermitten
 e latter is the mor
  cult case; it is harder to diagnose the problem when a system 
oscillates between the two states. Permanent failures are far easier to diagnose. 
 is 
 nition leads to two related terms: reliability and availability.
Reliability
 is a measure of the continuous service accomplishment—or, equivalently, 
of the time to failure—from a reference point. Hence, 
mean time to failure
 (MTTF) 
is a reliability measure. A related term is 
annual failure rate
 (AFR), which is just the 
percentage of devices that would be expected to fail in a year for a given MTTF. 

When MTTF gets large it can be misleading, while AFR leads to better intuition.
MTTF vs. AFR of DisksSome disks today are quoted to have a 1,000,000-hour MTTF. As 1,000,000 

hours is 1,000,000/(365 
 24)  114 years, it would seem like they practically 
never fail. Warehouse scale computers that run Internet services such as 

Search might have 50,000 servers. Assume each server has 2 disks. Use AFR to 

calculate how many disks we would expect to fail per year.
EXAMPLE
 5.5 Dependable Memory Hierarchy 
419One year is 365 
 24  8760 hours. A 1,000,000-hour MTTF means an AFR 
of 8760/1,000,000 
 0.876%. With 100,000 disks, we would expect 876 disks to 
fail per year, or on average more than 2 disk failures per day!
Service interruption is measured as 
mean time to repair
 (MTTR). 
Mean time 
between failures
 (MTBF) is simply the sum of MTTF + MTTR. Although MTBF 
is widely used, MTTF is o
 en the more appropriate term. 
Availability
 is then a 
measure of service accomplishment with respect to the alternation between the two 

states of accomplishment and interruption. Availability is statistically quan
 ed as
AvailabilityMTTF(MTTFMTTR)
Note that reliability and availability are actually quan
 able measures, rather than 
just synonyms for dependability. Shrinking MTTR can help availability as much as 

increasing MTTF. For example, tools for fault detection, diagnosis, and repair can 

help reduce the time to repair faults and thereby improve availability.
We want availability to be very high. One shorthand is to quote the number of 
“nines of availability” per year. For example, a very good Internet service today 

 ers 4 or 5 nines of availability. Given 365 days per year, which is 365 
 24  60  526,000 minutes, then the shorthand is decoded as follows:
One nine: 90% =>  36.5 days of repair/year

Two nines: 99% =>  3.65 days of repair/year

 ree nines: 99.9% =>  526 minutes of repair/year
Four nines: 99.99% =>  52.6 minutes of repair/year

Five nines: 99.999% =>  5.26 minutes of repair/year
and so on. 
To increase MTTF, you can improve the quality of the components or design 
systems to continue operation in the presence of components that have failed. 

Hence, failure needs to be
 ned with respect to a context, as failure of a component 
may not lead to a failure of the system. To make this distinction clear, the term 
fault
 is used to mean failure of a component. Here are three ways to improve MTTF:
1. Fault avoidanc
e: Preventing fault occurrence by construction.
2. Fault tolerance:
 Using redundancy to allow the service to comply with the 
service sp
 cation despite faults occurring. 
3. Fault forecasting:
 Predicting
 the presence and creation of faults, allowing 
the component to be replaced 
before
 it fails.
ANSWER
420 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
The Hamming Single Error Correcting, Double Error 
Detecting Code (SEC/DED)Richard Hamming invented a popular redundancy scheme for memory, for which 
he received the Turing Award in 1968. To invent redundant codes, it is helpful 

to talk about how “close” correct bit patterns can be. What we call the 
Hamming 
distance
 is just the minimum number of bits that ar
 erent between any two 
correct bit patterns. For example, the distance between 011011 and 001111 is two. 

What happens if the minimum distance between members of a codes is two, and 

we get a one-bit error? It will turn a valid pattern in a code to an invalid one.
 us, if we can detect whether members of a code are valid or not, we can detect single 

bit errors, and can say we have a single bit
 error detection code
.Hamming used a 
parity code
 for error detection. In a parity code, the number 
of 1s in a word is counted; the word has odd parity if the number of 1s is odd and 

even otherwise. When a word is written into memory, the parity bit is also written 

(1 for odd, 0 for ev at is, the parity of the N+1 bit word should always be even. 

 en, when the word is read out, the parity bit is read and checked. If the parity of the 
memory word and the stored parity bit do not match, an error has occurred.
Calculate the parity of a byte with the value 31
ten
 and show the pattern stored to 
memory. Assume the parity bit is on the right. Suppose th
 cant bit 
was inverted in memory, and then you read it back. Did you detect the error? 

What happens if the tw
 cant bits are inverted?
31ten
 is 00011111two
, whic
 ve 1s. To make parity even, we need to write a 1 
in the parity bit, or 000111111
two
. If th
 cant bit is inverted when we 
read it back, we would see 100111111
two
 which has seven 1s. Since we expect 
even parity and calculated odd parity, we would signal an error. If the 
two most  cant bits are inverted, we would see 110111111
two
 which has eight 1s or 
even parity and we would 
not signal an error.
If there are 2 bits of error, then a 1-bit parity scheme will not detect any errors, 

since the parity will match the data with two errors. (Actually, a 1-bit parity scheme 

can detect any odd number of errors; however, the probability of having 3 errors is 

much lower than the probability of having two, so, in practice, a 1-bit parity code is 

limited to detecting a single bit of error.) 
Of course, a parity code cannot correct errors, which Hamming wanted to do 
as well as detect them. If we used a code that had a minimum distance of 3, then 

any single bit error would be closer to the correct pattern than to any other valid 

pattern. He came up with an easy to understand mapping of data into a distance 3 

code that we call 
Hamming Error Correction Code
 (ECC) in his honor. We use extra 
error detection 

code A code that 
enables the detection of 
an error in data, but not 

the precise location and, 

hence, correction of the 

error.
EXAMPLEANSWER
 5.5 Dependable Memory Hierarchy 
421parity bits to allow the position iden
 cation of a single error. Here are the steps to 
calculate Hamming ECC
1. Start numbering bits from 1 on th
 , as opposed to the traditional 
numbering of the rightmost bit being 0.
2. Mark all bit positions that are powers of 2 as parity bits (positions 1, 2, 4, 8, 
16, …) .3. All other bit positions are used for data bits (positions 3, 5, 6, 7, 9, 10, 11, 12, 
13, 14, 15, …). e position of parity bit determines sequence of data bits that it checks 
(Figure 5.24 
shows this coverage graphically) is:
 Bit 1 (0001
two
) checks bits (1,3,5,7,9,11,...), which are bits where rightmost 
bit of address is 1 (0001
two
, 0011two
, 0101two
, 0111two
, 1001two
, 1011two
,…). Bit 2 (0010
two
) checks bits (2,3,6,7,10,11,14,15,…), which are the bits 
where the second bit to the right in the address is 1.
 Bit 4 (0100
two
) checks bits (4–7, 12–15, 20–23,...) , which are the bits where 
the third bit to the right in the address is 1.
 Bit 8 (1000
two
) checks bits (8–15, 24–31, 40–47,...), which are the bits 
where the fourth bit to the right in the address is 1.
 Note that each data bit is covered by two or more parity bits. 
5. Set parity bits to create even parity for each group.
Bit positionEncoded data bitsParitybitcoveragep1p1p2
p4
p8p2d1p4d2d3d4p8d5d6d7d8
XXXXXXXXXXXX
XXXXXXXXXX123456789101112
FIGURE 5.24 Parity bits, data bits, an
 eld coverage in a Hamming ECC code for 
eight data bits.
In what seems like a magic trick, you can then determine whether bits are 
incorrect by looking at the parity bits. Using the 12 bit code in 
Figure 5.24
, if the 
value of the four parity calculations (p8,p4,p2,p1) was 0000, then there was no 

error. However, if the pattern was, say, 1010, which is 10
ten
, then Hamming ECC 
tells us that bit 10 (d6) is an error. Since the number is binary, we can correct the 

error just by inverting the value of bit 10.

422 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Assume one byte data value is 10011010
two
. First show the Hamming ECC code 
for that byte, and then invert bit 10 and show that the ECC co
 nds and 
corrects the single bit error.
Leaving spaces for the parity bits, the 12 bit pattern is _ _ 1 _ 0 0 1 _ 1 0 1 0. 
Position 1 checks bits 1,3,5,7,9, and11, which we highlight: 
__ 1 _ 0 0 1 _ 1 0 1 0. To make the group even parity, we should set bit 1 to 0. 
Position 2 checks bits 2,3,6,7,10,11, which is 0 
_ 1 _ 0 0 1 _ 1 0 1 0 or odd parity, 
so we set position 2 to a 1. 

Position 4 checks bits 4,5,6,7,12, which is
 0 1 1 _ 0 0 1 _ 1 0 1, so we set it to a 1. 
Position 8 checks bits 8,9,10,11,12, which is 0 1 1 1 0 0 1 
_ 1 0 1 0, so
 we set it 
to a 0.  e 
 nal code word is 011100101010. Inverting bit 10 changes it to 
011100101110.
Parity bit 1 is 0 (
011100101110 is four 1s, so even parity; this group is OK).

Parity bit 2 is 1 (0
1110010111 ve 1s, so odd parity; there is an error 
somewhere).

Parity bit 4 is 1 (011
100101110 is two 1s, so even parity; this group is OK).
Parity bit 8 is 1 (0111001
01110 is three 1s, so odd parity; there is an error 

somewhere).

Parity bits 2 and 10 are incorrect. As 2 + 8 = 10, bit 10 must be wrong. Hence, 

we can correct the error by inverting bit 10: 011100101
010. Voila!
Hamming did not stop at single bit error correction code. At the cost of one more 

bit, we can make the minimum Hamming distance in a code b
 is means 
we can correct single bit errors 
and detect double bit errors
 e idea is to add a 
parity bit that is calculated over the whole word. Let’s use a four-bit data word as 

an example, which would only need 7 bits for single bit error detection. Hamming 

parity bits H (p1 p2 p3) are computed (even parity as usual) plus the even parity 

over the entire word, p4:
    1    2    3    4    5    6    7   8     p1  p2   d1   p3   d2   d3  d4   p4 en the algorithm to correct one error and detect two is just to calculate parity 
over the ECC groups (H) as before plus one more over the whole group (p
4 ere 
are four cases:
1. H is even and p
4 is even, so no error occurred.
2. H is odd and p
4 is odd, so a correctable single error occurred. (p
4 should 
calculate odd parity if one error occurred.)
3. H is even and p
4 is odd, a single error occurred in p
4 bit, not in the rest of the 
word, so correct the p
4 bit.
EXAMPLEANSWER
 5.5 Dependable Memory Hierarchy 
4234. H is odd and p
4 is even, a double error occurred. (p
4 should calculate even 
parity if two errors occurred.)
Single Error Correcting / Double Error Detecting (SEC/DED) is common in 
memory for servers today. Conveniently, eight byte data blocks can get SEC/DED 

with just one more byte, which is why many DIMMs are 72 bits wide.
Elaboration: To calculate how many bits are needed for SEC, let 
p be total number of parity bits and d number of data bits in p  d bit word. If p error correction bits are to 
point to error bit (
p + d cases) plus one case to indicate that no error exists, we need:
2p  p  d  1 bits, and thus 
p  log(p  d  1).For example, for 8 bits data means 
d  8 and 2p  p  8  1, so 
p  4. Similarly, 
p  5 for 16 bits of data, 6 for 32 bits, 7 for 64 bits, and so on. 
Elaboration: In very large systems, the possibility of multiple errors as well as 
complete failure of a single wide memor cant. IBM introduced 
chipkill to solve this problem, and many very large systems use this technology. (Intel 

calls their version SDDC.) Similar in nature to the RAID approach used for disks (see 
 Section 5.11), Chipkill distributes the data and ECC information, so that the complete 
failure of a single memory chip can be handled by supporting the reconstruction of the 
missing data from the remaining memory chips. Assuming a 10,000-processor cluster 

with 4 GiB per processor, IBM calculated the following rates of 
unrecoverable
 memory 

errors in three years of operation:
  Parity only—about 90,000, or one unrecoverable (or undetected) failure every 17 

minutes.  SEC/DED only—about 3500, or about one undetected or unrecoverable failure 

every 7.5 hours.
 Chipkill—6, or about one undetected or unrecoverable failure every 2 months.
Hence, Chipkill is a requirement for warehouse-scale computers.
Elaboration: While single or double bit errors are typical for memory systems, networks 
can have bursts of bit errors. One solution is called 
Cyclic Redundancy Check. For a 

block of k bits, a transmitter generates an 
n-k bit frame check sequence. It transmits 
n bits exactly divisible by some number. The receiver divides frame by that number. If 

there is no remainder, it assumes there is no error. If there is, the receiver rejects the 

message, and asks the transmitter to send again. As you might guess from Chapter 3, 

it is easy to calculate division for some binary numbers with a shift register, which made 

CRC codes popular even when hardware was more precious. Going even further, Reed-

 elds to 
correct
 multibit transmission errors, but now data is 
 cients of a polynomials and the code space is values of a polynomial. 

The Reed-Solomon calculation is considerably more complicated than binary division!

424 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 5.6 Virtual Machines
Virtual Machines
 (VM) wer
 rst developed in the mid-1960s, and they have 
remained an important part of mainframe computing over the years. Although 
largely ignored in the single user PC era in the 1980s and 1990s, they have recently 

gained popularity due to
 e increasing importance of isolation and security in modern systems
 e failures in security and reliability of standard operating systems
 e sharing of a single computer among many unrelated users, in particular 
for cloud computing
 e dramatic increases in raw speed of processors over the decades, which 
makes the overhead of VMs more acceptable
 e br
 nition of VMs includes basically all emulation methods that 
provide a standard so ware interface, such as the Java VM. In this section, we are 

interested in VMs that provide a complete system-level environment at the binary 

instruction set architecture
 (ISA) level. Although some VMs r
 erent ISAs in 
the VM from the native hardware, we assume they always match the hardware. Such 

VMs are called (Operating) 
System Virtual Machines
. IBM VM/370, VirtualBox, 
VMware ESX Server, and Xen are examples.
System virtual machines present the illusion that the users have an entire 
computer to themselves, including a copy of the operating system. A single 

computer runs multiple VMs and can support a number o
 erent 
operating 
systems
 (OSes). On a conventional platform, a single OS “owns” all the hardware 

resources, but with a VM, multiple OSes all share the hardware resources.
 e so
 ware that supports VMs is called a 
virtual machine monitor
 (VMM) or 
hypervisor;
 the VMM is the heart of virtual machine technology
 e underlying 
hardware platform is called the 
host, and its resources are shared among the 
guest
  e VMM determines how to map virtual resources to physical resources: a 

physical resource may be time-shared, partitioned, or even emulated in so
 ware. 
 e VMM is much smaller than a traditional OS; the isolation portion of a VMM 
is perhaps only 10,000 lines of code.
Although our interest here is in VMs for improving protection, VMs provide 
two other b
 ts that are commerciall
 cant:
1. Managing so
 ware
. VMs provide an abstraction that can run the complete 
so
 ware stack, even including old operating systems like DOS. A typical 
deployment might be some VMs running legacy OSes, many running the 

current stable OS release, and a few testing the next OS release.
2. Managing hardware
. One reason for multiple servers is to have each 
application running with the compatible version of the operating system 

on separate computers, as this separation can improve dependability. VMs 

 5.6 Virtual 
Machines 425allow these separate so
 ware stacks to run independently yet share hardware, 
thereby consolidating the number of servers. Another example is that some 
VMMs support migration of a running VM t
 erent computer, either 
to balance load or to evacuate from failing hardware.
Amazon Web Services
 (AWS) uses the virtual machines in its cloud computing 

 ering EC2 fo
 ve reasons:
1. It allows AWS to protect users from each other while sharing the same server.
2. It simp
 es so
 ware distribution within a warehouse scale computer. A 
customer installs a virtual machine image co
 gured with the appropriate 
so
 ware, and AWS distributes it to all the instances a customer wants to use.
3. Customers (and AWS) can reliably “kill” a VM to control resource usage 
when customers complete their work.
4. Virtual machines hide the identity of the hardware on which the customer is 
running, which means AWS can keep using old servers 
and
 introduce new, 
more
  cient servers. 
 e customer expects performance for instances to 
match their ratings in “EC2 Compute Units,” which AW
 nes: to “provide 
the equivalent CPU capacity of a 1.0–1.2 GHz 2007 AMD Opteron or 2007 
Intel Xeon processor.
 anks to 
Moore’s Law
, newer servers clearly o
 er more EC2 Compute Units than older ones, but AWS can keep renting old 

servers as long as they are economical.
5. Virtual Machine Monitors can control the rate that a VM uses the processor, 
the network, and disk space, which allows AWS to o
 er many price points 
of instances of
 erent types running on the same underlying servers. 
For example, in 2012 AWS o ered 14 instance types, from small standard 

instances at $0.08 per hour to high I/O quadruple extra large instances at 

$3.10 per hour.
In general, the cost of processor virtualization depends on the workload. User-

level processor-bound programs have zero virtualization overhead, because the 

OS is rarely invoked, so everything runs at native speeds. I/O-intensive workloads 

are generally also OS-intensive, executing many system calls and privileged 

instructions that can result in high virtualization overhead. On the other hand, if 

the I/O-intensive workload is also 
I/O-bound
, the cost of processor virtualization 
can be completely hidden, since the processor is o
 en idle waiting for I/O.
 e overhead is determined by both the number of instructions that must be 
emulated by the VMM and by how much time each takes to emulate them. Hence, 

when the guest VMs run the same ISA as the host, as we assume here, the goal 
Hardware/ 

Software 

Interface
426 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
of the architecture and the VMM is to run almost all instructions directly on the 
native hardware.
Requirements of a Virtual Machine Monitor
What must a VM monitor do? It presents a so
 ware interface to guest so
 ware, it 
must isolate the state of guests from each other, and it must protect itself from guest 

so
 ware (including guest OS
 e qualitative requirements are:
 Guest so
 ware should behave on a VM exactly as if it were running on the 
native hardware, except for performance-related behavior or limitations of 

 xed resources shared by multiple VMs.
 Guest so
 ware should not be able to change allocation of real system resources 
directly.
To “virtualize” the processor, the VMM must control just about everything—access 

to privileged state, I/O, exceptions, and interrupts—even though the guest VM and 

OS currently running are temporarily using them.
For example, in the case of a timer interrupt, the VMM would suspend the 
currently running guest VM, save its state, handle the interrupt, determine which 

guest VM to run next, and then load its state. Guest VMs that rely on a timer 

interrupt are provided with a virtual timer and an emulated timer interrupt by the 

VMM.To be in charge, the VMM must be at a higher privilege level than the guest 
VM, which generally runs in user mode; this also ensures that the execution of 

any privileged instruction will be handled by th
 e basic requirements of 
system virtual:
 At least two processor modes, system and user.
 A privileged subset of instructions that is available only in system mode, 
resulting in a trap if executed in user mode; all system resources must be 

controllable only via these instructions.
(Lack of) Instruction Set Architecture Support for Virtual 
MachinesIf VMs are planned for during the design of the ISA, it’s relatively easy to reduce 
both the number of instructions that must be executed by a VMM and improve 

their emulation speed. An architecture that allows the VM to execute directly on 

the hardware earns the title 
virtualizable,
 and the IBM 370 architecture proudly 
bears that label.
Alas, since VMs have been considered for PC and server applications only fairly 
recently, most instruction sets were created without virtualization in mind
 ese 
culprits include x86 and most RISC architectures, including ARMv7 and MIPS.

 5.7 Virtual Memory 
427Because the VMM must ensure that the guest system only interacts with virtual 
resources, a conventional guest OS runs as a user mode program on top of the 
 en, if a guest OS attempts to access or modify information related to 
hardware resources via a privileged instruction—for example, reading or writing 

a status bit that enables interrupts—it will trap to th
 e VMM can then 
 ect the appropriate changes to corresponding real resources.
Hence, if any instruction that tries to read or write such sensitive information 
traps when executed in user mode, the VMM can intercept it and support a virtual 

version of the sensitive information, as the guest OS expects.
In the absence of such support, other measures must be taken. A VMM must 
take special precautions to locate all problematic instructions and ensure that they 

behave correctly when executed by a guest OS, thereby increasing the complexity 

of the VMM and reducing the performance of running the VM.
Protection and Instruction Set Architecture
Protection is a join
 ort of architecture and operating systems, but architects 
had to modify some awkward details of existing instruction set architectures when 

virtual memory became popular. 
For example, the x86 instruction POPF loads th
 ag registers from the top of 
the stack in memory. One of th ags is the 
Interrupt Enable
 ag. If you run 
the POPF instruction in user mode, rather than trap it, it simply changes all the 

 ags except IE. In system mode, it does change the IE. Since a guest OS runs in user 
mode inside a VM, this is a problem, as it expects to see a changed IE.
Historically, IBM mainframe hardware and VMM took three steps to improve 
performance of virtual machines:
1. Reduce the cost of processor virtualization.
2. Reduce interrupt overhead cost due to the virtualization.

3. Reduce interrupt cost by steering interrupts to the proper VM without 
invoking VMM.
AMD and Intel tried to address th
 rst point in 2006 by reducing the cost of 
processor virtualization. It will be interesting to see how many generations of 
architecture and VMM mo
 cations it will take to address all three points, and 
how long before virtual machines of the 21st century will b
  cient as the IBM 
mainframes and VMMs of the 1970s.
 5.7 Virtual Memory
In earlier sections, we saw how caches provided fast access to recently used portions 

of a program’s code and data. Similarly, the main memory can act as a “cache” for 
… a system has 

been devised to 

make the core drum 

combination appear 

to the programmer 

as a single level 

store, the requisite 

transfers taking place 

automatically.
Kilburn et al.,
 One-level 
storage system,
 1962
428 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
the secondary storage, usually im
plemented wit
 is technique is 
called virtual memory
. Historically, there were two major motivations for virtual 
memory: to allow
  cient and safe sharing of memory among multiple programs, 
such as for the memory needed by multiple virtual machines for cloud computing, 
and to remove the programming burdens of a small, limited amount of main 

memory. Five decades a
 er its invention, it’s the former reason that reigns today.
Of course, to allow multiple virtual machines to share the same memory, we 
must be able to protect the virtual machines from each other, ensuring that a 

program can only read and write the portions of main memory that have been 

assigned to it. Main memory need contain only the active portions of the many 

virtual machines, just as a cache contains only the active portion of one program. 

 us, the principle of locality enables virtual memory as well as caches, and virtual 
memory allows us t
  ciently share the processor as well as the main memory.
We cannot know which virtual machines will share the memory with other 
virtual machines when we compile them. In fact, the virtual machines sharing 

the memory change dynamically while the virtual machines are running. Because 

of this dynamic interaction, we would like to compile each program into its 

own 
address space
—a separate range of memory locations accessible only to this 
program. Virtual memory implements the 
translation of a program’s address space 
to 
physical addresses
 is translation process enforces 
protection
 of a program’s 
address space from other virtual machines.
 e second motivation for virtual memory is to allow a single user program 
to exceed the size of primary memory. Formerly, if a program became too large 

for memory, it was up to the programmer to make i
 t. Programmers divided 
programs into pieces and then iden
 ed the pieces that were mutually exclusive. 
 ese 
overlays
 were loaded or unloaded under 
user program control during 
execution, with the programmer ensuring that the program never tried to access 

an overlay that was not loaded and that the overlays loaded never exceeded the 

total size of the memory. Overlays were traditionally organized as modules, each 

containing both code and data. Calls between procedur
 erent modules 
would lead to overlaying of one module with another.
As you can well imagine, this responsibility was a substantial burden on 
programmers. Virtual memory, which was invented to relieve programmers of 

th
  culty, automatically manages the two levels of the memory hierarchy 
represented by main memory (sometimes called 
physical memory
 to distinguish it 
from virtual memory) and secondary storage.
Although the concepts at work in virtual memory and in caches are the same, 
th
 ering historical roots have led to the use o
 erent terminology. A virtual 
memory block is called a 
page
, and a virtual memory miss is called a 
page fault
. With virtual memory, the processor produces a 
virtual address
, which is translated 
by a combination of hardware and so
 ware to a 
physical address
, which in turn can 
be used to access main memory. 
Figure 5.25
 shows the virtually addressed memory 

with pages mapped to main memory
 is process is called 
address mapping
 or 
virtual memory
 A technique that uses 
main memory as a “cache” 

for secondary storage.
physical address
 An address in main 

memory.
protection
 A set 
of mechanisms for 

ensuring that multiple 

processes sharing the 

processor, memory, 

or I/O devices cannot 

interfere, intentionally 

or unintentionally, with 

one another by reading or 

writing each other’s data. 

 ese mechanisms also 
isolate the operating system 

from a user process.
page fault
 An event that 
occurs when an accessed 

page is not present in 

main memory.
virtual address
 An address that 

corresponds to a location 

in virtual space and is 

translated by address 

mapping to a physical 

address when memory is 

accessed.

 5.7 Virtual Memory 
429address translation
. Today, the two memory hierarchy levels controlled by virtual 
memory are usually DRAMs an
 ash memory in personal mobile devices and 
DRAMs and magnetic disks in servers (see Section 5.2). If we return to our library 
analogy, we can think of a virtual address as the title of a book and a physical 

address as the location of that book in the library, such as might be given by the 

Library of Congress call number.
Virtual memory also simp
 es loading the program for execution by providing 
relocation
. Relocation maps the virtual addresses used by a program t
 erent 
physical addresses before the addresses are used to access memory
 is relocation 
allows us to load the program anywhere in main memory. Furthermore, all virtual 

memory systems in use today relocate the program as a set o
 xed-size blocks 
(pages), thereby eliminating the need t
 nd a contiguous block of memory to 
allocate to a program; instead, the operating system need onl
 
  cient 
number of pages in main memory.
In virtual memory, the address is broken into a 
virtual page number
 and a 
page 
 set
. Figure 5.26
 shows the translation of the virtual page number to a 
physical 
page number
 e physical page number constitutes the upper portion of the 
physical address, while the page o
 set, which is not changed, constitutes the lower 
portio e number of bits in the page o
 s
 eld determines the page size
 e number of pages addressable with the virtual address need not match the number 

of pages addressable with the physical address. Having a larger number of virtual 

pages than physical pages is the basis for 
the illusion of an essentially unbounded 

amount of virtual memory.
address translation
 Also called 
address 
mapping
 e process by 
which a virtual address 
is mapped to an address 

used to access memory.
Virtual addressesPhysical addressesAddress translationDisk addressesFIGURE 5.25 In virtual memory, blocks of memory (called 
pages
) are mapped from one set of addresses (called virtual addresses
) to another set (called physical addresses).  e processor generates virtual addresses while the 
memory is accessed using physical addresses. Both the 
virtual memory and the physical memory are broken into pages, so that a virtual page is mapped to a physical 

page. Of course, it is also possible for a virtual page to be absent from main memory and not be mapped to 

a physical address; in that case, the page resides on disk. 
Physical pages can be shared by having two virtual 
addresses point to the same physical addr
 is capability is used to allow tw
 erent programs to share 
data or code.

430 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Many design choices in virtual memory systems are motivated by the high cost 
of a page fault. A page fault to disk will
 take millions of clock cycles to process. 
 e table on page 378 shows that main memory latency is about 100,000 times 
quicker tha
 is enormous miss penalty, dominated by the time to get the 
 rst word for typical page sizes, leads to
 several key decisions in designing virtual 
memory systems:
 Pages should be large enough to try to amortize the high access time. Sizes 
from 4 KiB to 16 KiB are typical today. New desktop and server systems are 
being developed to support 32 KiB and 64 KiB pages, but new embedded 

systems are going in the other direction, to 1 KiB pages.
 Organizations that reduce the page fault rate are attractive
 e primary 
technique used here is to allow fully associative placement of pages in 

memory.
 Page faults can be handled in so
 ware because the overhead will be small 
compared to the disk access time. In addition, so
 ware can a
 ord to use clever 
algorithms for choosing how to place pages because even small reductions in 

the miss rate will pay for the cost of such algorithms.
 Write-through will not work for virtual memory, since writes take too long. 
Instead, virtual memory systems use write-back.
Virtual page numberPage offset31 30 29 28 273 2 1 015 14 13 12 11 10 9 8Physical page numberPage offset29 28 273 2 1 015 14 13 12 11 10 9 8Virtual addressPhysical addressTranslationFIGURE 5.26 Mapping from a virtual to a physical address.
 e page size is 2
12  4 KiB
 e number of physical pages allowed in memory is 2
18, since the physical page number has 18 bits in i
 us, main memory can have at most 1 GiB, while the virtual address space is 4 GiB.

 5.7 Virtual Memory 
431 e next few subsections address these factors in virtual memory design.
Elaboration: We present the motivation for virtual memory as many virtual machines 
sharing the same memory, but virtual memory was originally invented so that many 
programs could share a computer as part of a timesharing system. Since many readers 

today have no experience with time-sharing systems, we use virtual machines to motivate 

this section.Elaboration: For servers and even PCs, 32-bit address processors are problematic. 
Although we normally think of virtual addresses as much larger than physical addresses, 

the opposite can occur when the processor address size is small relative to the state 

of the memory technology. No single program or vir t, but a 

collection of programs or virtual machines r t from 

not having to be swapped to memory or by running on parallel processors. 
Elaboration: The discussion of virtual memory in this book focuses on paging, 
 xed-size blocks. There is also a variable-size block scheme called 
segmentation. In segmentation, an address consists of two parts: a segment number 
and a segment offset. The segment number is mapped to a physical address, and 
the offset is added
 nd the actual physical address. Because the segment can 
vary in size, a bounds check is also needed to make sure that the offset is within 

the segment. The major use of segmentation is to support more powerful methods 

of protection and sharing in an address space. Most operating system textbooks 

contain extensive discussions of segmentation compared to paging and of the use 

of segmentation to logically share the address space. The major disadvantage of 

segmentation is that it splits the address space into logically separate pieces that 

must be manipulated as a two-part address: the segment number and the offset. 

Paging, in contrast, makes the boundary between page number and offset invisible 

to programmers and compilers.
Segments have also been used as a method to extend the address space without 
changing the word size of the computer. Such attempts have been unsuccessful because 

of the awkwardness and performance penalties inherent in a two-part address, of which 

programmers and compilers must be aware.
Man xed-size blocks that simplify 
protection betw ciency 

of implementing paging. Although these divisions are often called “segments,” this 

mechanism is much simpler than variable block size segmentation and is not visible to 
user programs; we discuss it in more detail shortly.
Placing a Page and Finding It Again
Because of the incredibly high penalty fo
r a page fault, designers reduce page fault 
frequency by optimizing page placement. If we
 allow a virtual page to be mapped 
to any physical page, the operating system can then choose to replace any page 
it wants when a page fault occurs. For example, the operating system can use a 
segmentation
 A variable-size address 
mapping scheme in which 

an address consists of two 

parts: a segment number, 

which is mapped to a 

physical address, and a 

segment o
 set.

432 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
sophisticated algorithm and complex data structures that track page usage to try 
to choose a page that will not be needed for a long time
 e ability to use a clever 
an
 exible replacement scheme reduces the page fault rate and simp
 es the use 
of fully associative placement of pages.
As mentioned in Section 5.4, th
  culty in using fully associative placement 
is in locating an entry, since it can be anywhere in the upper level of the hierarchy. 

A full search is impractical. In virtual 
memory systems, we locate pages by using a 
table that indexes the memory; this structure is called a 
page table
, and it resides 
in memory. A page table is indexed with the page number from the virtual address 
to discover the corresponding physical page number. Each program has its own 

page table, which maps the virtual address space of that program to main memory. 

In our library analogy, the page table corresponds to a mapping between book 

titles and library locations. Just as the card catalog may contain entries for books 

in another library on campus rather than the local branch library, we will see that 

the page table may contain entries for pages not present in memory. To indicate the 

location of the page table in memory, the hardware includes a register that points to 

the start of the page table; we call this the 
page table register
. Assume for now that 
the page tab
 xed and contiguous area of memory.
 e page table, together with the program counter and the registers, sp
 es the 
state
 of a virtual machine. If we want to allow another virtual machine to use 
the processor, we must save this state. Later, a
 er restoring this state, the virtual 
machine can continue execution. We o
 en refer to this state as a 
process
 e process is considered 
active
 when it is in possession of the processor; otherwise, it 
is considered 
inactive
 e operating system can make a process active by loading 
the process’s state, including the program counter, which will initiate execution at 

the value of the saved program counter.
 e process’s address space, and hence all the data it can access in memory, is 
 ned by its page table, which resides in memory. Rather than save the entire page 
table, the operating system simply loads the 
page table register to point to the page 
table of the process it wants to make active. Each process has its own page table, 

 erent processes use the same virtual address
 e operating system is 
responsible for allocating the physical memory and updating the page tables, so 

that the virtual address spaces o
 erent processes do not collide. As we will see 
shortly, the use of separate page tables also provides protection of one process from 

another.
page table
 e table 
containing the virtual 
to physical address 

translations in a virtual 

memory syst
 e table, which is stored 

in memory, is typically 

indexed by the virtual 

page number; each entry 

in the table contains the 

physical page number 

for that virtual page if 

the page is currently in 

memory.
Hardware/ 
Software 
Interface
 5.7 Virtual Memory 
433Figure 5.27
 uses the page table register, the 
virtual address, and the indicated page 
table to show how the hardware can form a physical address. A valid bit is used 
in each page table entry, just as we did in a cache. If the bit is o
 , the page is not 
present in main memory and a page fault occurs. If the bit is on, the page is in 

memory and the entry contains the physical page number.
Because the page table contains a mapping for every possible virtual page, no 
tags are required. In cache terminology, the index that is used to access the page 

table consists of the full block address, which is the virtual page number.
Virtual page numberPage offset31 30 29 28 273 2 1 015 14 13 12 11 10 9 8Physical page numberPage offset29 28 273 2 1 015 14 13 12 11 10 9 8Virtual addressPhysical addressPage table registerPhysical page numberValidPage tableIf 0 then page is notpresent in memory201218FIGURE 5.27 The page table is indexed with the virtual page number to obtain the 
corresponding portion of the physical address.
 We assume a 32-bit addr
 e page table pointer 
gives the starting address of the page table. In t
 gure, the page size is 2
12 bytes, or 4 KiB
 e virtual 
address space is 2
32 bytes, or 4 GiB, and the physical address space is 2
30 bytes, which allows main memory 
of up to 1 GiB.
 e number of entries in the page table is 2
20, or 1 million entr
 e valid bit for each entry 
indicates whether the mapping is legal. If it is o
 , then the page is not present in memory. Although the 
page table entry shown here need only be 19 bits wide, it would typically be rounded up to 32 bits for ease of 
indexin
 e extra bits would be used to store additional information that needs to be kept on a per-page 
basis, such as protection.

434 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Page Faults
If the valid bit for a virtual page is o
 , a page fault occur
 e operating system 
must be given control.
 is transfer is done with the exception mechanism, which 
we saw in Chapter 4 and will discuss again later in this section. Once the operating 
system gets control, it mu
 nd the page in the next level of the hierarchy (usually 
 ash memory or magnetic disk) and de
cide where to place the requested page in 
main memory.
 e virtual address alone does not immediately
 tell us where the page is on disk. 
Returning to our library analogy, we canno
 nd the location of a library book on 
the shelves just by knowing its title. Instead, we go to the catalog and look up the 

book, obtaining an address for the location on the shelves, such as the Library of 

Congress call number. Likewise, in a virtual memory system, we must keep track 

of the location on disk of each page in virtual address space.
Because we do not know ahead of time when a page in memory will be replaced, 
the operating system usually creates the space o
 ash memory or disk for all the 
pages of a process when it creates the pro
 is space is called the 
swap space
. At that time, it also creates a data structure to record where each virtual page is 
stored on
 is data structure may be part of the page table or may be an 
auxiliary data structure indexed in the same way as the page table. 
Figure 5.28
 
shows the organization when a single table holds either the physical page number 

or the disk address.
 e operating system also creates a data structure that tracks which processes 
and which virtual addresses use each physical page. When a page fault occurs, 

if all the pages in main memory are in use, the operating system must choose a 

page to replace. Because we want to minimize the number of page faults, most 

operating systems try to choose a page that they hypothesize will not be needed 

in the near future. Using the past to predict the future, operating systems follow 

the 
least recently used
 (LRU) replacement scheme, which we mentioned in Section 
 e operating system searches for the le
ast recently used page, assuming that 
a page that has not been used in a long time is less likely to be needed than a more 

recently accessed page
 e replaced pages are written to swap space on the disk. 
In case you are wondering, the operating system is just another process, and these 

tables controlling memory are in memory; the details of this seeming contradiction 

will be explained shortly.
swap space
 e space on 
the disk reserved for the 
full virtual memory space 

of a process.

 5.7 Virtual Memory 
435Implementing a completely accurate LRU scheme is too expensive, since it requires 
updating a data structure on 
every
 memory reference. In
stead, most operating 
systems approximate LRU by keeping track of which pages have and which pages 

have not been recently used. To help the operating system estimate the LRU pages, 

some computers provide a 
reference bit
 or 
use bit
, which is set whenever a page 
is accessed.
 e operating system periodically clears the reference bits and later 
records them so it can determine which pages were touched during a particular 

time period. With this usage information, the operating system can select a page 

that is among the least recently referenced (detected by having its reference bit o ). 

If this bit is not provided by the hardware, the operating system mu
 nd another 
way to estimate which pages have been accessed.
Hardware/ 

Software 

Interfacereference bit
 Also called 
use bit
 eld that is 
set whenever a page 
is accessed and that is 

used to implement LRU 

or other replacement 

schemes.
Page tablePhysical page ordisk addressPhysical memoryVirtual pagenumberDisk storage111101
11
1100ValidFIGURE 5.28 The page table maps each page in virtual memory to either a page in main 
memory or a page stored on disk, which is the next level in the hierarchy.
 e virtual page 
number is used to index the page table. If the valid bit is on, the page table supplies the physical page number 
(i.e., the starting address of the page in memory) corr
esponding to the virtual page. If the valid bit is o
 , the 
page currently resides only on disk, at a sp
 ed disk address. In many syste
ms, the table of physical page 
addresses and disk page addresses, while logically one tabl
e, is stored in two separate data structures. Dual 
tables are ju
 ed in part because we must keep the disk addresses of all the pages, even if they are currently 
in main memory. Remember that the pages in main memory and the pages on disk are the same size.

436 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Elaboration: With a 32-bit virtual address, 4 KiB pages, and 4 bytes per page table 
entry, we can compute the total page table size:
Number of page table entries223220212Size of page table2 page table entries2
bytes
page tabl
202e
e entry
4 MiBThat is, we would need to use 4 MiB of memory for each program in execution at any 
time. This amount is not so bad for a single process. What if there are hundreds of 
processes running, each with their own page table? And how should we handle 64-bit 

addresses, which by this calculation would need 2
52 words?A range of techniques is used to reduce the amount of storage required for the page  ve techniques below aim at reducing the total maximum storage required as 

well as minimizing the main memory dedicated to page tables:
1.  The simplest technique is to keep a limit register that restricts the size of the 

page table for a given process. If the virtual page number becomes larger than 

the contents of the limit register, entries must be added to the page table. This 

technique allows the page table to grow as a process consumes more space. 
Thus, the page table will only be large if the process is using many pages of 

virtual address space. This technique requires that the address space expand in 

only one direction. 2.   cient, since most languages require 
two areas whose size is expandable: one area holds the stack and the other area 
holds the heap. Because of this duality, it is convenient to divide the page table 

and let it grow from the highest address down, as well as from the lowest address 

up. This means that there will be two separate page tables and two separate 
limits. The use of two page tables breaks the address space into two segments. 
The high-order bit of an address usually determines which segment and thus which 

 es the 
segment, each segment can be as large as one-half of the address space. A 

 es the current size of the segment, which 
grows in units of pages. This type of segmentation is used by many architectures, 

including MIPS. Unlike the type of segmentation discussed in the third elaboration 

on page 431, this form of segmentation is invisible to the application program, 

although not to the operating system. The major disadvantage of this scheme is 
that it does not work well when the address space is used in a sparse fashion 

rather than as a contiguous set of virtual addresses.
3.  Another approach to reducing the page table size is to apply a hashing function 
to the virtual address so that the page table need be only the size of the number 

of physical pages in main memory. Such a structure is called an 
inverted page 
table. Of course, the lookup process is slightly more complex with an inverted 

page table, because we can no longer just index the page table.
4.  Multiple levels of page tables can also be used to reduce the total amount of 

 rst le xed-size blocks of virtual address 

space, perhaps 64 to 256 pages in total. These large blocks are sometimes 

called segments, rst-level mapping table is sometimes called a 

 5.7 Virtual Memory 
437segment table, though the segments are again invisible to the user. Each entry 
in the segment table indicates whether any pages in that segment are allocated 

and, if so, points to a page table for that segment. Address translation happens 

b rst looking in the segment table, using the highest-order bits of the address. 

If the segment address is valid, the next set of high-order bits is used to index 

the page table indicated by the segment table entry. This scheme allows the 

address space to be used in a sparse fashion (multiple noncontiguous segments 

can be active) without having to allocate the entire page table. Such schemes 

are particularly useful with very large address spaces and in software systems 

that require noncontiguous allocation. The primary disadvantage of this two-level 

mapping is the more complex process for address translation.5.  To reduce the actual main memory tied up in page tables, most modern systems 

also allow the page tables to be paged. Although this sounds tricky, it works 

by using the same basic ideas of virtual memory and simply allowing the page 

tables to reside in the virtual address space. In addition, there are some small 

but critical problems, such as a never-ending series of page faults, which must 

be avoided. How these problems are overcome is both very detailed and typically 

 c. In brief, these problems are avoided by placing all the 
page tables in the address space of the operating system and placing at least 
some of the page tables for the operating system in a portion of main memory 

that is physically addressed and is always present and thus never on disk.
What about Writes? e 
 erence between the access time to the cache and main memory is tens to 
hundreds of cycles, and write-through schemes can be used, although we need a 
write bu
 er to hide the latency of the write from the processor. In a virtual memory 
system, writes to the next level of the hierarchy
 (disk) can take millions of processor 
clock cycles; therefore, building a write bu er to allow the system to write-through 

to disk would be completely impractical. Instead, virtual memory systems must use 

write-back, performing the individual writes into the page in memory, and copying 

the page back to disk when it is replaced in the memory.
A write-back scheme has another major advantage in a virtual memory system. 
Because the disk transfer time is small compared with its access time, copying back 

an entire page is much more e
  cient than writing individual words back to the disk. 
A write-back operation, although more
  cient than transferring individual words, is 
still costly.
 us, we would like to know whether a page 
needs
 to be copied back when 
we choose to replace it. To track whether a page has been written since it was read into 

the memory, a 
dirty bit
 is added to the page table
 e dirty bit is set when any word 
in a page is written. If the operating system chooses to replace the page, the dirty bit 

indicates whether the page needs 
to be written out before its location in memory can be 
given to another page. Hence, a mo
 ed page is o
 en called a 
dirty
 page.
Hardware/ 

Software 

Interface
438 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Making Address Translation Fast: the TLB
Since the page tables are stored in main memory, every memory access by a program 
can take at least twice as long: one memory access to obtain the physical address 

and a second access to get the dat
 e key to improving access performance is to 
rely on locality of reference to the page table. When a translation for a virtual page 

number is used, it will probably be needed again in the near future, because the 

references to the words on that page have both temporal and spatial locality.
Accordingly, modern processors include a special cache that keeps track of recently 
used translation
 is special address translation cache is traditionally referred to as 
a translation-lookaside bu
 er (TLB)
, although it would be more accurate to call it 
a translation cache. e TLB corresponds to that little piece of paper we typically use 
to record the location of a set of books we look up in the card catalog; rather than 

continually searching the entire catalog, we record the location of several books and 

use the scrap of paper as a cache of Library of Congress call numbers.
Figure 5.29
 shows that each tag entry in the TLB holds a portion of the virtual 
page number, and each data entry of the TLB holds a physical page number. 
translation-lookaside 
bu
 er (TLB)
 A cache 
that keeps track of 
recently used address 

mappings to try to avoid 

an access to the page 

table.
11
1
1
0
1
11110000
0
0
0
0
01110010
0
1
0
1
111100Physical pageor disk addressValidDirtyRef
Page tablePhysical memoryVirtual pagenumberDisk storage1111010110001
1
1
1
0
1Physical pageaddressValidDirtyRef
TLBTagFIGURE 5.29 The TLB acts as a cache of the page table for the entries that map to physical pages only.
 e TLB contains a subset of the virtual-to-physical page mappings that are in the 
page table.
 e TLB mappings are shown in color. Because the TLB is a cache, it must have a ta
 eld. If there 
is no matching entry in the TLB for a page, the page table must be examined
 e page table either supplies a 
physical page number for the page (which can then be us
ed to build a TLB entry) or indicates that the page 
resides on disk, in which case a page fault occurs. Since th
e page table has an entry for every virtual page, no 
ta
 eld is needed; in other words, unlike a TLB, a page table is 
not a cache.

 5.7 Virtual Memory 
439Because we access the TLB instead of the page table on every reference, the TLB 
will need to include other status bits, such as the dirty and the reference bits.
On every reference, we look up the virtual page number in the TLB. If we get a 
hit, the physical page number is used to form the address, and the corresponding 

reference bit is turned on. If the processor is performing a write, the dirty bit is also 

turned on. If a miss in the TLB occurs, we must determine whether it is a page fault 

or merely a TLB miss. If the page exis
ts in memory, then the TLB miss indicates 
only that the translation is missing. In such cases, the processor can handle the TLB 

miss by loading the translation from the page table into the TLB and then trying the 

reference again. If the page is not present in memory, then the TLB miss indicates 

a true page fault. In this case, the proces
sor invokes the operating system using an 
exception. Because the TLB has many fewer entries than the number of pages in 

main memory, TLB misses will be much more frequent than true page faults.
TLB misses can be handled either in hardware or in so
 ware. In practice, with 
care there can be little performa
 erence between the two approaches, because 
the basic operations are the same in either case.
 er a TLB miss occurs and the missing translation has been retrieved from the 
page table, we will need to select a TLB entry to replace. Because the reference and 

dirty bits are contained in the TLB entry, we need to copy these bits back to the page 

table entry when we replace an entry.
 ese bits are the only portion of the TLB 
entry that can be changed. Using write-back—that is, copying these entries back at 

miss time rather than when they are written—is ver
  cient, since we expect the 
TLB miss rate to be small. Some systems use other techniques to approximate the 

reference and dirty bits, eliminating the need to write into the TLB except to load 

a new table entry on a miss.
Some typical values for a TLB might be
 TLB size: 16–512 entries
 Block size: 1–2 page table entries (typically 4–8 bytes each)
 Hit time: 0.5–1 clock cycle
 Miss penalty: 10–100 clock cycles
 Miss rate: 0.01%–1%
Designers have used a wide variety of associativities in TLBs. Some systems use 

small, fully associative TLBs because a fully associative mapping has a lower miss 

rate; furthermore, since the TLB is small, the cost of a fully associative mapping is 

not too high. Other systems use large TLBs, o
 en with small associativity. With 
a fully associative mapping, choosing the entry to replace becomes tricky since 

implementing a hardware LRU scheme is too expensive. Furthermore, since TLB 

misses are much more frequent than page faults and thus must be handled more 

cheaply, we cannot a
 ord an expensive so
 ware algorithm, as we can for page faults. 
As a result, many systems provide some support for randomly choosing an entry 

to replace. We’ll examine replacement schemes in a little more detail in Section 5.8.

440 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
The Intrinsity FastMATH TLB
To see these ideas in a real processor, let’s take a closer look at the TLB of the 
Intrinsity FastMA
 e memory system uses 4 KiB pages and a 32-bit address 
space; thus, the virtual page number is 20 bits long, as in the top of 
Figure 5.30
. 
 e physical address is the same size as the virtual addr e TLB contains 16 
entries, it is fully associative, and it is shared between the instruction and data 

references. Each entry is 64 bits wide and contains a 20-bit tag (which is the virtual 

page number for that TLB entry), the corresponding physical page number (also 20 

bits), a valid bit, a dirty bit, and other bookkeeping bits. Like most MIPS systems, 

it uses so
 ware to handle TLB misses.
Figure 5.30
 shows the TLB and one of the caches, while 
Figure 5.31
 shows the 
steps in processing a read or write request. When a TLB miss occurs, the MIPS 

hardware saves the page number of the reference in a special register and generates 

an exceptio
 e exception invokes the operating system, which handles the miss 
in so ware. T nd the physical address for the mi
ssing page, the TLB miss routine 
indexes the page table using the page nu
mber of the virtual address and the page 
table register, which indicates the starting 
address of the active process page table. 
Using a special set of system instructions that can update the TLB, the operating 

system places the physical address from the page table into the TLB. A TLB miss 

takes about 13 clock cycles, assuming the code and the page table entry are in the 

instruction cache and data cache, respectively. (We will see the MIPS TLB code 

on page 449.) A true page fault occurs if the page table entry does not have a valid 

physical addr
 e hardware maintains an index that indicates the recommended 
entry to replace; the recommended entry is chosen randomly.
 ere is an extra complication for write requests: namely, the write access bit in 
the TLB must be checked.
 is bit prevents the program from writing into pages 
for which it has only read access. If the program attempts a write and the write 

access bit is o
 , an exception is generated
 e write access bit forms part of the 
protection mechanism, which we will discuss shortly.
Integrating Virtual Memory, TLBs, and Caches
Our virtual memory and cache systems work together as a hierarchy, so that data 

cannot be in the cache unless it is present in main memory
 e operating system 
helps maintain this hierarchy by
 ushing the contents of any page from the cache 
when it decides to migrate that page to disk. At the same time, the OS mo
 es the 
page tables and TLB, so that an attempt to access any data on the migrated page 

will generate a page fault.
Under the best of circumstances, a virtual address is translated by the TLB and 
sent to the cache where the appropriate data is found, retrieved, and sent back to 

the processor. In the worst case, a reference can miss in all three components of the 

memory hierarchy: the TLB, the page table, and the cache
 e following example 
illustrates these interactions in more detail.

 5.7 Virtual Memory 
441==20Virtual page numberPage offsetTagValidDirty
TLBPhysical page numberTagValidTLB hitCache hitDataDataByteoffset=====Physical page numberPage offsetPhysical address tagCache index1220BlockoffsetPhysical address1832842128Cache31   30   293   2   1   014   13   12   11   10   9Virtual addressFIGURE 5.30 The TLB and cache implement the process of going from a virtual address to a data item in the Intrinsity 
FastMATH.
 is 
 gure shows the organization of the TLB and the data cache, assuming a 4 KiB page size
 is diagram focuses on a read; 
Figure 5.31
 describes how to handle writes. Note that unlike 
Figure 5.12
, the tag and data RAMs are split. By addressing the lo
ng but narrow 
data RAM with the cache index concatenated with the block o
 set, we select the desired word in the block without a 16:1 multiplexor. While 
the cache is direct mapped, the TLB is fully associative. Implementing a fully associative TLB requires that every TLB tag be c
ompared against 
the virtual page number, since the entry of interest can be anywhere in the TLB. (See content addressable memories in the 
Elaboration
 on 
page 408.) If the valid bit of the matching entry is on, the access is a TLB hit, and bits from the physical page number togeth
er with bits from 
the page o
 set form the index that is used to access the cache.

442 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Yes
Write access
bit on?NoYes
Cache hit?NoWrite data into cache,
update the dirty bit, and
put the data and theaddress into the write buffer
Yes
TLB hit?Virtual address
TLB accessTry to read data
from cacheNoYes
Write?
NoCache miss stallwhile read block
Deliver data
to the CPUWrite protection
exception
Yes
Cache hit?NoTry to write data
to cacheCache miss stallwhile read block
TLB missexception
Physical address
FIGURE 5.31 Processing a read or a write-through in the Intrinsity FastMATH TLB and cache.
 If the TLB generates a hit, 
the cache can be accessed with the resulting physical address. For a read, the cache generates a hit or miss and supplies the d
ata or causes a stall 
while the data is brought from memory. If the operation is a write, a portion of the cache entry is overwritten for a hit and t
he data is sent to 
the write bu
 er if we assume write-through. A write miss is just like a read miss except that the block is mo
 ed a
 er it is read from memory. 
Write-back requires writes to set a dirty bit for the cache block, and a write bu
 er is loaded with the whole block only on a read miss or write 
miss if the block to be replaced is dirty. Notice that a TLB hit and a cache hit are independent events, but a cache hit can on
ly occur a er a TLB 
hit occurs, which means that the data must be present in memory
 e relationship between TLB misses and cache misses is examined further 
in the following example and the exercises at the end of this chapter.

 5.7 Virtual Memory 
443Overall Operation of a Memory Hierarchy
In a memory hierarchy like that of 
Figure 5.30
, which includes a TLB and a 
cache organized as shown, a memory reference can encounter thre
 erent 
types of misses: a TLB miss, a page 
fault, and a cache miss. Consider all 
the combinations of these three events with one or more occurring (seven 

possibilities). For each possibility, state whether this event can actually occur 

and under what circumstances.
Figure 5.32
 shows all combinations and whether each is possible in practice.
Elaboration: Figure 5.32
 assumes that all memory addresses are translated to 
physical addresses before the cache is accessed. In this organization, the cache is 
physically indexed
 and physically tagged
 (both the cache index and tag are physical, 

rather than virtual, addresses). In such a system, the amount of time to access memory, 

assuming a cache hit, must accommodate both a TLB access and a cache access; of 

course, these accesses can be 
pipelined.Alternatively, the processor can index the cache with an address that is completely 
or partially virtual. This is called a 
virtually addressed cache
, and it uses tags that 
are virtual addresses; hence, such a cache is 
virtually indexed
 and virtually tagged
. In such caches, the address translation hardware (TLB) is unused during the normal cache 

access, since the cache is accessed with a virtual address that has not been translated 

to a physical address. This takes the TLB out of the critical path, reducing cache latency. 

When a cache miss occurs, however, the processor needs to translate the address to a 

physical address so that it can fetch the cache block from main memory.
EXAMPLEANSWERvirtually addressed 
cache
 A cache that is 
accessed with a virtual 
address rather than a 

physical address.
TLBPage tableCache Possible? If so, under what circumstance?
HitHitMissPossible, although the page table is never really checked if TLB hits.
MissHitHitTLB misses, but entry found in page table; after retry, data is found in cache.
MissHitMissTLB misses, but entry found in page table; after retry, data misses in cache.

MissMissMissTLB 
misses and is followed by a page fault; after retry, data must miss in cache.
HitMissMissImpossible: cannot have a translation in TLB if page is not present in memory.

HitMissHitImpossible: cannot have a translation in TLB if page is not present in memory.
MissMissHitImpossible: data cannot be allowed in cache if the page is not in memory.
FIGURE 5.32 The possible combinations of events in the TLB, virtual memory system, 
and cache. ree of these combinations are impossible, and one is possible (TLB hit, virtual memory hit, 
cache miss) but never detected.

444 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
When the cache is accessed with a virtual address and pages are shared between 
processes (which may access them with different virtual addresses), there is the 
possibility of aliasing. Aliasing occurs when the same object has two names—in this 
case, two virtual addresses for the same page. This ambiguity creates a problem, because 
a word on such a page may be cached in two different locations, each corresponding 

to different virtual addresses. This ambiguity would allow one program to write the data 

without the other program being aware that the data had changed. Completely virtually 

addressed caches either introduce design limitations on the cache and TLB to reduce 
aliases or require the operating system, and possibly the user, to take steps to ensure 

that aliases do not occur.
A common compromise between these two design points is caches that are virtually 
indexed—sometimes using just the page-offset portion of the address, which is really 

a physical address since it is not translated—but use physical tags. These designs, 

which are virtually indexed but physically tagged
, attempt to achieve the performance 

advantages of virtually indexed caches with the architecturally simpler advantages of a 
physically addressed cache
. For example, there is no alias problem in this case. 
Figure
 5.30 assumed a 4 KiB page size, but it’s really 16 KiB, so the Intrinsity FastMATH can 
use this trick. To pull it off, there must be careful coordination between the minimum 

page size, the cache size, and associativity.
Implementing Protection with Virtual Memory
Perhaps the most important function of virtual memory today is to allow sharing of 
a single main memory by multiple processes, while providing memory protection 

among these processes and the operating syst
 e protection mechanism must 
ensure that although multiple processes are sharing the same main memory, one 

renegade process cannot write into the addres
s space of another user process or into 
the operating system either intentionally or unintentionally.
 e write access bit in 
the TLB can protect a page from being written. Without this level of protection, 

computer viruses would be even more widespread.
To enable the operating system to implement protection in the virtual memory 
system, the hardware must provide at leas
t the three basic capabilities summarized 
below. Note that th
 rst two are the same requirements as needed for virtual 
machines (Section 5.6).
1. Support at least two modes that indicate whether the running process is a 
user process or an operating system process, variously called a 
supervisor
 process, a 
kernel process, or an 
executive
 process.
2. Provide a portion of the processor state that a user process can read but not 
write.
 is includes the user/supervisor mode bit, which dictates whether 
the processor is in user or supervisor mode, the page table pointer, and the 
aliasing
 A situation 
in which two addresses 
access the same object; 

it can occur in virtual 

memory when there are 

two virtual addresses for 

the same physical page.
physically addressed 
cache
 A cache that is 
addressed by a physical 
address.
Hardware/ 
Software 
Interfacesupervisor mode
 Also 
called kernel mode
. A mode indicating that a 
running process is an 

operating system process.

 5.7 Virtual Memory 
445TLB. To write these elements, the operating system uses special instructions 
that are only available in supervisor mode.
3. Provide mechanisms whereby the processor can go from user mode to 
supervisor mode and vice vers
 e 
 rst direction is typically accomplished 
by a 
system call
 exception, implemented as a special instruction (
syscall
 in the MIPS instruction set) that transfers control to a dedicated location in 

supervisor code space. As with any other exception, the program counter 

from the point of the system call is saved in the exception PC (EPC), and 

the processor is placed in supervisor mode. To return to user mode from the 

exception, use the 
return from exception
 (ERET) instruction, which resets to 
user mode and jumps to the address in EPC.
By using these mechanisms and storing the page tables in the operating system’s 
address space, the operating system can change the page tables while preventing a 

user process from changing them, ensuring that a user process can access only the 

storage provided to it by the operating system.
We also want to prevent a process from reading the data of another process. For 
example, we wouldn’t want a student program to read the grades while they were 

in the processor’s memory. Once we begin sharing main memory, we must provide 

the ability for a process to protect its data from both reading and writing by another 

process; otherwise, sharing the main memory will be a mixed blessing!
Remember that each process has its own virtual address space
 us, if the 
operating system keeps the page tables organized so that the independent virtual 

pages map to disjoint physical pages, one pr
ocess will not be able to access another’s 
data. Of course, this also requires that a user process be unable to change the page 

table mappin e operating system can assure safety if it prevents the user process 

from modifying its own page tables. However, the operating system must be able 

to modify the page tables. Placing the page tables in the protected address space of 

the operating system sa
 es both requirements.
When processes want to share information in a limited way, the operating system 
must assist them, since accessing the infor
mation of another process requires 

changing the page table of the accessing pro
 e write access bit can be used 
to restrict the sharing to just read sharing, and, like the rest of the page table, this 

bit can be changed only by the operating system. To allow another process, say, P1, 

to read a page owned by process P2, P2 would ask the operating system to create 

a page table entry for a virtual page in P1’s address space that points to the same 

physical page that P2 wants to share
 e operating system could use the write 
protection bit to prevent P1 from writing the data, if that was P2’s wish. Any bits 

that determine the access rights for a page mu
st be included in both the page table 
and the TLB, because the page table is accessed only on a TLB 
miss
.system call
 A special 
instruction that transfers 
control from user mode 

to a dedicated location 

in supervisor code space, 

invoking the exception 

mechanism in the process.

446 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Elaboration: When the operating system decides to change from running process 
P1 to running process P2 (called a 
context switch or process switch), it must ensure 
that P2 cannot get access to the page tables of P1 because that would compromise protection. If there is no TLB, ces to change the page table register to point to P2
’s page table (rather than to P1’s); with a TLB, we must clear the TLB entries that belong to 

P1—both to protect the data of P1 and to force the TLB to load the entries for P2. If the 
process switch rate were high, cient. For example, P2 might load 

only a few TLB entries before the operating system switched back to P1. Unfortunately, 

 nd that all its TLB entries were gone and would have to pay TLB misses 

to reload them. This problem arises because the virtual addresses used by P1 and P2 

are the same, and we must clear out the TLB to avoid confusing these addresses.
A common alternative is to extend the virtual address space by adding a 
process identiﬁ er
 or task identiﬁ er
. The Intrinsity FastMATH has an 8-bit address space ID (ASID) 
 eld for this pur  es the currently running process; it is kept 

in a register loaded by the operating system when it switches processes. The process 

 er is concatenated to the tag portion of the TLB, so that a TLB hit occurs only if 
both the page number and er match. This combination eliminates the 

need to clear the TLB, except on rare occasions.
Similar problems can occur for a cache, since on a process switch the cache will 
contain data from the running process. These problems arise in different ways for 

physically addressed and virtually addressed caches, and a variety of different solutions, 

 ers, are used to ensure that a process gets its own data.
Handling TLB Misses and Page Faults
Although the translation of virtual to physical addresses with a TLB is 
straightforward when we get a TLB hit, as we saw earlier, handling TLB misses and 

page faults is more complex. A TLB miss occurs when no entry in the TLB matches 

a virtual address. Recall that a TLB miss can indicate one of two possibilities:
 e page is present in memory, and we need only create the missing TLB 
entry.
 e page is not present in memory, and we need to transfer control to the 
operating system to deal with a page fault.
MIPS traditionally handles a TLB miss in so
 ware. It brings in the page table 
entry from memory and then re-executes the instruction that caused the TLB miss. 

Upon re-executing, it will get a TLB hit. If the page table entry indicates the page is 

not in memory, this time it will get a page fault exception.
Handling a TLB miss or a page fault requires using the exception mechanism 
to interrupt the active process, transferring control to the operating system, and 

later resuming execution of the interrupted process. A page fault will be recognized 

sometime during the clock cycle used to access memory. To restart the instruction 

 er the page fault is handled, the program counter of the instruction that caused 
the page fault must be saved. Just as in Chapter 4, the 
exception program counter 
(EPC) is used to hold this value.
context switch
 A changing of the internal 
state of the processor to 

allow
 erent process 
to use the processor 

that includes saving the 

state needed to return to 

the currently executing 

process.

 5.7 Virtual Memory 
447In addition, a TLB miss or page fault exception must be asserted by the end 
of the same clock cycle that the memory access occurs, so that the next clock 
cycle will begin exception processing rather than continue normal instruction 

execution. If the page fault was not recognized in this clock cycle, a load instruction 

could overwrite a register, and this could be disastrous when we try to restart the 

instruction. For example, consider the instruction 
lw $1,0($1): the computer 

must be able to prevent the write pipeline stage from occurring; otherwise, it could 

not properly restart the instruction, since the contents of $
1 would have been 
destroyed. A similar complication arises on stores. We must prevent the write into 

memory from actually completing when there is a page fault; this is usually done 

by deasserting the write control line to the memory.
Between the time we begin executing the exception handler in the operating 
system and the time that the operating system has saved all the state of the process, 

the operating system is particularly vulnerable. For example, if another exception 

occurred when we were processing th
 rst exception in the operating system, the 
control unit would overwrite the exception program counter, making it impossible 

to return to the instruction that caused the page fault! We can avoid this disaster 

by providing the ability to 
disable 
and 
enable exceptions
. When an exceptio
 rst occurs, the processor sets a bit that disables all other exceptions; this could happen 

at the same time the processor sets the supervisor mode bi
 e operating system 
will then save just enough state to allow it to recover if another exception occurs—

namely, the 
exception program counter
 (EPC) and Cause registers. EPC and Cause 
are two of the special control registers that help with exceptions, TLB misses, and 

page faults; 
Figure 5.33
 shows the r
 e operating system can then re-enable 
exception
 ese steps make sure that exceptions will not cause the processor 
to lose any state and thereby be unable to restart execution of the interrupting 

instruction.
Once the operating system knows the virtual 
address that caused the page fault, it 
must complete three steps:
1. Look up the page table entry using the virtual address an
 nd the location 
of the referenced page on disk.
2. Choose a physical page to replace; if the chosen page is dirty, it must be 
written out to disk before we can bring a new virtual page into this physical 
page.
3. Start a read to bring the referenced page from disk into the chosen physical 
page.
Hardware/ 

Software 

Interfaceexception enable
 Also 
called interrupt enable. 
A signal or action that 

controls whether the 

process responds to 

an exception or not; 

necessary for preventing 

the occurrence of 

exceptions during 

intervals before the 

processor has safely saved 

the state needed to restart.

448 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Of course, this last step will take millions of processor clock cycles (so will the 
second if the replaced page is dirty); accordingly, the operating system will usually 

select another process to execute in th
e processor until the disk access completes. 
Because the operating system has saved the state of the process, it can freely give 

control of the processor to another process.
When the read of the page from disk is complete, the operating system can 
restore the state of the process that originally caused the page fault and execute 

the instruction that returns from the exceptio
 is instruction will reset the 
processor from kernel to user mode, as well as restore the program counter.
 e user process then re-executes the instruction that faulted, accesses the requested 

page successfully, and continues execution.
Page fault exceptions for data accesses ar
  cult to implement properly in a 
processor because of a combination of three characteristics:
 ey occur in the middle of instructio
ns, unlike instruction page faults.
 e instruction cannot be completed before handling the exception.
 er handling the exception, the instruction must be restarted as if nothing 
had occurred.
Making instructions 
restartable
, so that the exception can be handled and the 
instruction later continued, is relatively easy in an architecture like the MIPS. 
Because each instruction writes only one data item and this write occurs at the end 

of the instruction cycle, we can simply prevent the instruction from completing (by 

not writing) and restart the instruction at the beginning.
Let’s look in more detail at MIPS. When a TLB miss occurs, the MIPS hardware 
saves the page number of the reference in a special register called BadVAddr and 

generates an exception.
restartable 
instruction
 An instruction that can 
resume execution a
 er an exception is resolved 

without the exception’s 

 ecting the result of the 
instruction.
RegisterCP0 register number
DescriptionEPC14Where to restart after exception
Cause13Cause of exceptionBadVAddr
8Address that caused exceptionIndex0Location in TLB to be read or writtenRandom1Pseudorandom location in TLBEntryLo
2Physical page address and ßags
EntryHi
10Virtual page address
Context4Page table address and page number
FIGURE 5.33 MIPS control registers.
 ese are considered to be in coprocessor 0, and hence are 
read using 
mfc0 and written using 
mtc0.
 5.7 Virtual Memory 
449 e exception invokes the operating system, which handles the miss in so
 ware. 
Control is transferred to address 8000 0000
hex, the location of the TLB miss 
handler
. To nd the physical address for the missin
g page, the TLB miss routine indexes the 
page table using the page number of the vi
rtual address and the page table register, 
which indicates the starting address of the active process page table. To make this 
indexing fast, MIPS hardware places everything you need in the special 
Context register: the upper 12 bits have the address of the base of the page table, and the 

next 18 bits have the virtual address of the 
missing page. Each page table entry is 
one word, so the last 2 bits ar
 us, th
 rst two instructions copy the Context 
register into the kernel temporary register 
$k1 and then load the page table entry 
from that address into 
$k1. Recall that 
$k0 and 
$k1 are reserved for the operating 
system to use without saving; a major reason for this convention is to make the TLB 

miss handler fast. Below is the MIPS code for a typical TLB miss handler:
TLBmiss:mfc0 $k1,Context # copy address of PTE into temp $k1
lw $k1,0($k1) # put PTE into temp $k1

mtc0 $k1,EntryLo # 
put PTE into special register EntryLotlbwr  # put EntryLo into TLB entry at Randomeret  
# return from TLB miss exceptionAs shown above, MIPS has a special set of system instructions to update the 
TLB
 e instruction 
tlbwr copies from control register 
EntryLo into the TLB 
entry selected by the control register 
Random. Random implements random 
replacement, so it is basically a free-running counter. A TLB miss takes about a 
dozen clock cycles.
Note that the TLB miss handler does not check to see if the page table entry is 
valid. Because the exception for TLB entry missing is much more frequent than 

a page fault, the operating system loads the TLB from the page table without 

examining the entry and restarts the instruction. If the entry is invalid, another 

an
 erent exception occurs, and the operating system recognizes the page fault. 
 is method makes the frequent case of a TLB miss fast, at a slight performance 
penalty for the infrequent case of a page fault.
Once the process that generated the page fault has been interrupted, it transfers 
control to 8000 0180
hex erent address than the TLB miss handler
 is is 
the general address for exception; TLB miss has a special entry point to lower the 

penalty fo
 e operating system uses the exception Cause register 
to diagnose the cause of the exception. Because the exception is a page fault, the 

operating system knows that extensive processing will be required
 us, unlike a 
TLB miss, it saves the entire state of the active pro
 is state includes all the 
general-purpose an
 oating-point registers, the page table address register, the 
EPC, and the exception Cause register. Since exception handlers do not usually use 

th
 oating-point registers, the general entry point does not save them, leaving that 
to the few handlers that need them.
handler
 Name of a 
so
 ware routine invoked 
to “handle” an exception 
or interrupt.

450 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Figure 5.34
 sketches the MIPS code of an exception handler. Note that we 
save and restore the state in MIPS code, taking care when we enable and disable 
exceptions, but we invoke C code to handle the particular exception.
 e virtual address that caused the fault depends on whether the fault was an 
instruction or data faul
 e address of the instruction that generated the fault is 
in the EPC. If it was an instruction page fault, the EPC contains the virtual address 

of the faulting page; otherwise, the faulting virtual address can be computed by 

examining the instruction (whose address is in the EPC) t
 nd the base register 
and
 set 
 eld.Elaboration:  ed version assumes that the 
stack pointer (sp) is valid. To 
avoid the problem of a page fault during this low-level exception code, MIPS sets aside 
a portion of its address space that cannot have page faults, called 
unmapped. The operating system places the exception entry point code and the exception stack in unmapped memory. MIPS hardware translates virtual addresses 8000 0000
hex to BFFF FFFFhex to physical addresses simply by ignoring the upper bits of the virtual address, 
thereby placing these addresses in the low part of physical memory. Thus, the operating 

system places exception entry points and exception stacks in unmapped memory.
Elaboration: The code in 
Figure 5.34
 shows the MIPS-32 exception return sequence. 
The older MIPS-I architecture uses rfe and jr instead of eret.Elaboration: For processors with more complex instructions that can touch many 
memory locations and write many data items, making instructions restartable is much 

harder. Processing one instruction may generate a number of page faults in the middle 

of the instruction. For example, x86 processors have block move instructions that touch 

thousands of data words. In such processors, instructions often cannot be restarted 

from the beginning, as we do for MIPS instructions. Instead, the instruction must be 

interrupted and later continued midstream in its execution. Resuming an instruction in 

the middle of its execution usually requires saving some special state, processing the 

exception, and restoring that special state. Making this work properly requires careful 

and detailed coordination between the exception-handling code in the operating system 

and the hardware.
Elaboration: Rather than pay an extra level of indirection on every memory access, the 
VMM maintains a shadow page table
 that maps directly from the guest virtual address 

space to the physical address space of the hardw cations to 

the guest’s page table, the VMM can ensure the shadow page table entries being used 

by the hardware for translations correspond to those of the guest OS environment, with 

the exception of the correct physical pages substituted for the real pages in the guest 

tables. Hence, the VMM must trap any attempt by the guest OS to change its page table 

or to access the page table pointer. This is commonly done by write protecting the guest 

page tables and trapping any access to the page table pointer by a guest OS. As noted 

above, the latter happens naturally if accessing the page table pointer is a privileged 

operation.unmapped
 A portion 
of the address space that 
cannot have page faults.

 5.7 Virtual Memory 
451Save stateSave GPR addi $k1,$sp, -XCPSIZE # save space on stack for state 
 sw $sp, XCT_SP($k1) # save $sp on stack 

 sw $v0, XCT_V0($k1) # save $v0 on stack 

 ...   # save $v1, $ai, $si, $ti,...
on stack sw $ra, XCT_RA($k1) # save $ra on stack
Save hi, lo
 mfhi $v0  #
 copy Hi  mßo $v1  # copy Lo 
 sw $v0, XCT_HI($k1) # save Hi value on stack 

 sw $v1, XCT_LO($k1) # save Lo value on stack
Save exception
registers
 mfc0 $a0,
 $cr  
# copy cause register  sw $a0, XCT_CR($k1) # save $cr value on stack 
 ...   # save $v1,.... 

 mfc0 $a3, $sr 
 # copy status register  sw $a3, XCT_SR($k1) # save $sr on stack
Set sp move $sp,
 $k1  
# sp = sp - XCPSIZEEnable nested exceptions andi $v0, $a3, MASK1 # $v0 = $sr & MASK1, enable exceptions 

 mtc0 $v0, $sr 
 # $sr = value that enables exceptions
Call C exception handlerSet $gp move $gp, GPINIT # set $gp to point to heap area
Call C code move $a0, $sp 
 # arg1 = pointer to exception stack 
 jal xcpt_deliver  # call C code to handle exception
Restoring stateRestore most GPR, hi, lo move $at, $sp 
 # temporary value of $sp 
 lw $ra, XCT_RA($at) # restore $ra from stack 
 ...   # restore $t0,...., $a1 

 lw $a0, XCT_A0($k1) # restore $a0 from stack
Restore status register lw $v0,
 XCT_SR($at) # load old $sr from stack 
 li $v1, MASK2 # mask to disable exceptions 
 and $v0, $v0, $v1 # $v0 = $sr & MASK2, disable exceptions 

 mtc0 $v0, $sr 
 # set status register
Exception return
Restore $sp and rest of 
GPR used as 
temporary 
registers lw $sp, XCT_SP($at) # restore $sp from stack 
 lw $v0, XCT_V0($at) # restore $v0 from stack 

 lw $v1, XCT_V1($at) # restore $v1 from stack 
 lw $k1, XCT_EPC($at) # copy old $epc from stack 
 lw $at, XCT_AT($at) # restore $at from stack
Restore ERC and return mtc0 $k1, $epc # restore $epc 
 eret $ra  # return to interrupted instruction
FIGURE 5.34 MIPS code to save and restore state on an exception.

452 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Elaboration:  nal portion of the architecture to virtualize is I/O. This is by far 
 cult part of system virtualization because of the increasing number of 
I/O devices attached to the computer 
and the increasing diversity of I/O device types. 

 culty is the sharing of a real device among multiple VMs, and yet another 

comes from supporting the myriad of device drivers that are required, especially if 

different guest OSes are supported on the same VM system. The VM illusion can be 

maintained by giving each VM generic versions of each type of I/O device driver, and then 

leaving it to the VMM to handle real I/O.
Elaboration: In addition to virtualizing the instruction set for a virtual machine, 
another challenge is virtualization of virtual memory, as each guest OS in every virtual 

machine manages its own set of page tables. To make this work, the VMM separates 

the notions of real
 and physical memory
 (which are often treated synonymously), and 
makes real memory a separate, intermediate level between virtual memory and physical 

memory. (Some use the terms 
virtual memory, physical memory,
 and machine memory
 to name the same three levels.) The guest OS maps virtual memory to real memory 

via its page tables, and the VMM page tables map the guest
’s real memory to physical 

memory. The virtual memor ed either via page tables, as in IBM 

VM/370 and the x86, or via the TLB structure, as in MIPS.
Summary
Virtual memory is the name for the level of memory hierarchy that manages 
caching between the main memory and secondary memory. Virtual memory 

allows a single program to expand its address space beyond the limits of main 

memory. More importantly, virtual memory supports sharing of the main memory 

among multiple, simultaneously active processes, in a protected manner.
Managing the memory hierarchy between main memory and disk is challenging 
because of the high cost of page faults. Several techniques are used to reduce the 

miss rate:
1. Pages are made large to take advantage of spatial locality and to reduce the 
miss rate.
 e mapping between virtual addresses and physical addresses, which is 
implemented with a page table, is made fully associative so that a virtual 

page can be placed anywhere in main memory.
 e operating system uses techniques, such as LRU and a reference bit, to 
choose which pages to replace.

 5.7 Virtual Memory 
453Writes to secondary memory are expensive, so virtual memory uses a write-back 
scheme and also tracks whether a page is unchanged (using a dirty bit) to avoid 

writing unchanged pages.
 e virtual memory mechanism provides address translation from a virtual 
address used by the program to the physical address space used for accessing 

memory
 is address translation allows protected sharing of the main memory 
and provides several additional be
 ts, such as simplifying memory allocation. 
Ensuring that processes are protected from each other requires that only the 

operating system can change the address translations, which is implemented by 

preventing user programs from changing the page tables. Controlled sharing of 

pages among processes can be implemented with the help of the operating system 

and access bits in the page table that indicate whether the user program has read or 

write access to a page.
If a processor had to access a page table resident in memory to translate every 
access, virtual memory would be too expensive, as caches would be pointless! 

Instead, a TLB acts as a cache for translations from the page table. Addresses are 

then translated from virtual to physical using the translations in the TLB.
Caches, virtual memory, and TLBs all rely on a common set of principles and 
po
 e next section discusses this common framework.
Although virtual memory was invented to enable a small memory to act as a large 

one, the performa
 erence between secondary memory and main memory 
means that if a program routinely accesses more virtual memory than it has 

physical memory, it will run very slowly. Such a program would be continuously 

swapping pages between memory and disk, called 
thrashing
 rashing is a disaster 
if it occurs, but it is rare. If your program thrashes, the easiest solution is to run it on 

a computer with more memory or buy more memory for your computer. A more 

complex choice is to re-examine your algorithm and data structures to see if you 

can change the locality and thereby reduce the number of pages that your program 

uses simultaneously
 is set of popular pages is informally called the 
working set
.A more common performance problem is TLB misses. Since a TLB might 
handle only 32–64 page entries at a time, a program could easily see a high TLB 

miss rate, as the processor may access less than a quarter mebibyte directly: 64 

 4 KiB 
 0.25 MiB. For example, TLB misses are o
 en a challenge for Radix 
Sort. To try to alleviate this problem, most computer architectures now support 

variable page sizes. For example, in addition to the standard 4 KiB page, MIPS 

hardware supports 16 KiB, 64 KiB, 256 KiB, 1 MiB, 4 MiB, 16 MiB, 64 MiB, and 

256 MiB pages. Hence, if a program uses large page sizes, it can access more 

memory directly without TLB misses.
 e practical challenge is getting the operating system to allow programs to 
select these larger page sizes. Once again, the more complex solution to reducing 
Understanding 

Program 

Performance

454 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
TLB misses is to re-examine the algorithm and data structures to reduce the 
working set of pages; given the importance of memory accesses to performance 

and the frequency of TLB misses, some programs with large working sets have 

been redesigned with that goal.
Match th
 nitions in the right column to the terms in th
  column.
1. L1 cache
a. A cache for a cache
2. L2 cache
b. A cache for disks
3. Main memoryc. A cache for a main memory
4. TLB
d. A cache for page table entries
 5.8  A Common Framework for Memory 
Hierarchy
By now, you’ve recognized that th
 erent types of memory hierarchies have a 
great deal in common. Although many of the aspects of memory hierarc
 er quantitatively, many of the policies and features that determine how a hierarchy 
functions are similar qualitatively. 
Figure 5.35
 shows how some of the quantitative 

characteristics of memory hierarchies ca
 er. In the rest of this section, we will 
discuss the common operational alternatives for memory hierarchies, and how 

these determine their behavior. We will examine these policies as a series of four 

questions that apply between any two levels of a memory hierarchy, although for 

simplicity we will primarily use terminology for caches.
Check Yourself
Feature
Typical values for L1 cachesTypical values for L2 cachesTypical values for paged memory
Typical values for a TLBTotal size in blocks
250Ð20002,500Ð25,000
16,000Ð250,00040Ð1024Total size in kilobytes
16Ð64125Ð20001,000,000Ð1,000,000,0000.25Ð16
Block size in bytes
16Ð6464Ð1284000Ð64,0004Ð32Miss penalty in clocks10Ð25100Ð100010,000,000Ð100,000,00010Ð1000
Miss rates (global for L2)2%Ð5%
0.1%Ð2%0.00001%Ð0.0001%0.01%Ð2%FIGURE 5.35 The key quantitative design parameters that characterize the major elements of memory hierarchy in a 
computer.
 ese are typical values for these levels as of 2012. Although the range of values is wide, this is partially because many of th
e values 
that have
 ed over time are related; for example, as caches become larger to overcome larger miss penalties, block sizes also grow. While
 not 
shown, server microprocessors today also have L3 caches, which can be 2 to 8 MiB and contain many more blocks than L2 caches. L
3 caches 
lower the L2 miss penalty to 30 to 40 clock cycles.

 5.8 A Common Framework for Memory Hierarchy 
455Question 1: Where Can a Block Be Placed?We have seen that block placement in the upper level of the hierarchy can use a range 
of schemes, from direct mapped to set associative to fully associative. As mentioned 

above, this entire range of schemes can be thought of as variations on a set-associative 

scheme where the number of sets and the number of blocks per set varies:
Scheme nameNumber of setsBlocks per setDirect mappedNumber of blocks in cache
1Set associativeNumber of blocks in the cache
Associativity
Associativity (typically 2–16)Fully associative1Number of blocks in the cache e advantage of increasing the degree of associativity is that it usually decreases 
the miss rate.
 e improvement in miss rate comes from reducing misses that 
compete for the same location. We will examine these in more detail shortly. First, 

let’s look at how much improvement is gained. 
Figure 5.36
 shows the miss rates 

for several cache sizes as associativity varies from direct mapped to eight-way set 

associative.
 e largest gains are obtained in going from direct mapped to two-way 
set associative, which yields between a 20% and 30% reduction in the miss rate. 

As cache sizes grow, the relative improvement from associativity increases only 
AssociativityMiss rate
0One-wayTwo-way
3%6%
9%12%
15%Four-wayEight-way
1 KiB2 KiB4 KiB8 KiB16 KiB32 KiB64 KiB128 KiBFIGURE 5.36 The data cache miss rates for each of eight cache sizes improve as the 
associativity increases. While the b
 t of going from one-way (direct mapped) to two-way set 
associativ
 cant, the b
 ts of further associativity are smaller (e.g., 1%–10% improvement going 
from two-way to four-way versus 20%–30% improvement going from one-way to two-wa
 ere is even 
less improvement in going from four-way to eight-way set associative, which, in turn, comes very close to 
the miss rates of a fully associative cache. Smaller caches obta
 cantly larger absolute be
 t from 
associativity because the base miss rate of a small cache is larger. 
Figure 5.16
 explains how this data was 

collected.

456 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
slightly; since the overall miss rate of a larger cache is lower, the opportunity for 
improving the miss rate decreases and the absolute improvement in the miss rate 

from associativity shr
 cantly. 
 e potential disadvantages of associativity, 
as we mentioned earlier, are increased cost and slower access time.
Question 2: How Is a Block Found?
 e choice of how we locate a block depends on the block placement scheme, since 
that dictates the number of possible locations. We can summarize the schemes as 

follows:
AssociativityLocation methodComparisons requiredDirect mappedIndex1Set associativeIndex the set, search among elements
Degree of associativityFullSearch all cache entriesSize of the cacheSeparate lookup table0 e choice among direct-mapped, set-associative, or fully associative mapping 
in any memory hierarchy will depend on the cost of a miss versus the cost of 

implementing associativity, both in time and in extra hardware. Including the 

L2 cache on the chip enables much higher associativity, because the hit times are 

not as critical and the designer does not have to rely on standard SRAM chips as 

the building blocks. Fully associative caches are prohibitive except for small sizes, 

where the cost of the comparators is not overwhelming and where the absolute 

miss rate improvements are greatest.
In virtual memory systems, a separate mapping table—the page table—is kept 
to index the memory. In addition to the storage required for the table, using an 

index table requires an extra memor
 e choice of full associativity for 
page placement and the extra table is motivated by these facts:
1. Full associativity is b cial, since misses are very expensive.
2. Full associativity allows so
 ware to use sophisticated replacement schemes 
that are designed to reduce the miss rate.
 e full map can be easily indexed with no extra hardware and no searching 
required.
 erefore, virtual memory systems almost always use fully associative placement.
Set-associative placement is o
 en used for caches and TLBs, where the access 
combines indexing and the search of a small set. A few systems have used direct-
mapped caches because of their advantage in access time and simplicity
 e advantage in access time occurs becaus
 nding the requested block does not 
depend on a comparison. Such design choices depend on many details of the 

 5.8 A Common Framework for Memory Hierarchy 
457implementation, such as whether the cache is on-chip, the technology used for 
implementing the cache, and the critical role of cache access time in determining 

the processor cycle time.
Question 3: Which Block Should Be Replaced on a Cache Miss?When a miss occurs in an associative cache, we must decide which block to replace. 
In a fully associative cache, all blocks are candidates for replacement. If the cache is 

set associative, we must choose among the blocks in the set. Of course, replacement 

is easy in a direct-mapped cache because there is only one candidate.
 ere are the two primary strategies for replacement in set-associative or fully 
associative caches:
 Random
: Candidate blocks are randomly selected, possibly using some hardware 
assistance. For example, MIPS supports random replacement for TLB misses.
 Least recently used
 (LRU):
 e block replaced is the one that has been unused 
for the longest time.
In practice, LRU is too costly to implement for hierarchies with more than a small 

degree of associativity (two to four, typically), since tracking the usage information 

is costly. Even for four-way set associativity, LRU is o
 en approximated—for 
example, by keeping track of which pair of blocks is LRU (which requires 1 bit), 

and then tracking which block in each pair is LRU (which requires 1 bit per pair).
For larger associativity, either LRU is approximated or random replacement is 
used. In caches, the replacement algorithm is in hardware, which means that the 

scheme should be easy to implement. Random replacement is simple to build in 

hardware, and for a two-way set-associative cache, random replacement has a miss 

rate about 1.1 times higher than LRU replacement. As the caches become larger, the 

miss rate for both replacement strategies falls, and the absolut
 erence becomes 
small. In fact, random replacement can sometimes be better than the simple LRU 

approximations that are easily implemented in hardware.
In virtual memory, some form of LRU is always approximated, since even a tiny 
reduction in the miss rate can be important when the cost of a miss is enormous. 

Reference bits or equivalent functionality are o
 en provided to make it easier for 
the operating system to track a set of less recently used pages. Because misses are 

so expensive and relatively infrequent, approximating this information primarily 

in so
 ware is acceptable.
Question 4: What Happens on a Write?A key characteristic of any memory hierarchy is how it deals with writes. We have 

already seen the two basic options:
 Write-through
 e information is written to both the block in the cache and 
the block in the lower level of the memory hierarchy (main memory for a 

cach
 e caches in Section 5.3 used this scheme.

458 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 Write-back
 e information is written only to the block in the cache
 e mo
 ed block is written to the lower level of the hierarchy only when it 
is replaced. Virtual memory systems always use write-back, for the reasons 
discussed in Section 5.7.
Both write-back and write-through have their advantag
 e key advantages of 
write-back are the following:
 Individual words can be written by the processor at the rate that the cache, 
rather than the memory, can accept them.
 Multiple writes within a block require only one write to the lower level in the 
hierarchy.
 When blocks are written back, the system can mak
 ective use of a high-
bandwidth transfer, since the entire block is written.
Write-through has these advantages:
 Misses are simpler and cheaper because they never require a block to be 
written back to the lower level.
 Write-through is easier to implement than write-back, although to be 
practical, a write-through cache w
ill still need to use a write bu
 er.
Caches, TLBs, and virtual memory may initially look ver
 erent, but 
they rely on the same two principles of locality, and they can be understood 

by their answers to four questions:
Question 1:Where can a block be placed?
Answer:
One place (direct mapped), a few places (set associative), 

or any place (fully associative).
Question 2:How is a block found?
Answer:
 ere are four methods: indexing (as in a direct-mapped 
cache), limited search (as in a set-associative cache), full 

search (as in a fully associative cache), and a separate 

lookup table (as in a page table).
Question 3:What block is replaced on a miss?
Answer:
Typically, either the least recently used or a random block.
Question 4:How are writes handled?
Answer:
Each level in the hierarchy can use either write-through 

or write-back.
The BIGPicture
 5.8 A Common Framework for Memory Hierarchy 
459In virtual memory systems, only a write-back policy is practical because of the long 
latency of a write to the lower level of the hierarchy
 e rate at which writes are 
generated by a processor generally exceeds 
the rate at which the memory system can 
process them, even allowing for physically and logically wider memories and burst 

modes for DRAM. Consequently, today lowest-level caches typically use write-back.
The Three Cs: An Intuitive Model for Understanding the 
Behavior of Memory Hierarchies
In this subsection, we look at a model that provides insight into the sources of 
misses in a memory hierarchy and how the misses will be a
 ected by changes 
in the hierarchy. We will explain the ideas in terms of caches, although the ideas 

carry over directly to any other level in the hierarchy. In this model, all misses are 

cl
 ed into one of three categories (the 
three Cs
): Compulsory misses
 ese are cache misses caused by th
 rst access to 
a block that has never been in the cache
 ese are also called 
cold-start 
misses. Capacity misses
 
ese are cache misses caused when the cache cannot 
contain all the blocks needed during execution of a program. Capacity misses 

occur when blocks are replaced and then later retrieved.
 Con
 ict misses
 ese are cache misses that occur in set-associative or 
direct-mapped caches when multiple blocks compete for the same set. 

Con
 ict misses are those misses in a direct-mapped or set-associative cache 
that are eliminated in a fully associative cache of the same size
 ese cache 
misses are also called 
collision misses
.Figure 5.37 shows how the miss rate divides into the three sour
 ese sources of 
misses can be directly attacked by changing some aspect of the cache design. Since 

co
 ict misses arise directly from contention for the same cache block, increasing 
associativity reduces co
 ict misses. Associativity, however, may slow access time, 
leading to lower overall performance.
Capacity misses can easily be reduced by enlarging the cache; indeed, second-
level caches have been growing steadily larger for many years. Of course, when we 

make the cache larger, we must also be careful about increasing the access time, 

which could lead to lower overall performance
 us, 
 rst-level caches have been 
growing slowly, if at all.
Because compulsory misses are generated by th
 rst reference to a block, the 
primary way for the cache system to reduce the number of compulsory misses is 

to increase the block size
 is will reduce the number of references required to 
touch each block of the program once, because the program will consist of fewer 
three Cs model
 A cache 
model in which all cache 
misses are cl
 ed into 
one of three categories: 

compulsory misses, 

capacity misses, and 

co
 ict misses.
compulsory miss
 Also 
called cold-start miss
. A cache miss caused by 
th
 rst access to a block 
that has never been in the 

cache.
capacity miss
 A cache 
miss that occurs because 

the cache, even with 

full associativity, cannot 

contain all the blocks 

needed to satisfy the 

request.
co
 ict miss
 Also called 
collision miss
. A cache 
miss that occurs in a 

set-associative or direct-

mapped cache when 

multiple blocks compete 

for the same set and that 

are eliminated in a fully 

associative cache of the 

same size.

460 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Cache size (KiB)
Miss rate
per type0%8321%2%
3%
4%
5%1285126%
7%16642564Capacity8%
9%10%1024One-way
Two-way
Four-way
FIGURE 5.37 The miss rate can be broken into three sources of misses. is graph shows 
the total miss rate and its components for a range of cach
 is data is for the SPEC CPU2000 integer 
an oating-point benchmarks and is from the same source as the data in 
Figure 5.36
 e compulsory 
miss component is 0.006% and cannot be seen in this grap
 e next component is the capacity miss rate, 
which depends on cache size
 e co
 ict portion, which depends both on associativity and on cache size, is 
shown for a range of associativities from one-way to eight-way. In each case, the labeled section corresponds 
to the increase in the miss rate that occurs when the associativity is changed from the next higher degree to 

the labeled degree of associativity. For example, the section labeled 
two-way
 indicates the additional misses 
arising when the cache has associativity of two rather than four
 us, th
 erence in the miss rate incurred 
by a direct-mapped cache versus a fully associative cache of the same size is given by the sum of the sections 

marked 
four-way, two-way,
 and 
one-way
 e 
 erence between eight-way and four-way is so small that it 
  cult to see on this graph.
 e challenge in designing memory hierarchies is that every change 
that potentially improves the miss rate can also negatively a
 ect overall 
performance, as 
Figure 5.38
 summar
 is combination of positive 
and negative
 ects is what makes the design of a memory hierarchy 
interesting.
The BIGPicture
 5.9 Using a Finite-State Machine to Control a Simple Cache 
461cache blocks. As mentioned above, increasing the block size too much can have a 
negative
 ect on performance because of the increase in the miss penalty.
 e decomposition of misses into the three Cs is a useful qualitative model. In 
real cache designs, many of the design choices interact, and changing one cache 

characteristic will o
 en a
 ect several components of the miss rate. Despite such 
shortcomings, this model is a useful way to gain insight into the performance of 

cache designs.
Which of the following statements (if any) are generally true?
 ere is no way to reduce compulsory misses.
2. Fully associative caches have no co
 ict misses.
3. In reducing misses, associativity is more important than capacity.
 5.9  Using a Finite-State Machine to Control a 
Simple CacheWe can now implement control for a cache, just as we implemented control for 
the single-cycle and pipelined datapaths in Chapt
 is section starts with a 
 nition of a simple cache and then a description of 
 nite-state machines
 (FSMs). 
It
 nishes with the FSM of a controller for this simple cache. 
 Section 5.12
 goes 
into more depth, showing the cache and controller in a new hardware description 
language.
A Simple CacheWe’re going to design a controller for a simple cache. Here are the key characteristics 

of the cache:
 Direct-mapped cache
Check Yourself
Design changeEffect on miss rate
Possible negative  performance effect
Increases cache sizeDecreases capacity misses
May increase access time
Increases associativityDecreases miss rate due to conßict 
missesMay increase access time
Increases block sizeDecreases miss rate for a wide range of 
block sizes due to spatial localityIncreases miss penalty. Very large 
block could increase miss rateFIGURE 5.38 Memory hierarchy design challenges.

462 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 Write-back using write allocate
 Block size is 4 words (16 bytes or 128 bits)
 Cache size is 16 KiB, so it holds 1024 blocks
 32-byte addresses
 e cache includes a valid bit and dirty bit per block
From Section 5.3, we can now calculate th
 elds of an address for the cache:
 Cache index is 10 bits
 Block o
 set is 4 bits
 Tag size is 32 
 (10  4) or 18 bits
 e signals between the processor to the cache are
 1-bit Read or Write signal
 1-bit Valid signal, saying whether there is a cache operation or not
 32-bit address
 32-bit data from processor to cache
 32-bit data from cache to processor
 1-bit Ready signal, saying the cache operation is complete
 e interface between the memory and the cache has the sa
 elds as between 
the processor and the cache, except that the dat
 elds are now 128 bits wide
 e extra memory width is generally found in microprocessors today, which deal with 
either 32-bit or 64-bit words in the processor while the DRAM controller is o
 en 128 bits. Making the cache block match the width of the DRAM simp
 ed the 
design. Here are the signals:
 1-bit Read or Write signal
 1-bit Valid signal, saying whether there is a memory operation or not
 32-bit address
 128-bit data from cache to memory
 128-bit data from memory to cache
 1-bit Ready signal, saying the memory operation is complete
Note that the interface to memory is no
 xed number of cycles. We assume a 
memory controller that will notify the cache via the Ready signal when the memory 

read or writ
 nished.
Before describing the cache controller, we need to revie
 nite-state machines, 
which allow us to control an operation that can take multiple clock cycles.

 5.9 Using a Finite-State Machine to Control a Simple Cache 
463Finite-State Machines
To design the control unit for the single-cycle datapath, we used a set of truth tables 
that sp
 ed the setting of the control signals based on the instruction class. For a 
cache, the control is more complex because the operation can be a series of steps. 

 e control for a cache must specify both the signals to be set in any step and the 
next step in the sequence.
 e most common multistep control method is based on 
 nite-state machines
, which are usually represented graphically.
 nite-state machine consists of a set 
of states and directions on how to change stat
 e directions ar
 ned by a 
next-state function
, which maps the current state and the inputs to a new state. 
When we us
 nite-state machine for control, each state also sp
 es a set of 
outputs that are asserted when the machine is in that state.
 e implementation 
o
 nite-state machine usually assumes that all outputs that are not explicitly 
asserted are deasserted. Similarly, the correct operation of the datapath depends on 

the fact that a signal that is not explicitly asserted is deasserted, rather than acting 

as a don’t care.
Multiplexor controls are slightl
 erent, since they select one of the inputs 
whether they are 0 o us, in th
 nite-state machine, we always specify the 
setting of all the multiplexor controls that we care about. When we implement 

th
 nite-state machine with logic, setting a control to 0 may be the default and 
thus may not require any gates. A simple example o
 nite-state machine appears 
in Appendix B, and if you are unfamiliar with the concept o
 nite-state machine, 
you may want to examine Appendix B before proceeding.
 nite-state machine can be implemented with a temporary register that holds 
the current state and a block of combinational logic that determines both the 

data-path signals to be asserted and the next state. 
Figure 5.39
 shows how such an 

implementation might look. 
 Appendix D
 describes in detail how th
 nite-state 
machine is implemented using this structure. In Section B.3, the combinational 

control logic fo
 nite-state machine is implemented both with either a ROM 
(read-only memory
) or a PLA (
programmable logic array
). (Also see Appendix B 
for a description of these logic elements.)
Elaboration: Note that this simple design is called a 
blocking cache, in that the 
processor must w nished the request. 
 Section 5.12 describes the alternative, which is called a 
nonblocking cache.Elaboration:  nite-state machine in this book is called a Moore machine, 
after Edward Moore. Its identifying characteristic is that the output depends only on the 
current state. For a Moore machine, the box labeled combinational control logic can be 

split into two pieces. One piece has the control output and only the state input, while the 

other has only the next-state output.An alternative style of machine is a Mealy machine, named after George Mealy. The 
Mealy machine allows both the input and the current state to be used to determine the 

output. Moore machines have potential implementation advantages in speed and size 

of the control unit. The speed advantages arise because the control outputs, which are 
 nite-state machine
 A sequential logic 
function consisting of a 

set of inputs and outputs, 

a next-state function that 

maps the current state and 

the inputs to a new state, 

and an output function 

that maps the current 

state and possibly the 

inputs to a set of asserted 

outputs.
next-state function
 A combinational function 

that, given the inputs 

and the current state, 

determines the next state 

o
 nite-state machine.

464 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
needed early in the clock cycle, do not depend on the inputs, but only on the current 
state. In Appendix B, nite-state machine is taken down 
to logic gates, the size advantage can be clearly seen. The potential disadvantage of a 

Moore machine is that it may require additional states. For example, in situations where 

there is a one-state difference between two sequences of states, the Mealy machine 

may unify the states by making the outputs depend on the inputs.
FSM for a Simple Cache ControllerFigure 5.40
 shows the four states of our simple cache controller:
 Idle
 is state waits for a valid read or write request from the processor, 
which moves the FSM to the Compare Tag state.
 Compare Tag
: As the name suggests, this state tests to see if the requested read 
or write is a hit o
 e index portion of the address selects the tag to 
be compared. If the data in the cache block referred to by the index portion 
of the address is valid, and the tag portion of the address matches the tag, 

then it is a hit. Either the data is read from the selected word if it is a load or  

written to the selected word if it is a store
 e Cache Ready signal is then 
Combinationalcontrol logic
OutputsInputsState registerNext state
Datapath control outputsInputs from cachedatapathFIGURE 5.39 Finite-state machine controllers are typically implemented using a block of 
combinational logic and a register to hold the current state.
 e outputs of the combinational 
logic are the next-state number and the control signals to be asserted for the current state
 e inputs to the 
combinational logic are the current state and any inputs used to determine the next state. Notice that in the 
 nite-state machine used in this chapter, the outputs depend only on the current state, not on the inpu
 e Elaboration
 explains this in more detail.

 5.9 Using a Finite-State Machine to Control a Simple Cache 
465set. If it is a write, the dirty bit is set to 1. Note that a write hit also sets the 
valid bit and the ta
 eld; while it seems unnecessary, it is included because 
the tag is a single memory, so to change the dirty bit we also need to change 

the valid and ta
 elds. If it is a hit and the block is valid, the FSM returns to 
the idle state.
 rst updates the cache tag and then goes either to the 
Write-Back state, if the block at this location has dirty bit value of 1, or to the 

Allocate state if it is 0.
 Write-Back
 is state writes the 128-bit block to memory using the address 
composed from the tag and cache index. We remain in this state waiting for 

the Ready signal from memory. When the memory write is complete, the 

FSM goes to the Allocate state.
 Allocate
 e new block is fetched from memory. We remain in this state 
waiting for the Ready signal from memory. When the memory read is 

complete, the FSM goes to the Compare Tag state. Although we could 

have gone to a new state to complete the operation instead of reusing the 

Compare Tag state, there is a good deal of overlap, including the update of the 

appropriate word in the block if the access was a write.
CacheMiss
and
Old Block
is DirtyCache
MissandOld Blockis CleanValid CPU requestMark Cache ReadyIdleCache HitCompare TagIf Valid && Hit ,Set Valid, SetTag,if Write Set DirtyMemory ReadyMemory ReadyMemorynotReadyMemorynotReadyWrite OldBlock toMemoryWrite-BackRead new blockfrom MemoryAllocateFIGURE 5.40 Four states of the simple controller.

466 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 is simple model could easily be extended with more states to try to improve 
performance. For example, the Compare Tag state does both the compare and the 
read or write of the cache data in a single clock cycle.
 en the compare and cache 
access are done in separate states to try to improve the clock cycle time. Another 

optimization would be to add a write bu
 er so that we could save the dirty block 
and then read the new bloc
 rst so that the processor doesn’t have to wait for two 
memory accesses on a dir
 e cache would then write the dirty block from 
the write bu
 er while the processor is operating on the requested data.
 Section 5.12
, goes into more detail about the FSM, showing the full controller 
in a hardware description language and a block diagram of this simple cache.
  5.10
  Parallelism and Memory Hierarchy: 
Cache CoherenceGiven that a multicore multiprocessor means multiple processors on a single chip, 
these processors very likely share a common physical address space. Caching shared 

data introduces a new problem, because the view of memory held by tw
 erent 
processors is through their individual caches, which, without any additional 

precautions, could end up seeing tw
 erent values. 
Figure 5.41
 illustrates the 
problem and shows how tw
 erent processors can have tw
 erent values 
for the same locatio
 is 
  culty is generally referred to as the 
cache coherence 
problem
.Informally, we could say that a memory system is coherent if any read of a data 
item returns the most recently written value of that data it
 is 
 nition, 
although intuitively appealing, is vague and simplistic; the reality is much more 

comp
 is simp
 nition contains tw
 erent aspects of memory system 
behavior, both of which are critical to writing correct shared memory programs. 

 e 
 rst aspect, called 
coherence,
 nes what values
 can be returned by a read
 e second aspect, called 
consistency,
 determines 
when a written value will be returned 
by a read.
Let’s look at cohere
 rst. A memory system is coherent if
1. A read by a processor P to a location X that follows a write by P to X, with no 
writes of X by another processor occurring between the write and the read 

by P, always returns the value written by P.
 us, in 
Figure 5.41
, if CPU A were to read X a
 er time step 3, it should see the value 1.
2. A read by a processor to location X that follows a write by another processor 
to X returns the written value if the read and write ar
  ciently separated 
in time and no other writes to X occur between the two access
 us, in 
Figure 5.41
, we need a mechanism so that the value 0 in the cache of CPU B 

is replaced by the value 1 a
 er CPU A stores 1 into memory at address X in 
time step 3.

 5.10 Parallelism and Memory Hierarchy: Cache Coherence 
4673. Writes to the same location are 
serialized
; that is, two writes to the same 
location by any two processors are seen in the same order by all processors. 
For example, if CPU B stores 2 into memory at address X a
 er time step 3, 
processors can never read the value at location X as 2 and then later read 

it as 1.
 e 
 rst property simply preserves program order—we certainly expect this 
property to be true in uniprocessors, for example
 e second proper
 nes the notion of what it means to have a coherent view of memory: if a processor 

could continuously read an old data value, we would clearly say that memory was 

incoherent.
 e need for 
write serialization
 is more subtle, but equally important. Suppose 
we did not serialize writes, and processor P1 writes location X followed by P2 

writing location X. Serializing the writes ensures that every processor will see the 

write done by P2 at some point. If we did not serialize the writes, it might be the 

case that some processor could see the write o
 rst and then see the write of P1, 
maintaining the value written b
 nitely. 
 e simplest way to avoid such 
  culties is to ensure that all writes to the same location are seen in the same 
order, which we call 
write serialization
.Basic Schemes for Enforcing CoherenceIn a cache coherent multiprocessor, the caches provide both 
migration
 and 
replication
 of shared data items:
 Migration
: A data item can be moved to a local cache and used there in a 
transparent fashion. Migration reduces both the latency to access a shared 

data item that is allocated remotely and the bandwidth demand on the shared 

memory.
Time
stepEventCache  contents for CPU ACache  contents 
for CPU BMemory 
contents for location X001CPU A reads X00
2CPU B reads X000
3CPU A stores 1 into X101
FIGURE 5.41 The cache coherence problem for a single memory location (X), read and 
written by two processors (A and B).
 We initially assume that neither cache contains the variable and 
that X has the value 0. We also assume a write-through cache; a write-back cache adds some additional but 

similar complication
 er the value of X has been written by A, A’s cache and the memory both contain the 
new value, but B’s cache does not, and if B reads the value of X, it will receive 0!

468 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 Replication
: When shared data are being simultaneously read, the caches 
make a copy of the data item in the local cache. Replication reduces both 
latency of access and contention for a read shared data item.
Supporting migration and replication is critical to performance in accessing 

shared data, so many multiprocessors introduce a hardware protocol to maintain 

coherent cach
 e protocols to maintain coherence for multiple processors are 
called cache coherence protocols
. Key to implementing a cache coherence protocol 
is tracking the state of any sharing of a data block.
 e most popular cache coherence protocol is 
snooping
. Every cache that has a 
copy of the data from a block of physical memory also has a copy of the sharing 

status of the block, but no centralized state is kep
 e caches are all accessible via 
some broadcast medium (a bus or network), and all cache controllers monitor or 

snoop
 on the medium to determine whether or not they have a copy of a block that 
is requested on a bus or switch access.
In the following section we explain snooping-based cache coherence as 
implemented with a shared bus, but any communication medium that broadcasts 

cache misses to all processors can be used to implement a snooping-based 

coherence scheme
 is broadcasting to all caches makes snooping protocols 
simple to implement but also limits their scalability.
Snooping ProtocolsOne method of enforcing coherence is to ensure that a processor has exclusive 

access to a data item before it writes that it
 is style of protocol is called a 
write 
invalidate protocol
 because it invalidates copies in other caches on a write. Exclusive 
access ensures that no other readable or writable copies of an item exist when the 

write occurs: all other cached copies of the item are invalidated.
Figure 5.42
 shows an example of an invalidation protocol for a snooping bus 
with write-back caches in action. To see how this protocol ensures coherence, 

consider a write followed by a read by another processor: since the write requires 

exclusive access, any copy held by the reading processor must be invalidated (hence 

the protocol na
 us, when the read occurs, it misses in the cache, and the 
cache is forced to fetch a new copy of the data. For a write, we require that the 

writing processor have exclusive access, preventing any other processor from being 

able to write simultaneously. If two processors do attempt to write the same data 

simultaneously, one of them wins the race, causing the other processor’s copy to be 

invalidated. For the other processor to complete its write, it must obtain a new copy 

of the data, which must now contain the updated value
 erefore, this protocol 
also enforces write serialization.

 5.10 Parallelism and Memory Hierarchy: Cache Coherence 
469One insight is that block size plays an important role in cache coherency. For 
example, take the case of snooping on a cache with a block size of eight words, 

with a single word alternatively written and read by two processors. Most protocols 

exchange full blocks between processors, thereby increasing coherency bandwidth 

demands.
Large blocks can also cause what is called 
false sharing
: when two unrelated 
shared variables are located in the same cache block, the full block is exchanged 
between processors even though the processors are accessin
 erent variables. 
Programmers and compilers should lay out data carefully to avoid false sharing.
Elaboration: Although the three proper cient to 
ensure coherence, the question of when a written value will be seen is also important. To 
see why, observe that we cannot require that a read of X in 
Figure 5.41
 instantaneously 
sees the value written for X by some other processor. If, for example, a write of X on one 

processor precedes a read of X on another processor very shortly beforehand, it may be 

impossible to ensure that the read returns the value of the data written, since the written 

data may not even have left the processor at that point. The issue of exactly 
when a written value must be seen b ned by a 
memory consistency model
.Hardware/ 
Software 

Interfacefalse sharing
 When two 
unrelated shared variables 
are located in the same 

cache block and the 

full block is exchanged 

between processors even 

though the processors 

are accessin
 erent 
variables.
FIGURE 5.42 An example of an invalidation protocol working on a snooping bus for a 
single cache block (X) with write-back caches. We assume that neither cache initially holds X 
and that the value of X in memor
 e CPU and memory contents show the value a
 er the processor 
and bus activity have both completed. A blank indicates no activity or no copy cached. When the second 

miss by B occurs, CPU A responds with the value canceling the response from memory. In addition, both 

the contents of B’s cache and the memory contents of X are updated
 is update of memory, which occurs 
when a block becomes shared, simp
 es the protocol, but it is possible to track the ownership and force the 
write-back only if the block is replaced. is requires the introduction of an additional state called “owner,” 

which indicates that a block may be shared, but the owning processor is responsible for updating any other 

processors and memory when it changes the block or replaces it.
Processor activityBus activity
Contents of  CPU A™s cache
Contents of  CPU B™s cacheContents of  
memory  
location X000XrofssimehcaC
XsdaerAUPC
CPU B reads XCache miss for X
00001XrofnoitadilavnIXot1asetirwAUPC
CPU B reads XCache miss for X
111
470 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
We make the following two assumptions. First, a write does not complete (and allow 
the next write to occur) until all processors have seen the effect of that write. Second, 

the processor does not change the order of any write with respect to any other memory 

access. These two conditions mean that if a processor writes location X followed by 

location Y, any processor that sees the new value of Y must also see the new value of 

X. These restrictions allow the processor to reorder reads, but forces the processor to 

 nish a write in program order.
Elaboration: Since input can change memory behind the caches and since output 
could need the latest value in a write back cache, there is also a cache coherency 

problem for I/O with the caches of a single processor as well as just between caches 

of multiple processors. The cache coherence problem for multiprocessors and I/O 

(see Chapter 6), although similar in origin, has different characteristics that affect the 

appropriate solution. Unlike I/O, where multiple data copies are a rare event
—one to 
be avoided whenever possible
—a program running on multiple processors will normally 

have copies of the same data in several caches.
Elaboration: In addition to the snooping cache coherence protocol where the status 
of shared blocks is distributed, a 
directory-based
 cache coherence protocol keeps the 

sharing status of a block of physical memory in just one location, called the 
directory
. Directory-based coherence has slightly higher implementation overhead than snooping, 

 c between caches and thus scale to larger processor counts.
5.11  Parallelism and Memory Hierarchy: 
Redundant Arrays of Inexpensive Disks 
 is online section describes how using many disks in conjunction can o
 er much 
higher throughput, which was the orginal inspiration of 
Redundant Arrays of 
Inexpensive Disks
 e real popularlity of RAID, however, was due more to 
the much greater dependability o
 ered by including a modest number of redundant 
 e section explains th
 erences in performance, cost, and 
dependability 
between th
 erent RAID levels.
5.12  Advanced Material: Implementing Cache 
Controllers 
 is online section shows how to implement control for a cache, just as we 
implemented control for the single-cycle and pipelined datapaths in Chapter 4. 
 is section starts with a description o
 nite-state machines and the implemention 
of a cache controller for a simple data cache, including a description of the cache 

controller in a hardware description language. It then goes into details of an example 

cache coherence protocol and th
  culties in implementing such a protocol.

  Parallelism and the Memory Hierarchy: 
Redundant Arrays of Inexpensive Disks
Amdahl’s law in Chapter 1 reminds us that neglecting I/O in this parallel revolution 
is foolhardy. A simple example demonstrates this.
Impact of I/O on System Performance
Suppose we have a benchmark that executes in 100 seconds of elapsed time, of 

which 90 seconds is CPU time and the rest is I/O time. Suppose the number 

of processors doubles every two years, but the processors remain at the same 

speed, and I/O time doesn’t improve. How much faster will our program run 

at the end of six years?
We know that
Elapsed timeCPU timeI/O time
I/O time
I/O time s
10090
10e
econds
 e new CPU times and the resulting elapsed times are computed in the 
following table.
After n yearsCPU timeI/O timeElapsed time% I/O time
0 years90 seconds10 seconds100 seconds10%
2 years
90245 seconds
10 seconds55 seconds18%
4 years
45223 seconds
10 seconds33 seconds31%
6 years
23211 seconds
10 seconds21 seconds47%
 e improvement in CPU performance a
 er six years is
90118EXAMPLEANSWER5.11
 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks 
5.11-3However, the improvement in elapsed time is only
1002147.and the I/O time has increased from 10% to 47% of the elapsed time.
Hence, the parallel revolution needs to come to I/O as well as to computation, or 
th
 ort spent in parallelizing could be squandered whenever programs do I/O, 
which they all must do.
Accelerating I/O performance was the original motivation of disk arrays. In 
the late 1980s, the high performance storage of choice was large, expensive disks. 

 e argument was that by replacing a fe
w large disks with many small disks, 
performance would improve because there would be more read he
 is 
  is 
a good match for multiple processors as well, since many read/write heads mean 

the storage system could support many more independent accesses as well as large 

transfers spread across many
 at is, you could get both high I/Os per second 
and high data transfer rates. In addition to higher performance, there could be 

advantages in cost, power, an
 oor space, since smaller di
sks are generally more 
  cient per gigabyte than larger disks.
 e 
 aw in the argument was that disk arrays could make reliability much 
worse.
 ese smaller, inexpensive drives had lower MTTF ratings than the large 
drives, but more importantly, by replacing a single drive with, say, 50 small drives, 

the failure rate would go up by at least a factor of 50.
 e solution was to add redundancy so that the system could cope with disk 
failures without losing information. By having many small disks, the cost of extra 

redundancy to improve dependability is small, relative to the solutions for a few 

larg
 us, dependability was more a
 ordable if you constructed a redundant 
array of inexpensive
 is observation led to its name: 
redundant arrays of 
inexpensive disks
, abbreviated 
RAID
. In retrospect, although its invention was motivated by performance, 
dependability was the key reason for the widespread popularity of RAID
 e parallel revolution has resurfaced the original performance side of the argument 

for RAID
 e rest of this section surveys the options for dependability and their 
impacts on cost and performance.
How much redundancy do you need? Do you need extra information t
 nd the 
faults? Does it matter how you organize the data and the extra check information 

on thes
 e paper that coined the term gave an evolutionary answer to 
these questions, starting with the simplest but most expensive solution. Figure 

5.11.1 shows the evolution and example cost in number of extra check disks. To 

keep track of the evolution, the authors numbered the stages of RAID, and they are 

still used today.
redundant arrays of 
inexpensive disks 

(RAID)
 An organization 
of disks that uses an array 
of small and inexpensive 

disks so as to increase 

both performance and 

reliability.

5.11-4 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks
No Redundancy (RAID 0)Simply spreading data over multiple disks, called 
striping
, automatically forces 
accesses to several disks. Striping across a s
et of disks makes the collection appear 
to so
 ware as a single large disk, which simp
 es storage management. It also 
improves performance for large accesses, since many disks can operate at once. 
Video-editing systems, for example, o
 en stripe their data and may not worry 
about dependability as much as, say, databases.
RAID 0 is something of a misnomer, as there is no redundancy. However, RAID 
levels are o
 en 
  to the operator to set when creating a storage system, and RAID 
0 is o
 en listed as one of the options. Hence, the term RAID 0 has become widely 
used.
striping
 Allocation of 
logically sequential blocks 
to separate disks to allow 

higher performance than 

a single disk can deliver.
FIGURE 5.11.1 RAID for an example of four data disks showing extra check disks per RAID level and companies that use each level.
 Figures 5.11.2 and 5.11.3 explain th
 erence 
between RAID 3, RAID 4, and RAID 5.
RAID 0(No redundancy)
Widely usedData disksRAID 1
(Mirroring)
EMC, HP(Tandem), IBMRAID 2
(Error detection and
correction code) Unused RAID 3
(Bit-interleaved parity)
Storage conceptsRAID 4
(Block-interleaving parity)
Network applianceRAID 5
(Distributed block-
interleaved parity)
Widely usedRAID 6(P + Q redundancy)Recently popularRedundant check disks
 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks 
5.11-5Mirroring (RAID 1)
 is traditional scheme for tolerating disk failure, called 
mirroring
 or 
shadowing,
 uses twice as many disks as does RAID 0. Whenever data is written to one disk, 
that data is also written to a redundant disk, so that there are always two copies 

of the information. If a disk fails, the system just goes to the “mirror” and reads 

its contents to get the desired information. Mirroring is the most expensive RAID 

solution, since it requires the most disks.
Error Detecting and Correcting Code (RAID 2)
RAID 2 borrows an error detection and correction scheme most o
 en used for 
memories (see Section 5.5). Since RAID 2 has fallen into disuse, we’ll not describe 

it here.
Bit-Interleaved Parity (RAID 3)
 e cost of higher availability can be reduced to 1/
n, where 
n is the number of 
disks in a protection group
. Rather than have a complete copy of the original data 
for each disk, we need only add enough redundant information to restore the lost 

information on a failure. Reads or writes go to all disks in the group, with one extra 

disk to hold the check information in case there is a failure. RAID 3 is popular in 

applications with large data sets, such as multimedia and some scien
 c codes.
Parity
 is one such scheme. Readers unfamiliar with parity can think of the 
redundant disk as having the sum of all th
e data in the other disks. When a disk fails, 
then you subtract all the data in the good 
disks from the parity disk; the remaining 
information must be the missing information. Parity is simply the sum modulo two.
Unlike RAID 1, many disks must be read to determine the missing dat
 e assumption behind this technique is that taking longer to recover from failure but 

spending less on redundant storage is a good tradeo
 .Block-Interleaved Parity (RAID 4)
RAID 4 uses the same ratio of data disks and check disks as RAID 3, but they 

access data
 erently. 
 e parity is stored as blocks and associated with a set of 
data blocks.
In RAID 3, every access went to all disks. However, some applications prefer 
smaller accesses, allowing independent 
accesses to occur in parallel
 at is the 
purpose of the RAID levels 4 to 7. Since error detection information in each sector 

is checked on reads to see if the data is correct, such “small reads” to each disk can 

occur independently as long as the minimum access is one sector. In the RAID 

context, a small access goes to just one disk in a protection group while a large 

access goes to all the disks in a protection group.
Writes are another matter. It would seem that each small write would demand 
that all other disks be accessed to read the rest of the information needed to 

recalculate the new parity, as in th
  in Figure 5.11.2. A “small write” would 
mirroring
 Writing 
identical data to multiple 
disks to increase data 

availability.
protection group
 e group of data disks 

or blocks that share a 

common check disk or 

block.

5.11-6 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks
FIGURE 5.11.2 Small write update on RAID 4. is optimization for small writes reduces the 
number of disk accesses as well as the number of disks occupied
 is 
 gure assumes we have four blocks 
of data and one block of parity
 e naive RAID 4 parity calculation in th
  of th
 gure reads blocks D1, 
D2, and D3 before adding block D0? to calculate the new parity P?. (In case you were wondering, the new 
data D0? comes directly from the CPU, so disks are not involved in reading i
 e RAID 4 shortcut on the 
right reads the old value D0 and compares it to the new value D0? to see which bits will change. You then 

read the old parity P and then change the corresponding bits to for
 e logical function exclusive OR 
does exactly what we wan
 is example replaces three disk reads (D1, D2, D3) and two disk writes (D0?, P?) 
involving all the disks for two disk reads (D0, P) and 
two disk writes (D0?, P?), which involve just two disks. 
Increasing the size of the parity group increases the savings of the shortcut. RAID 5 uses the same shortcut.
require reading the old data and old parity, adding the new information, and then 
writing the new parity to the parity disk and the new data to the data disk.
 e key insight to reduce this overhead is that parity is simply a sum of 
information; by watching which bits change when we write the new information, 

we need only change the correspo
nding bits on the pari
 e right of Figure 
5.11.2 shows the shortcut. We must read the old data from the disk being written, 

compare old data to the new data to see which bits change, read the old parity, 

change the corresponding bits, and then write the new data and new parity
 us, the small write involves four disk access
es to two disks instead of accessing all 
 is organization is RAID 4.
Distributed Block-Interleaved Parity (RAID 5)
  ciently supports a mixture of large reads, large writes, and small reads, 
plus it allows small writes. One drawback to the system is that the parity disk must be 

updated on every write, so the parity disk is the bottleneck for back-to-back writes.
To
 x the parity-write bottleneck, the parity information can be spread 
throughout all the disks so that there is no single bottleneck for writ
 e distributed parity organization is RAID 5.
Figure 5.11.3 shows how data is distributed in RAID 4 versus RAID 5. As the 
organization on the right shows, in RAID 5 the parity associated with each row of 

data blocks is no longer restricted to a sing
 is organization allows multiple 
writes to occur simultaneously as long as the parity blocks are not located on the 

same disk. For example, a write to block 8 on the right must also access its parity 
D0D0D1D2D3PD0D1D2D3PNew Data1. Read2. Read3. Read
4. Write 
5. Write 
XORD0D0D1D2D3PD0D1D2D3P+New Data1. Read2. Read
3. Write
4. Wr
iteXOR+XOR+
 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks 
5.11-7block P2, thereby occupying th
 rst and third disks. A second write to block 5 on 
the right, implying an update to its parity block P1, accesses the second and fourth 
disks and thus could occur concurrently with the write to bloc
 ose same 
writes to the organization on th
  result in changes to blocks P1 and P2, both on 
th
 h disk, which is a bottleneck.
P  Q Redundancy (RAID 6)Parity-based schemes protect against a single self-identifying failure. When a 

single failure correction is no
  cient, parity can be generalized to have a second 
calculation over the data and another check disk of informatio
 is second check 
block allows recovery from a second failure.
 us, the storage overhead is twice 
that of
 e small write shortcut of Figure 5.11.2 works as well, except now 
there are six disk accesses instead of four to update both P and Q information.
RAID Summary
RAID 1 and RAID 5 are widely used in servers; one estimate is that 80% of disks in 

servers are found in a RAID organization.
One weakness of the RAID systems is repair. First, to avoid making the data 
unavailable during repair, the array must be designed to allow the failed disks to be 

replaced without having to turn o
  the system. RAIDs have enough redundancy 
to allow continuous operation, but 
hot-swapping
 disks place demands on the 
physical and electrical design of the array and the disk interfaces. Second, another 

failure could occur during repair, so the repair time a
 ects the chances of losing 
data: the longer the repair time, the greater the chances of another failure that will 
hot-swapping
 Replacing 
a hardware component 
while the system is 

running.
FIGURE 5.11.3 Block-interleaved parity (RAID 4) versus distributed block-interleaved 
parity (RAID 5). By distributing parity blocks to all disks, 
some small writes can be performed in parallel.
048121620. . .159131721. . .2610141822. . .3711151923. . .P0P1P2P3P4P5. . .04812P420. . .159P31621. . .26P2131722. . .3P110141823. . .P07111519P5. . .RAID 4RAID 5

5.11-8 5.11 Parallelism and the Memory Hierarchy: Redundant Arrays of Inexpensive Disks
lose data. Rather than having to wait for the operator to bring in a good disk, some 
systems include 
standby spares
 so that the data can be reconstructed immediately 
upon discovery of the failure.
 e operator can then replace the failed disks in a 
more leisurely fashion. Note that a human operator ultimately determines which 

disks to remove. Operators are only human, so they occasionally remove the good 

disk instead of the broken disk, leading to an unrecoverable disk failure.
In addition to designing the RAID system for repair, there are questions about 
how disk technology changes over time. Although disk manufacturers quote very 

high MTTF for their products, those numbers are under nominal conditions. 

If a particular disk array has been subject to temperature cycles due to, say, the 

failure of the air conditioning system, or to shaking due to a poor rack design, 

construction, or installation, the failure rates can be three to six times higher (see 

the fallacy on page
 e calculation of RAID reliability assumes independence 
between disk failures, but disk failures could be correlated, because such damage 

due to the environment would likely happen to all the disks in the array. Another 

concern is that since disk bandwidth is growing more slowly than disk capacity, the 

time to repair a disk in a RAID system is increasing, which in turn increases the 

chances of a second failure. For example, a 3 TB disk could take almost nine hours 

to read sequentially, assuming no interference. Given that the damaged RAID is 

likely to continue to serve data, reconstruction could be stretched considerably. 

Besides increasing that time, another concern is that reading much more data 

during reconstruction means increasing the chance of an uncorrectable read 

media failure, which would result in data loss. Other arguments for concern about 

simultaneous multiple failures are the increasing number of disks in arrays and the 

use of higher capacity disks.
Hence, these trends have led to a growing interest in protecting against more 
than one failure, and so RAID 6 is increasingly being o
 ered as an option and being 
used in th
 eld.Which of the following are true about RAID levels 1, 3, 4, 5, and 6?
1. RAID systems rely on redundancy to achieve high availability.
2. RAID 1 (mirroring) has the highest check disk overhead.

3. For small writes, RAID 3 (bit-interleaved parity) has the worst throughput.

4. For large writes, RAID 3, 4, and 5 have the same throughput.
Elaboration: One issue is how mirroring interacts with striping. Suppose you had, 
say, four disks’ worth of data to store and eight physical disks to use. Would you create 
four pairs of disks—each organized as RAID 1—and then stripe data across the four 

RAID 1 pairs? Alternatively, would you create two sets of four disks—each organized as 

RAID 0—and then mirror writes to both RAID 0 sets? The RAID terminology has evolved 

to call the former RAID 1 
 0 or RAID 10 (“striped mirrors”) and the latter RAID 0 
 1 or RAID 01 (“mirrored stripes”).
standby spares
 Reserve 
hardware resources that 
can immediately take 

the place of a failed 

component.
Check Yourself

 5.13 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Memory Hierarchies 
471  5.13
  Real Stuff: The ARM Cortex-A8 and Intel 
Core i7 Memory Hierarchies
In this section, we will look at the memory hierarchy of the same two microprocessors 
described in Chapter 4: the ARM Cortex-A8 and Intel Cor
 is section is based 
on Section 2.6 of 
Computer Architecture: A Quantitative Approach
, 5th edition.
Figure 5.43
 summarizes the address sizes and TLBs of the two processors. Note 
that the A8 has two TLBs with a 32-bit virtual address space and a 32-bit physical 

address space
 e Core i7 has three TLBs with a 48-bit virtual address and a 44-bit 
physical address. Although the 64-bit registers of the Core i7 could hold a larger 

virtual address, there was no so
 ware need for such a large space and 48-bit virtual 
addresses shrinks both the page table memory footprint and the TLB hardware.
Figure 5.44
 shows their caches. Keep in mind that the A8 has just one processor 
or core while the Core i7 has four. Both have identically organized 32 KiB, 4-way 

set associative, L1 instruction caches (per core) with 64 byte bloc
 e A8 uses the 
same design for data cache, while the Core i7 keeps everything the same except the 

associativity, which it increases to 8-way. Both use an 8-way set associativ
 ed L2 cache (per core) with 64 byte blocks, although the A8 varies in size from 128 KiB 

to 1 MiB while the Cor
 xed at 256 KiB. As the Core i7 is used for servers, it 
CharacteristicARM Cortex-A8
Intel Core i7Virtual address 32 bits
48 bitsPhysical address32 bits44 bits
Page sizeVariable: 4, 16, 64 KiB, 1, 16 MiBVariable: 4 KiB, 2/4 MiB

TLB organization1 TLB for instructions and 1 TLB
for dataBoth TLBs are fully associative,with 32 entries, round robin
replacementTLB misses handled in hardware1 TLB for instructions and 1 TLB fordata per coreBoth L1 TLBs are four-way setassociative, LRU replacementL1 I-TLB has 128 entries for smallpages, 7 per thread for large pagesL1 D-TLB has 64 entries for small pages, 32 for large pagesThe L2 TLB is four-way set associative,LRU replacementThe L2 TLB has 512 entries 
TLB misses handled in hardwareFIGURE 5.43 Address translation and TLB hardware for the ARM Cortex-A8 and Intel 
Core i7 920. Both processors provide support for large pages, which are used for things like the operating 
system or mapping a frame bu
 er
 e large-page scheme avoids using a large number of entries to map a 
single object that is always present. 

472 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
also o
 ers an L3 cache shared by all the cores on the chip. Its size varies depending 
on the number of cores. With four cores, as in this case, the size is 8 MiB.
 cant challenge facing cache designers is to support processors like the 
A8 and the Core i7 that can execute more than one memory instruction per clock 
cycle. A popular technique is to break the cache into banks and allow multiple, 

independent, 
parallel
 accesses, provided the accesses are t
 erent ba
 e technique is similar to interleaved DRAM banks (see Section 5.2).
 e Core i7 has additional optimizations that allow them to reduce the miss 
penalty.
 e 
 rst of these is the return of the requested wor rst on a miss. It also 
continues to execute instructions that access the data cache during a cache miss. 

Designers who are attempting to hide the cache miss latency commonly use this 

technique, called a 
nonblocking cache
, when building out-of-order processors. 
 ey implement tw
 avors of nonblocking. 
Hit under miss
 allows additional cache 
hits during a miss, while 
miss under miss
 allows multiple outstanding cache misses. 
 e aim of th
 rst of these two is hiding some miss latency with other work, while 
the aim of the second is overlapping the latency of tw
 erent misses.
Overlapping a large fraction of miss
 times for multiple outstanding misses 
requires a high-bandwidth memory system capable of handling multiple misses in 

parallel. In a personal mobile device, the memory may only be able to take limited 
nonblocking cache
 A cache that allows 
the processor to make 

references to the cache 

while the cache is 

handling an earlier miss.
CharacteristicARM Cortex-A8Intel NehalemL1 cache organizationSplit instruction and data cachesSplit instruction and data caches
L1 cache size32 KiB each for instructions/data 32 KiB each for instructions/data
per coreL1 cache associativityy4-way (I), 4-way (D) set associative4-way (I), 8-way (D) set associative
L1 replacementRandom 
Approximated LRU L1 block size64 bytes
64 bytesL1 write policyWrite-back, Write-allocate(?)
Write-back, No-write-allocateL1 hit time (load-use))1 clock cycle
4 clock cycles, pipelinedL2 cache organizationUnified (instruction and data)
Unified (instruction and data) per coreL2 cache size128 KiB to 1 MiB
256 KiB (0.25 MiB)L2 cache associativity8-way set associative
8-way set associativeL2 replacementRandom(?)
Approximated LRU L2 block size64 bytes
64 bytesL2 write policyWrite-back, Write-allocate (?)
Write-back, Write-allocateL2 hit time11 clock cycles10 clock cyclesL3 cache organization------
--
--
--
--Unified (instruction and data)8 MiB, sharedL3 cache sizeL3 cache associativity16-way set associativeL3 replacementApproximated LRUL3 block size64 bytes
L3 write policyWrite-back, Write-allocateL3 hit time35 clock cyclesFIGURE 5.44 Caches in the ARM Cortex-A8 and Intel Core i7 920.

 5.13 Real Stuff: The ARM Cortex-A8 and Intel Core i7 Memory Hierarchies 
473advantage of this capability, but large servers and multiprocessors o
 en have 
memory systems capable of handling more than one outstanding miss in parallel.
 e Core i7 has a prefetch mechanism for data accesses. It looks at a pattern 
of data misses and use this information to try to predict the next address to start 
fetching the data before the miss occurs. Such techniques generally work best when 

accessing arrays in loops.
 e sophisticated memory hierarchies of these chips and the large fraction of 
the dies dedicated to caches and TLBs show th
 cant
 ort expended 
to try to close the gap between processor cycle times and memory latency.
Performance of the A8 and Core i7 Memory Hierarchies
 e memory hierarchy of the Cortex-A8 was simulated with a 1 MiB eight-way 
set associative L2 cache using the integer Minnespec benchmarks. As mentioned 

in Chapter 4, Minnespec is a set of benchmarks consisting of the SPEC2000 

benchmarks but wit
 erent inputs that reduce the running times by several 
orders of magnitude. Although the use of smaller inputs does not change the 

instruction mix, it does a
 ect the cache behavior. For example, on mcf, the most 
memory-intensive SPEC2000 integer benchmark, Minnespec has a miss rate for a 

32 KiB cache that is only 65% of the miss rate for the full SPEC2000 version. For 

a 1 MiB cache th
 erence is a factor of six! For this reason, one cannot compare 
the Minnespec benchmarks against the SPEC2000 benchmarks, much less the even 

larger SPEC2006 benchmarks used for the Core i7 in 
Figure 5.47
. Instead, the data 

are useful for looking at the relative impact of L1 and L2 misses and on overall CPI, 

which we used in Chapter 4.
 e A8 instruction cache miss rates for these benchmarks (and also for the 
full SPEC2000 versions on which Minnespec is based) are very small even for 

just the L1: close to zero for most and under 1% for all of th
 is low rate 
probably results from the computationally intensive nature of the SPEC programs 

and the four-way set associative cache that eliminates most co
 ict misses. 
Figure
 5.45 shows the data cache results for the A8, which hav
 cant L1 and L2 
miss rat
 e L1 miss penalty for a 1 GHz Cortex-A8 is 11 clock cycles, while 
the L2 miss penalty is assumed to be 60 clock cycles. Using these miss penalties, 

Figure 5.46
 shows the average miss penalty per data access. 
Figure 5.47
 shows the miss rates for the caches of the Core i7 using the SPEC2006 
benchmar
 e L1 instruction cache miss rate varies from 0.1% to 1.8%, 
averaging just ov
 is rate is in keeping with other studies of instruction 
cache behavior for the SPECCPU2006 benchmarks, which show low instruction 

cache miss rates. With L1 data cache miss rates running 5% to 10%, and sometimes 

higher, the importance of the L2 and L3 caches should be obvious. Since the cost 

for a miss to memory is over 100 cycles and the average data miss rate in L2 is 4%, 

L3 is obviously critical. Assuming about ha
lf the instructions are loads or stores, 
without L3 the L2 cache misses could add two cycles per instruction to the CPI! In 

comparison, the average L3 data miss rate of 1% is s
 cant but four times 
lower than the L2 miss rate and six times less than the L1 miss rate.

25.0%20.0%
15.0%Miss Rate10.0%5.0%0.0%twolf
bzip2gzipparsergapperlbmk
gcccrafty
vprvortex
conmcfL1 Data Miss RateL2 Data Miss RateFIGURE 5.45 Data cache miss rates for ARM Cortex-A8 when running Minnespec, a small 
version of SPEC2000. Applications with larger memory footprints tend to have higher miss rates in both 
L1 and L2. Note that the L2 rate is the global miss rate; that is, counting all references, including those that hit 
in L1. (See Elaboration in Section 5.4.) Mcf is known as a cache buster. Note that t
 gure is for the same 
systems and benchmarks as Figure 4.76 in Chapter 4.
FIGURE 5.46 The average memory access penalty in clock cycles per data memory 
reference coming from L1 and L2 is shown for the ARM processor when running Minnespec.
 Although the miss rates for L1 ar
 cantly higher, the L2 miss penalty, which is more than
 ve times 
higher, means that the L2 misses can contribut
 cantly.
00.511.522.533.544.55gzipvpr
gccmcfcraftyparsereonperlbmkgapvortexbzip2
L1 data average memory penaltyL2 data average memory penaltyMiss penalty per data reference
 5.14 Going Faster: Cache Blocking and Matrix Multiply 
47525%20%
15%
10%5%
0%libquantumh264refhummerperlbench
bzip2xalancbmksjenggpbmlastargccomnetppmcfL1 Data Miss RateL2 Data Miss Rate
L3 Data Miss RateFIGURE 5.47 The L1, L2, and L3 data cache miss rates for the Intel Core i7 920 running 
the full integer SPECCPU2006 benchmarks.
 Elaboration: Because speculation may sometimes be wrong (see Chapter 4), there 
are references to the L1 data cache that do not correspond to loads or stores that 
eventually complete execution. The data in 
Figure 5.45
 is measured against all data 
requests including some that are cancelled. The miss rate when measured against only 
completed data accesses is 1.6 times higher (an average of 9.5% versus 5.9% for L1 

Dcache misses)  5.14
   
Going Faster: Cache Blocking and Matrix 
MultiplyOur next step in the continuing saga of improving performance of DGEMM by 
tailoring it to the underlying hardware is to add cache blocking to the subword 

parallelism and instruction level parallelism optimizations of Chapters 3 and 4. 

Figure 5.48
 shows the blocked version of DGEMM from Figur
 e changes 
are the same as was made earlier in going from unoptimized DGEMM in Figure 

3.21 to blocked DGEMM in 
Figure 5.21
 above
 is time we taking the unrolled 
version of DGEMM from Chapter 4 and invoke it many times on the submatrices 

476 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
#include <x86intrin.h>#define UNROLL (4)
#define BLOCKSIZE 32
void do_block (int n, int si, int sj, int sk, 
               double *A, double *B, double *C)
{
  for ( int i = si; i < si+BLOCKSIZE; i+=UNROLL*4 )
    for ( int j = sj; j < sj+BLOCKSIZE; j++ ) {
      __m256d c[4];
      for ( int x = 0; x < UNROLL; x++ ) 
        c[x] = _mm256_load_pd(C+i+x*4+j*n);
     /* c[x] = C[i][j] */
      for( int k = sk; k < sk+BLOCKSIZE; k++ )
      {
        __m256d b = _mm256_broadcast_sd(B+k+j*n);
     /* b = B[k][j] */
        for (int x = 0; x < UNROLL; x++)
          c[x] = _mm256_add_pd(c[x], /* c[x]+=A[i][k]*b */
                 _mm256_mul_pd(_mm256_load_pd(A+n*k+x*4+i), b));
      }      for ( int x = 0; x < UNROLL; x++ )         _mm256_store_pd(C+i+x*4+j*n, c[x]);
        /* C[i][j] = c[x] */
    }
}void dgemm (int n, double* A, double* B, double* C){
  for ( int sj = 0; sj < n; sj += BLOCKSIZE ) 
    for ( int si = 0; si < n; si += BLOCKSIZE )
      for ( int sk = 0; sk < n; sk += BLOCKSIZE )
        do_block(n, si, sj, sk, A, B, C);
}1
2
3
4
5
6
7
8
910
11
12
13
14
15
16
17
18
19
20
21
22
23
242526
27
28
29
30
31
32
33
34FIGURE 5.48 Optimized C version of DGEMM from Figure 4.80 using cache blocking.
  ese changes 
are the same ones found in Figur
 e assembly language produced by the compiler for the 
do_block function 
is nearly identical to 
Figure 4.81
. Once again, there is no overhead to call the 
do_block because the compiler inlines 
the function call.

of 
A, B, and 
C. Indeed, lines 28 – 34 and lines 7 – 8 in 
Figure 5.48
 are identical to 
lines 14 – 20 and lines 5 – 6 in 
Figure 5.21
, with the exception of incrementing the 
for loop in line 7 by the amount unrolled.
Unlike the earlier chapters, we do not show the resulting x86 code because the 
inner loop code is nearly identical to Figure 4.81, as the blocking does not a
 ect the 
computation, just the order that it accesses data in memory. What does change is 

the bookkeeping integer instructions to implement the for loops. It expands from 

14 instructions before the inner loop and 8 a er the loop for Figure 4.80 to 40 and 

28 instructions respectively for the bookkeeping code generated for 
Figure 5.48
. 
Nevertheless, the extra instructions executed pale in comparison to the performance 

improvement of reducing cache misses. 
Figure 5.49
 compares unoptimzed to 

optimizations for subword parallelism, instruction level parallelism, and caches. 

Blocking improves performance over unrolled AVX code by factors of 2 to 2.5 for 

the larger matrices. When we compare unoptimized code to the code with all three 

optimizations, the performance improvement is factors of 8 to 15, with the largest 

increase for the largest matrix.
32x32160x160480x480960x960
16.012.08.04.0UnoptimizedAVXAVX + unrollAVX + unroll +
blocked
Œ1.71.51.30.86.43.52.32.514.66.64.75.113.612.711.712.0GFLOPSFIGURE 5.49 Performance of four versions of DGEMM from matrix dimensions 32x32 to 
960x960.  e fully optimized code for largest matrix is almost 15 times as fast the unoptimized version in 
Figure 3.21 in Chapter 3.
Elaboration: As mentioned in the Elaboration in Section 3.8, these results are 
with Turbo mode turned off. As in Chapters 3 and 4, when we turn it on we improve all 
the results by the temporary increase in the clock rate of 3.3/2.6 
 1.27. Turbo mode 
works particularly well in this case because it is using only a single core of an eight-

core chip. However, if we want to run fast we should use all cores, which we’ll see in 

Chapter 6. 5.14 Going Faster: Cache Blocking and Matrix Multiply 
477
478 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
 5.15 Fallacies and Pitfalls
As one of the most naturally quantitative aspects of computer architecture, the 
memory hierarchy would seem to be less vulnerable to fallacies and pitfalls. Not 

only have there been many fallacies propagated and pitfalls encountered, but some 

have led to major negative outcomes. We start with a pitfall that o
 en traps students 
in exercises and exams.
Pitfall: Ignoring memory system behavior when writing programs or when 

generating code in a compiler.
 is could easily be rewritten as a fallacy: “Programmers can ignore memory 
hierarchies in writing code.
 e evaluation of sort in 
Figure 5.19
 and of cache blocking 
in Section 5.14 demonstrate that programmers can easily double performance if they 

factor the behavior of the memory system into the design of their algorithms.
Pitfall: Forgetting to account for byte addressing or the cache block size in simulating 

a cache.
When simulating a cache (by hand or by computer), we need to make sure we 

account for th
 ect of byte addressing and multiword blocks in determining into 
which cache block a given address maps. For example, if we have a 32-byte direct-

mapped cache with a block size of 4 bytes, the byte address 36 maps into block 1 

of the cache, since byte address 36 is block address 9 and (9 modulo 8) = 1. On the 

other hand, if address 36 is a word address, then it maps into block (36 mod 8) = 4. 

Make sure the problem clearly states the base of the address.
In like fashion, we must account for the block size. Suppose we have a cache with 
256 bytes and a block size of 32 bytes. Into which block does the byte address 300 

fall? If we break the address 300 int
 elds, we can see the answer:
313029. . .. . .. . .11109876543210
000. . .. . .. . .000100101100
Cache block numberBlock offsetBlock addressByte address 300 is block address
300329 e number of blocks in the cache is
256328Block number 9 falls into cache block number (9 modulo 8) 
 1.
 5.15 Fallacies and Pitfalls 
479 is mistake catches many people, including the authors (in earlier dra
 s) and 
instructors who forget whether they intended the addresses to be in words, bytes, 
or block numbers. Remember this pitfall when you tackle the exercises.
Pitfall: Having less set associativity for a shared cache than the number of cores or 

threads sharing that cache.
Without extra care, a 
parallel
 program running on 2
n processors or threads can 
easily allocate data structures to addresses that would map to the same set of a 

shared L2 cache. If the cache is at least 2
n-way associative, then these accidental 
co
 icts are hidden by the hardware from the program. If not, programmers could 
face apparently mysterious performance bugs—actually due to L2 co
 ict misses—
when migrating from, say, a 16-core design to 32-core design if both use 16-way 

associative L2 caches.
Pitfall: Using average memory access time to evaluate the memory hierarchy of an 

out-of-order processor.
If a processor stalls during a cache miss, then you can separately calculate the 

memory-stall time and the processor execution time, and hence evaluate the memory 

hierarchy independently using average 
memory access time (see page 399).
If the processor continues to execute instructions, and may even sustain more 
cache misses during a cache miss, then the only accurate assessment of the memory 

hierarchy is to simulate the out-of-order processor along with the memory hierarchy.
Pitfall: Extending an address space by adding segments on top of an unsegmented 

address space.
During the 1970s, many programs grew so large that not all the code and data could 

be addressed with just a 16-bit address. Computers were then revised to o
 er 32-
bit addresses, either through an unsegmented 32-bit address space (also called a 
 at 
address space
) or by adding 16 bits of segment to the existing 16-bit address. From 
a marketing point of view, adding segments that were programmer-visible and that 

forced the programmer and compiler to decompose programs into segments could 

solve the addressing problem. Unfortunately, there is trouble any time a programming 

language wants an address that is larger than one segment, such as indices for large 

arrays, unrestricted pointers, or reference parameters. Moreover, adding segments 

can turn every address into two words—one for the segment number and one for the 

segment o
 set—causing problems in the use of addresses in registers.
Fallacy: Disk failure rates in th
 eld match their speci
 cations.
Two recent studies evaluated large collections of disks to check the relationship 

between results in th
 eld compared to sp
 cations. One study was of almost 
100,000 disks that had quoted MTTF of 1,000,000 to 1,500,000 hours, or AFR of 

0.6% t
 ey found AFRs of 2% to 4% to be common, o
 en three t
 ve 
times higher than the sp
 ed rates [Schroeder and Gibson, 2007]. A second study 
of more than 100,000 disks at Google, which had a quoted AFR of about 1.5%, saw 

failure rates of 1.7% for drives in th
 rst year rise to 8.6% for drives in their third 
year, or abou
 ve to six times the sp
 ed rate [Pinheiro, Weber, and Barroso, 
2007].
480 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Fallacy:
 Operating systems are the best place to schedule disk accesses.
As mentioned in Section 5.2, higher-level disk interfaces o
 er logical block 
addresses to the host operating system. Given this high-level abstraction, the best 
an OS can do to try to help performance is to sort the logical block addresses into 

increasing order. However, since the disk knows the actual mapping of the logical 

addresses onto the physical geometry of sectors, tracks, and surfaces, it can reduce 

the rotational and seek latencies by rescheduling.
For example, suppose the workload is four reads [Anderson, 2003]:
OperationStarting LBALength
Read 724  8Read 100 16Read9987  1Read  26128 e host might reorder the four reads into logical block order:
OperationStarting LBALength
Read  26128Read 100 16Read 724  8Read9987  1Depending on the relative location of the data on the disk, reordering could 
make it worse, as 
Figure 5.50
 sho
 e disk-scheduled reads complete in three-
quarters of a disk revolution, but the OS-scheduled reads take three revolutions.
Host-ordered queueDrive-ordered queue724100269987FIGURE 5.50 Example showing OS versus disk schedule accesses, labeled host-ordered 
versus drive-ordered.  e former takes three revolutions to complete the four reads, while the latter 
completes them in just three-fourths of a revolution (from Anderson [2003]).

FIGURE 5.51 Summary of 18 x86 instructions that cause problems for virtualization 
[Robin and Irvine, 2000].
 e 
 rst 
 ve instructions in the top group allow a program in user mode to 
read a control register, such as descriptor table registers, without causing a trap
 e po
 ags instruction 
mo
 es a control register with sensitive information but fails silently when in user mode
 e protection 
checking of the segmented architecture of the x86 is the downfall of the bottom group, as each of these 
instructions checks the privilege level implicitly as part of instruction execution when reading a control 

register
 e checking assumes that the OS must be at the highest privilege level, which is not the case for 
guest VMs. Only the Move to segment register tries to modify control state, and protection checking foils it 

as well.
Problem category
Problem x86 instructions
Access sensitive registers without 
trapping when running in user mode 
Store global descriptor table register (SGDT) 
Store local descriptor table register (SLDT) 
Store interrupt descriptor table register (SIDT)

Store machine status word (SMSW)
Push ßags (PUSHF, PUSHFD)

Pop ßags (POPF, POPFD)
When accessing virtual memory 

mechanisms in user mode, instructions 

fail the x86 protection checksLoad access rights from segment descriptor (LAR)
Load segment limit from segment descriptor (LSL)
Verify if segment descriptor is readable (VERR)

Verify if segment descriptor is writable (VERW)

Pop to segment register (POP CS, POP SS, . . .)

Push segment register (PUSH CS, PUSH SS, . . .)

Far call to different privilege level (CALL)

Far return to different privilege level (RET)

Far jump to different privilege level (JMP)

Software interrupt (INT)

Store segment selector register (STR)
Move to/from segment registers (MOVE)
Pitfall: Implementing a virtual machine monitor on an instruction set architecture 
that wasn’t designed to be virtualizable.
Many architects in the 1970s and 1980s weren’t careful to make sure that all 

instructions reading or writing information related to hardware resource 

information were privileged
 is laissez-faire
 attitude causes problems for VMMs 
for all of these architectures, including the x86, which we use here as an example.
Figure 5.51
 describes the 18 instructions that cause problems for virtualization 
[Robin and Irvine
 e two broad classes are instructions that
 Read control registers in user mode that reveals that the guest operating 
system is running in a virtual machine (such as POPF, mentioned earlier)
 Check protection as required by the segmented architecture but assume that 
the operating system is running at the highest privilege level
To simplify implementations of VMMs on the x86, both AMD and Intel have 
proposed extensions to the architecture via a new mode. Intel’s VT-x provides 

a new execution mode for running VMs, an architecte
 nition of the VM 
 5.15 Fallacies and Pitfalls 
481
482 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
state, instructions to swap VMs rapidly, and a large set of parameters to select the 
circumstances where a VMM must be invoked. Altogether, VT-x adds 11 new 

instructions for the x86. AMD’s P
 ca makes similar proposals.
An alternative to modifying the hardware is to make small mo
 cations to the 
operating system to avoid using the troublesome pieces of the architecture
 is technique is called 
paravirtualization
, and the open source Xen VMM is a good 
example
 e Xen VMM provides a guest OS with a virtual machine abstraction 
that uses only the easy-to-virtualize parts of the physical x86 hardware on which 

the VMM runs.
 5.16 Concluding Remarks
 e 
  culty of building a memory system to keep pace with faster processors 
is underscored by the fact that the raw material for main memory, DRAMs, is 

essentially the same in the fastest computers as it is in the slowest and cheapest.
It is the principle of locality that gives us a chance to overcome the long latency of 
memory access—and the soundness of this strategy is demonstrated at all levels of 

the 
memory hierarchy
. Although these levels of the hierarchy look quit
 erent 
in quantitative terms, they follow similar strategies in their operation and exploit 

the same properties of locality.
Multilevel caches make it possible to use more cache optimizations more easily 
for two reasons. First, the design parameters of a lower-level cache ar
 erent 
fro
 rst-level cache. For example, because a lower-level cache will be much 
larger, it is possible to use larger block sizes. Second, a lower-level cache is not 

constantly being used by the processor
 rst-level cach
 is allows us to 
consider having the lower-level cache do something when it is idle that may be 

useful in preventing future misses.
Another trend is to seek so
 ware help.
  ciently managing the memory 
hierarchy using a variety of program transformations and hardware facilities is a 

major focus of compiler enhancements. Tw
 erent ideas are being explored. 
One idea is to reorganize the program to en
hance its spatial and temporal locality. 
 is approach focuses on loop-oriented programs that use large arrays as the 
major data structure; large linear algebra problems are a typical example, such as 

DGEMM. By restructuring the loops that access the arrays, substantially improved 

locality—and, therefore, cache performance—can be obtained.
Another approach is 
prefetching
. In prefetching, a block of data is brought into 
the cache before it is actually referenced. Many microprocessors use hardware 
prefetching to try to 
predict
 accesses that may b
  cult for so
 ware to notice.
A third approach is special cache-aware instructions that optimize memory 
transfer. For example, the microprocessors in Section 6.10 in Chapter 6 use 

an optimization that does not fetch the contents of a block from memory on a 

write miss because the program is going to write the full bloc
 is optimization 
 cantly reduces memory tra
  c for one kernel.
prefetching 
A technique in which 
data blocks needed in the 

future are brought into 

the cache early by the use 

of special instructions that 

specify the address of the 

block.

As we will see in Chapter 6, memory systems are a central design issue for parallel 
processor
 e growing importance of the memory hierarchy in determining 
system performance means that this important area will continue to be a focus for 
both designers and researchers for some years to come.
5.17  Historical Perspective and Further 
Reading is section, which appears online, gives an overview of memory technologies, 
from mercury delay lines to DRAM, the invention of the memory hierarchy, 
protection mechanisms, and virtual machines, and concludes with a brief history 

of operating systems, including CTSS, MULTICS, UNIX, BSD UNIX, MS-DOS, 

Windows, and Linux.
 5.18 Exercises5.1 In this exercise we look at memory locality properties of matrix computation. 
 e following code is written in C, where elements within the same row are stored 
contiguously.  Assume each word is a 32-bit integer.
for (I=0; I<8; I++)  for (J=0; J<8000; J++)
    A[I][J]=B[I][0]+A[J][I];5.1.1 [5] <§5.1> How many 32-bit integers can be stored in a 16-byte cache block?
5.1.2 [5] <
§5.1> References to which variables exhibit temporal locality?
5.1.3 [5] <
§5.1> References to which variables exhibit spatial locality?
Locality is a
 ected by both the reference order and data layou
 e same computation 
can also be written below in Matlab, whic
 ers from C by storing matrix elements 
within the same column contiguously in memory.
for I=1:8
  for J=1:8000
    A(I,J)=B(I,0)+A(J,I);
  end
end 5.18 Exercises 483
484 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
5.1.4 [10] <
§5.1> How many 16-byte cache blocks are needed to store all 32-bit 
matrix elements being referenced?
5.1.5 [5] <
§5.1> References to which variables exhibit temporal locality?
5.1.6 [5] <
§5.1> References to which variables exhibit spatial locality?
5.2 Caches are important to providing a high-performance memory hierarchy 
to processors. Below is a list of 32-bit memory address references, given as word 
addresses.
3, 180, 43, 2, 191, 88, 190, 14, 181, 44, 186, 253
5.2.1 [10] <
§5.3> For each of these references, identify the binary address, the tag, 
and the index given a direct-mapped cache with 16 one-word blocks. Also list if each 

reference is a hit or a miss, assuming the cache is initially empty.
5.2.2 [10] <
§5.3> For each of these references, identify the binary address, the tag, 
and the index given a direct-mapped cache with two-word blocks and a total size of 8 

blocks. Also list if each reference is a hit or
 a miss, assuming the cache is initially empty.
5.2.3 [20] <
§§5.3, 5.4> You are asked to optimize a cache design for the given 
refere
 ere are three direct-mapped cache designs possible, all with a total of 8 
words of data: C1 has 1-word blocks, C2 has 2-word blocks, and C3 has 4-word blocks. 

In terms of miss rate, which cache design is the best? If the miss stall time is 25 cycles, 

and C1 has an access time of 2 cycles, C2 takes 3 cycles, and C3 takes 5 cycles, which is 

the best cache design?
 ere are man
 erent design parameters that are important to a cache’s overall 
performance. Below are listed parameters fo
 erent direct-mapped cache designs.
Cache Data Size:
  32 KiB
Cache Block Size:
  2 words

Cache Access Time:
  1 cycle
5.2.4 [15] <
§5.3> Calculate the total number of bits required for the cache listed 
above, assuming a 32-bit address. Given that total size nd the total size of the closest 
direct-mapped cache with 16-word blocks of equal size or greater. Explain why the 

second cache, despite its larger data size, might provide slower performance than the 

 rst cache.
5.2.5 [20] <
§§5.3, 5.4> Generate a series of read requests that have a lower miss rate 
on a 2 KiB 2-way set associative cache than the cache listed above. Identify one possible 

solution that would make the cache listed have an equal or lower miss rate than the 2 

KiB cache. Discuss the advantages and disadvantages of such a solution.
5.2.6 [15] <
§ e formula shown in Section 5.3 shows the typical method to 
index a direct-mapped cache, sp
 cally (Block address) modulo (Number of blocks in 
the cache). Assuming a 32-bit address and 1024 blocks in the cache, con
 erent 

indexing function, sp
 cally (Block address[31:27] XOR Block address[26:22]). Is it 
possible to use this to index a direct-mapped cache? If so, explain why and discuss any 
changes that might need to be made to the cache. If it is not possible, explain why.
5.3 For a direct-mapped cache design with a 32-bit address, the following bits of the 
address are used to access the cache.
 TagIndexOffset
31–109Ð54–05.3.1 [5] <
§5.3> What is the cache block size (in words)?
5.3.2 [5] <
§5.3> How many entries does the cache have?
5.3.3 [5] <
§5.3> What is the ratio between total bits required for such a cache 
implementation over the data storage bits?
Starting from power on, the following byte-addressed cache references are recorded.
Address041613223216010243014031001802180
5.3.4 [10] <
§5.3> How many blocks are replaced?
5.3.5 [10] <
§5.3> What is the hit ratio?
5.3.6 [20] <
§5.3> List th
 nal state of the cache, with each valid entry represented as 
a record of <index, tag, data>.
5.4 Recall that we have two write policies and write allocation policies, and their 
combinations can be implemented either in L1 or L2 cache. Assume the following 
choices for L1 and L2 caches:
L1L2 Write through, non-write allocate
Write back, write allocate
5.4.1 [5] <
§§5.3, 5.8> B
 ers are employed betw
 erent levels of memory 
hierarchy to reduce access latency. For this given co
 guration, list the possible bu
 ers 
needed between L1 and L2 caches, as well as L2 cache and memory.
5.4.2 [20] <
§§5.3, 5.8> Describe the procedure of handling an L1 write-miss, 
considering the component involved and the possibility of replacing a dirty block.
5.4.3 [20] <
§§5.3, 5.8> For a multilevel exclusive cache (a block can only reside in 
one of the L1 and L2 caches), co guration, describe the procedure of handling an L1 

write-miss, considering the component involved and the possibility of replacing a dirty 

block.
 5.18 Exercises 485
486 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Consider the following program and cache behaviors.
Data Reads per 1000 Instructions
Data Writes per 1000 Instructions
Instruction Cache 
Miss RateData Cache Miss RateBlock Size (byte)
2501000.30%2%645.4.4 [5] <
§§5.3, 5.8> For a write-through, write-allocate cache, what are the minimum 
read and write bandwidths (measured by byte per cycle) needed to achieve a CPI of 2?
5.4.5 [5] <
§§5.3, 5.8> For a write-back, write-allocate cache, assuming 30% of 
replaced data cache blocks are dirty, what are the minimal read and write bandwidths 
needed for a CPI of 2?
5.4.6 [5] <
§§5.3, 5.8> What are the minimal bandwidths needed to achieve the 
performance of CPI=1.5?
5.5 Media applications that play audio or
 les are part of a class of workloads 
called “streaming” workloads; i.e., they bring in large amounts of data but do not reuse 

much of it. Consider a video streaming wo
rkload that accesses a 512 KiB working set 
sequentially with the following address stream:
0, 2, 4, 6, 8, 10, 12, 14, 16, …
5.5.1 [5] <
§§5.4, 5.8> Assume a 64 KiB direct-mapped cache with a 32-byte block. 
What is the miss rate for the address stream above? How is this miss rate sensitive to 

the size of the cache or the working set? How would you categorize the misses this 

workload is experiencing, based on the 3C model?
5.5.2 [5] <
§§5.1, 5.8> Re-compute the miss rate when the cache block size is 16 bytes, 
64 bytes, and 128 bytes. What kind of locality is this workload exploiting?
5.5.3 [10] <
§5.13>“Prefetching” is a technique that leverages predictable address 
patterns to speculatively bring in additional cache blocks when a particular cache block 

is accessed. One example of prefetching is a stream b
 er that prefetches sequentially 
adjacent cache blocks into a separate bu
 er when a particular cache block is brought 
in. If the data is found in the prefetch b
 er, it is considered as a hit and moved into 
the cache and the next cache block is prefetched. Assume a two-entry stream b
 er and assume that the cache latency is such that a cache block can be loaded before the 

computation on the previous cache block is completed. What is the miss rate for the 

address stream above?
Cache block size (B) can a
 ect both miss rate and miss latency. Assuming a 1-CPI 
machine with an average of 1.35 references (both instruction and data) per instruction, 
hel
 nd the optimal block size given the following miss rates for various block sizes.
8: 4%16: 3%32: 2%64: 1.5%128: 1%
5.5.4 [10] <
§5.3> What is the optimal block size for a miss latency of 20×B cycles?
5.5.5 [10] <
§5.3> What is the optimal block size for a miss latency of 24+B cycles?
5.5.6 [10] <
§5.3> For constant miss latency, what is the optimal block size?

5.6 In this exercise, we will look at th
 erent ways capacity a
 ects overall 
performance. In general, cache access time is proportional to capacity. Assume that 
main memory accesses take 70 ns and that memory accesses are 36% of all instructions. 

 e following table shows data for L1 caches attached to each of two processors, P1 and 
P2. L1 SizeL1 Miss RateL1 Hit Time
P12 KiB8.0%0.66 nsP24 KiB6.0%0.90 ns5.6.1 [5] <
§5.4> Assuming that the L1 hit time determines the cycle times for P1 and 
P2, what are their respective clock rates?
5.6.2 [5] <
§5.4> What is the Average Memory Access Time for P1 and P2?
5.6.3 [5] <
§5.4> Assuming a base CPI of 1.0 without any memory stalls, what is the 
total CPI for P1 and P2? Which processor is faster?
For the next three problems, we will consider the addition of an L2 cache to P1 to 
presumably make up for its limited L1 cache capacity. Use the L1 cache capacities 

and hit times from the previous table when solving these problem
 e L2 miss rate 
indicated is its local miss rate.
L2 SizeL2 Miss RateL2 Hit Time
1 MiB95%5.62 ns5.6.4 [10] <
§5.4> What is the AMAT for P1 with the addition of an L2 cache? Is the 
AMAT better or worse with the L2 cache?
5.6.5 [5] <
§5.4> Assuming a base CPI of 1.0 without any memory stalls, what is the 
total CPI for P1 with the addition of an L2 cache?
5.6.6 [10] <
§5.4> Which processor is faster, now that P1 has an L2 cache? If P1 is 
faster, what miss rate would P2 need in its L1 cache to match P1’s performance? If P2 is 

faster, what miss rate would P1 need in its L1 cache to match P2’s performance?
5.7  is exercise examines the impact o
 erent cache designs, sp
 cally 
comparing associative caches to the direct-mapped caches from Section 5.4. For these 

exercises, refer to the address stream shown in Exercise 5.2.
5.7.1 [10] <
§5.4> Using the sequence of references from Exercise 5.2, show th
 nal 
cache contents for a three-way set associative cache with two-word blocks and a total 

size of 24 words. Use LRU replacement. For each reference identify the index bits, the 

tag bits, the block o
 set bits, and if it is a hit or a miss.
5.7.2 [10] <
§5.4> Using the references from Exercise 5.2, show th
 nal cache 
contents for a fully associative cache with one-word blocks and a total size of 8 words. 

Use LRU replacement. For each reference identify the index bits, the tag bits, and if it 

is a hit or a miss.
 5.18 Exercises 487
488 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
5.7.3 [15] <
§5.4> Using the references from Exercise 5.2, what is the miss rate for 
a fully associative cache with two-word blocks and a total size of 8 words, using LRU 
replacement? What is the miss rate using MRU (most recently used) replacement? 

Finally what is the best possible miss rate for this cache, given any replacement policy?
Multilevel caching is an important technique to overcome the limited amount of 
space tha
 rst level cache can provide while still maintaining its speed. Consider a 
processor with the following parameters:
Base CPI, No Memory 
StallsProcessor SpeedMain Memory Access 
Time
First Level Cache 
MissRate per Instruction 
Second Level Cache, 
Direct-Mapped SpeedGlobal Miss Rate with Second Level Cache, 
Direct-MappedSecond Level Cache, 
Eight-Way Set 
Associative SpeedGlobal Miss Rate with Second Level 
Cache, Eight-Way Set 
Associative1.52 GHz100 ns7%12 cycles3.5%28 cycles1.5%5.7.4 [10] <
§5.4> Calculate the CPI for the processor in the table using: 1) only a 
 rst level cache, 2) a second level direct-mapped cache, and 3) a second level eight-way 
set associative cache. How do these numbers change if main memory access time is 

doubled? If it is cut in half?
5.7.5 [10] <
§5.4> It is possible to have an even greater cache hierarchy than two 
levels. Given the processor above with a second level, direct-mapped cache, a designer 

wants to add a third level cache that takes 50 cycles to access and will reduce the global 

miss rate to 1.3%. Would this provide better performance? In general, what are the 

advantages and disadvantages of adding a third level cache?
5.7.6 [20] <
§5.4> In older processors such as the Intel Pentium or Alpha 21264, the 
second level of cache was external (located o
 erent chip) from the main processor 
and th
 rst level cache. While this allowed for large second level caches, the latency to 
access the cache was much higher, and the bandwidth was typically lower because the 

second level cache ran at a lower frequency. Assume a 512 KiB o
 -chip second level 
cache has a global miss rate of 4%. If each additional 512 KiB of cache lowered global 

miss rates by 0.7%, and the cache had a total access time of 50 cycles, how big would 

the cache have to be to match the performance of the second level direct-mapped cache 

listed above? Of the eight-way set associative cache?
5.8 Mean Time Between Failures (MTBF), Mean Time To Replacement (MTTR), and 
Mean Time To Failure (MTTF) are useful metrics for evaluating the reliability and 

availability of a storage resource. Explore these concepts by answering the questions 

about devices with the following metrics.
MTTFMTTR3 Years
1 Day

5.8.1 [5] <
§5.5> Calculate the MTBF for each of the devices in the table.
5.8.2 [5] <
§5.5> Calculate the availability for each of the devices in the table.
5.8.3 [5] <
§5.5> What happens to availability as the MTTR approaches 0? Is this a 
realistic situation?
5.8.4 [5] <
§5.5> What happens to availability as the MTTR gets very high, i.e., a 
de
  cult to repair? Does this imply the device has low availability?
5.9  is Exercise examines the single error correcting, double error detecting (SEC/
DED) Hamming code.
5.9.1 [5] <
§5.5> What is the minimum number of parity bits required to protect a 
128-bit word using the SEC/DED code?
5.9.2 [5] <
§5.5> Section 5.5 states that modern server memory modules (DIMMs) 
employ SEC/DED ECC to protect each 64 bits with 8 parity bits.  Compute the cost/
performance ratio of this code to the code from 5.9.1. In this case, cost is the relative 

number of parity bits needed while performance is the relative number of errors that 

can be corrected.  Which is better?
5.9.3 Consider a SEC code that protects 8 bit words with 4 parity bits.  If we read the 
value 0x375, is there an error?  If so, correct the error.
5.10 For a high-performance system such as a B-tree index for a database, the page 
size is determined mainly by the data size and disk performance. Assume that on 

average a B-tree index page is 70% full wit
 x-sized entr
 e utility of a page is 
its B-tree depth, calculated as log
2(entr
 e following table shows that for 16-byte 
entries, and a 10-year-old disk with a 10 ms latency and 10 MB/s transfer rate, the 

optimal page size is 16K.
Page Size (KiB)
Page Utility or B-Tree 
Depth (Number of Disk Accesses Saved)
Index Page 
Access Cost (ms)Utility/Cost
26.49 (or log2(2048/16×0.7))10.20.6447.4910.40.72
88.4910.80.79
169.4911.60.82

3210.4913.20.79

6411.4916.40.70
12812.4922.80.55

25613.4935.60.38
5.10.1 [10] <
§5.7> What is the best page size if entries now become 128 bytes?
5.10.2 [10] <
§5.7> Based on 5.10.1, what is the best page size if pages are half full?
5.10.3 [20] <
§5.7> Based on 5.10.2, what is the best page size if using a modern disk 
with a 3 ms latency and 100 MB/s transfer rate? Explain why future servers are likely 
to have larger pages.
 5.18 Exercises 489
490 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Keeping “frequently used” (or “hot”) pages in DRAM can save disk accesses, but how 
do we determine the exact meaning of “frequently used” for a given system? Data 

engineers use the cost ratio between DRAM and disk access to quantify the reuse time 

threshold for hot pag
 e cost of a disk access is 
$Disk/accesses_per_sec, while the 
cost to keep a page in DRAM is $DRAM_MiB/page_size
 e typical DRAM and disk 
costs and typical database page sizes at several time points are listed below:
Year
DRAM Cost ($/MiB)Page Size 
(KiB)Disk Cost ($/disk)Disk Access Rate (access/sec)19875000115,000151997158200064
20070.05648083
5.10.4 [10] <§§5.1, 5.7> What are the reuse time thresholds for these three 
technology generations?
5.10.5 [10] <
§5.7> What are the reuse time thresholds if we keep using the same 4K 
page size? What’s the trend here?
5.10.6 [20] <
§5.7> What other factors can be changed to keep using the same page 
size (thus avoiding so
 ware rewrite)? Discuss their likeliness with current technology 
and cost trends.
5.11 As described in Section 5.7, virtual memory uses a page table to track the 
mapping of virtual addresses to physical address
 is exercise shows how this table 
must be updated as addresses are accessed
 e following data constitutes a stream of 
virtual addresses as seen on a system. Assume
 4 KiB pages, a 4-entry fully associative 
TLB, and true LRU replacement. If pages must 
be brought in from disk, increment the 
next largest page number.
4669, 2227, 13916, 34587, 48870, 12608, 49225
TLBValid
Tag
Physical Page 
Number1111
2174

136

049

Page table
ValidPhysical Page or in Disk
150Disk
0Disk

16
19
111

0Disk

14
0Disk

0Disk

13
112
5.11.1 [10] <
§5.7> Given the address stream shown, and the initial TLB and page 
table states provided above, show th nal state of the system. Also list for each reference 
if it is a hit in the TLB, a hit in the page table, or a page fault.
5.11.2 [15] <
§5.7> Repeat 5.11.1, but this time use 16 KiB pages instead of 4 KiB 
pages. What would be some of the advantages of having a larger page size? What are 

some of the disadvantages?
5.11.3 [15] <
§§5.4, 5.7> Show th nal contents of the TLB if it is 2-way set 
associative. Also show the contents of the TLB if it is direct mapped. Discuss the 

importance of having a TLB to high performance. How would virtual memory 

accesses be handled if there were no TLB?
 ere are several parameters that impact the overall size of the page table. Listed below 
are key page table parameters.
Virtual Address Size
Page Size
Page Table Entry Size
32 bits8 KiB4 bytes
5.11.4 [5] <
§5.7> Given the parameters shown above, calculate the total page table 
size for a system running 5 applications that utilize half of the memory available.
5.11.5 [10] <
§5.7> Given the parameters shown above, calculate the total page table 
size for a system running 5 applications that utilize half of the memory available, given 
a two level page table approach with 256 entries. Assume each entry of the main page 

table is 6 bytes. Calculate the minimum and maximum amount of memory required.
5.11.6 [10] <
§5.7> A cache designer wants to increase the size of a 4 KiB virtually 
indexed, physically tagged cache. Given the page size shown above, is it possible to 

make a 16 KiB direct-mapped cache, assuming 2 words per block? How would the 

designer increase the data size of the cache?
 5.18 Exercises 491
492 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
5.12 In this exercise, we will examine space/time optimizations for page tab
 e following list provides parameters of a virtual memory system.
Virtual Address (bits)
Physical DRAM 
InstalledPage SizePTE Size (byte)
4316 GiB4 KiB45.12.1 [10] <
§5.7> For a single-level page table, how many page table entries (PTEs) 
are needed? How much physical memory is needed for storing the page table?
5.12.2 [10] <
§5.7> Using a multilevel page table can reduce the physical memory 
consumption of page tables, by only keeping active PTEs in physical memory. How 
many levels of page tables will be needed in this case? And how many memory 

references are needed for addres
s translation if missing in TLB?
5.12.3 [15] <
§5.7> An inverted page table can be used to further optimize space 
and time. How many PTEs are needed to store the page table? Assuming a hash table 

implementation, what are the common case and worst case numbers of memory 

references needed for servicing a TLB miss?
 e following table shows the contents of a 4-entry TLB.
Entry-IDValidVA PageModiﬁ
 edProtectionPA Page
111401RW
3020400RX34312001RO
32412800RW
315.12.4 [5] <
§5.7> Under what scenarios would entry 2’s valid bit be set to zero?
5.12.5 [5] <
§5.7> What happens when an instruction writes to VA page 30? When 
would a so
 ware managed TLB be faster than a hardware managed TLB?
5.12.6 [5] <
§5.7> What happens when an instruction writes to VA page 200?
5.13 In this exercise, we will examine how replacement policies impact miss rate. 
Assume a 2-way set associative cache with 4 blocks. To solve the problems in this 
exercise, you ma
 nd it helpful to draw a table like the one below, as demonstrated for 
the address sequence “0, 1, 2, 3, 4.”
Address of Memory
Block AccessedHit or Miss
Evicted BlockContents of Cache Blocks After ReferenceSet 0Set 0Set 1Set 1
0MissMem[0]1MissMem[0]Mem[1]2MissMem[0]Mem[2]Mem[1]
3MissMem[0]Mem[2]Mem[1]Mem[3]
4Miss0Mem[4]Mem[2]Mem[1]Mem[3]
…
Consider the following address sequence:  
0, 2, 4, 8, 10, 12, 14, 16, 0
5.13.1 [5] <
§§5.4, 5.8> Assuming an LRU replacement policy, how many hits does 
this address sequence exhibit?
5.13.2 [5] <
§§5.4, 5.8> Assuming an MRU (most recently used) replacement policy, 
how many hits does this address sequence exhibit?
5.13.3 [5] <
§§5.4, 5.8> Simulate a random replacement policy b
 ipping a coin. For 
example, “heads” means to evict th
 rst block in a set and “tails” means to evict the 
second block in a set. How many hits does this address sequence exhibit?
5.13.4 [10] <
§§5.4, 5.8> Which address should be evicted at each replacement to 
maximize the number of hits? How many hits does this address sequence exhibit if you 
follow this “optimal” policy?
5.13.5 [10] <
§§5.4, 5.8> Describe why i  cult to implement a cache replacement 
policy that is optimal for all address sequences.
5.13.6 [10] <
§§5.4, 5.8> Assume you could make a decision upon each memory 
reference whether or not you want the requested address to be cached. What impact 

could this have on miss rate?
5.14 To support multiple virtual machines, two levels of memory virtualization are 
needed. Each virtual machine still controls the mapping of virtual address (VA) to 

physical address (PA), while the hypervisor maps the physical address (PA) of each 

virtual machine to the actual machine address (MA). To accelerate such mappings, 

a so
 ware approach called “shadow paging
” duplicates each virtual machine’s page 
tables in the hypervisor, and intercepts VA to PA mapping changes to keep both copies 

consistent. To remove the complexity of shadow page tables, a hardware approach 

called nested page table (NPT) explicitly supports two classes of page tables (VA 
 PA 
and PA 
 MA) and can walk such tables purely in hardware.
Consider the following sequence of operations: (1) Create process; (2) TLB miss; 
(3) page fault; (4) context switch;
5.14.1 [10] <
§§5.6, 5.7> What would happen for the given operation sequence for 
shadow page table and nested page table, respectively?
5.14.2 [10] <
§§5.6, 5.7> Assuming an x86-based 4-level page table in both guest and 
nested page table, how many memory references are needed to service a TLB miss for 

native vs. nested page table?
5.14.3 [15] <
§§5.6, 5.7> Among TLB miss rate, TLB miss latency, page fault rate, and 
page fault handler latency, which metrics are more important for shadow page table? 

Which are important for nested page table?
 5.18 Exercises 493
494 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
Assume the following parameters for a shadow paging system.
TLB Misses per 1000 Instructions
NPT TLB Miss LatencyPage Faults per 
1000 Instructions
Shadowing Page 
Fault Overhead
0.2200 cycles0.00130,000 cycles5.14.4 [10] <
§5.6> For a benchmark with native execution CPI of 1, what are the CPI 
numbers if using shadow page tables vs. NPT (a
ssuming only page table virtualization 
overhead)?
5.14.5 [10] <
§5.6> What techniques can be used to reduce page table shadowing 
induced overhead?
5.14.6 [10] <
§5.6> What techniques can be used to reduce NPT induced overhead?
5.15 One of the biggest impediments to widespread use of virtual machines is the 
performance overhead incurred by running a virtual machine. Listed below are various 
performance parameters and application behavior.
Base CPIPriviliged O/S Accesses per 10,000 Instructions
Performance 
Impact to Trap to the 
Guest O/SPerformance 
Impact to Trap 
to VMMI/O Access per 10,000 Instructions
I/O Access Time 
(Includes Time 
to Trap to Guest 
O/S)1.512015 cycles175 cycles30
1100 cycles5.15.1 [10] <
§5.6> Calculate the CPI for the system listed above assuming that there 
are no accesses to I/O. What is the CPI if the VMM performance impact doubles? If it is 

cut in half? If a virtual machine so
 ware company wishes to obtain a 10% performance 
degradation, what is the longest possible penalty to trap to the VMM?
5.15.2 [10] <
§5.6> I/O accesses o
 en have a large impact on overall system 
performance. Calculate the CPI of a machine using the performance characteristics 

above, assuming a non-virtualized system. Calculate the CPI again, this time using a 

virtualized system. How do these CPIs change if the system has half the I/O accesses? 

Explain why I/O bound applications have a smaller impact from virtualization.
5.15.3 [30] <
§§5.6, 5.7> Compare and contrast the ideas of virtual memory and 
virtual machines. How do the goals of each compare? What are the pros and cons of 

each? List a few cases where virtual memory is desired, and a few cases where virtual 

machines are desired.
5.15.4 [20] <
§5.6> Section 5.6 discusses virtualization under the assumption that 
the virtualized system is running the same ISA as the underlying hardware. However, 

one possible use of virtualization is to emulate non-native ISAs. An example of this is 

QEMU, which emulates a variety of ISAs such as MIPS, SPARC, and PowerPC. What 

are some of th
  culties involved in this kind of virtualization? Is it possible for an 
emulated system to run faster than on its native ISA?

5.16 In this exercise, we will explore the control unit for a cache controller for a 
processor with a write bu
 er. Use th
 nite state machine found in Figure 5.40 as a 
starting point for designing your ow
 nite state machines. Assume that the cache 
controller is for the simple direct-mapped cache described on page 465 (
Figure 5.40 
in  
Section 5.9), but you will add a write bu
 er with a capacity of one block.
Recall that the purpose of a write bu
 er is to serve as temporary storage so that the 
processor doesn’t have to wait for two memory accesses on a dirty miss. Rather than 

writing back the dirty block before reading the new block, it b ers the dirty block and 

immediately begins reading the new bloc
 e dirty block can then be written to main 
memory while the processor is working.
5.16.1 [10] <
§§5.8, 5.9> What should happen if the processor issues a request that 
hits
 in the cache while a block is being written back to main memory from the write 
bu
 er?5.16.2 [10] <
§§5.8, 5.9> What should happen if the processor issues a request that 
misses
 in the cache while a block is being written back to main memory from the write 
bu
 er?5.16.3 [30] <
§§5.8, 5.9> D
 nite state machine to enable the use of a write 
bu
 er.
5.17 Cache coherence concerns the views of multiple processors on a given cache 
bloc
 e following data shows two processors and their read/write operations on two 
 erent words of a cache block X (initially X[0] = X[1] = 0).  Assume the size of integers is 
32 bits.
P1P2X[0] ++; X[1] = 3;X[0] = 5; X[1] +=2;5.17.1 [15] <
§5.10> List the possible values of the given cache block for a correct 
cache coherence protocol implementation. List at least one more possible value of the 

block if the protocol doesn’t ensure cache coherency.
5.17.2 [15] <
§5.10> For a snooping protocol, list a valid operation sequence on each 
processor/cache t
 nish the above read/write operations.
5.17.3 [10] <
§5.10> What are the best-case and worst-case numbers of cache misses 
needed to execute the listed read/write instructions?
Memory consistency concerns the views of multiple data item
 e following data 
shows two processors and their read/write operations o
 erent cache blocks (A and 
B initially 0).
P1P2A = 1; B = 2; A+=2; B++;C = B; D = A; 5.18 Exercises 495
496 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
5.17.4 [15] <
§5.10> List the possible values of C and D for an implementation that 
ensures both consistency assumptions on page 470.
5.17.5 [15] <
§5.10> List at least one more possible pair of values for C and D if such 
assumptions are not maintained.
5.17.6 [15] <
§§5.3, 5.10> For various combinations of write policies and write 
allocation policies, which combinations make the protocol implementation simpler?
5.18 Chip multiprocessors (CMPs) have multiple cores and their caches on a single 
chip. CMP on-chip L2 cache design has interesting trade-o
 s. 
 e following table 
shows the miss rates and hit latencies for two benchmarks with private vs. shared L2 
cache designs. Assume L1 cache misses once every 32 instructions.
 PrivateShared
Benchmark A misses-per-instruction0.30%0.12%
Benchmark B misses-per-instruction0.06%0.03%
Assume the following hit latencies:
Private CacheShared Cache
Memory
5201805.18.1 [15] <
§5.13> Which cache design is better for each of these benchmarks? Use 
data to support your conclusion.
5.18.2 [15] <
§5.13> Shared cache latency increases with the CMP size. Choose 
the best design if the shared cache latency doub
 -chip bandwidth becomes the 
bottleneck as the number of CMP cores increases. Choose the best design if o
 -chip 
memory latency doubles.
5.18.3 [10] <
§5.13> Discuss the pros and cons of shared vs. private L2 caches for both 
single-threaded, multi-threaded, and multiprogrammed workloads, and reconsider 

them if having on-chip L3 caches.
5.18.4 [15] <
§5.13> Assume both benchmarks have a base CPI of 1 (ideal L2 cache). 
If having non-blocking cache improves the average number of concurrent L2 misses 

from 1 to 2, how much performance improvement does this provide over a shared L2 

cache? How much improvement can be achieved over private L2?
5.18.5 [10] <
§5.13> Assume new generations of processors double the number of 
cores every 18 months. To maintain the same level of per-core performance, how much 

more o
 -chip memory bandwidth is needed for a processor released in three years?
5.18.6 [15] <
§5.13> Consider the entire memory hierarchy. What kinds of 
optimizations can improve the number of concurrent misses?

5.19 In this exercise we show th
 nition of a web server log and examine code 
optimizations to improve log processing speed
 e data structure for th
 ned 
as follows:
struct entry {int srcIP;   // remote IP addresschar URL[128]; // request URL (e.g., “GET index.html”)
long long refTime; // reference time
int status;  // connection status
char browser[64]; // client browser name} log [NUM_ENTRIES];Assume the following processing function for the log:
topK_sourceIP (int hour);5.19.1 [5] <
§5.15> Whic
 elds in a log entry will be accessed for the given log 
processing function? Assuming 64-byte cache blocks and no prefetching, how many 
cache misses per entry does the given function incur on average?
5.19.2 [10] <
§5.15> How can you reorganize the data structure to improve cache 
utilization and access locality? Show your structur
 nition code.
5.19.3 [10] <
§5.15> Give an example of another log processing function that would 
pref
 erent data structure layout. If both functions are important, how would you 
rewrite the program to improve the overall performance? Supplement the discussion 

with code snippet and data.
For the problems below, use data from “Cache Performance for SPEC CPU2000 
Benchmarks” 
(http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/)
 for the 

pairs of benchmarks shown in the following table.
a.Mesa / gccb.mcf / swim5.19.4 [10] <
§5.15> For 64 KiB data caches with varying set associativities, what are 
the miss rates broken down by miss types (cold, capacity, and co
 ict misses) for each 
benchmark?
5.19.5 [10] <
§5.15> Select the set associativity to be used by a 64 KiB L1 data cache 
shared by both benchmarks. If the L1 cache has to be directly mapped, select the set 

associativity for the 1 MiB L2 cache.
5.19.6 [20] <
§5.15> Give an example in the miss rate table where higher set 
associativity actually increases miss rate. Construct a cache co
 guration and reference 
stream to demonstrate this.
 5.18 Exercises 497
498 Chapter 5 Large and Fast: Exploiting Memory Hierarchy
§5.1, page 377: 1 and 4. (3 is false because the cost of the memory hierarchy varies 
per computer, but in 2013 the highest cost is usually the DRAM.)

§5.3, page 398: 1 and 4: A lower miss penalty can enable smaller blocks, since you 

don’t have that much latency to amortize, yet higher memory bandwidth usually 

leads to larger blocks, since the miss penalty is only slightly larger.

§5.4, page 417: 1.

§5.7, page 454: 1-a, 2-c, 3-b, 4-d.

§5.8, page 461: 2. (Both large block sizes and prefetching may reduce compulsory 

misses, so 1 is false.)
Answers to 
Check Yourself


6“I swing big, with 
everything I’ve got. 

I hit big or I miss big. 

I like to live as big as 

I can.”
Babe RuthAmerican baseball player
Parallel Processors 
from Client to Cloud6.1 Introduction 5026.2 The Difﬁ
 culty of Creating Parallel Processing 
Programs 5046.3 SISD, MIMD, SIMD, SPMD, and Vector 
5096.4 Hardware Multithreading 
5166.5 Multicore and Other Shared Memory 
Multiprocessors 
5196.6 Introduction to Graphics Processing 
Units 524Computer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
6.7 Clusters, Warehouse Scale Computers, and Other Message-
Passing Multiprocessors 
5316.8 Introduction to Multiprocessor Network Topologies 
5366.9 Communicating to the Outside World: Cluster Networking 
5396.10 Multiprocessor Benchmarks and Performance Models 
5406.11 Real Stuff: Benchmarking Intel Core i7 versus NVIDIA 
Tesla GPU 
5506.12 Going Faster:  Multiple Processors and Matrix Multiply 
5556.13 Fallacies and Pitfalls 
5586.14 Concluding Remarks 
5606.15 Historical Perspective and Further Reading 
5636.16 Exercises 563ComputerComputerComputer
ComputerNetworkMultiprocessor or Cluster Organization
502 Chapter 6 Parallel Processors from Client to Cloud
 6.1 IntroductionComputer architects have long sought the “
 e City of Gold” (El Dorado) of 
computer design: to create powerful computers simply by connecting many existing 
smaller on
 is golden vision is the fountainhead of 
multiprocessors
. Ideally, 
customers order as many processors as they can a
 ord and receive a commensurate 
amount of performance.
 us, multiprocessor so
 ware must be designed to work 
with a variable number of processors. As mentioned in Chapter 1, energy has 

become the overriding issue for both microprocessors and datacenters. Replacing 

larg
  cient processors with many smaller
  cient processors can deliver 
better performance per joule both in the large and in the small, if so
 ware can 
  ciently use th
 us, improved energ
  ciency joins scalable performance 
in the case for multiprocessors.
Since multiprocessor so
 ware should scale, some designs support operation 
in the presence of broken hardware; that is, if a single processor fails in a 

multiprocessor with 
n processors, these system would continue to provide service 
with 
n – 1 processors. Hence, multiprocessors can also improve availability (see 
Chapter 5).
High performance can mean high throughput for independent tasks, called 
task-level parallelism 
or process-level parallelism
 ese tasks are independent 
single-threaded applications, and they are an important and popular use of 

multiple processor
 is approach is in contrast to running a single job on 
multiple processors. We use the term 
parallel processing program
 to refer to a 
single program that runs on multiple processors simultaneously.
 ere have long been scien
 c problems that have needed much faster 
computers, and this class of problems has been used to justify many novel parallel 

computers over the decades. Some of these problems can be handled simply today, 

using a 
cluster
 composed of microprocessors housed in many independent servers 
(see Section 6.7). In addition, clusters can serve equally demanding applications 
outside the sciences, such as search engines, Web servers, email servers, and 

databases.
As described in Chapter 1, multiprocessors have been shoved into the spotlight 
because the energy problem means that future increases in performance will 

primarily come from explicit hardware parallelism rather than much higher 

clock rates or vastly improved CPI. As we said in Chapter 1, they are called 
Over the Mountains Of 

the Moon, Down the 

Valley of the Shadow, 

Ride, boldly ride the 

shade replied— If you 

seek for El Dorado!
Edgar Allan Poe, 
“El Dorado,” 

stanza 4, 1849
multiprocessor
 A computer system with at 
least two processor
 is 
computer is in contrast to 

a uniprocessor, which has 

one, and is increasingly 

hard
 nd today.
task-level parallelism 
or process-level 
parallelism
 Utilizing 
multiple processors by 

running independent 

programs simultaneously.
parallel processing 
program
 A single 
program that runs on 
multiple processors 

simultaneously.
cluster
 A set of 
computers connected over 

a local area network that 

function as a single large 

multiprocessor.

 6.1 Introduction 503multicore microprocessors
 instead of multiprocessor microprocessors, 
presumably to avoid redundancy in naming. Hence, processors are o
 en called 
cores
 in a multicore chip
 e number of cores is expected to increase with 
Moore’s Law
 ese multicores are almost always 
Shared Memory Processors 
(SMPs)
, as they usually share a single physical address space. We’ll see SMPs 
more in Section 6.5.
 e state of technology today means that programmers who care about 
performance must become parallel programmers, for sequential code now means 
slow code.
 e tall challenge facing the industry is to create hardware and so
 ware that 
will make it easy to write correct parallel processing programs that will execute 

  ciently in performance and energy as the number of cores per chip scales.
 is abrupt
  in microprocessor design caught many o  guard, so there is a 
great deal of confusion about the terminology and what it means. 
Figure 6.1
 tries to 

clarify the terms serial, parallel, sequential, and concurren
 e columns of t
 gure 
represent the so ware, which is either inherently sequential or concurren
 e rows 
of the
 gure represent the hardware, which is either serial or parallel. For example, the 
programmers of compilers think of them as
 sequential programs: the steps include 

parsing, code generation, optimization, and so on. In contrast, the programmers 

of operating systems normally think of them as concurrent programs: cooperating 

processes handling I/O events due to independent jobs running on a computer.
 e point of these two axes of 
Figure 6.1
 is that concurrent so
 ware can run on 
serial hardware, such as operating systems for the Intel Pentium 4 uniprocessor, 

or on parallel hardware, such as an OS on the more recent Intel Cor
 e same 
is true for sequential so
 ware. For example, the MATLAB programmer writes 
a matrix multiply thinking about it sequentially, but it could run serially on the 

Pentium 4 or in parallel on the Intel Core i7. 
You might guess that the only challenge of the parallel revolutio
 guring out how 
to make naturally sequential so
 ware have high performance on parallel hardware, but 
it is also to make concurrent programs have high performance on multiprocessors as the 

number of processors increases. With this distinction made, in the rest of this chapter 

we will use 
parallel processing program
 or 
parallel so
 ware
 to mean either sequential 
or concurrent so
 ware running on parallel hardware
 e next section of this chapter 
describes why it is hard to create
  cient parallel processing programs. 
Software
SequentialConcurrent
Hardware
SerialMatrix Multiply written in MatLabrunning on an Intel Pentium 4
Windows Vista Operating System

running on an Intel Pentium 4
Parallel
Matrix Multiply written in MATLAB

running on an Intel Core i7
Windows Vista Operating System

running on an Intel Core i7
FIGURE 6.1 Hardware/software categorization and examples of application perspective 
on concurrency versus hardware perspective on parallelism.
multicore 
microprocessor
 A microprocessor 
containing multiple 

processors (“cores”) 

in a single integrated 

circuit. Virtually all 

microprocessors today in 

desktops and servers are 

multicore.
shared memory 
multiprocessor 

(SMP)
 A parallel 
processor with a single 
physical address space.

504 Chapter 6 Parallel Processors from Client to Cloud
Before proceeding further down the path to parallelism, don
t forget our initial 
incursions from the earlier chapters:
 Chapter 2, Section 2.11: Parallelism and Instructions: Synchronization
 Chapter 3, Section 3.6: Parallelism and Computer Arithmetic: Subword 
Parallelism
 Chapter 4, Section 4.10: Parallelism via Instructions
 Chapter 5, Section 5.10: Parallelism and Memory Hierarchy: Cache Coherence
True or false: To b
 t from a multiprocessor, an application must be concurrent.
 6.2 The Difﬁ
 culty of Creating Parallel 
Processing Programs e 
  culty with parallelism is not the hardware; it is that too few important 
application programs have been rewritten to complete tasks sooner on multiprocessors. 
It
  cult to write s
 ware that uses multiple processors to complete one task 
faster, and the problem gets worse as the number of processors increases.
Why has this been so? Why have parallel processing programs been so much 
harder to develop than sequential programs?
 e 
 rst reason is that you 
must
 get better performance or better energy 
  ciency from a parallel processing program on a multiprocessor; otherwise, you 
would just use a sequential program on a uniprocessor, as sequential programming 

is simpler. In fact, uniprocessor design techniques such as superscalar and out-of-

order execution take advantage of instruction-level parallelism (see Chapter 4), 

normally without the involvement of the programmer. Such innovations reduced 

the demand for rewriting programs for multiprocessors, since programmers 

could do nothing and yet their sequential programs would run faster on new 

computers.
Why is i
  cult to write parallel processin
g programs that are fast, especially 
as the number of processors increases? In Chapter 1, we used the analogy of 

eight reporters trying to write a single story in hopes of doing the work eight 

times faster. To succeed, the task must be broken into eight equal-sized pieces, 

because otherwise some reporters would be idle while waiting for the ones with 

larger pieces to
 nish. Another speed-up obstacle could be that the reporters 
would spend too much time communicating with each other instead of writing 

their pieces of the story. For both this analogy and parallel programming, 

the challenges include scheduling, partitioning the work into parallel pieces, 

balancing the load evenly between the workers, time to synchronize, and 
Check Yourself

overhead for communication between the part
 e challenge
 er with the 
more reporters for a newspaper story and with the more processors for parallel 
programming.
Our discussion in Chapter 1 reveals another obstacle, namely Amdahl
s Law. It 
reminds us that even small parts of a program must be parallelized if the program 

is to make good use of many cores.
Speed-up ChallengeSuppose you want to achieve a speed-up of 90 times faster with 100 processors. 

What percentage of the original computation can be sequential?
Amdahl
s Law (Chapter 1) says
Execution time a
 imp
rovement=
Execution time  b
yy improvement
Amount of improvement
Execution time c
+t
ted
We can reformulate Amdahl
s Law in terms of speed-up versus the original 
execution time:
Speed-up=
Execution time before
(Execution time beforeExecu
t
tion time aected)
Execution time 
+Amount of impr
o
ovement is formula is usually rewritten assuming that the execution time before is 
1 for some unit of time, and the execution time a
 ected by improvement is 
considered the fraction of the original execution time:
Speed-up=
1(1Fraction time aected)
Fraction time ae
dd
Amount of improvement
Substituting 90 for speed-up and 100 for amount of improvement into the 
formula above:
90=
1(1Fraction time aected)
Fraction time aected
100EXAMPLEANSWER 6.2 The Difﬁ culty of Creating Parallel Processing Programs 
505
506 Chapter 6 Parallel Processors from Client to Cloud
 en simplifying the formula and solving for fraction time a
 ected:
90(10.99Fraction ti
me a)=1
90(900.99Fraction t
×
×
i
ime)=1
90=900.99Fraction time 
Fractio
××
n
n time =89/89.1=0.999
 us, to achieve a speed-up of 90 from 100 processors, the sequential 
percentage can only be 0.1%.
Yet, there are applications with plenty of parallelism, as we shall see next.
Speed-up Challenge: Bigger ProblemSuppose you want to perform two sums: one is a sum of 10 scalar variables, and 
one is a matrix sum of a pair of two-dimensional arrays, with dimensions 10 by 10. 

For now let’s assume only the matrix sum is parallelizable; we’ll see soon how to 

parallelize scalar sums. What speed-up do you get with 10 versus 40 processors? 

Next, calculate the speed-ups assuming the matrices grow to 20 by 20.
If we assume performance is a function of the time for an addition, 
t, then 
there are 10 additions that do not b
 t from parallel processors and 100 
additions that do. If the time for a single processor is 110 
t, the execution time 
for 10 processors is
Execution timeer improvement=
Execution time  b
yy improvement
Amount of improvement
Execution time c
+t
ted
Execution timeer improvement=
10010ttt+=1020
so the speed-up with 10 processors is 110
t/20t e execution time for 
40 processors is
Execution time a
 imp
rovement=
10040ttt+=10125
.so the speed-up with 40 processors is 110
t/12.5t us, for this problem 
size, we get about 55% of the potential speed-up with 10 processors, but only 
22% with 40. 
EXAMPLEANSWER
Look what happens when we increase the matr
 e sequential program now 
takes 10
t + 400t e execution time for 10 processors is
Execution time improvement=
40010ttt+=1050
so the speed-up with 10 processors is 410
t/50t e execution time for 
40 processors is
Execution time improvement=
40040ttt+=1020
so the speed-up with 40 processors is 410
t/20t us, for this larger problem 
size, we get 82% of the potential speed-up with 10 processors and 51% with 40.
 ese examples show that getting good speed-up on a multiprocessor while 
keeping the prob
 xed is harder than getting good speed-up by increasing 
the size of the prob
 is insight allows us to introduce two terms that describe 
ways to scale up.
Strong scaling
 means measuring speed-up while keeping the prob
 xed. 
Weak scaling
 means that the problem size grows proportionally to the increase in 
the number of processors. Let’s assume that the size of the problem, M, is the working 
set in main memory, and we have P processor
 en the memory per processor for 
strong scaling is approximately M/P, and for weak scaling, it is approximately M.
Note that the 
memory hierarchy
 can interfere with the conventional wisdom 
about weak scaling being easier than strong scaling. For example, if the weakly 
scaled dataset no long
 ts in the last level cache of a multicore microprocessor, 
the resulting performance could be much worse than by using strong scaling.
Depending on the application, you can argue for either scaling approach. For 
example, the TPC-C debit-credit database benchmark requires that you scale up 

the number of customer accounts in proportion to the higher transactions per 

minute
 e argument is that it
s nonsensical to think that a given customer base 
is suddenly going to start using ATMs 100 times a day just because the bank gets a 

faster computer. Instead, if you
re going to demonstrate a system that can perform 
100 times the numbers of transactions per minute, you should run the experiment 

with 100 times as many customers. Bigger problems o
 en need more data, which 
is an argument for weak scaling.
 is 
 nal example shows the importance of load balancing.
Speed-up Challenge: Balancing LoadTo achieve the speed-up of 20.5 on the previous larger problem with 40 

processors, we assumed the load was perfectly balanced
 at is, each of the 40 
strong scaling
 Speed-
up achieved on a 
multiprocessor without 

increasing the size of the 

problem.
weak scaling
 Speed-
up achieved on a 

multiprocessor while 

increasing the size of the 

problem proportionally 

to the increase in the 

number of processors.
EXAMPLE 6.2 The Difﬁ culty of Creating Parallel Processing Programs 
507
508 Chapter 6 Parallel Processors from Client to Cloud
processors had 2.5% of the work to do. Instead, show the impact on speed-up if 
one processor
s load is higher than all the rest. Calculate at twice the load (5%) 

an
 ve times the load (12.5%) for that hardest working processor. How well 
utilized are the rest of the processors?
If one processor has 5% of the parallel load, then it must do 5% × 400 or 20 
additions, and the other 39 will share the remaining 380. Since they are operating 

simultaneously, we can just calculate the execution time as a maximum
Execution time a
 imp
rovement=Max
38039201tt,+110tt=30 e speed-up drops from 20.5 to 410
t/30t e remaining 39 processors 
are utilized less than half the time: 
while waiting 20t for hardest working 
processor to
 nish, they only compute for 380
t/39 = 9.7t. If one processor has 12.5% of the load, it must perform 50 addition
 e formula is:
Execution time a
 imp
rovement=Max
35039501tt,+110tt=60 e speed-up drops even further to 410
t/60t e rest of the processors 
are utilized less than 20% of the time (9
t/50t is example demonstrates the 
importance of balancing load, for just a single processor with twice the load 
of the others cuts speed-up by a third, an
 ve times the load on just one 
processor reduces speed-up by almost a factor of three.
Now that we better understand the goals and challenges of parallel processing, 
we give an overview of the rest of the chapter.
 e next Section (6.3) describes 
a much older cl
 cation scheme than in 
Figure 6.1
. In addition, it describes 
two styles of instruction set architectures that support running of sequential 

applications on parallel hardware, namely 
SIMD
 and 
vector
. Section 6.4 then 
describes 
multithreading
, a term o
 en confused with multiprocessing, in part 
because it relies upon similar concurrency in programs. Section 6.5 describes the 

 rst the two alternatives of a fundamental parallel hardware characteristic, which is 
whether or not all the processors in the systems rely upon a single physical address 

space. As mentioned above, the two popular versions of these alternatives are called 

shared memory multiprocessors 
(SMPs) and 
clusters
, and this section covers the 
former. Section 6.6 describes a relatively new style of computer from the graphics 

hardware community, called a 
graphics-processing unit
 (GPU) that also assumes 
a single physical address. (
 Appendix C
 describes GPUs in even more detail.) 
Section 6.7 describes clusters, a popular example of a computer with multiple 
physical address spaces.  Section 6.8 shows typical topologies used to connect many 

processors together, either server nodes in a cluster or cores in a microprocessor. 
 Section 6.9
 describes the hardware and so
 ware for communicating between 
ANSWER
nodes in a cluster using Ethernet. It shows how to optimize its performance using 
custom so
 ware and hardware. We next discuss th
  culty of 
 nding parallel 
benchmarks in Sectio
 is section also includes a simple, yet insightful 
performance model that helps in the design of applications as well as architectures. 

We use this model as well as parallel benchmarks in Section 6.11 to compare a 

multicore computer to a GPU. Section 6.12 divulges th
 nal and largest step in 
our journey of accelerating matrix multiply. For matrices that don’
 t in the cache, 
parallel processing uses 16 cores to improve performance by a factor of 14. We 

close with fallacies and pitfalls and our conclusions for parallelism.
In the next section, we introduce acronyms that you probably have already seen 
to identify
 erent types of parallel computers.
True or false: Strong scaling is not bound by Amdahl
s Law.
 6.3 SISD, MIMD, SIMD, SPMD, and Vector
One categorization of parallel hardware proposed in the 1960s is still used today. It 

was based on the number of instruction streams and the number of data streams. 

Figure 6.2
 shows the categor
 us, a conventional uniprocessor has a single 
instruction stream and single data stream, and a conventional multiprocessor has 

multiple instruction streams and multiple data stream
 ese two categories are 
abbreviated 
SISD
 and 
MIMD, respectively.
While it is possible to write separate programs that run o
 erent processors 
on a MIMD computer and yet work together for a grander, coordinated goal, 

programmers normally write a single program that runs on all processors of an 
MIMD computer, relying on conditional statements wh
 erent processors 
should execut
 erent sections of code
 is style is called 
Single Program 
Multiple Data (SPMD)
, but it is just the normal way to program a MIMD computer.
 e closest we can come to multiple instruction streams and single data stream 
(MISD)
 processor might be a “stream processor” that would perform a series of 
computations on a single data stream in a pipelined fashion: parse the input from 
the network, decrypt the data, decompress it, search for match, and so o
 e inverse of MISD is much more popular. 
SIMD
 computers operate on vectors of 
Check Yourself
SISD
 or Single 
Instruction stream, 

Single Data stream. 

A uniprocessor.
MIMD or Multiple 
Instruction streams, 

Multiple Data streams. 

A multiprocessor.
SPMD
 Single Program, 
Multiple Data streams. 

 e conventional MIMD 
programming model, 

where a single program 

runs across all processors.
SIMD
 or Single 
Instruction stream, 

Multiple Data streams. 

 e same instruction 
is applied to many data 

streams, as in a vector 

processor.
FIGURE 6.2 Hardware categorization and examples based on number of instruction 
streams and data streams: SISD, SIMD, MISD, and MIMD.
Data Streams
SingleMultipleInstruction 
StreamsSingleSISD: Intel Pentium 4
SIMD: SSE instructions of x86
MultipleMISD: No examples today
MIMD: Intel Core i7 6.3 SISD, MIMD, SIMD, SPMD, and Vector 
509
510 Chapter 6 Parallel Processors from Client to Cloud
data. For example, a single SIMD instruction might add 64 numbers by sending 64 
data streams to 64 ALUs to form 64 sums within a single clock cycle.
 e subword 
parallel instructions that we saw in Sections 3.6 and 3.7 are another example of 

SIMD; indeed, the middle letter of Intel’s SSE acronym stands for SIMD.
 e virtues of SIMD are that all the parallel execution units are synchronized and 
they all respond to a single instruction that emanates from a single 
program counter
 (PC). From a programmer
s perspective, this is close to the already familiar SISD. 
Although every unit will be executing the same instruction, each execution unit has 

its own address registers, and so each unit can hav
 erent data address
 us, 
in terms of 
Figure 6.1
, a sequential application might be compiled to run on serial 

hardware organized as a SISD or in parallel hardware that was organized as a SIMD.
 e original motivation behind SIMD was to amortize the cost of the control 
unit over dozens of execution units. Another advantage is the reduced instruction 

bandwidth and space
SIMD needs only one copy of the code that is being 
simultaneously executed, while message-passing MIMDs may need a copy in every 

processor, and shared memory MIMD will need multiple instruction caches.
SIMD works best when dealing with arrays in 
for loops. Hence, for parallelism 
to work in SIMD, there must be a great deal of identically structured data, which 

is called 
data-level parallelism
. SIMD is at its weakest in 
case or switch statements, where each execution unit must perfor
 erent operation on its 
data, depending on what data it has. Execution units with the wrong data must be 

disabled so that units with proper data may continue. If there are 
n cases, in these 
situations SIMD processors essentially run at 1/
nth of peak performance.
 e so-called array processors that inspired the SIMD category have faded 
into history (see 
 Section 6.15
 online), but two current interpretations of SIMD 
remain active today.
SIMD in x86: Multimedia ExtensionsAs described in Chapter 3, subword parallelism for narrow integer data was the 

original inspiration of the Multimedia Extension (MMX) instructions of the x86 

in 1996. As 
Moore’s Law
 continued, more instructions were added, leadin
 rst 
to 
Streaming SIMD Extensions
 (SSE) and now 
Advanced Vector Extensions
 (AVX). 
AVX supports the simultaneous execution of four 64-bi
 oating-point numbers. 
 e width of the operation and the registers is encoded in the opcode of these 
multimedia instructions. As the data width of the registers and operations grew, 

the number of opcodes for multimedia instructions exploded, and now there are 

hundreds of SSE and AVX instructions (see Chapter 3).
Vector
An older and, as we shall see, more elegant interpretation of SIMD is called a 
vector 
architecture
, which has been closely iden
 ed with computers designed by Seymour 
Cray starting in the 1970s. It is also a great match to problems with lots of data-level 

parallelism. Rather than having 64 ALUs perform 64 additions simultaneously, like 

the old array processors, the vector architectures pipelined the ALU to get good 

performance at low
 e basic philosophy of vector architecture is to collect 
data-level 
parallelism
 Parallelism 
achieved by performing 
the same operation on 

independent data.

data elements from memory, put them in order into a large set of registers, operate 
on them sequentially in registers using 
pipelined execution units
, and then write 
the results back to memory. A key feature of vector architectures is then a set of 

vector register
 us, a vector architecture might have 32 vector registers, each 
with 64 64-bit elements.
Comparing Vector to Conventional Code
Suppose we extend the MIPS instruction set architecture with vector 

instructions and vector registers. Vector operations use the same names as 

MIPS operations, but with the letter 
V appended. For example, 
addv.d adds two double-precision vector
 e vector instructions take as their input 
either a pair of vector registers (
addv.d) or a vector register and a scalar 
register (
addvs.d). In the latter case, the value in the scalar register is used 
as the input for all operations
the operation 
addvs.d will add the contents 
of a scalar register to each element in a vector register
 e names 
lv and 
sv denote vector load and vector store, and they load or store an entire vector 

of double-precision data. One operand is the vector register to be loaded or 

stored; the other operand, which is a MIPS general-purpose register, is the 

starting address of the vector in memory. Given this short description, show 

the conventional MIPS code versus the vector MIPS code for
YaXY
=×+where 
X and 
Y are vectors of 64 double precisio
 oating-point numbers, 
initially resident in memory, and 
a is a scalar double precision variable
 is example is the so-called DAXPY loop that forms the inner loop of the Linpack 

benchmark; DAXPY stands for double precision 
a ×X plus 
Y.). Assume that 
the starting addresses of 
X and 
Y are in 
$s0 and 
$s1, respectively.
Here is the conventional MIPS code for DAXPY:
 l.d $f0,a($sp) :load scalar a
 addiu $t0,$s0,#512 :upper bound of what to load
loop: l.d $f2,0($s0) :load x(i)
 mul.d $f2,$f2,$f0 :a x x(i)
 l.d $f4,0($s1) :load y(i)

 add.d $f4,$f4,$f2 :a x x(i) + y(i)

 s.d $f4,0($s1) :store into y(i)

 addiu $s0,$s0,#8 :increment index to x

 addiu $s1,$s1,#8 :increment index to y

 subu $t1,$t0,$s0 :compute bound

 bne $t1,$zero,loop :check if done
Here is the vector MIPS code for DAXPY:
EXAMPLEANSWER 6.3 SISD, MIMD, SIMD, SPMD, and Vector 
511
512 Chapter 6 Parallel Processors from Client to Cloud
 l.d $f0,a($sp) :load scalar a
 lv $v1,0($s0) :load vector x

 mulvs.d $v2,$v1,$f0 :vector-scalar multiply

 lv $v3,0($s1) :load vector y

 addv.d $v4,$v2,$v3 :add y to product

 sv $v4,0($s1) :store the result
 ere are some interesting comparisons between the two code segments in 
this example
 e most dramatic is that the vector processor greatly reduces the 
dynamic instruction bandwidth, executing only 6 instructions versus almost 600 
for the traditional MIPS architecture
 is reduction occurs both because the vector 
operations work on 64 elements at a time and because the overhead instructions 

that constitute nearly half the loop on MIPS are not present in the vector code. As 

you might expect, this reduction in instructions fetched and executed saves energy.
Another importan
 erence is the frequency of 
pipeline
 hazards (Chapter 4). 
In the straightforward MIPS code, every 
add.d must wait for a 
mul.d, every 
s.d must wait for the 
add.d and every 
add.d and
 mul.d must wait on 
l.d. On the vector processor, each vector instruction will only stall for th
 rst element 
in each vector, and then subsequent elemen
 ow smoothly down the pipeline. 
 us, pipeline stalls are required only once per vector 
operation
, rather than once 
per vector 
element
. In this example, the pipeline stall frequency on MIPS will be 

about 64 times higher than it is on the vector version o
 e pipeline stalls 
can be reduced on MIPS by using loop unrolling (see Chapter 4). However, the 

large
 erence in instruction bandwidth cannot be reduced.
Since the vector elements are independent, they can be operated on in parallel, 
much like subword parallelism for AVX instructions. All modern vector computers 

have vector functional units with multiple parallel pipelines (called 
vector
 lanes
; see 
Figures 6.2 and 6.3
) that can produce two or more results per clock cycle.
Elaboration: The loop in the example above exactly matched the vector length. When 
loops are shorter, vector architectures use a register that reduces the length of vector 
operations. When loops are larger, we add bookkeeping code to iterate full-length vector 

operations and to handle the leftovers. This latter process is called 
strip mining.Vector versus Scalar
Vector instructions have several important properties compared to conventional 
instruction set architectures, which are called 
scalar architectures
 in this context:
 A single vector instruction sp
 es a great deal of work
it is equivalent 
to executing an entire loop
 e instruction fetch and decode bandwidth 
needed is dramatically reduced.
 By using a vector instruction, the compiler or programmer indicates that the 
computation of each result in the vector is independent of the computation of 

other results in the same vector, so hardware does not have to check for data 

hazards within a vector instruction.
 Vector architectures and compilers have a reputation of making it much 
easier than when using MIMD multiprocessors to writ
  cient applications 
when they contain data-level parallelism.

 Hardware need only check for data hazards between two vector instructions 
once per vector operand, not once for every element within the vectors. 
Reduced checking can save energy as well as time.
 Vector instructions that access memory have a known access pattern. If 
the vector
s elements are all adjacent, then fetching the vector from a set 
of heavily interleaved memory banks works very well
 us, the cost of the 
latency to main memory is seen only once for the entire vector, rather than 

once for each word of the vector.
 Because an entire loop is replaced by a vector instruction whose behavior 
is predetermined, control hazards that would normally arise from the loop 

branch are nonexistent.
 e savings in instruction bandwidth and hazard checking plus th
  cient 
use of memory bandwidth give vector architectures advantages in power and 

energy versus scalar architectures.
For these reasons, vector operations can be made faster than a sequence of 
scalar operations on the same number of data items, and designers are motivated 

to include vector units if the application domain can o
 en use them.
Vector versus Multimedia Extensions
Like multimedia extensions found in the x86 AVX instructions, a vector instruction 

sp
 es multiple operations. However, multimedia extensions typically specify a 
few operations while vector sp
 es dozens of operations. Unlike multimedia 
extensions, the number of elements in a vector operation is not in the opcode but in a 

separate register
 is distinction mean
 erent versions of the vector architecture 
can be implemented wit
 erent number of elements just by changing the 
contents of that register and hence retain binary compatibility. In contrast, a new 

large set of opcodes is added each time the 
vector
 length changes in the multimedia 
extension architecture of the x86: MMX, SSE, SSE2, AVX, AVX2, … .
Also unlike multimedia extensions, the data transfers need not be contiguous. 
Vectors support both strided accesses, where the hardware loads every 
nth data 
element in memory, and indexed accesses, where hardware
 nds the addresses of 
the items to be loaded in a vector register. Indexed accesses are also called 
gather-
scatter
, in that indexed loads gather elements from main memory into contiguous 
vector elements and indexed stores scatter vector elements across main memory.
Like multimedia extensions, vector architectures easily capture th
 exibility 
in data widths, so it is easy to make a vector operation work on 32 64-bit data 

elements or 64 32-bit data elements or 128 
16-bit data elements or 256 8-bit data 
elemen
 e parallel semantics of a vector instruction allows an implementation 
to execute these operations using a deeply 
pipelined
 functional unit, an array of 
parallel functional units, or a combination of parallel and pipelined functional 

units. 
Figure 6.3
 illustrates how to improve vector performance by using parallel 

pipelines to execute a vector add instruction.
Vector arithmetic instructions usually only allow element N of one vector 
register to take part in operations with element N from other vector register
 is  6.3 SISD, MIMD, SIMD, SPMD, and Vector 
513
514 Chapter 6 Parallel Processors from Client to Cloud
dramatically simp
 es the construction of a highly parallel vector unit, which can 
be structured as multiple parallel 
vector lanes
. As with a tra
  c highway, we can 
increase the peak throughput of a vector unit by adding more lanes. 
Figure 6.4
 shows the structure of a four-lane vector uni
 us, going to four lanes from one 
lane reduces the number of clocks per vector instruction by roughly a factor of four. 

For multiple lanes to be advantageous, both the applications and the architecture 

must support long vectors. Otherwise, they will execute so quickly that you’ll run 

out of instructions, requiring instruction level 
parallel
 techniques like those in 
Chapter 4 to supply enough vector instructions.
Generally, vector architectures are a very  cient way to execute data parallel 
processing programs; they are better matches to compiler technology than 

multimedia extensions; and they are easier to evolve over time than the multimedia 

extensions to the x86 architecture.
Given these classic categories, we next see how to exploit parallel streams of 
instructions to improve the performance of a 
single
 processor, which we will reuse 
with multiple processors.
True or false: As exemp
 ed in the x86, multimedia extensions can be thought of 
as a vector architecture with short vectors that supports only contiguous vector 
data transfers.
vector lane
 One or 
more vector functional 
units and a portion of 

the vector regist
 le. 
Inspired by lanes on 

highways that increase 

tra
  c speed, multiple 
lanes execute vector 

operations 

simultaneously.
Check Yourself
A[9]A[8]A[7]A[6]A[5]A[4]A[3]A[2]A[1]B[9]B[8]B[7]B[6]B[5]B[4]B[3]B[2]B[1]C[0]+C[0]C[1]C[2]C[3]A[8]A[4]B[8]B[4]A[9]A[5]B[9]B[5]A[6]B[6]A[7]B[7](a)(b)Element group
++++FIGURE 6.3 Using multiple functional units to improve the performance of a single vector 
add instruction, C = A + B.
  e vector processor (a) on th
  has a single add pipeline and can complete 
one addition per cycle.
 e vector processor (b) on the right has four add pipelines or lanes and can complete 
four additions per cycle
 e elements within a single vector add instruction are interleaved across the four 
lanes.

Elaboration: Given the advantages of vector, why aren’t they more popular outside 
high-performance computing? There were concerns about the larger state for vector 
register culty of handling page faults in 

vector loads and stores, and SIMD instructions achie ts of vector 

instructions.  In addition, as long as advances in instruction level parallelism could 

deliver on the performance promise of Moore’s Law, there was little reason to take the 

chance of changing architecture styles.Elaboration: Another advantage of vector and multimedia extensions is that it is relatively easy to extend a scalar instruction set architecture with these instructions to 

improve performance of data parallel operations.
Elaboration: The Haswell-generation x86 processors from Intel support AVX2, which 
has a gather operation but not a scatter operation.Lane 0Lane 1Lane 2Lane 3FP addpipe 0FP mul
pipe 0Vector
registers:elements0,4,8,...FP addpipe 1FP mul
pipe 1Vector
registers:elements1,5,9,...FP addpipe 2FP mul
pipe 2Vector
registers:elements2,6,10,...FP addpipe 3FP mul
pipe 3Vector
registers:elements3,7,11,...Vector load store unit
FIGURE 6.4 Structure of a vector unit containing four lanes. 
 e vector-register storage is 
divided across the lanes, with each lane holding every fourth element of each vector register
 e 
 gure 
shows three vector functional units: an FP add, an FP multiply, and a load-store unit. Each of the vector 
arithmetic units contains four execution pipelines, one per lane, which acts in concert to complete a single 

vector instruction. Note how each section of the vector-regist
 le only needs to provide enough read and 
write ports (see Chapter 4) for functional units local to its lane. 
 6.3 SISD, MIMD, SIMD, SPMD, and Vector 
515
516 Chapter 6 Parallel Processors from Client to Cloud
 6.4 Hardware Multithreading
A related concept to MIMD, especially from the programmer’s perspective, is 
hardware multithreading
. While MIMD relies on multiple 
processes
 or 
threads
 to try to keep multiple processors busy, hardware multithreading allows multiple 
threads to share the functional units of a 
single
 processor in an overlapping fashion 
to try to utilize the hardware resour
  ciently. To permit this sharing, the 
processor must duplicate the independent state of each thread. For example, each 
thread would have a separate copy of the regist
 le and the program counter. 
 e memory itself can be shared through the virtual memory mechanisms, which 
already support multi-programming. In addition, the hardware must support the 

ability to change to
 erent thread relatively quickly. In particular, a thread 
switch should be much more
  cient than a process switch, which typically 
requires hundreds to thousands of processor cycles while a thread switch can be 

instantaneous.
 ere are two main approaches to hardware multithreading. 
Fine-grained 
multithreading
 switches between threads on each instruction, resulting in 
interleaved execution of multiple thre
 is interleaving is o
 en done in a 
round-robin fashion, skipping any threads that are stalled at that clock cycle. To 

make
 ne-grained multithreading practical, the processor must be able to switch 
threads on every clock cycle. One advantage of
 ne-grained multithreading is 
that it can hide the throughput losses that arise from both short and long stalls, 

since instructions from other threads can be executed when one thread st
 e primary disadvantage of
 ne-grained multithreading is that it slows down the 
execution of the individual threads, since a thread that is ready to execute without 

stalls will be delayed by instructions from other threads.
Coarse-grained multithreading
 was invented as an alternative to
 ne-grained multithreading. Coarse-grained multithreading switches threads only on costly 

stalls, such as last-level cache miss
 is change relieves the need to have thread 
switching be extremely fast and is much less likely to slow down the execution of an 

individual thread, since instructions from other threads will only be issued when 

a thread encounters a costly stall. Coarse-grained multithreadin
 ers, however, 
from a major drawback: it is limited in its ability to overcome throughput losses, 

especially from shorter st
 is limitation arises from the 
pipeline
 start-up 
costs of coarse-grained multithreading. Be
cause a processor with coarse-grained 
multithreading issues instructions from a single thread, when a stall occurs, the 

pipeline must be emptied or fro
 e new thread that begins executing a
 er the stall mu
 ll the pipeline before instructions will be able to complete. Due 
to this start-up overhead, coarse-grained multithreading is much more useful for 

reducing the penalty of high-cost stalls, where pipeline r
 ll is negligible compared 
to the stall time.
hardware 
multithreading
 Increasing utilization of a 
processor by switching to 

another thread when one 

thread is stalled.
thread
 A thread includes 
the program counter, the 

register state, and the 

stack. It is a lightweight 

process; whereas threads 

commonly share a single 

address space, processes 

don’t.
process
 A process 
includes one or more 

threads, the address space, 

and the operating system 

state. Hence, a process 

switch usually invokes the 

operating system, but not 

a thread switch.
 ne-grained 
multithreading
 A version of hardware 

multithreading that 

implies switching between 

threads a
 er every 
instruction.
coarse-grained 
multithreading
 A version of hardware 
multithreading that 

implies switching between 

threads only a
 er  cant events, such as 
a last-level cache miss.

Simultaneous multithreading (SMT)
 is a variation on hardware multithreading 
that uses the resources of a multiple-issue, dynamically scheduled 
pipelined
 processor to exploit thread-level parallelism at the same time it exploits instruction-
level parallelism (see Chapt
 e key insight that motivates SMT is that 
multiple-issue processors o
 en have more functional unit parallelism available 
than most single threads ca
 ectively use. Furthermore, with register renaming 
and dynamic scheduling (see Chapter 4), multiple instructions from independent 

threads can be issued without regard to the dependences among them; the resolution 

of the dependences can be handled by the dynamic scheduling capability.
Since SMT relies on the existing dynamic mechanisms, it does not switch 
resources every cycle. Instead, SMT is 
always
 executing instructions from multiple 
threads, leaving it up to the hardware to associate instruction slots and renamed 

registers with their proper threads.
Figure 6.5
 conceptually illustrates th
 erences in a processor
s ability to exploit 
superscalar resources for the following processor co
 gurations. 
 e top portion shows 
simultaneous 
multithreading 

(SMT)
 A version 
of multithreading 
that lowers the cost 

of multithreading by 

utilizing the resources 

needed for multiple issue, 

dynamically scheduled 

microarchitecture.
FIGURE 6.5 How four threads use the issue slots of a superscalar processor in different 
approaches. e four threads at the top show how each would execute running alone on a standard 
superscalar processor without multithreading suppor
 e three examples at the bottom show how they 
would execute running together in three multithreading option
 e horizontal dimension represents the 
instruction issue capability in each clock cycle e vertical dimension represents a sequence of clock cycles. 
An empty (white) box indicates that the corresponding issue slot is unused in that clock cycle.
 e shades of 
gray and color correspond to fo
 erent threads in the multithreading processor
 e additional pipeline 
start-u
 ects for coarse multithreading, which are not illustrated in th
 gure, would lead to further loss 
in throughput for coarse multithreading.
Issue slotsThread CThread D
Thread AThread B
TimeTimeSMTCoarse MTFine MT
Issue slots 6.4 Hardware 
Multithreading 517
518 Chapter 6 Parallel Processors from Client to Cloud
how four threads would execute independently on a superscalar with no multithreading 
suppor
 e bottom portion shows how the four threads could be combined to execute 
on the processor more
  ciently using three multithreading options:
 A superscalar with coarse-grained multithreading
 A superscalar wi
 ne-grained multithreading
 A superscalar with simultaneous multithreading
In the superscalar without hardware multithreading support, the use of issue 
slots is limited by a lack of 
instruction-level parallelism
. In addition, a major stall, 
such as an instruction cache miss, can leave the entire processor idle.
In the coarse-grained multithreaded superscalar, the long stalls are partially 
hidden by switching to another thread that uses the resources of the processor. 

Although this reduces the number of completely idle clock cycles, the pipeline 

start-up overhead still leads to idle cycles, and limitations to ILP means all issue 

slots will not be used. In th
 ne-grained case, the interleaving of threads mostly 
eliminates idle clock cycles. Because only a single thread issues instructions in a 

given clock cycle, however, limitations in instruction-level parallelism still lead to 

idle slots within some clock cycles.
2.001.751.501.251.000.75i7 SMT performance and energy efficiency ratioBlackscholesBodytrackCannealFerretFluidanimateRaytraceStreamclusterSwaptions×264Energy efficiencySpeedupFacesimVipsFIGURE 6.6 The speed-up from using multithreading on one core on an i7 processor averages 1.31 for the PARSEC benchmarks (see 
 Section 6.9) and the energy efﬁ
 ciency improvement is 1.07.  is data was collected and analyzed by Esmaeilzadeh et. al. [2011].

In the SMT case, thread-level parallelism and instruction-level parallelism are 
both exploited, with multiple threads using the issue slots in a single clock cycle. 
Ideally, the issue slot usage is limited by imbalances in the resource needs and 

resource availability over multiple threads. In practice, other factors can restrict 

how many slots are used. Although 
Figure 6.5
 greatly simp
 es the real operation 
of these processors, it does illustrate the potential performance advantages of 

multithreading in general and SMT in particular. 
Figure 6.6
 plots the performance and energy be
 ts of multithreading on a 
single processors of the Intel Core i7 960, which has hardware support for two 

thre
 e average speed-up is 1.31, which is not bad given the modest extra 
resources for hardware multithreadin
 e average improvement in energy 
  ciency is 1.07, which is excellent. In general, you’d be happy with a performance 
speed-up being energy neutral.
Now that we have seen how multiple threads can utilize the resources of a single 
processor more
 ectively, we next show how to use them to exploit multiple 
processors.
1. True or false: Both multithreading and multicore rely on parallelism to get 
more
  ciency from a chip.
2. True or false: 
Simultaneous multithreading
 (SMT) uses threads to improve 
resource utilization of a dynamically scheduled, out-of-order processor.
 6.5 Multicore and Other Shared Memory 
Multiprocessors
While hardware multithreading improved th
  ciency of processors at modest 
cost, the big challenge of the last decade has been to deliver on the performance 

potential of Moore’s Law by
  ciently programming the increasing number of 
processors per chip.
Given th
  culty of rewriting old programs to run well on parallel hardware, 
a natural question is: what can computer designers do to simplify the task? One 

answer was to provide a single physical address space that all processors can share, 

so that programs need not concern themselves with where their data is, merely that 

programs may be executed in parallel. In this approach, all variables of a program 

can be made available at any time to any processor
 e alternative is to have a 
separate address space per processor that requires that sharing must be explicit; 

well describe this option in the Section 6.7. When the physical address space is 
common
then the hardware typically provides cache coherence to give a consistent 
view of the shared memory (see Section 5.8).
As mentioned above, a 
shared memory multiprocessor
 (SMP) is one that o
 ers 
the programmer a 
single physical address space
 across all processors
which is 
Check Yourself
 6.5 Multicore and Other Shared Memory Multiprocessors 
519
520 Chapter 6 Parallel Processors from Client to Cloud
nearly always the case for multicore chips
although a more accurate term would 
have been shared-
address
 multiprocessor. Processors communicate through shared 
variables in memory, with all processors capable of accessing any memory location 
via loads and stores. 
Figure 6.7
 shows the classic organization of an SMP. Note that 

such systems can still run independent jobs in their own virtual address spaces, 

even if they all share a physical address space.
Single address space multiprocessors come in two styles. In th
 rst style, the 
latency to a word in memory does not depend on which processor asks for it. 

Such machines are called 
uniform memory access (UMA)
 multiprocessors. In the 
second style, some memory accesses are much faster than others, depending on 
which processor asks for which word, typically because main memory is divided 

and attached to
 erent microprocessors or t erent memory controllers on 
the same chip. Such machines are called 
nonuniform memory access (NUMA)
 multiprocessors. As you might expect, the programming challenges are harder for 
a NUMA multiprocessor than for a UMA multiprocessor, but NUMA machines 

can scale to larger sizes and NUMAs can have lower latency to nearby memory.
As processors operating in parallel will 
normally share data, they also need to 
coordinate when operating on shared data; otherwise, one processor could start 

working on data before anot
 nished with i
 is coordination is called 
synchronization
, which we saw in Chapter 2. When sharing is supported with a 
single address space, there must be a separate mechanism for synchronization. One 
approach uses a 
lock
 for a shared variable. Only one processor at a time can acquire 
the lock, and other processors interested in shared data must wait until the original 
processor unlocks the variable. Section 2.11 of Chapter 2 describes the instructions 

for locking in the MIPS instruction set.
uniform memory access 
(UMA)
 A multiprocessor 
in which latency to any 
word in main memory is 

about the same no matter 

which processor requests 

the access.
nonuniform memory 
access (NUMA)
 A type 
of single address space 
multiprocessor in which 

some memory accesses 

are much faster than 

others depending on 

which processor asks for 

which word.
synchronization
 e process of coordinating 

the behavior of two or 

more processes, which 

may be running on 

 erent processors.
lock
 A synchronization 
device that allows access 

to data to only one 

processor at a time.
FIGURE 6.7 Classic organization of a shared memory multiprocessor.
ProcessorMemoryI/OProcessorProcessorCacheCacheCacheInterconnection Network. . .. . .
A Simple Parallel Processing Program for a Shared Address Space
Suppose we want to sum 64,000 numbers on a shared memory multiprocessor 
computer with uniform memory access time. Let
s assume we have 64 
processors.
 e 
 rst step is to ensure a balanced load per processor, so we split the set 
of numbers into subsets of the same size. We do not allocate the subsets to a 
 erent memory space, since there is a single memory space for this machine; 
we just give di
 erent starting addresses to each processor. Pn is the number that 
iden
 es the processor, between 0 and 63. All processors start the program by 
running a loop that sums their subset of numbers:
   sum[Pn] = 0;   for (i = 1000*Pn; i < 1000*(Pn+1); i += 1)
     sum[Pn] += A[i]; /*sum the assigned areas*/(Note the C code 
i += 1 is just a shorter way to say 
i = i + 1.) e next step is to add these 64 partial sum
 is step is called a 
reduction
, where we divide to conquer. Half of the processors add pairs of partial sums, 
and then a quarter add pairs of the new partial sums, and so on until we 
have the single,
 nal sum. 
Figure 6.8
 illustrates the hierarchical nature of this 
reduction.
In this example, the two processors must synchronize before the 
consumer
 processor tries to read the result from the memory location written by the 

producer
 processor; otherwise, the consumer may read the old value of 
EXAMPLEANSWERreduction
 A function 
that processes a data 
structure and returns a 

single value.
001012301234567(half = 1)(half = 2)
(half = 4)FIGURE 6.8 The last four levels of a reduction that sums results from each processor, 
from bottom to top. For all processors whose number i is less than half, add the sum produced by 
processor number (i + half) to its sum.
 6.5 Multicore and Other Shared Memory Multiprocessors 
521
522 Chapter 6 Parallel Processors from Client to Cloud
the data. We want each processor to have its own version of the loop counter 
variable  
i, so we must indicate that it is a 
private
 variable. Here is the code 
(half is private also):
   half = 64; /*64 processors in multiprocessor*/   do
       synch(); /*wait for partial sum completion*/
       if (half%2 != 0 && Pn == 0)
           sum[0] += sum[half–1];
           /*Conditional sum needed when half is
           odd; Processor0 gets missing element */
           half = half/2; /*dividing line on who sums */
           if (Pn < half) sum[Pn] += sum[Pn+half];
   while (half > 1); /*exit with final sum in Sum[0] */Given the long-term interest in parallel programming, there have been hundreds 
of attempts to build parallel programming systems. A limited but popular example 

is OpenMP
. It is just an 
Application Programmer Interface
 (API) along with a set of 
compiler directives, environment variables, and runtime library routines that can 

extend standard programming languages. It o
 ers a portable, scalable, and simple 
programming model for shared memory multiprocessors. Its primary goal is to 

parallelize loops and to perform reductions. 
Most C compilers already have support for OpenMP
 e command to uses the 
OpenMP API with the UNIX C compiler is just:
cc –fopenmp foo.cOpenMP extends C using 
pragmas
, which are just commands to the C macro 
preprocessor like 
#define and 
#include. To set the number of processors we 
want to use to be 64, as we wanted in the example above, we just use the command
#define P 64 /* define a constant that we’ll use a few times */#pragma omp parallel num_threads(P) at is, the runtime libraries should use 64 parallel threads. 
To turn the sequential for loop into a parallel for loop that divides the work 
equally between all the threads that we told it to use, we just write (assuming 
sum is initialized to 0)
#pragma omp parallel for
for (Pn = 0; Pn < P; Pn += 1)
  for (i = 0; 1000*Pn; i < 1000*(Pn+1); i += 1)
    sum[Pn] += A[i]; /*sum the assigned areas*/Hardware/
Software 
InterfaceOpenMP
 An API 
for shared memory 
multiprocessing in C, 

C++, or Fortran that runs 

on UNIX and Microso
  platforms. It includes 

compiler directives, a 

library, and runtime 

directives.

To perform the reduction, we can use another command that tells OpenMP 
what the reduction operator is and what variable you need to use to place the result 
of the reduction.
#pragma omp parallel for reduction(+ : FinalSum)for (i = 0; i < P; i += 1)
     FinalSum += sum[i]; /* Reduce to a single number */Note that it is now up to the OpenMP library t
 nd 
  cient code to sum 64 
number
  ciently using 64 processors.
While OpenMP makes it easy to write simple parallel code, it is not very helpful 
with debugging, so many parallel programmers use more sophisticated parallel 
programming systems than OpenMP, just as many programmers today use more 

productive languages than C.
Given this tour of classic MIMD hardware and so
 ware, our next path is a more 
exotic tour of a type of MIMD architecture with
 erent heritage and thus a very 
 erent perspective on the parallel programming challenge. 
True or false: Shared memory multiprocessors cannot take advantage of task-level 

parallelism.
Elaboration: Some writers repurposed the acronym SMP to mean 
symmetric multiprocessor, to indicate that the latency from processor to memory was about the 
same for all processors. This shift was done to contrast them from large-scale NUMA 

multiprocessors, as both classes used a single address space. As clusters proved much 

more popular than large-scale NUMA multiprocessors, in this book we restore SMP to 

its original meaning, and use it to contrast against that use multiple address spaces, 

such as clusters.
Elaboration: An alternative to sharing the physical address space would be to have 
separate physical address spaces but share a common virtual address space, leaving 

it up to the operating system to handle communication. This approach has been tried, 

but it has too high an overhead to offer a practical shared memory abstraction to the 

performance-oriented programmer.
Check Yourself
 6.5 Multicore and Other Shared Memory Multiprocessors 
523
524 Chapter 6 Parallel Processors from Client to Cloud
 6.6 Introduction to Graphics Processing Units
 e original ju
 cation for adding SIMD instructions to existing architectures 
was that many microprocessors were connected to graphics displays in PCs and 
workstations, so an increasing fraction of processing time was used for graphics. 

As 
Moore’s Law
 increased the number of transistors available to microprocessors, 
it therefore made sense to improve graphics processing.
A major driving force for improving graphics processing was the computer game 
industry, both on PCs and in dedicated game consoles such as the Sony PlayStation. 

 e rapidly growing game market encouraged many companies to make increasing 
investments in developing faster graphics hardware, and this positive feedback loop 

led graphics processing to improve at a faster rate than general-purpose processing 

in mainstream microprocessors.
Given that the graphics and game communi
 erent goals than the 
microprocessor development community, it evolved its own style of processing and 

terminology. As the graphics processors increased in power, they earned the name 

Graphics Processing Units
 or 
GPUs
 to distinguish themselves from CPUs. 
For a few hundred dollars, anyone can buy a GPU today with hundreds of 
parallel
 oating-point units, which makes high-performance computing more 
accessible.
 e interest in GPU computing blossomed when this potential was 
combined with a programming language that made GPUs easier to program. 

Hence, many programmers of scien
 c and multimedia applications today are 
pondering whether to use GPUs or CPUs.
 is section concentrates on using GPUs for computing. To see how GPU 
computing combines with the traditional role of graphics acceleration, see 
 Appendix C
.)Here are some of the key characteristics as to how GPUs vary from CPUs:
 GPUs are accelerators that supplement a CPU, so they do not need be able 
to perform all the tasks of a CPU
 is role allows them to dedicate all their 
resources to graphics. It
 ne for GPUs to perform some tasks poorly or not 
at all, given that in a system with both a CPU and a GPU, the CPU can do 

them if needed. 
 e GPU problems sizes are typically hundreds of megabytes to gigabytes, 
but not hundreds of gigabytes to terabytes.
 ese 
 erences led t
 erent styles of architecture:
 Perhaps the bigg
 erence is that GPUs do not rely on multilevel caches 
to overcome the long latency to memory, as do CPUs. Instead, GPUs rely on 

hardware multithreading (Section 6.4) to hide the latency to memory
 at is, 
between the time of a memory request and the time that data arrives, the GPU 

executes hundreds or thousands of threads that are independent of that request.

 e GPU memory is thus oriented toward bandwidth rather than latency. 
 ere are even special graphics DRAM chips for GPUs that are wider and 
have higher bandwidth than DRAM chips for CPUs. In addition, GPU 
memories have traditionally had smaller main memories than conventional 

microprocessors. In 2013, GPUs typically have 4 to 6 GiB or less, while 

CPUs have 32 to 256 GiB. Finally, keep in mind that for general-purpose 

computation, you must include the time to transfer the data between CPU 

memory and GPU memory, since the GPU is a coprocessor.
 Given the reliance on many threads to deliver good memory bandwidth, 
GPUs can accommodate many parallel processors (MIMD) as well as many 

threads. Hence, each GPU processor is more highly multithreaded than a 

typical CPU, plus they have more processors.
Although GPUs were designed for a narrower set of applications, some programmers 

wondered if they could specify their applications in a form that would let them 

tap the high potential performance of GPU
 er tiring of trying to specify their 
problems using the graphics APIs and languages, they developed C-inspired 

programming languages to allow them to write programs directly for the GPUs. 

An example is NVIDIA
s CUDA (Compute U
 ed Device Architecture), which 
enables the programmer to write C programs to execute on GPUs, albeit with some 

restrictions. 
 Appendix C
 gives examples of CUDA code. (OpenCL is a multi-
company initiative to develop a portable programming language that provides 
many of the be
 ts of CUDA.) 
NVIDIA decided that the unifying theme of all these forms of parallelism is 
the 
CUD
 read
. Using this lowest level of parallelism as the programming 
primitive, the compiler and the hardware can gang thousands of CUD
 reads 
together to utilize the various styles of parallelism within a GPU: multithreading, 

MIMD, SIMD, and instruction-level pa
 ese threads are blocked 
together and executed in groups of 32 at a time. A multithreaded processor inside 

a GPU executes these blocks of threads, and a GPU consists of 8 to 32 of these 

multithreaded processors
. An Introduction to the NVIDIA GPU ArchitectureWe use NVIDIA systems as our example as they are representative of GPU 

architectures. Sp
 cally, we follow the terminology of the CUDA parallel 
programming language and use the Fermi architecture as the example.
Like vector architectures, GPUs work well only with data-level parallel problems. 
Both styles have gather-scatter data transfers, and GPU processors have even more 
Hardware/

Software 

Interface 6.6 Introduction to Graphics Processing Units 
525
526 Chapter 6 Parallel Processors from Client to Cloud
registers than do vector processors. Unlike most vector architectures, GPUs also 
rely on hardware multithreading within a single multi-threaded SIMD processor 

to hide memory latency (see Section 6.4).
A multithreaded SIMD processor is similar to a Vector Processor, but the former 
has many parallel functional units instead of just a few that are deeply pipelined, 

as does the latter. 
As mentioned above, a GPU contains a collection of multithreaded SIMD 
processors; that is, a GPU is a MIMD composed of multithreaded SIMD processors. 

For example, NVIDIA has four implementations of the Fermi architecture at 

 erent price points with 7, 11, 14, or 15 multithreaded SIMD processors. To 
provide transparent scalability across models of GPUs wit
 ering number of 
multithreaded SIMD processors, th
 read Block Scheduler hardware assigns 
blocks of threads to multithreaded SIMD processors.  
Figure 6.9
 shows a simp
 ed block diagram of a multithreaded SIMD processor.
Dropping down one more level of detail, the machine object that the hardware 
creates, manages, schedules, and executes is a 
thread of SIMD instructions
, which 

we will also call a 
SIMD thread
. It is a traditional thread, but it contains exclusively 
SIMD instruction
 ese SIMD threads have their own program counters and 
they run on a multithreaded SIMD processor
 e S
 read Scheduler
 includes 
a controller that lets it know which threads of SIMD instructions are ready to 

run, and then it sends them o
  to a dispatch unit to be run on the multithreaded 
FIGURE 6.9 Simpliﬁ ed block diagram of the datapath of a multithreaded SIMD Processor. 
It has 16 SIMD la
 e S
 read Scheduler has many independent SIMD threads that it chooses from 
to run on this processor.
Instruction register
Regi-sters1K×32LoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitAddress coalescing unitInterconnection network
Local Memory
64KiB
To Global
 Memory
LoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitLoad
storeunitLoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitLoadstoreunitReg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32SIMD Lanes(ThreadProcessors)
SIMD processor. It is identical to a hardware thread scheduler in a traditional 
multithreaded processor (see Section 6.4), except that it is scheduling threads of 

SIMD instruction
 us, GPU hardware has two levels of hardware schedulers: 
 e  read Block Scheduler
 that assigns blocks of threads to multithreaded 
SIMD processors, and 
2. the S
 read Scheduler 
within
 a SIMD processor, which schedules 
when SIMD threads should run.
 e SIMD instructions of these threads are 32 wide, so each thread of SIMD 
instructions would compute 32 of the elements of the computation. Since the 

thread consists of SIMD instructions, the SIMD processor must have parallel 

functional units to perform the operation. We call them 
SIMD Lanes
, and they are 
quite similar to the Vector Lanes in Section 6.3.
Elaboration: The number of lanes per SIMD processor varies across GPU generations. With Fermi, each 32-wide thread of SIMD instructions is mapped to 16 SIMD Lanes, 
so each SIMD instruction in a thread of SIMD instructions takes two clock cycles to 

complete. Each thread of SIMD instructions is executed in lock step. Staying with the 

analogy of a SIMD processor as a vector processor, you could say that it has 16 lanes, 

and the vector length would be 32. This wide but shallow nature is why we use the term 

SIMD processor instead of vector processor, as it is more intuitive.
Since b nition the threads of SIMD instructions are independent, the SIMD 
Thread Scheduler can pick whatever thread of SIMD instructions is ready, and need not 

stick with the next SIMD instruction in the sequence within a single thread. Thus, using 

the terminology of Section 6.4,
 ne-grained multithreading.
To hold these memory elements, a Fermi SIMD processor has an impressive 32,768 
32-bit registers. Just like a vector processor, these registers are divided logically across 

the vector lanes or, in this case, SIMD Lanes. Each SIMD Thread is limited to no more than 

64 registers, so you might think of a SIMD Thread as having up to 64 vector registers, 

with each vector register having 32 elements and each element being 32 bits wide.
Since Fermi has 16 SIMD Lanes, each contains 2048 registers. Each CUDA Thread 
gets one element of each of the vector registers.  Note that a CUDA thread is just a 

vertical cut of a thread of SIMD instructions, corresponding to one element executed by 

one SIMD Lane. Beware that CUDA Threads are very different from POSIX threads; you 

cant make arbitrary system calls or synchronize arbitrarily in a CUDA Thread.
NVIDIA GPU Memory Structures
Figure 6.10
 shows the memory structures of an NVIDIA GPU. We call the on-
chip memory that is local to each multithreaded SIMD processor 
Local Memory
. It is shared by the SIMD Lanes within a multithreaded SIMD processor, but this 

memory is not shared between multithreaded SIMD processors. We call the o
 -chip DRAM shared by the whole GPU and all thread blocks 
GPU Memory
. Rather than rely on large caches to contain the whole working sets of an 
application, GPUs traditionally use smaller streaming caches and rely on extensive 

multithreading of threads of SIMD instructions to hide the long latency to DRAM, 
 6.6 Introduction to Graphics Processing Units 
527
528 Chapter 6 Parallel Processors from Client to Cloud
since their working sets can be hundreds of megabyt
 us, they will no
 t in the last level cache of a multicore microprocessor. Given the use of hardware 
multithreading to hide DRAM latency, the chip area used for caches in system 

processors is spent instead on computing resources and on the large number of 

registers to hold the state of the many threads of SIMD instructions. 
Elaboration: While hiding memory latency is the underlying philosophy, note that the 
latest GPUs and vector processors have added caches. For example, the recent Fermi 
architecture has added caches, but they are thought of as either band lters to 

reduce demands on GPU Memory or as accelerators for the few variables whose latency 

cannot be hidden by multithreading. Local memory for stack frames, function calls, 

and register spilling is a good match to caches, since latency matters when calling a 

function. Caches can also save energy, since on-chip cache accesses take much less 

energy than accesses to multiple, external DRAM chips.
CUDA Thread
Thread block
Per-Block
Local Memory
Grid 0 
. . . Grid 1 
. . . GPU Memory
SequenceInter-Grid Synchronization
Per-CUDA Thread Private MemoryFIGURE 6.10 GPU Memory structures. 
GPU Memory is shared by the vectorized loops. All threads 
of SIMD instructions within a thread block share Local Memory.

Putting GPUs into Perspective
At a high level, multicore computers with SIMD instruction extensions do share 
similarities with GPUs. 
Figure 6.11
 summarizes the similarities an
 erences. 
Both are MIMDs whose processors use multiple SIMD lanes, although GPUs 
have more processors and many more lanes. Both use hardware multithreading 

to improve processor utilization, although GPUs have hardware support for many 

more threads. Both use caches, although GPUs use smaller streaming caches and 

multicore computers use large multilevel caches that try to contain whole working 

sets completely. Both use a 64-bit address space, although the physical main 

memory is much smaller in GPUs. While GPUs support memory protection at the 

page level, they do not yet support demand paging.
SIMD processors are also similar to vector processor
 e multiple SIMD 
processors in GPUs act as independent MIMD cores, just as many vector computers 

have multiple vector processor
 is view would consider the Fermi GTX 580 as 
a 16-core machine with hardware support for multithreading, where each core has 

16 la
 e bigg
 erence is multithreading, which is fundamental to GPUs 
and missing from most vector processors.
GPUs and CPUs do not go back in computer architecture genealogy to a 
common ancestor; there is no Missing Link that explains both. As a result of this 

uncommon heritage, GPUs have not used  the terms common in the computer 

architecture community, which has led to confusion about what GPUs are and 

how they work. To help resolve the confusion, 
Figure 6.12
 (fro
  to right) lists 
the more descriptive term used in this section, the closest term from mainstream 

computing, the o
  cial NVIDIA GPU term in case you are interested, and then 
a short description of the ter
 is “GPU Rosetta Stone” may help relate this 
section and ideas to more conventional GPU descriptions, such as those found in 
 Appendix C
.While GPUs are moving toward mainstream computing, they can
t abandon 
their responsibility to continue to excel at grap
 us, the design of GPUs may 
FeatureMulticore with SIMDGPU
SIMD processorsSIMD lanes/processor
Multithreading hardware support for SIMD threads
Largest cache size
Size of memory address
Size of main memory
Memory protection at level of page
Demand pagingCache coherent4 to 88 to 168 to 1616 to 322 to 42 to 48 MiB0.75 MiB8 GiB to 256 GiB4 GiB to 6 GiB
64-bit64-bitYes
Yes
NoNoYes

Yes
FIGURE 6.11 Similarities and differences between multicore with Multimedia SIMD 
extensions and recent GPUs. 6.6 Introduction to Graphics Processing Units 
529
530 Chapter 6 Parallel Processors from Client to Cloud
make more sense when architects ask, given the hardware invested to do graphics 
well, how can we supplement it to improve the performance of a wider range of 

applications?
Having covered tw
 erent styles of MIMD that have a shared address 
space, we next introduce parallel processors where each processor has its 

own private address space, which makes it much easier to build much larger 

system
 e Internet services that you use every day depend on these large scale 
systems.
TypeMore descriptivenameVectorizable
LoopBody ofVectorized LoopBody of a
(Strip-Mined)
Vectorized LoopThread BlockSequence of
SIMD LaneOperationsOne iteration ofa Scalar LoopCUDA ThreadA Thread of
SIMDInstructionsThread of VectorInstructionsWarpSIMDInstructionVector InstructionPTX InstructionMultithreadedSIMDProcessor(Multithreaded)
Vector ProcessorStreamingMultiprocessorThread Block
SchedulerScalar ProcessorGiga Thread
EngineSIMD Thread
SchedulerThread scheduler
in a Multithreaded
CPUWarp SchedulerSIMD LaneVector laneThread Processor
GPU MemoryMain MemoryGlobal Memory
Local MemoryLocal MemoryShared Memory
SIMD LaneRegistersVector Lane
RegistersThread Processor
RegistersA vectorized loop executed on a multithreaded
SIMD Processor, made up of one or more threads
of SIMD instructions. They can communicate via
Local Memory.Program abstractionsMachine objectProcessing hardwareMemory hardwareA vertical cut of a thread of SIMD instructions
corresponding to one element executed by one
SIMD Lane. Result is stored depending on maskand predicate register.A traditional thread, but it contains just SIMDinstructions that are executed on a multithreaded
SIMD Processor. Results stored depending on a
per-element mask.A single SIMD instruction executed across SIMDLanes.A multithreaded SIMD Processor executesthreads of SIMD instructions, independent of
other SIMD Processors.Assigns multiple Thread Blocks (bodies ofvectorized loop) to multithreaded SIMDProcessors.Hardware unit that schedules and issues threadsof SIMD instructions when they are ready to
execute; includes a scoreboard to track SIMD
Thread execution.A SIMD Lane executes the operations in a thread
of SIMD instructions on a single element. Results
stored depending on mask.DRAM memory accessible by all multithreaded
SIMD Processors in a GPU.Fast local SRAM for one multithreaded SIMDProcessor, unavailable to other SIMD Processors.Registers in a single SIMD Lane allocated acrossa full thread block (body of vectorized loop).Vectorizable LoopGrid
A vectorizable loop, executed on the GPU, made
up of one or more Thread Blocks (bodies of
vectorized loop) that can execute in parallel.Closest old term
outside of GPUsOfficial CUDA/
NVIDIA GPU termBook definitionFIGURE 6.12 Quick guide to GPU terms. 
We use th
 rst column for hardware terms. Four groups 
cluster these 12 terms. From top to bottom: Program Abstractions, Machine Objects, Processing Hardware, 
and Memory Hardware.

Elaboration: While the GPU was introduced as having a separate memory from the 
CPU, both AMD and Intel have announced “fused” products that combine GPUs and 
CPUs to share a single memory. The challenge will be to maintain the high bandwidth 

memory in a fused architecture that has been a foundation of GPUs.
True or false: GPUs rely on graphics DRAM chips to reduce memory latency and 
thereby increase performance on graphics applications.
 6.7 Clusters, Warehouse Scale Computers, 
and Other Message-Passing 
Multiprocessors
 e alternative approach to sharing an address space is for the processors to 
each have their own private physical address space. 
Figure 6.13
 shows the classic 
organization of a multiprocessor with multiple private addr
 is alternative multiprocessor must communicate via explicit 
message passing
, which traditionally is the name of such style of computers. Provided the system 
has routines to 
send
 and 
receive messages
, coordination is built in with message 
passing, since one processor knows when a message is sent, and the receiving 
processor knows when a message arrives. If the sender needs co
 rmation that the 
message has arrived, the receiving processor can then send an acknowledgment 

message back to the sender.
 ere have been several attempts to build large-scale computers based on 
high-performance message-passing networks, and they do o
 er better absolute 
Check Yourself
message passing
 Communicating between 

multiple processors by 

explicitly sending and 

receiving information.
send message routine
 A routine used by a 

processor in machines 

with private memories to 

pass a message to another 

processor.
receive message routine
 A routine used by a 

processor in machines 

with private memories 

to accept a message from 

another processor.
CacheCacheCacheMemoryMemoryMemoryInterconnection Network. . .. . .ProcessorProcessorProcessor. . .FIGURE 6.13 Classic organization of a multiprocessor with multiple private address spaces, traditionally called a message-passing multiprocessor.
 Note that unlike the SMP in 
Figure 6.7
, the interconnection network is not between the caches and memory but is instead between 
processor-memory nodes.
 6.7 Clusters, Warehouse Scale Computers, and Other Message-Passing Multiprocessors 
531
532 Chapter 6 Parallel Processors from Client to Cloud
communication performance than clusters built using local area networks. Indeed, 
many supercomputers today use custom networ e problem is that they are 

much more expensive than local area networks like Ethernet. Few applications today 

outside of high performance computing can justify the higher communication 

performance, given the much higher costs. 
Computers that rely on message passing for communication rather than cache 
coherent shared memory are much easier for hardware designers to build (see 

Sectio ere is an advantage for programmers as well, in that communication 

is explicit, which means there are fewer performance surprises than with the implicit 

communication in cache-coherent shared memory computer
 e downside 
for programmers is that it
s harder to port a sequential program to a message-
passing computer, since every communication must be iden
 ed in advance or 
the program doesn
t work. Cache-coherent shared memory allows the hardware to 
 gure out what data needs to be communicated, which makes porting easier
 ere 
ar
 erences of opinion as to which is the shortest path to high performance, 
given the pros and cons of implicit communication, but there is no confusion in the 

marketplace today. Multicore microprocessors use shared physical memory and 

nodes of a cluster communicate with each other using message passing.
Some concurrent applications run well on parallel hardware, independent of 
whether it o
 ers shared addresses or message passing. In particular, task-level 
parallelism and applications with little communication
like Web search, mail 
servers, an
 le servers
do not require shared addressing to run well. As a result, 
clusters
 have become the most widespread example today of the message-passing 
parallel computer. Given the separate memories, each node of a cluster runs a 
distinct copy of the operating system. In
 contrast, the cores inside a microprocessor 
are connected using a high-speed network inside the chip, and a multichip shared-

memory system uses the memory interconnect for communicatio
 e memory 
interconnect has higher bandwidth and lower latency, allowing much better 

communication performance for shared memory multiprocessors. 
 e weakness of separate memories for user memory from a parallel programming 
perspective turns into a strength in system dependability (see Section 5.5). Since a 

cluster consists of independent computers co
nnected through a local area network, it 
is much easier to replace a computer without bringing down the system in a cluster 

than in an shared memory multiprocessor. Fundamentally, the shared address means 

that it is di
  cult to isolate a processor and replace it without heroic work by the 
operating system and in the physical design of the server. It is also easy for clusters 

to scale down gracefully when a server fails, thereby improving 
dependability
. Since 
the cluster so
 ware is a layer that runs on top of the local operating systems running 
on each computer, it is much easier to disconnect and replace a broken computer.
Hardware/
Software 
Interfaceclusters
 Collections of 
computers connected 
via I/O over standard 

network switches to 

form a message-passing 

multiprocessor.

Given that clusters are constructed from whole computers and independent, 
scalable networks, this isolation also makes it easier to expand the system without 
bringing down the application that runs on top of the cluster.
 eir lower cost, higher availability, and rapid, incremental expandability make 
clusters attractive to service Internet providers, despite their poorer communication 

performance when compared to large-scale shared memory multiprocessor
 e search engines that hundreds of millions of us use every day depend upon this 

technology. Amazon, Facebook, Google, Microso
 , and others all have multiple 
datacenters each with clusters of tens of thousands of servers. Clearly, the use of 

multiple processors in Internet service companies has been hugely successful.
Warehouse-Scale Computers
Internet services, such as those described above, necessitated the construction 

of new buildings to house, power, and cool 100,000 servers. Although they may 

be cl
 ed as just large clusters, their architecture and operation are more 
sophisticated.
 ey act as one giant computer and cost on the order of $150M 
for the building, the electrical and cooling infrastructure, the servers, and the 

networking equipment that connects and houses 50,000 to 100,000 servers. We 

consider them a new class of computer, called 
Warehouse-Scale Computers 
(WSC). 
 e most popular framework for batch processing in a WSC is MapReduce [Dean, 
2008] and its open-source twin Hadoop. Inspired by the Lisp functions of the same 

name, Ma
 rst applies a programmer-supplied function to each logical input 
record. Map runs on thousands of servers to produce an intermediate result of key-

value pairs. Reduce collects the output of those distributed tasks and collapses them 

using another programmer
 ned function. With appropriate so
 ware support, 
both are highly parallel yet easy to understand and to use. Within 30 minutes, a 

novice programmer can run a MapReduce task on thousands of servers.
For example, one MapReduce program calculates the number of occurrences of 
every English word in a large collection of documents. Below is a simp
 ed version 
of that program, which shows just the inner loop and assumes just one occurrence 

of all English words found in a document:
Hardware/

Software 

Interface 6.7 Clusters, Warehouse Scale Computers, and Other Message-Passing Multiprocessors 
533Anyone can build a fast 
CPU
 e trick is to build a 
fast system.
Seymour Cray, considered 
the father of the 

supercomputer.
map(String key, String value):      // key: document name
     // value: document contents 
     for each word w in value:EmitIntermediate(w, “1”); // Produce list of all words reduce(String key, Iterator values):// key: a word 
// values: a list of counts 
     int result = 0; 
     for each v in values:
     result += ParseInt(v); // get integer from key-value pair
     Emit(AsString(result));
534 Chapter 6 Parallel Processors from Client to Cloud
 e function 
EmitIntermediate used in the Map function emits each 
word in the document and the value one
 en the Reduce function sums all the 
values per word for each document using 
ParseInt() to get the number of 
occurrences per word in all documen
 e MapReduce runtime environment 
schedules map tasks and reduce tasks to the servers of a WSC.
At this extreme scale, which requires innovation in power distribution, cooling, 
monitoring, and operations, the WSC is a modern descendant of the 1970s 
supercomputers—making Seymour Cray the godfather of today’s WSC architects. 

His extreme computers handled computations
 that could be done nowhere else, but 
were so expensive that only a few companies could a
 ord them. 
 is time the target 
is providing information technology for the world instead of high performance 

computing for scientists and engineers. Hence, WSCs surely play a more important 

societal role today than Cray’s supercomputers did in the past.
While they share some common goals with servers, WSCs have three major 
distinctions:
1. Ample, easy parallelism
: A concern for a server architect is whether the 
applications in the targeted marketplace have enough parallelism to justify 

the amount of parallel hardware and whether the cost is too high fo
  cient 
communication hardware to exploit this parallelism. A WSC architect has 

no such concern. First, batch applications like MapReduce b
 t from the 
large number of independent data sets 
that need independent processing, 
such as billions of Web pages from a Web crawl. Second, interactive Internet 

service applications, also known as 
So
 ware as a Service (SaaS)
, can b
 t from millions of independent users of interactive Internet services. Reads 
and writes are rarely dependent in SaaS, so SaaS rarely needs to synchronize. 

For example, search uses a read-only index and email is normally reading 

and writing independent information. We call this type of easy parallelism 

Request-Level Parallelism
, as many independen
 orts can proceed in 
parallel naturally with little need for communication or synchronization.
2. Operational Costs Count
: Traditionally, server architects design their systems 
for peak performance within a cost budget and worry about energy only to 

make sure they don’t exceed the cooling capacity of their enclosure
 ey 
usually ignored operational costs of a server, assuming that they pale in 

comparison to purchase costs. WSC have longer lifetimes—the building and 

electrical and cooling infrastructure are o
 en amortized over 10 or more 
years—so the operational costs add up: energy, power distribution, and 

cooling represent more than 30% of the costs of a WSC over 10 years.
3. Scale and the Opportunities/Problems Associated with Scale
: To construct a 
single WSC, you must purchase 100,000 servers along with the supporting 

infrastructure, which means volume discounts. Hence, WSCs are so massive 
so
 ware as a service 
(SaaS)
 Rather than 
selling so
 ware that 
is installed and run 
on customers’ own 

computers, so
 ware is run 
at a remote site and made 

available over the Internet 

typically via a Web 

interface to customers. 

SaaS customers are 

charged based on use 

versus on ownership.

internally that you get economy of scale even if there are not many WSCs. 
 ese economies of scale led to 
cloud computing
, as the lower per unit costs 
of a WSC meant that cloud companies could rent servers at a pro
 table rate 
and still be below what it costs outsiders to do it themselv
 e 
 ip side 
of the economic opportunity of scale is the need to cope with the failure 

frequency of scale. Even if a server had a Mean Time To Failure of an amazing 

25 years (200,000 hours), the WSC architect would need to design for 5 

server failures every day. Section 5.15 mentioned annualized disk failure rate 

(AFR) was measured at Google at 2% to 4%. If there were 4 disks per server 

and their annual failure rate was 2%, the WSC architect should expect to see 

one disk fail every 
hour
 us, fault tolerance is even more important for the 
WSC architect than the server architect.
 e economies of scale uncovered by WSC have realized the long dreamed of 
goal of computing as a utility. Cloud computing means anyone anywhere with good 

ideas, a business model, and a credit card 
can tap thousands of servers to deliver 
their vision almost instantly around the world. Of course, there are important 

obstacles that could limit the growth of cloud computing—such as security, 

privacy, standards, and the rate of growth of Internet bandwidth—but we foresee 

them being addressed so that WSCs and cloud computing ca
 ourish.
To put the growth rate of cloud computing into perspective, in 2012 Amazon 
Web Services announced that it adds enough new server capacity 
every day
 to 
support all of Amazon’s global infrastructure as of 2003, when Amazon was a 

$5.2Bn annual revenue enterprise with 6000 employees.
Now that we understand the importance of message-passing multiprocessors, 
especially for cloud computing, we next cover ways to connect the nodes of a WSC 

together
 anks to 
Moore’s Law
 and the increasing number of cores per chip, we 
now need networks inside a chip as well, so these topologies are important in the 

small as well as in the large.
Elaboration: The MapReduce framewor es and sorts the key-value pairs at the 
end of the  Map phase to produce groups that all share the same key.  These groups are 
then passed to the Reduce phase.Elaboration: Another form of large scale computing is 
grid computing, where the 
computers are spread across large areas, and then the programs that run across them 

must communicate via long haul networks. The most popular and unique form of grid 

computing was pioneered by the SETI@home project. As millions of PCs are idle at 

any one time doing nothing useful, they could be harvested and put to good uses if 

someone developed software that could run on those computers and then gave each PC 

an independent piece of the problem to wor rst example was the Search for 

ExtraTerrestrial Intelligence
 (SETI), which was launched at UC Berkeley in 1999. Over 5 

million computer users in more than 200 countries have signed up for SETI@home, with 

more than 50% outside the US. By the end of 2011, the average performance of the 

SETI@home grid was 3.5 PetaFLOPS.
 6.7 Clusters, Warehouse Scale Computers, and Other Message-Passing Multiprocessors 
535
536 Chapter 6 Parallel Processors from Client to Cloud
1. True or false: Like SMPs, message-passing computers rely on locks for 
synchronization.
2. True or false: Clusters have separate memories and thus need many copies of 
the operating system.
 6.8 Introduction to Multiprocessor Network 
Topologies
Multicore chips require on-chip networks to connect cores together, and clusters 
require local area networks to connect servers together
 is section reviews the 
pros and cons o
 erent interconnection network topologies.
Network costs include the number of switches, the number of links on a switch 
to connect to the network, the width (number of bits) per link, and length of the 

links when the network is mapped into silicon. For example, some cores or servers  

may be adjacent and others may be on the other side of the chip or the other side of 

the datacenter. Network performance is multifaceted as well. It includes the latency 

on an unloaded network to send and receive a message, the throughput in terms of 

the maximum number of messages that can be transmitted in a given time period, 

delays caused by contention for a portion of the network, and variable performance 

depending on the pattern of communication. Another obligation of the network 

may be fault tolerance, since systems may be required to operate in the presence 

of broken components. Finally, in this era of energy-limited systems, the energy 

  ciency of 
 erent organizations may trump other concerns.
Networks are normally drawn as graphs, with each edge of the graph representing 
a link of the communication network. In th
 gures in this section, the processor-
memory node is shown as a black square and the switch is shown as a colored 

circle. We assume here that all links are 
bidirectional;
 that is, information ca
 ow 
in either direction. All networks consist of 
switches
 whose links go to processor-
memory nodes and to other switch
 e 
 rst network connects a sequence of 
nodes together:
 is topology is called a 
ring
. Since some nodes are not directly connected, some 
messages will have to hop along intermediate nodes until they arrive at th
 nal 
destination.
Unlike a bus—a shared set of wires that allows broadcasting to all connected 
devices—a ring is capable of many simultaneous transfers. 
Check Yourself

Because there are numerous topologies to choose from, performance metrics 
are needed to distinguish these designs. Two are popular
 e 
 rst is 
total 
network 
bandwidth
, which is the bandwidth of each link multiplied by the number of links. 
 is represents the peak bandwidth. For the ring network above, with 
P processors, 
the total network bandwidth would be 
P times the bandwidth of one link; the total 
network bandwidth of a bus is just the bandwidth of that bus.
To balance this best bandwidth case, we include another metric that is closer to 
the worst case: the 
bisection bandwidth
 is metric is calculated by dividing the 
machine into two halv
 en you sum the bandwidth of the links that cross that 
imaginary dividing line
 e bisection bandwidth of a ring is two times the link 
bandwidth. It is one times the link bandwidth for the bus. If a single link is as fast 
as the bus, the ring is only twice as fast as a bus in the worst case, but it is 
P times faster in the best case.
Since some network topologies are not symmetric, the question arises 
of where to draw the imaginary line when bisecting the machine. Bisection 

bandwidth is a worst-case metric, so the answer is to choose the division that 

yields the most pessimistic network performance. Stated alternatively, calculate 

all possible bisection bandwidths and pick the smallest. We take this pessimistic 

view because parallel programs are o
 en limited by the weakest link in the 
communication chain.
At the other extreme from a ring is a 
fully connected network
, where every 
processor has a bidirectional link to every other processor. For fully connected 
networks, the total network bandwidth is 
P ×(P – 1)/2, and the bisection bandwidth 
is (P/2)2. e tremendous improvement in performa
nce of fully connected networks is 
 set by the tremendous increas
 is consequence inspires engineers 
to invent new topologies that are between the cost of rings and the performance 

of fully connected networ
 e evaluation of success depends in large part on 
the nature of the communication in the workload of parallel programs run on the 

computer.
 e number o
 erent topologies that have been discussed in publications 
would be
  cult to count, but only a few have been used in commercial parallel 
processors. 
Figure 6.14
 illustrates two of the popular topologies. 
An alternative to placing a processor at every node in a network is to leave only 
the switch at some of these no
 e switches are smaller than processor-memory-
switch nodes, and thus may be packed more densely, thereby lessening distance and 

increasing performance. Such networks are frequently called 
multistage networks
 to re
 ect the multiple steps that a message may travel. Types of multistage networks 
are as numerous as single-stage networks; 
Figure 6.15
 illustrates two of the popular 

multistage organizations. A 
fully connected
 or 
crossbar network
 allows any 
node to communicate with any other node in one pass through the network. An 
Omega network
 uses less hardware than the crossbar network (2
n log2 n versus 
n2 switches), but contention can occur between messages, depending on the pattern 
network 
bandwidth
 Informally, 
the peak transfer rate of a 
network; can refer to the 

speed of a single link or 

the collective transfer rate 

of all links in the network.
bisection 
bandwidth
 e bandwidth between 
two equal parts of 

a multiprocessor. 

 is measure is for a 
worst case split of the 

multiprocessor.
fully connected 
network
 A network 
that connects processor-
memory nodes by 

supplying a dedicated 

communication link 

between every node.
multistage network
 A network that supplies a 

small switch at each node.
crossbar network
 A network that allows 

any node to communicate 

with any other node in 

one pass through the 

network.
 6.8 Introduction to Multiprocessor Network Topologies 
537
538 Chapter 6 Parallel Processors from Client to Cloud
of communication. For example, the Omega network in 
Figure 6.15
 cannot send a 
message from P
0 to P
6 at the same time that it sends a message from P
1 to P
4.Implementing Network Topologies
 is simple analysis of all the networks in this section ignores important practical 
considerations in the construction of a networ
 e distance of each link a
 ects 
the cost of communicating at a high clock rate
generally, the longer the distance, 
the more expensive it is to run at a high clock rate. Shorter distances also make 

it easier to assign more wires to the link, as the power to drive many wires is less 

if the wires are short. Shorter wires are also cheaper than longer wires. Another 

practical limitation is that the three-dimensional drawings must be mapped onto 

chips that are essentially two-dimensio
 e 
 nal concern is energy. 
Energy concerns may force multicore chips to rely on simple grid topologies, for 

example
 e bottom line is that topologies that appear elegant when sketched on 
the blackboard may be impractical when constructed in silicon or in a datacenter.
Now that we understand the importance of clusters and have seen topologies 
that we can follow to connect them together, we next look at the hardware and 

so
 ware of the interface of the network to the processor. 
True or false: For a ring with P nodes, the ratio of the total network bandwidth to 

the bisection bandwidth is P/2.
Check Yourself
a. 2-D grid or mesh of 16 nodesb. 
n-cube tree of 8 nodes (8 = 23 so n = 3)FIGURE 6.14 Network topologies that have appeared in commercial parallel processors.
  e colored circles represent switches and the black squares represent processor-memory nodes. Even 
though a switch has many links, generally only one goes to the processor
 e Boolean n-cube topology is 
an n-dimensional interconnect with 2n nodes, requiring n links per switch (plus one for the processor) and 
thus n nearest-neighbor nodes. Frequently, these basic topologies have been supplemented with extra arcs to 

improve performance and reliability.

 5.96.9 Communicating to the Outside World: 
Cluster Networking
 is online section describes the networking hardware and so
 ware used to 
connect the nodes of a cluster together
 e example is 10 gigabit/second Ethernet 
connected to the computer  using 
Peripheral Component Interconnect Express 
(PCIe). It shows both so
 ware and hardware optimizations how to improve 
network performance, including zero copy messaging, user space communication, 
using polling instead of I/O interrupts, and hardware calculation of checksums. 

While the example is networking, the techniques in this section apply to storage 

controllers and other I/O devices as well. 
a. Crossbarb. Omega networkc. Omega network switch boxCDA
BP0P1P2P3P4P5P6P7P0P1P2P3P4P5P6P7FIGURE 6.15 Popular multistage network topologies for eight nodes.
  e switches in these 
drawings are simpler than in earlier drawings because the links are unidirectional; data comes in at th
  and exits out the righ
 e switch box in c can pass A to C and B to D or B to C and A to D
 e crossbar 
uses n
2 switches, where n is the number of processors, while the Omega network uses 2n log
2n of the large 
switch boxes, each of which is logically composed of 
four of the smaller switches. In this case, the crossbar 
uses 64 switches versus 12 switch boxes, or 48 switches, in the Omega networ
 e crossbar, however, can 
support any combination of messages between processors, while the Omega network cannot.
 6.9 Communicating to the Outside World: Cluster Networking 
539
  Communicating to the Outside World: 
Cluster Networking
 is online section describes the networking hardware and so
 ware used to 
connect the nodes of cluster together. As there are whole books and courses just on 
networking, this section only introduces the main terms and concepts. While our 

example is networking, the techniques we describe apply to storage controllers and 

other I/O devices as well.
Ethernet has dominated local area networks for decades, so it is not surprising 
that clusters primarily rely on Ethernet as the cluster interconnect. It became 

commercially popular at 10 Megabits per second link speed in the 1980s, but 

today 1 Gigabit per second Ethernet is standard and 10 Gigabit per second is being 

deployed in datacenters. Figure 6.9.1 shows a network interface card (NIC) for 10 

Gigabit Ethernet.
Computers o
 er high-speed links to plug in fast I/O devices like this NIC. While 
there used to be separate chips to connect the microprocessor to the memory and 

high-speed I/O devices, thanks to 
Moore’s Law
 these functions have been absorbed 
into the main chip in recent o
 erings like Intel’s Sandy Bridge. A popular high-
speed link today is 
PCIe
, which stands for 
Peripheral Component Interconnect 
Express
. It is called a 
link in that the basic building block, called a 
serial lane
, consists of just four wires: two for receiving data and two for transmitting data. 

 is small number contrasts with an earlier version of PCI that consisted of 64 
5.96.9FIGURE 6.9.1 The NetFPGA 10-Gigabit Ethernet card
 (see http://netfpga.org/), which 
connects up to four 10-Gigabit/sec Ethernet links. It is an FPGA-based open platform for 
network research and classroom experimentation.
 e DMA engine and the four “MAC chips” 
in Figure 6.9.2 are just portions of the Xilinx Virtex FPGA in the middle of the board
 e four PHY chips 
in Figure 6.9.2 are the four black squares just to the right of the four white rectangles on th
  edge of the 
board, which is where the Ethernet cables are plugged in.

 6.9 Communicating to the Outside World: Cluster Networking 
6.9-3wires, which was called a 
parallel bus
. PCIe allows anywhere from 1 to 32 lanes to 
be used to connect to I/O devices, depending on i
 is NIC uses PCI 1.1, 
so each lane transfers at 2 Gigabits/second.
 e NIC in Figure 6.9.1 connects to the host computer over an 8-lane PCIe link, 
which o
 ers 16 Gigabits/second in both directions. To communicate, a NIC must 
both send or transmit messages and receive them, o
 en abbreviated as TX and 
RX, respectively. For this NIC, each 10G link uses separate transmit and receive 
queues, each of which can store two full-length Ethernet packets, used between 

the Ethernet links and the NIC. Figure 6.9.2 is a block diagram of the NIC showing 

the TX and RX q
 e NIC also has two 32-entry queues for transmitting and 
receiving between the host computer and the NIC.
To give a command to the NIC, the processor must be able to address the device 
and to supply one or more command words. In 
memory-mapped I/O
, portions of 
the address space are assigned to I/O devices. 
During initialization (at boot time), 
PCIe devices can request to be assigned an address region of a sp
 ed length. 
All subsequent processor reads and writes to that address region are forwarded 

over PCIe to that device. Reads and writes to those addresses are interpreted as 

commands to the I/O device.
For example, a write operation can be used to send data to the network interface 
where the data will be interpreted as a command. When the processor issues the 

address and data, the memory system ignores the operation because the address 

indicates a portion of the memory space used for I/O
 e NIC, however, sees the 
operation and records the data. User programs are prevented from issuing I/O 

operations directly, because the OS does not provide access to the address space 

assigned to the I/O devices, and thus the addresses are protected by the address 

translation. Memory-mapped I/O can also be used to transmit data by writing or 

reading to select address
 e device uses the address to determine the type of 
command, and the data may be provided by a write or obtained by a read. In any 

event, the address encodes both the device identity and the type of transmission 

between processor and device.
memory-mapped 
I/O An I/O scheme in 
which portions of the 
address space are assigned 

to I/O devices, and reads 

and writes to those 

addresses are interpreted 

as commands to the I/O 

device.
PCIeTXRXDMAMAC
MAC
MAC
MAC
PHYPHYPHYPHYPort 0
Port 1

Port 2

Port 3
ControlDataFIGURE 6.9.2 Block diagram of the NetFPGA Ethernet card in Figure 6.9.1 showing the 
control paths and the data paths. e control path allows the DMA engine to read the status of the 
queues, such as empty vs. on-empty, and the content of the next available queue entry
 e DMA engine also 
controls port multiplexin
 e data path simply passes through the DMA block to the TX/RX queues or 
to main memory
 e “MAC chips” are described below
 e PHY chips, which refer to the physical layer, 
connect the “MAC chips” to physical networking medium, such as copper wire or opt
 ber.

6.9-4 6.9 Communicating to the Outside World: Cluster Networking
While the processor could transfer the data from the user space into the I/O 
space by itself, the overhead for transferring data from or to a high-speed network 
could be intolerable, since it could consume a large fraction of the processor
 us, 
computer designers long ago invented a mechanism for o
  oading the processor and 
having the device controller transfer data directly to or from the memory without 

involving the processor
 is mechanism is called 
direct memory access
 (DMA).
DMA is implemented with a specialized controller that transfers data between 
the network interface and memory independent of the processor, and in this case 

the DMA engine is inside the NIC. 
To notify the operating system (and eventually the application that will receive 
the packet) that a transfer is complete, the DMA sends an 
I/O interrupt
.An I/O interrupt is just like the exceptions we saw in Chapters 4 and 5, with two 
important distinctions:
1. An I/O interrupt is asynchronous with respect to the instruction execution. 
 at is, the interrupt is not associated with any instruction and does not 
prevent the instruction completion, so it is ver
 erent from either page fault 
exceptions or exceptions such as arithmetic over
 ow. Our control unit needs 
only check for a pending I/O interrupt at the time it starts a new instruction.
2. In addition to the fact that an I/O interrupt has occurred, we would like to 
convey further information, such as the identity of the device generating 

the interrupt. Furthermore, the interrupts represent devices that may have 

 erent priorities and whose interrupt requests hav
 erent urgencies 
associated with them.
To communicate information to the processor, such as the identity of the device 

raising the interrupt, a system can use either vectored interrupts or an exception 

iden
 cation register, called the Cause register in MIPS (see Section 4.9). When 
the processor recognizes the interrupt, the device can send either the vector 

address or a stat
 eld to place in the Cause register. As a result, when the OS 
gets control, it knows the identity of the device that caused the interrupt and can 

immediately interrogate the device. An interrupt mechanism eliminates the need 

for the processor to keep checking the device and instead allows the processor to 

focus on executing programs.
The Role of the Operating System in Networking
 e operating system acts as the interface between the hardware and the program 
that requests I/O
 e network responsibilities of the operating system arise from 
three characteristics of networks:
1. Multiple programs using the processor share the network.
2. Networks o
 en use interrupts to communicate information about the 
operations. Because interrupts cause a transfer to kernel or supervisor mode, 
they must be handled by the operating system (OS).
direct memory access 
(DMA)
 A mechanism 
that provides a device 
controller with the ability 

to transfer data directly 

to or from the memory 

without involving the 

processor.
interrupt-driven 
I/O An I/O scheme that 
employs interrupts to 
indicate to the processor 

that an I/O device needs 

attention.

 6.9 Communicating to the Outside World: Cluster Networking 6.9
-5 e low-level control of an network is complex, because it requires managing 
a set of concurrent events and because the requirements for correct device 
control are o
 en very detailed.
 ese three characteristics of networks sp
 cally and I/O systems in general lead 
to sev
 erent functions the OS must provide:
 e OS guarantees that a user’s program accesses only the portions of an I/O 
device to which the user has rights. For example, the OS must not allow a 

program to read or writ
 le on disk if the owner of th
 le has not granted 
access to this program. In a system with shared I/O devices, protection could 

not be provided if user programs could perform I/O directly.
 e OS provides abstractions for accessing devices by supplying routines 
that handle low-level device operations.
 e OS handles the interrupts generated by I/O devices, just as it handles the 
exceptions generated by a program.
 e OS tries to provide equitable access to the shared I/O resources, as well 
as schedule accesses to enhance system throughput.
 e so
 ware inside the operating system that interfaces to a sp
 c I/O device 
like this NIC is called a 
device driver
 e driver for this NIC follo
 ve steps 
when transmitting or receiving a message. Figure 6.9.3 shows the relationship of 

these steps as an Ethernet packet is sent from one node of the cluster and received 

by another node in the cluster. 
First, the transmit steps:
 e driv
 rst prepares a packet bu
 er in host memory. It copies a packet 
from the user address space into a b
 er that it allocates in the operating 
system address space.
2. Next, it “talks” to the NI
 e driver writes an 
I/O descriptor
 to the 
appropriate NIC register that gives the address of the b
 er and its length.
 e DMA in the NIC next copies the outgoing Ethernet packet from the host 
bu
 er over PCIe.
4. When the transmission is complete, the DMA interrupts the processor to 
notify the processor that the packet has been successfully transmitted.
5. Finally, the driver de-allocates the transmit bu
 er.
Hardware/ 
Software 

Interfacedevice driver
 A program 
that controls an I/O device 
that is attached to the 

computer.

6.9-6 6.9 Communicating to the Outside World: Cluster Networking
Next, the receive steps:
1. First, the driver prepares a packet b
 er in host memory, allocating a new 
bu
 er in which to place the received packet.
2. Next, it “talks” to the NI
 e driver writes an I/O descriptor to the 
appropriate NIC register that gives the address of the b
 er and its length.
 e DMA in the NIC next copies the incoming Ethernet packet over PCIe 
into the allocated host b
 er. 
4. When the transmission is complete, the DMA interrupts the processor to 
notify the host of the newly received packet and its size.
5. Finally, the driver copies the received packet into the user address space.
As you can see in Figure 6.9.3, th
 rst three steps are time critical when transmitting 
a packet (since the last two occur a
 er the packet is sent), and the last three steps 
are time critical when receiving a packet (since th
 rst two occur before a packet 
arrives). However, these non-critical steps must be completed before individual 
nodes run out of resources, such as memory space. Failure to do so negatively 

 ects network performance.
SourceStep 1Step 2Step 3Step 3NICCPURAMStep 2Step 1Step 4Step 5DestinationEthernet
Step 4Step 5RAMCPUNICPCIePCIeFIGURE 6.9.3 Relationship of the ﬁ ve steps of the driver when transmitting an Ethernet 
packet from one node and receiving that packet on another node.
 6.9 Communicating to the Outside World: Cluster Networking 
6.9-7Improving Network Performance
 e importance of networking in clusters means it is certainly worthwhile to try to 
improve performance. We show both so
 ware and hardware techniques.
Starting with so
 ware optimizations, one performance target is reducing the 
number of times the packet is copied, which you may have noticed happening 
repeatedly in th
 ve steps of the driver above.
 e zero-copy
 optimization allows 
the DMA engine to get the message directly from the user program data space 

during transmission and be placed where the user wants it when the message is 

received, rather than go through intermediary bu
 ers in the operating system 
along the way.
A second so
 ware optimization is to cut out the operating system almost entirely 
by moving the communication into the user address space. By not invoking the 

operating system and not causing a context switch, we can reduce the so
 ware 
overhead considerably. 
In this more radical scenario, a third step would be to drop interrupts. One 
reason is that modern processors normally go into lower power mode while 

waiting for an interrupt, and it takes time to come out of low power to service the 

interrupt as well for the disruption to the pipeline, which increases latency
 e alternative to interrupts is for the processor to periodically check status bits to see 

if I/O operation is complete, which is called 
polling
. Hence, we can require the user 
program to poll the NIC continuously to see when the DMA unit has delivered a 

message, an
 ect the processor does not go into low power mode.
Looking at hardware optimizations, one potential target for improvement is 
in calculating the values of th
 elds of the Ethernet pack
 e 48-bit Ethernet 
address, called the 
Media Access Control address
 or 
MAC address
, is a unique 
number assigned to each Ethernet NIC. To improve performance, the “MAC 

chip”—actually just a portion of the FPGA on this NIC—calculates the value for 

the preamb
 elds and the CR
 eld (see Sectio
 e driv
  with 
placing the MAC destination address, MAC source address, message type, the 

data payload, and padding if needed. (Ethernet requires that the minimum packet, 

including the header and CR
 elds but not the preamble, be 64 bytes.) Note that 
even the least expensive Ethernet NICs do CRC calculation in hardware today.
A second hardware optimization, available on the most recent Intel processors 
such as Ivy Bridge, improves the performance of the NIC with respect to the memory 

hierarchy. 
Direct Data IO (DDIO)
 allowing up to 10% of the last level cache is used 
as a fast scratchpad for the DMA engine. Data is copied directly into the last level 

cache rather than to DRAM by the DMA, and only written to DRAM upon eviction 

from the cache.
 is optimization helps with latency, but also with bandwidth; some 
memory regions used for control might be written by the NIC repeatedly, and these 

writes no longer need to go to DR
 us, DDIO o
 ers be
 ts similar to those of 
a write back cache versus a write through cache (Chapter 5).
Let’s look at an object store that follows a client-server architecture and uses most 
of the optimizations above: zero copy messaging, user space communication, polling 

instead of interrupts, and hardware calculation of preamble and CR
 e driver 
polling
 e process of 
periodically checking the 
status of an I/O device 

to determine the need to 

service the device.

6.9-8 6.9 Communicating to the Outside World: Cluster Networking
operates in user address space as a library that the application invokes. It grants this 
application exclusive and direct access to the NIC. All of the I/O register space on the 

NIC is mapped into the application, and all of the driver state is kept in the application. 

 e OS kernel doesn’t even see the NIC as such, which avoids the overheads of context 
switching, the standard kernel network so
 ware stack, and interrupts. 
Figure 6.9.4 shows the time to send an object from one node to another. It varies 
from about 9.5 to 12.5 microseconds, depending on the size of the object. Here is 

the time for each step in microseconds:
0.7 – for the client “driver” (library) to make the request (Driver TX in Figure 6.9.4).
6.4 to 8.7 – for the NIC hardware to transmit the client’s request over the PCIe bus 
to the Ethernet, depending on the size of the object (NIC TX).
0.02 – to send object over the 10 G Ethernet (Time of Fligh
 e time o
 ight 
is limited by speed of light to 5 ns per meter
 e three-meter cables used in this 
measurement mean the time o
 ight is 15 ns, which is too small to be clearly 
visible in th
 gure.
02468101214064128192
256
3203844485125766407047688328969601024108811521216128013441408Latency (microseconds)
Object Size (B)
Driver RX
NIC RX
Time of Flight
NIC TX
Driver TX
FIGURE 6.9.4 Time to send an object broken into transmit driver and NIC hardware time 
vs. receive driver and NIC hardware time.
 NIC transmit time is much larger than the NIC receive 
time because transmit requires more PCIe round-tri
 e NIC does PCIe reads to read the descriptor and 
data, but on receive the NIC does PCIe writes of data, length of data, and interrupt. PCIe reads incur a round 
trip latency because NIC waits for the reply, but PCIe writes require no response because PCIe is reliable, so 

PCIe writes can be sent back-to-back.

 6.9 Communicating to the Outside World: Cluster Networking 
6.9-91.8 to 2.5 – for the NIC hardware to receive the object, depen
ding on its size (NIC 
RX).
0.6 – for the server “driver” to transmit the message with the requested object to 
the app (Driver RX).
Now that we have seen how to measure the performance of network at a low 
level of detail, let’s raise the perspective to see how to benchmark multiprocessors 
of all kinds with much higher level programs.
Elaboration: There are three versions of PCIe. This NIC uses PCIe 1.1, which transfers 
at 2 gigabits per second per lane, so this NIC transfers at up to 16 gigabits per second 
in each direction. PCIe 2.0, which is found on most PC motherboards today, doubles 

the lane bandwidth to 4 gigabits per second. PCIe 3.0 doubles again to 8 gigabits per 

second, and it is starting to be found on some motherboards. We applaud the standard 

committee’s logical rate of bandwidth improvement, which has been about 2
version number 
gigabits/second. The limitations of the Virtex 5 FPGA prevented the NIC from using 

faster versions of PCIe.
Elaboration: While Ethernet is the foundation of cluster communication, clusters 
commonly use higher-level protocols for reliable communication. Transmission Control 

Protocol and Internet Protocol (TCP/IP), although invented for planet-wide communication, 

is often used inside a warehouse scale computer, due in part to its dependability. While 

IP makes no deliver guarantees in the protocol, TCP does. The sender keeps the packet 

sent until it gets the acknowledgment message back that it was received correctly from 

the receiver. The receiver knows that the message was not corrupted along the way, by 

 eld. To ensure that IP delivers to the right 
destination, the IP header includes a checksum to make sure the destination number 

remains unchanged. The success of the Internet is due in large part to the elegance 

and popularity of TCP/IP, which allows independent local area networks to communicate 
dependably. Given its importance in the Internet and in clusters, many have accelerated 

TCP/IP, using techniques like those listed in this section [Regnier, 2004].
Elaboration: Adding DMA is another path to the memory system—one that does not 
go through the address translation mechanism or the cache hierarchy. This difference 

generates some problems both in virtual memory and in caches. These problems are 

usually solved with a combination of hardware techniques and software support. The 

 culties in having DMA in a virtual memory system arise because pages have both 

a physical and a virtual address. DMA also creates problems for systems with caches, 

because there can be two copies of a data item: one in the cache and one in memory. 

Because the DMA issues memory requests directly to the memory rather than through 

the processor cache, the value of a memory location seen by the DMA unit and the 

processor may differ. Consider a read from a NIC that the DMA unit places directly 

into memory. If some of the locations into which the DMA writes are in the cache, the 

processor will receive the old value when it does a read. Similarly, if the cache is write-

back, the DMA may read a value directly from memory when a newer value is in the 

6.9-10 6.9 Communicating to the Outside World: Cluster Networking
cache, and the value has not been written back. This is called the 
stale data problem or coherence problem (see Chapter 5). Similar solutions for coherence are used with DMA.Elaboration: Virtual Machine support clearly can negatively impact networking 
performance. As a result, microprocessor designers have been adding hardware 
to reduce the performance overhead of virtual machines for networking in particular 

and I/O in general. Intel offers 
Virtualization Technology for Directed I/O
 (VT-d
) to help virtualize I/O. It is an I/O memory management unit that enables guest virtual machines 

to directly use I/O devices, such as Ethernet. It supports 
DMA remapping
, which allows 
the DMA to read or write the data directly in the I/O buffers of the guest virtual machine, 

rather than into the host I/O buffers and then copy them into the guest I/O buffers. It 

also supports 
interrupt remapping
, which lets the virtual machine monitor route interrupt 
requests directly to the proper virtual machine.
Two options for networking are using interrupts or polling, and using DMA or 
using the processor via load and store instructions.
1. If we want the lowest latency for small packets, which combination is likely 
best?
2. If we want the lowest latency for large packets, which combination is likely 
best?
Check Yourself

540 Chapter 6 Parallel Processors from Client to Cloud
 er covering the performance of network at a low level of detail in this online 
section, the next section shows how to benchmark multiprocessors of all kinds 
with much higher-level programs.
 6.10 Multiprocessor Benchmarks and 
Performance Models
As we saw in Chapter 1, benchmarking systems is always a sensitive topic, because 

it is a highly visible way to try to determine which system is better e results a
 ect not only the sales of commercial systems, but also the reputation of the designers 

of those systems. Hence, all participants want to win the competition, but they also 

want to be sure that if someone else wins, they deserve to win because they have 

a genuinely better syst
 is desire leads to rules to ensure that the benchmark 
results are not simply engineering tricks for that benchmark, but are instead 

advances that improve performance of real applications.
To avoid possible tricks, a typical rule is that you can
t change the benchmark. 
 e source code and data sets ar
 xed, and there is a single proper answer. Any 
deviation from those rules makes the results invalid.
Many multiprocessor benchmarks follow these traditions. A common exception 
is to be able to increase the size of the problem so that you can run the benchmark 

on systems with a widel
 erent number of processor
 at is, many benchmarks 
allow weak scaling rather than require strong scaling, even though you must take 

care when comparing results for programs runnin
 erent problem sizes.
Figure 6.16
 gives a summary of several parallel benchmarks, also described below:
 Linpack
 is a collection of linear algebra routines, and the routines for 
performing Gaussian elimination constitute what is known as the Linpack 

benchmar
 e DGEMM routine in the example on page 215 represents a 
small fraction of the source code of the Linpack benchmark, but it accounts 

for most of the execution time for the benchmark. It allows weak scaling, 

letting the user pick any size problem. Moreover, it allows the user to rewrite 

Linpack in almost any form and in any language, as long as it computes the 

proper result and performs the same number o
 oating point operations 
for a given problem size. Twice a year, the 500 computers with the fastest 

Linpack performance are published at www.top500.org
 e 
 rst on this list 
is considered by the press to be the world
s fastest computer.
 SPECrate
 is a throughput metric based on the SPEC CPU benchmarks, 
such as SPEC CPU 2006 (see Chapter 1). Rather than report performance 

of the individual programs, SPECrate runs many copies of the program 

simultaneously.
 us, it measures task-level parallelism, as there is no 

communication between the tasks. You can run as many copies of the 
programs as you want, so this is again a form of weak scaling.
 SPLASH
 and 
SPLASH 2
 (Stanford Parallel Applications for Shared Memory) 
were e
 orts by researchers at Stanford University in the 1990s to put together 
a parallel benchmark suite similar in goals to the SPEC CPU benchmark 

suite. It includes both kernels and applications, including many from the 

high-performance computing community
 is benchmark requires strong 
scaling, although it comes with two data sets.
BenchmarkScaling?Reprogram?
DescriptionLinpackWeak
YesDense matrix linear algebra [Dongarra, 1979]
SPECrateWeak
NoIndependent job parallelism [Henning, 2007]
Stanford Parallel 
Applications for Shared Memory 
SPLASH 2 [Woo 
et al., 1995]
Strong  (although  offers  
two problem sizes)NoComplex 1D FFTBlocked LU Decomposition

Blocked Sparse Cholesky Factorization

Integer Radix Sort

Barnes-Hut

Adaptive Fast Multipole

Ocean Simulation
Hierarchical Radiosity
Ray Tracer

Volume Renderer

Water Simulation with Spatial Data Structure
Water Simulation without Spatial Data Structure
NAS Parallel 
Benchmarks 
[Bailey et al., 
1991]Weak
Yes  
(C or  Fortran only)
EP: embarrassingly parallel
MG: simpliÞed multigridCG: unstructured grid for a conjugate gradient method
FT: 3-D partial differential equation solution using FFTs  
IS: large integer sort
PARSEC 
Benchmark Suite 
[Bienia et al., 
2008]Weak
NoBlackscholesÑOption pricing with Black-Scholes PDEBodytrackÑBody tracking of a person
CannealÑSimulated cache-aware annealing to optimize routing
DedupÑNext-generation compression with data deduplication
FacesimÑSimulates the motions of a human face

FerretÑContent similarity search server

FluidanimateÑFluid dynamics for animation with SPH methodFreqmineÑFrequent itemset mining
StreamclusterÑOnline clustering of an input streamSwaptionsÑPricing of a portfolio of swaptions
VipsÑImage processing
x264ÑH.264 video encodingBerkeley  
Design  Patterns 
[Asanovic et al., 
2006]Strong or  Weak
Yes
Finite-State Machine
Combinational LogicGraph Traversal
Structured Grid

Dense Matrix
Sparse Matrix
Spectral Methods (FFT)Dynamic ProgrammingN-BodyMapReduce
Backtrack/Branch and BoundGraphical Model InferenceUnstructured Grid
FIGURE 6.16 Examples of parallel benchmarks.
 6.10 Multiprocessor Benchmarks and Performance Models 
541
542 Chapter 6 Parallel Processors from Client to Cloud
 e NAS (NASA Advanced Supercomputing) parallel benchmarks
 were 
another attempt from the 1990s to benchmark multiprocessors. Taken from 
computatio
 uid dynamics, they consist of
 ve ker
 ey allow weak 
scaling by
 ning a few data sets. Like Linpack, these benchmarks can be 
rewritten, but the rules require that the programming language can only be C 

or Fortran.
 e recent 
PARSEC (Princeton Application Repository for Shared Memory 
Computers) benchmark suite
 consists of multithreaded programs that use 
Pthreads
 (POSIX threads) and OpenMP (Open MultiProcessing; see 
Sectio
 ey focus on emerging computational domains and consist of 
nine applications and three kernels. Eight rely on data parallelism, three rely 

on pipelined parallelism, and on
e on unstructured parallelism.
 On the cloud front, the goal of the 
Yahoo! Cloud Serving Benchmark
 (YCSB) 
is to compare performance of cloud data services. It o
 ers a framework that 
makes it easy for a client to benchmark new data services, using Cassandra 

and HBase as representative examples. [Cooper, 2010]
 e downside of such traditional restrictions to benchmarks is that innovation is 
c
 y limited to the architecture and compiler. Better data structures, algorithms, 
programming languages, and so on o
 en cannot be used, since that would give a 
misleading resul
 e system could win because of, say, the algorithm, and not 
because of the hardware or the compiler.
While these guidelines are understandable when the foundations of computing 
are relatively stable
as they were in the 1990s and th
 rst half of this decade
they are undesirable during a programming revolution. For this revolution to 

succeed, we need to encourage
 innovation at all levels.
Researchers at the University of California at Berkeley have advocated one 
approac
 ey iden
 ed 13 design patterns that they claim will be part of 
applications of the future. Frameworks or kernels implement these design 

patterns. Examples are sparse matrices, structured gr
 nite-state machines, 
map reduce, and graph traversal. By keeping th
 nitions at a high level, they 
hope to encourage innovations at any level of the syst
 us, the system with the 
fastest sparse matrix solver is welcome to use any data structure, algorithm, and 

programming language, in addition to novel architectures and compilers. 
Performance Models
A topic related to benchmarks is performance models. As we have seen with the 
increasing architectural diversity in this chapter—multithreading, SIMD, GPUs—

it would be especially helpful if we had a simple model that o
 ered insights into the 
performance of
 erent architectures. It need not be perfect, just insightful.
 e 3Cs for cache performance from Chapter 5 is an example performance 
model. It is not a perfect performance model, since it ignores potentially important 
Pthreads
 A UNIX 
API for creating and 
manipulating threads. It is 

structured as a library.

factors like block size, block allocation policy, and block replacement policy. 
Moreover, it has quirks. For example, a miss can be ascribed due to capacity in one 

design and to a co
 ict miss in another cache of the same size. Yet 3Cs model has 
been popular for 25 years, because it o
 ers insight into the behavior of programs, 
helping both architects and programmers improve their creations based on insights 

from that model.
To
 nd such a model for parallel computers, let
s start with small kernels, 
like those from the 13 Berkeley design patterns in 
Figure 6.16
. While there are 

versions with
 erent data types for these ker
 oating point is popular in 
several implementations. Hence, pe
 oating-point performance is a limit on the 
speed of such kernels on a given computer. For multicore chips, pe
 oating-point 
performance is the collective peak performance of all the cores on the chip. If there 

were multiple microprocessors in the system, you would multiply the peak per chip 

by the total number of chips.
 e demands on the memory system can be estimated by dividing this peak 
 oating-point performance by the average number o
 oating-point operations per 
byte accessed:
 FloatingPoint Operations/Sec
FloatingPoint Operations/By
--tte=Bytes/Sec
 e ratio of
 oating-point operations per byte of memory accessed is called the 
arithmetic intensity
. It can be calculated by taking the total number of
 oating-
point operations for a program divided by the total number of data bytes transferred 
to main memory during program execution. 
Figure 6.17
 shows the arithmetic 

intensity of several of the Berkeley design patterns from 
Figure 6.16
.arithmetic intensity
  e ratio of
 oating-
point operations in a 
program to the number 

of data bytes accessed by 

a program from main 

memory.
A r i t h m e t i c   I n t e n s i t y O(N) O(log(N)) O(1) SparseMatrix
(SpMV)Structured
Grids
(Stencils,
PDEs)Structured
Grids
(Lattice
Methods)Spectral
Methods
(FFTs)Dense
Matrix
(BLAS3)N-body
(Particle
Methods)FIGURE 6.17 Arithmetic intensity, speciﬁ ed as the number of ﬂ oat-point operations to 
run the program divided by the number of bytes accessed in main memory [Williams, 
Waterman, and Patterson 2009]. 
Some kernels have an arithmetic intensity that scales with problem 

size, such as Dense Matrix, but there are many kernels with arithmetic intensities independent of problem 

size. For kernels in this former case, weak scaling can lead t
 erent results, since it puts much less demand 
on the memory system.
 6.10 Multiprocessor Benchmarks and Performance Models 
543
544 Chapter 6 Parallel Processors from Client to Cloud
The Rooﬂ ine Model
 is simple mo
 oating-point performance, arithmetic intensity, and memory 
performance together in a two-dimensional graph [Williams, Waterman, and 
Patterson 2009]. Pe
 oating-point performance can be found using the hardware 
sp
 cations mentioned above.
 e working sets of the kernels we consider here 
do no
 t in on-chip caches, so peak memory performance may b
 ned by the 
memory system behind the caches. One way to
 nd the peak memory performance 
is the Stream benchmark. (See the 
Elaboration
 on page 381 in Chapter 5).
Figure 6.18
 shows the model, which is done once for a computer, not for each 
ker
 e vertical Y-axis is achievab
 oating-point performance from 0.5 to 
64.0 GFLOPs/second
 e horizontal X-axis is arithmetic intensity, varying from 
1/8 FLOPs/DRAM byte accessed to 16 FLOPs/DRAM byte accessed. Note that the 

graph is a log-log scale.
For a given kernel, we ca
 nd a point on the X-axis based on its arithmetic 
intensity. If we draw a vertical line through that point, the performance of the kernel 

on that computer must lie somewhere along that line. We can plot a horizontal line 

showing peak
 oating-point performance of the computer. Obviously, the actual 
 oating-point performance can be no higher than the horizontal line, since that is 
a hardware limit.
Arithmetic Intensity: FLOPs/Byte RatioAttainable GFLOPs/second0.51.02.04.08.016.0
32.0
64.01/81/41/2124816
peak floating-point performancepeak memory BW (stream)Kernel 1(Memory
Bandwidth
limited) Kernel 2
(Computation
limited)FIGURE 6.18 Rooﬂ ine Model [Williams, Waterman, and Patterson 2009]. 
 is example has a 
peak
 oating-point performance of 16 GFLOPS/sec and a peak memory bandwidth of 16 GB/sec from the 
Stream benchmark. (Since Stream is actually four measurements, this line is the average of the four
 e dotted vertical line in color on th
  represents Kernel 1, which has an arithmetic intensity of 0.5 FLOPs/
byte. It is limited by memory bandwidth to no more than 8 GFLOPS/sec on this Optero
 e dotted 
vertical line to the right represents Kernel 2, which has an arithmetic intensity of 4 FLOPs/byte. It is limited 
only computationally to 16 GFLO
 is data is based on the AMD Opteron X2 (Revision F) using dual 
cores running at 2 GHz in a dual socket system.)

How could we plot the peak memory performance, which is measured in bytes/
second? Since the X-axis is FLOPs/byte and the Y-axis FLOPs/second, bytes/second 
is just a diagonal line at a 45-degree angle in t
 gure. Hence, we can plot a third 
line that gives the maxim
 oating-point performance that the memory system 
of that computer can support for a given arithmetic intensity. We can express the 

limits as a formula to plot the line in the graph in 
Figure 6.18
: Attainable GFLOP
s/sec=Mi
n (Peak Memory BWA
rithmetic Inte
×nnsity, Peak
FloatingPoint Performance)- e horizontal and diagonal lines give this simple model its name and indicate its 
value.
 e ro
 ine sets an upper bound on performance of a kernel depending on 
its arithmetic intensity. Given a roo
 ine of a computer, you can apply it repeatedly, 
since it doesn
t vary by kernel. 
If we think of arithmetic intensity as a pole that hits the roof, either it hits 
the slanted part of the roof, which means performance is ultimately limited by 
memory bandwidth, or it hits th
 at part of the roof, which means performance is 
computationally limited. In 
Figure 6.18
, kernel 1 is an example of the former, and 

kernel 2 is an example of the latter. 
Note that the 
ridge point,
 where the diagonal and horizontal roofs meet, o
 ers 
an interesting insight into the computer. If it is far to the right, then only kernels 

with very high arithmetic intensity can achieve the maximum performance of 

that computer. If it is far to th
 , then almost any kernel can potentially hit the 
maximum performance. 
Comparing Two Generations of Opterons
 e AMD Opteron X4 (Barcelona) with four cores is the successor to the Opteron 
X2 with two cores. To simplify board design, they use the same socket. Hence, they 

have the same DRAM channels and thus the same peak memory bandwidth. In 

addition to doubling the number of cores, the Opteron X4 also has twice the peak 

 oating-point performance per core: Opteron X4 cores can issue tw
 oating-point 
SSE2 instructions per clock cycle, while Opteron X2 cores issue at most one. As the 

two systems we
re comparing have similar clock rates
2.2 GHz for Opteron X2 
versus 2.3 GHz for Opteron X4
the Opteron X4 has about four times the peak 
 oating-point performance of the Opteron X2 with the same DRAM bandwidth. 
 e Opteron X4 also has a 2MiB L3 cache, which is not found in the Opteron X2.
In 
Figure 6.19
 the roo
 ine models for both systems are compared. As we would 
expect, the ridge point moves to the right, from 1 in the Opteron X2 to 5 in the 

Opteron X4. Hence, to see a performance gain in the next generation, kernels need 

an arithmetic intensity higher than 1, or their working sets mu
 t in the caches 
of the Opteron X4.
 e roo
 ine model gives an upper bound to performance. Suppose your 
program is far below that bound. What optimizations should you perform, and in 

what order?
 6.10 Multiprocessor Benchmarks and Performance Models 
545
546 Chapter 6 Parallel Processors from Client to Cloud
To reduce computational bottlenecks, the following two optimizations can help 
almost any kernel:
1. Floating-point operation mix
. Pe
 oating-point performance for a computer 
typically requires an equal number of nearly simultaneous additions and 
multiplication
 at balance is necessary either because the computer 
supports a fused multiply-add instruction (see the 
Elaboration
 on page 220 
in Chapter 3) or because th
 oating-point unit has an equal number of 
 oating-point adders an
 oating-point multiplier
 e best performance 
also requires tha
 cant fraction of the instructio
 oating-
point operations and not integer instructions.
2. Improve 
instruction-level parallelism
 and apply SIMD
. For modern archi-
tectures, the highest performance comes when fetching, executing, and 

committing three to four instructions per clock cycle (see Sectio
 e goal for this step is to improve the code from the compiler to increase ILP. One 

way is by unrolling loops, as we saw in Section 4.12. For the x86 architectures, 

a single AVX instruction can operate on four double precision operands, so 

they should be used whenever possible (see Sections 3.7 and 3.8).
To reduce memory bottlenecks, the following two optimizations can help:
1. So
 ware prefetching
. Usually the highest performance requires keeping many 
memory operation
 ight, which is easier to do by performing 
predicting
 accesses via so ware prefetch instructions rather than waiting until the data 
is required by the computation.
Actual FLOPbyte ratioAttainable GFLOP/s128.064.032.0
16.08.04.02.01.00.51/81/41/216842
1Opteron X4 (Barcelona)Opteron X2FIGURE 6.19 Rooﬂ ine models of two generations of Opterons. 
 e Opteron X2 roo
 ine, which 
is the same as in 
Figure 6.18
, is in black, and the Opteron X4 roo
 ine is in color
 e bigger ridge point of 
Opteron X4 means that kernels that were computationally bound on the Opteron X2 could be memory-
performance bound on the Opteron X4.

2. Memory a
  nity.
 Microprocessors today include a memory controller on 
the same chip with the microprocessor, which improves performance of the 
memory hierarchy
. If the system has multiple chips, this means that some 
addresses go to the DRAM that is local to one chip, and the rest require 

accesses over the chip interconnect to access the DRAM that is local to 

another chip
 is split results in non-uniform memory accesses, which we 
described in Section 6.5. Accessing memory through another chip lowers 

performance.
 is second optimization tries to allocate data and the threads 
tasked to operate on that data to the same memory-processor pair, so that 

the processors rarely have to access the memory of the other chips.
 e roo
 ine model can help decide which of these two optimizations to 
perform and the order in which to perform them. We can think of each of these 

optimizations as a 
ceiling
 below the appropriate roo
 ine, meaning that you 
cannot break through a ceiling without performing the associated optimization.
 e computational roo
 ine can be found from the manuals, and the memory 
ro
 ine can be found from running the Stream benchmar
 e computational 
ceilings, suc
 oating-point balance, can also come from the manuals for 
that computer. A memory ceiling, such as memory a
  nity, requires running 
experiments on each computer to determine the gap between th
 e good 
news is that this process only need be done once per computer, for once someone 

characterizes a computer
s ceilings, everyone can use the results to prioritize their 

optimizations for that computer.
Figure 6.20
 adds ceilings to the roo
 ine model in 
Figure 6.18
, showing the 
computational ceilings in the top graph and the memory bandwidth ceilings on the 

bottom graph. Although the higher ceilings are not labeled with both optimizations, 

they are implied in t gure; to break through the highest ceiling, you need to 

have already broken through all the ones below.
 e width of the gap between the ceiling and the next higher limit is the reward 
for trying that optimizatio
 us, 
Figure 6.20
 suggests that optimization 2, which 
improves ILP, has a large b
 t for improving computation on that computer, and 
optimization 4, which improves memory a
  nity, has a large b
 t for improving 
memory bandwidth on that computer.
Figure 6.21
 combines the ceilings of 
Figure 6.20
 into a single grap
 e arithmetic intensity of a kernel determines the optimization region, which in turn 

suggests which optimizations to try. Note that the computational optimizations 

and the memory bandwidth optimizations overlap for much of the arithmetic 

intensity
 ree regions ar
 erently in 
Figure 6.21
 to indicate th
 erent 
optimization strategies. For example, Kernel 2 falls in the blue trapezoid on the 

right, which suggests working only on the computational optimizations. Kernel 1 

falls in the blue-gray parallelogram in the middle, which suggests trying both types 

of optimizations. Moreover, it suggests starting with optimizations 2 and 4. Note 

that the Kernel 1 vertical lines fall below th
 oating-point imbalance optimization, 
so optimization 1 may be unnecessary. If a kernel fell in the gray triangle on the 

low
 , it would suggest trying just memory optimizations.
 6.10 Multiprocessor Benchmarks and Performance Models 
547
548 Chapter 6 Parallel Processors from Client to Cloud
0.51.02.04.08.016.032.064.01/81/41/2124816
peak floating-point performance1. Fl. Pt. imbalance2. Without ILP or SIMDAMD Opteronpeak memory BW (stream)Arithmetic Intensity: FLOPs/Byte RatioAttainable GFLOPs/second0.51.02.04.08.016.0
32.0
64.01/81/41/2124816
AMD Opteronpeak memory BW (stream)Arithmetic Intensity: FLOPs/Byte RatioAttainable GFLOPs/second3. w/out SW prefetching4. w/out Memory Affinitypeak floating-point performanceFIGURE 6.20 Rooﬂ ine model with ceilings. 
 e top graph shows the computational “ceilings” of 
8 GFLOPs/sec if th
 oating-point operation mix is imbalanced and 2 GFLOPs/sec if the optimizations to 
increase ILP and SIMD are also missin
 e bottom graph shows the memory bandwidth ceilings of 11 GB/
sec without so
 ware prefetching and 4.8 GB/sec if memory a
  nity optimizations are also missing.

 us far, we have been assuming that the arithmetic intensit
 xed, but that is 
not really the case. First, there are kernels where the arithmetic intensity increases 
with problem size, such as for Dense Matrix and N-body problems (see 
Figure 6.17
). 
Indeed, this can be a reason that programmers have more success with weak scaling 

than with strong scaling. Second, th
 ectiveness of the 
memory hierarchy
  ects the number of accesses that go to memory, so optimizations that improve 
cache performance also improve arithmetic intensity. One example is improving 

temporal locality by unrolling loops and then grouping together statements with 

similar addresses. Many computers have special cache instructions that allocate 

data in a cache but do no
 rst 
 ll the data from memory at that address, since it 
will soon be over-written. Both these optimizations reduce memory tra
  c, thereby 
moving the arithmetic intensity pole to the right by a factor of, say
 is 
  right could put the ker
 erent optimization region.
While the examples above show how to help programmers improve performance, 
architects can also use the model to decide where they should optimize hardware to 

improve performance of the kernels that they think will be important.
 e next section uses the roo
 ine model to demonstrate the performance 
 erence between a multicore microprocessor and a GPU and to see whether 
thes
 erences r
 ect performance of real programs.
0.51.02.04.08.016.032.064.0124816
peak memory BW (stream) Arithmetic Intensity: FLOPs/Byte Ratio Attainable GFLOPs/second Kernel 1Kernel 2
2. Without ILP or SIMD4. w/out Memory Affinity1. Fl. Pt. imbalance3. w/out SW prefetchingpeak floating-point performance1/81/41/2FIGURE 6.21 Rooﬂ ine model with ceilings, overlapping areas shaded, and the two kernels 
from Figure 6.18
. Kernels whose arithmetic intensity land in the blue trapezoid on the right should focus 
on computation optimizations, and kernels whose arithmetic intensity land in the gray triangle in the lower 

  should focus on memory bandwidth optimization
 ose that land in the blue-gray parallelogram in 
the middle need to worry about both. As Kernel 1 falls in the parallelogram in the middle, try optimizing 

ILP and SIMD, memory a
  nity, and so
 ware prefetching. Kernel 2 falls in the trapezoid on the right, so try 
optimizing ILP and SIMD and the balance o
 oating-point operations.
 6.10 Multiprocessor Benchmarks and Performance Models 
549
550 Chapter 6 Parallel Processors from Client to Cloud
Elaboration: The ceilings are ordered so that lower ceilings are easier to optimize. 
Clearly, a programmer can optimize in any order, but following this sequence reduces the 
chances of wasting effor t due to other constraints. 

Like the 3Cs model, ine model delivers on insights, a model can 

have assumptions that may prove optimistic. For example, ine assumes the load is 

balanced between all processors.
Elaboration: An alternative to the Stream benchmark is to use the raw DRAM 
band ine. While the raw band nitely is a hard upper bound, 

actual memory performance is often so far from that boundary that it
s not that useful. 
That is, no program can go close to that bound. The downside to using Stream is that 

very careful programming may exceed the Stream results, so the memor ine may 

 ine. We stick with Stream because few 

programmers will be able to deliver more memory bandwidth than Stream discovers.
Elaboration:  ine model shown is for multicore processors, it clearly 
would work for a uniprocessor as well.
True or fals
 e main drawback with conventional approaches to benchmarks 
for parallel computers is that the rules that ensure fairness also slow so
 ware 
innovation.
 6.11 Real Stuff: Benchmarking and Rooﬂ ines 
of the Intel Core i7 960 and the NVIDIA Tesla GPU
A group of Intel researchers published a paper [Lee et al., 2010] comparing a 
quad-core Intel Core i7 960 with multimedia SIMD extensions to the previous 

generation GPU, the NVIDIA Tesla GTX 280. 
Figure 6.22
 lists the characteristics 

of the two systems. Both products were purchased in F
 e Core i7 is 
in Intel
s 45-nanometer semiconductor technology while the GPU is in TSMC
s 65-nanometer technology. Although it might have been fairer to have a comparison 

by a neutral party or by both interested parties, the purpose of this section is 
not to 
determine how much faster one product is than another, but to try to understand 

the relative value of features of these two contrasting architecture styles. 
 e roo
 ines of the Core i7 960 and GTX 280 in 
Figure 6.23
 illustrate the 
 erences in the computers. Not only does the GTX 280 have much higher 
memory bandwidth and double-precisio
 oating-point performance, but also its 
double-precision ridge point is considerably to th
 . 
 e double-precision ridge 
point is 0.6 for the GTX 280 versus 3.1 for the Core i7. As mentioned above, it is 

much easier to hit peak computational performance the further the ridge point of 
Check Yourself

the roo
 ine is to th . For single-precision performance, the ridge point moves 
far to the right for both computers, so it
s much harder to hit the roof of single-
precision performance. Note that the arithmetic intensity of the kernel is based on 

the bytes that go to main memory, not the bytes that go to cache memory
 us, as mentioned above, caching can change the arithmetic intensity of a kernel on a 

particular computer, if most references really go to the cache. Note also that this 

bandwidth is for unit-stride accesses in both architectures. Real gather-scatter 

addresses can be slower on the GTX 280 and on the Core i7, as we shall see.
 e researchers selected the benchmark programs by analyzing the computational 
and memory characteristics of four recently proposed benchmark suites and then 

formulated the set of 
throughput computing kernels
 that capture these characteristics.
 Figure 6.24
 shows the performance results, with larger numbers meaning faster
 e Roo
 ines help explain the relative performance in this case study.
Given that the raw performance sp
 cations of the GTX 280 vary from 2.5
 ×  slower (clock rate) to 7.5
 × faster (cores per chip) while the performance varies 
Core i7-960Number of processing elements (cores or SMs)Clock frequency (GHz)
Die size
Technology
Power (chip, not module)
Transistors
Memory brandwith (GBytes/sec)
Single-precision SIMD width
Double-precision SIMD width
Peak Single-precision scalar FLOPS (GFLOP/sec)
Peak Single-precision SIMD FLOPS (GFLOP/Sec)
(SP 1 add or multiply)
(SP 1 instruction fused multiply-adds)
(Rare SP dual issue fused multiply-add and multiply)
Peal double-precision SIMD FLOPS (GFLOP/sec)43.2263Intel 45 nm130700 M324
226102N.A.
N.A.
N.A.51301.3576TSMC 65 nm1301400 M1418
1117311 to 933(311)
(622)
(933)78151.4520TSMC 40 nm1673030 M17732
16
63515 or 1344(515)(1344)N.A.5157.50.412.2
1.6
1.0
2.0
4.4
2.0
0.5
4.63.0Ð9.1(3.0)
(6.1)
(9.1)1.53.80.442.0
1.0
1.3
4.4
5.5
8.0
8.0
2.56.6Ð13.1(6.6)(13.1)Ð10.1GTX 280GTX 480
Ratio280/i7Ratio480/i7FIGURE 6.22 Intel Core i7-960, NVIDIA GTX 280, and GTX 480 speciﬁ
 cations.  e rightmost columns show the ratios of the 
Tesla GTX 280 and the Fermi GTX 480 to Core i7. Although the case study is between the Tesla 280 and i7, we include the Fermi 4
80 to show 
its relationship to the Tesla 280 since it is described in this chapter. Note that these memory bandwidths are higher than in 
Figure 6.23
 because 
these are DRAM pin bandwidths and those in 
Figure 6.23
 are at the processors as measured by a benchmark program. (From Table 2 
in Lee 
et al. [2010].)
 6.11 Real Stuff: Benchmarking and Rooﬂ ines of the Intel Core i7 960 and the NVIDIA Tesla GPU 
551
552 Chapter 6 Parallel Processors from Client to Cloud
FIGURE 6.23 Rooﬂ ine model
 [Williams, Waterman, and Patterson 2009].
  ese roo
 ines show double-precisio
 oating-point 
performance in the top row and single-precision performance in the bottom row
 e DP FP performance ceiling is also in the bottom row 
to give perspective.)
 e Core i7 960 on th
  has a peak DP FP performance of 51.2 GFLOP/sec, a SP FP peak of 102.4 GFLOP/sec, and a 
peak memory bandwidth of 16.4 GBytes/s
 e NVIDIA GTX 280 has a DP FP peak of 78 GFLOP/sec, SP FP peak of 624 GFLOP/sec, and 
127 GBytes/sec of memory bandwidt e dashed vertical line on th
  represents an arithmetic intensity of 0.5 FLOP/byte. It is limited by 
memory bandwidth to no more than 8 DP GFLOP/sec or 8 SP GFLOP/sec on the Cor
 e dashed vertical line to the right has an arithmetic 
intensity of 4 FLOP/byte. It is limited only computationally to 51.2 DP GFLOP/sec and 102.4 SP GFLOP/sec on the Core i7 and 78 
DP GFLOP/
sec and 512 DP GFLOP/sec on the GTX 280. To hit the highest computation rate on the Core i7 you need to use all 4 cores and SSE
 instructions 
with an equal number of multiplies and adds. For the GTX 280, you need to use fused multiply-add instructions on all multithrea
ded SIMD 
processors. 
1286432168
4211286432168
421Core i7 960(Nehalem)102451225612864
32
16812Arithmetic intensity481632
1/81/41/2
12Arithmetic intensity481632
1/81/41/2
12Arithmetic intensity481632
321/81/41/2
12Arithmetic intensity4816
1/81/41/2
Core i7 960(Nehalem)NVIDIA GTX2801024512256
128643281644NVIDIA GTX280GFlop/sGFlop/sGFlop/sGFlop/s51.2 GF/sDouble PrecisionStream = 16.4 GB/sStream=127GB/s
Peak = 78GF/s
Double Precision78GF/s
Double PrecisionStream=127GB/s
624GF/s
Single PrecisionStream = 16.4 GB/s102.4 GF/sSingle Precision51.2 GF/sDouble Precision
from 2.0
 × slower (Solv) to 15.2
 × faster (GJK), the Intel researchers decided to 
 nd the reasons for th
 erences:
 Memory bandwidth
 e GPU has 4.4
 ×the memory bandwidth, which helps 
explain why LBM and SAXPY run 5.0 and 5.3
 × faster; their working sets are 
hundreds of megabytes and hence don
 t into the Core i7 cache. (So as to 
access memory intensively, they purposely did not use cache blocking as in 
Chapter 5.) Hence, the slope of the roo ines explains their performance. SpMV 

also has a large working set, but it only runs 1.9
 × faster because the double-
precisio
 oating point of the GTX 280 is only 1.5 × as faster as the Core i7. 
 Compute bandwidth
. Five of the remaining kernels are compute bound: 
SGEMM, Conv, FFT, MC, and Bila
 e GTX is faster by 3.9, 2.8, 3.0, 1.8, and 
5.7 ×, respectively
 e 
 rst three of these use single-precisio
 oating-point 
arithmetic, and GTX 280 single precision is 3 to 6
 ×faster. MC uses double 
precision, which explains why it
s only 1.8 × faster since DP performance 
is only 1.5 × faster. Bilat uses transcendental functions, which the GTX 

280 supports directly
 e Core i7 spends two-thirds of its time calculating 
transcendental functions for Bilat, so the GTX 280 is 5.7
 × faster
 is 
observation helps point out the value of hardware support for operations that 

occur in your workload: double-precisio
 oating point and perhaps even 
transcendentals.
KernelUnitsCore i7-960GTX 280
GTX 280/i7-960Million pixels/sec
SGEMMGFLOP/sec
Billion paths/secMCMillion pixels/sec
Conv
GFLOP/secFFTGBytes/secSAXPYMillion lookups/secLBMFrames/sec
SolvGFLOP/secSpMVFrames/sec
GJKMillion elements/secSort
Frames/sec
RCMillion queries/sec
SearchMillion pixels/sec
83940.8125071.416.8851034.96725055015173.95.71.82.83.05.35.00.51.915.20.81.61.81.73644751.4350021388.8426529.110201988.1902583HistBilatFIGURE 6.24 Raw and relative performance measured for the two platforms. 
In this study, 
SAXPY is just used as a measure of memory bandwidth, so the right unit is GBytes/sec and not GFLOP/sec. 
(Based on Table 3 in [Lee et al., 2010].)
 6.11 Real Stuff: Benchmarking and Rooﬂ ines of the Intel Core i7 960 and the NVIDIA Tesla GPU 
553
554 Chapter 6 Parallel Processors from Client to Cloud
 Cache ben
 ts. Ray casting
 (RC) is only 1.6
 ×faster on the GTX because 
cache blocking with the Core i7 caches prevents it from becoming memory 
bandwidth bound (see Sections 5.4 and 5.14), as it is on GPUs. Cache 

blocking can help Search, too. If the index trees are small so that they
 t in 
the cache, the Core i7 is twice as fast. Larger index trees make them memory 

bandwidth bound. Overall, the GTX 280 runs search 1.8
 × faster. Cache 
blocking also helps Sort. While most programmers wouldn
t run Sort on 
a SIMD processor, it can be written with a 1-bit Sort primitive called 
split. However, the split algorithm executes many more instructions than a scalar 

sort does. As a result, the Core i7 runs 1.25
 × as fast as the GTX 280. Note 

that caches also help other kernels on the Core i7, since cache blocking allows 

SGEMM, FFT, and SpMV to become compute bound
 is observation re-
emphasizes the importance of cache blocking optimizations in Chapter 5. 
 Gather-Scatter
 e multimedia SIMD extensions are of little help if the data are 
scattered throughout main memory; optimal performance comes only when 

accesses are to data are aligned on 16-byte boundar
 us, GJK gets little b
 t from SIMD on the Core i7. As mentioned above, GPUs o
 er gather-scatter 
addressing that is found in a vector architecture but omitted from most SIMD 

extension
 e memory controller even batches accesses to the same DRAM 
page together (see Sectio
 is combination means the GTX 280 runs GJK 
a startling 15.2
 ×as fast as the Core i7, which is larger than any single physical 
parameter in 
Figure 6.22
 is observation reinforces the importance of gather-
scatter to vector and GPU architectures that is missing from SIMD extensions.
 Synchronization
 e performance of synchronization is limited by atomic 
updates, which are responsible for 28% of the total runtime on the Core i7 

despite its having a hardware fetch-and-increment instructio
 us, Hist is only 
1.7 ×faster on the GTX 280. Solv solves a batch of independent constraints in 
a small amount of computation followed by barrier synchronizatio
 e Core 
i7 be
 ts from the atomic instructions and a memory consistency model that 
ensures the right results even if not all previous accesses to memory hierarchy 

have completed. Without the memory consistency model, the GTX 280 

version launches some batches from the system processor, which leads to the 

GTX 280 running 0.5
 × as fast as the Core
 is observation points out how 
synchronization performance can be important for some data parallel problems.
It is striking how o
 en weaknesses in the Tesla GTX 280 that were uncovered by 
kernels selected by Intel researchers were already being addressed in the successor 

architecture to Tesla: Fermi has faster double-precisio
 oating-point performance, 
faster atomic operations, and caches. It was also interesting that the gather-scatter 

support of vector architectures that predate the SIMD instructions by decades was 

so important to th
 ective usefulness of these SIMD extensions, which some had 
predicted before the compariso
 e Intel researchers noted that 6 of the 14 kernels 
would exploit SIMD better with mor
  cient gather-scatter support on the Core 
 is study certainly establishes the importance of cache blocking as well. 

Now that we seen a wide range of results of benchmarkin
 erent 
multiprocessors, let’s return to our DGEMM example to see in detail how much we 
have to change the C code to exploit multiple processors.
 6.12 Going Faster:  Multiple Processors and 
Matrix Multiply is section is th
 nal and largest step in our incremental performance journey of 
adapting DGEMM to the underlying hardware of the Intel Core i7 (Sandy Bridge).  

Each Core i7 has 8 cores, and the computer we have been using has 2 Core i7s. 

 us, we have 16 cores on which to run DGEMM. 
Figure 6.25
 shows the OpenMP version of DGEMM that utilizes those cores. 
Note that line 30 is the 
single
 line added to Figure 5.48 to make this code run on 
multiple processors: an OpenMP pragma that tells the compiler to use multiple 

threads in the outermost for loop. It tells the computer to spread the work of the 

outermost loop across all the threads. 
Figure 6.26
 plots a classic multiprocessor speedup graph, showing the 
performance improvement versus a single thread as the number of threads increase. 

 is graph makes it easy to see the challenges 
of strong scaling versus weak scaling.  
When everythin ts in th
 rst level data cache, as is the case for 32
 × 32 matrices, 
adding threads actually hurts performance
 e 16-threaded version of DGEMM 
is almost half as fast as the single-threaded version in this case. In contrast, the two 

largest matrices get a 14
 × speedup from 16 threads, and hence the classic two “up 
and to the right” lines in 
Figure 6.26
. Figure 6.27
 shows the absolute performance increase as we increase the number 
of threads from 1 to 16.  DGEMM operates now operates at 174 GLOPS for 960
 × 960 
matrices. As our unoptimized C version of DGEMM in Figure 3.21 ran this code at 

just 0.8 GFOPS, the optimizations in Chapters 3 to 6 that tailor the code to the 

underlying hardware result in a speedup of over 200 times!
Next up is our warnings of the fallacies and pitfalls of multiprocessin
 e computer architecture graveyar
 lled with parallel processing projects that have 
ignored them.
Elaboration: These results are with Turbo mode turned off. We are using a dual chip 
system in this system, so not surprisingly, we can get the full Turbo speedup (3.3/2.6 
= 1.27) with either 1 thread (only 1 core on one of the chips) or 2 threads (1 core per 
chip). As we increase the number of threads and hence the number of active cores, the 

 t of Turbo mode decreases, as there is less of the power budget to spend on the 

active cores. For 4 threads the average Turbo speedup is 1.23, for 8 it is 1.13, and for 

16 it is 1.11. 6.12 Going Faster:  Multiple Processors and Matrix Multiply 
555
556 Chapter 6 Parallel Processors from Client to Cloud
#include <x86intrin.h>#define UNROLL (4)
#define BLOCKSIZE 32
void do_block (int n, int si, int sj, int sk, 
               double *A, double *B, double *C)
{
  for ( int i = si; i < si+BLOCKSIZE; i+=UNROLL*4 )
    for ( int j = sj; j < sj+BLOCKSIZE; j++ ) {      __m256d c[4];
      for ( int x = 0; x < UNROLL; x++ ) 
        c[x] = _mm256_load_pd(C+i+x*4+j*n);
     /* c[x] = C[i][j] */
      for( int k = sk; k < sk+BLOCKSIZE; k++ )
      {
        __m256d b = _mm256_broadcast_sd(B+k+j*n);
     /* b = B[k][j] */
        for (int x = 0; x < UNROLL; x++)          c[x] = _mm256_add_pd(c[x], /* c[x]+=A[i][k]*b */
                 _mm256_mul_pd(_mm256_load_pd(A+n*k+x*4+i), b));      }      for ( int x = 0; x < UNROLL; x++ )         _mm256_store_pd(C+i+x*4+j*n, c[x]);
        /* C[i][j] = c[x] */
    }
}void dgemm (int n, double* A, double* B, double* C){
#pragma omp parallel for
  for ( int sj = 0; sj < n; sj += BLOCKSIZE )     for ( int si = 0; si < n; si += BLOCKSIZE )      for ( int sk = 0; sk < n; sk += BLOCKSIZE )        do_block(n, si, sj, sk, A, B, C);
}1
2
3
4
5
6
7
8
910
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
323334
35FIGURE 6.25 OpenMP version of DGEMM from Figure 5.48. 
Line 30 is the only OpenMP code, making 
the outermost for loop operate in parallel
 is line is the only
 erence from Figure 5.48.
Elaboration: Although the Sandy Bridge supports two hardware threads per core, we 
do not get more performance from 32 threads. The reason is that a single AVX hardware 
is shared between the two threads multiplexed onto one core, so assigning two threads 

per core actually hurts performance due to the multiplexing overhead.

Œ1234567
8
9101112
1314048Threads1216Speedup relative to 1 core960 X 960480 X 480160 X 16032 X 32FIGURE 6.26 Performance improvements relative to a single thread as the number of 
threads increase.  e most honest way to present such graphs is to make performance relative to the best 
version of a single processor program, which we did is plot is relative to the performance of the code in 
Figure 5.48 
without
 including OpenMP pragmas.
14 12 11 11 8 13 20 31 61 60 12 22 43 85 169 12 23 44 87 174 -5010015020012481
6GFLOPSThreads32x32160x160480x480960x960
FIGURE 6.27 DGEMM performance versus the number of threads for four matrix sizes. 
  e performance improvement compared unoptimized code in Figure 3.21 for th
×960 matrix with 16 
threads is an astounding 212 times faster!
 6.12 Going Faster:  Multiple Processors and Matrix Multiply 
557
558 Chapter 6 Parallel Processors from Client to Cloud
 6.13 Fallacies and Pitfalls
 e many assaults on parallel processing have uncovered numerous fallacies and 
pitfalls. We cover four here.
Fallacy: Amdahl’s Law doesn’t apply to parallel computers.
In 1987, the head of a research organization claimed that a multiprocessor machine 
had broken Amdahl’s Law. To try to understand the basis of the media reports, let
s see the quote that gave us Amdahl
s Law [1967, p. 483]:
A fairly obvious conclusion which can be drawn at this point is that th
 ort 
expended on achieving high parallel processing rates is wasted unless it is 

accompanied by achievements in sequential processing rates of very nearly the 

same magnitude.
 is statement must still be true; the neglected portion of the program must limit 
performance. One interpretation of the law leads to the following lemma: portions 

of every program must be sequential, so there must be an economic upper bound 

to the number of processors
say, 100. By showing linear speed-up with 1000 
processors, this lemma is disproved; hence the claim that Amdahl
s Law was broken.
 e approach of the researchers was just to use weak scaling: rather than going 
1000 times faster on the same data set, they computed 1000 times more work in 

comparable time. For their algorithm, the sequential portion of the program was 

constant, independent of the size of the input, and the rest was fully parallel
hence, linear speed-up with 1000 processors.
Amdahl
s Law obviously applies to parallel processors. What this research does 
point out is that one of the main uses of faster computers is to run larger problems. 

Just be sure that users really care about those problems versus being a ju
 cation 
to buying an expensive computer b
 nding a problem that just keeps lots of 
processors busy.
Fallacy: Peak performance tracks observed performance.
 e supercomputer industry once used this metric in marketing, and the fallacy 
is exacerbated with parallel machines. Not only are marketers using the nearly 

unattainable peak performance of a uniprocessor node, but also they are then 

multiplying it by the total number of processors, assuming perfect speed-up! 

Amdahl
s Law suggests ho
  cult it is to reach either peak; multiplying the two 
together multiplies the sin
 e roo
 ine model helps put peak performance in 
perspective.
Pitfall: Not developing the so
 ware to take advantage of, or optimize for, a 
multiprocessor architecture.
 ere is a long history of parallel so
 ware lagging behind on parallel hardware, 
possibly because the so
 ware problems are much harder. We give one example to 
show the subtlety of the issues, but there are many examples we could choose!
For over a decade 

prophets have voiced 

the contention that the 

organization of a single 

computer has reached 

its limits and that truly 

signi
 cant advances 
can be made only 

by interconnection 

of a multiplicity of 

computers in such a 

manner as to permit 

cooperative solution. 

…Demonstration is 

made of the continued 

validity of the single 

processor approach …
Gene Amdahl, “Validity 
of the single processor 

approach to achieving 

large scale computing 

capabilities,” Spring Joint 

Computer Conference, 

1967
One frequently encountered problem occurs when so
 ware designed for a 
uniprocessor is adapted to a multiprocessor environment. For example, the Silicon 
Graphics operating system originally protected the page table with a single lock, 

assuming that page allocation is infrequent. In a uniprocessor, this does not 

represent a performance problem. In a multiprocessor, it can become a major 

performance bottleneck for some programs. Consider a program that uses a large 

number of pages that are initialized at start-up, which UNIX does for statically 

allocated pages. Suppose the program is parallelized so that multiple processes 

allocate the pages. Because page allocation requires the use of the page table, which 

is locked whenever it is in use, even an OS kernel that allows multiple threads in the 

OS will be serialized if the processes all try to allocate their pages at once (which is 

exactly what we might expect at initialization time!).
 is page table serialization eliminat
es parallelism in initialization and has 
 cant impact on overall 
parallel performance.
 is performance bottleneck 
persists even for task-level parallelism. For example, suppose we split the parallel 

processing program apart into separate 
jobs and run them, one job per processor, 
so that there is no sharing between th
 is is exactly what one user did, 
since he reasonably believed that the performance problem was due to unintended 

sharing or interference in his application.) Unfortunately, the lock still serializes all 

the jobs
so even the independent job performance is poor.
 is pitfall indicates the kind of subtle bu
 cant performance bugs 
that can arise when so
 ware runs on multiprocessors. Like many other key 
so
 ware components, the OS algorithms and data structures must be rethought 
in a multiprocessor context. Placing locks on smaller portions of the page table 

 ectively eliminated the problem.
Fallacy: You can get good vector performance without providing memory 

bandwidth.
As we saw with the Roo
 ine model, memory bandwidth is quite important to 
all architectures. DAXPY requires 1.5 memory references p
 oating-point 
operation, and this ratio is typical of many scien
 c codes. Even if th
 oating-point 
operations took no time, a Cray-1 could not increase the DAXPY performance of 

the vector sequence used, since it was memory limited. e Cray-1 performance on 

Linpack jumped when the compiler used blocking to change the computation so 

that values could be kept in the vector register
 is approach lowered the number 
of memory references per FLOP and improved the performance by nearly a factor 

of tw
 us, the memory bandwidth on the Cray-1 beca
  cient for a loop 
that formerly required more bandwidth, which is just what the Roo
 ine model 
would predict.
 6.13 Fallacies and Pitfalls 
559
560 Chapter 6 Parallel Processors from Client to Cloud
 6.14 Concluding Remarks
 e dream of building computers by simply aggregating processors has been 
around since the earliest days of computing. Progress in building and usin
 ective 
an
  cient parallel processors, however, has been slow
 is rate of progress has 
been limited b
  cult so
 ware problems as well as by a long process of evolving 
the architecture of multiprocessors to enhance usability and improv
  ciency. 
We have discussed many of the so
 ware challenges in this chapter, including the 
  culty of writing programs that obtain good speed-up due to Amdahl
s Law
 e wide variety o
 erent architectural approaches and the limited success and short 
life of many of the parallel architectures of the past have compounded the so
 ware 
  culties. We discuss the history of the development of these multiprocessors 
in online 
 Section 6.15
. To go into even greater depth on topics in this chapter, 
see Chapter 4 of 
Computer Architecture: A Quantitative Approach, Fi
 h Edition
 for 
more on GPUs and comparisons between GPUs and CPUs and Chapter 6 for more 
on WSCs. 
As we said in Chapter 1, despite this long and checkered past, the information 
technology industry has now tied its future to parallel computing. Although it is 

easy to make the case that this e
 ort will fail like many in the past, there are reasons 
to be hopeful:
 Clearly, 
so
 ware as a service
 (SaaS) is growing in importance, and clusters 
have proven to be a very successful way to deliver such services. By providing 

redundancy at a higher-level, including geographically distributed datacenters, 

such services have delivered 24 ×
7 × 365 availability for customers around 
the world. 
 We believe that Warehouse-Scale Computers are changing the goals and 
principles of server design, just as the needs of mobile clients are changing the 

goals and principles of microprocessor design. Both are revolutionizing the 

so
 ware industry as well. Performance per dollar and performance per joule 
drive both mobile client hardware and the WSC hardware, and parallelism is 

the key to delivering on those sets of goals.
 SIMD and vector operations are a good match to multimedia applications, 
which are playing a larger role in the P
 ey share the advantage of 
being easier for the programmer than classic parallel MIMD programming 

and being more energy
  cient than MIMD. To put into perspective the 
importance of SIMD versus MIMD, 
Figure 6.28
 plots the number of cores 

for MIMD versus the number of 32-bit and 64-bit operations per clock cycle 

in SIMD mode for x86 computers over time. For x86 computers, we expect 

to see two additional cores per chip about every two years and the SIMD 

width to double about every four years. Given these assumptions, over the 

next decade the potential speed-up from SIMD parallelism is twice that of 
We are dedicating 

all of our future 

product development 

to multicore designs. 

We believe this is a 

key in
 ection point 
for the industry. …

 is is not a race. 
 is is a sea change in 
computing…”
Paul Otellini, Intel 
President, Intel 

Developers Forum, 2004

MIMD parallelism. Given th
 ectiveness of SIMD for multimedia and its 
increasing importance in the PostPC Era, that emphasis may be appropriate. 
Hence, it’s as least as important to understand SIMD parallelism as MIMD 

parallelism, even though the latter has received much more attention. 
 e use of parallel processing in domains such as scien
 c and engineering 
computation is popular
 is application domain has an almost limitless 
thirst for more computation. It also has many applications that have lots of 

natural concurrency. Once again, clusters dominate this application area. For 

example, using the 2012 Top 500 report, clusters are responsible for more 

than 80% of the 500 fastest Linpack results. 
 All desktop and server microprocessor manufacturers are building 
multiprocessors to achieve higher performance, so, unlike in the past, there 

is no easy path to higher performance for sequential applications. As we said 

earlier, sequential programs are now slow programs. Hence, programmers 

who need higher performance 
must
 parallelize their codes or write new 
parallel processing programs.
2003110100Potential parallel speedup
100020072011201520192023
MIMD*SIMD (32b)
SIMD (32b)
MIMD*SIMD (64b)
MIMDSIMD (64b)
FIGURE 6.28 Potential speed-up via parallelism from MIMD, SIMD, and both MIMD and 
SIMD over time for x86 computers. 
 is 
 gure assumes that two cores per chip for MIMD will be 
added every two years and the number of operations for SIMD will double every four years.
 6.14 Concluding Remarks 
561
562 Chapter 6 Parallel Processors from Client to Cloud
 In the past, microprocessors and 
multiprocessors were subject to 
 erent 
 nitions of success. When scaling uniprocessor performance, 
microprocessor architects were happy if single thread performance went up 
by the square root of the increased silicon are
 us, they were happy with 
sublinear performance in terms of 
resources. Multiprocessor success used 
to b
 ned as 
linear
 speed-up as a function of the number of processors, 
assuming that the cost of purchase or cost of administration of 
n processors 
was n times as much as one processor. Now that parallelism is happening on-
chip via multicore, we can use the traditional microprocessor metric of being 

successful with sublinear 
performance improvement.
 e success of just-in-time runtime co
mpilation and autotuning makes it 
feasible to think of so
 ware adapting itself to take advantage of the increasing 
number of cores per chip, which prov
 exibility that is not available when 
limited to static compilers.
 Unlike in the past, the open source movement has become a critical portion 
of the so
 ware industry
 is movement is a meritocracy, where better 
engineering solutions can win the mind share of the developers over legacy 

concerns. It also embraces innovation, inviting change to old so
 ware and 
welcoming new languages and so
 ware products. Such an open culture could 
be extremely helpful in this time of rapid change.
To motivate readers to embrace this revolution, we demonstrated the potential 
of parallelism concretely for matrix multiply on the Intel Core i7 (Sandy Bridge) in 

the Going Faster sections of Chapters 3 to 6:
 Data-level parallelism in Chapter 3 improved performance by a factor of 3.85 
by executing four 64-bit
 oating-point operations in parallel using the 256-
bit operands of the AVX instructions, demonstrating the value of SIMD.
 Instruction-level parallelism in Chapter 4 pushed performance up by another 
factor of 2.3 by unrolling loops 4 times to give the out-of-order execution 

hardware more instructions to schedule.
 Cache optimizations in Chapter 5 improved performance of matrices that 
didn’
 t into the L1 data cache by another factor of 2.0 to 2.5 by using cache 
blocking to reduce cache misses.
 read-level parallelism in this chapter improved performance of matrices 
that don’
 t into a single L1 data cache by another factor of 4 to 14 by utilizing 
all 16 cores of our multicore chips, demonstrating the value of MIMD. We 

did this by adding a single line using an OpenMP pragma.
Using the ideas in this book and tailoring the so
 ware to this computer added 
24 lines of code to DGEMM. For the matrix sizes of 32x32, 160x160, 480x480, and 

960x960, the overall performance speedup from these ideas realized in those two-

dozen lines of code is factors of 8, 39, 129, and 212!

 is parallel revolution in the hardware/so
 ware interface is perhaps the 
greatest challenge facing th
 eld in the last 60 years. You can also think of it as 
the greatest opportunity, as our Going Faster sections demonstrate
 is revolution 
will provide many new research and business prospects inside and outside the IT 
 eld, and the companies that dominate the multicore era may not be the same 
ones that dominated the uniprocesso
 er understanding the underlying 
hardware trends and learning to adapt so
 ware to them, perhaps you will be one 
of the innovators who will seize the opportunities that are certain to appear in the 

uncertain times ahead. We look forward to b
 ting from your inventions!
 5.96.15 Historical Perspective and Further 
Reading is section online gives the rich and o
 en disastrous history of multiprocessors 
over the last 50 years.
ReferencesG. Regnier, S. Makineni, R. Illikkal, R. Iyer, D. Minturn, R. Huggahalli, D. Newell
, 
L. Cline, and A. Foong. TCP onloading for data center servers. IEEE Computer,
 
37(11):48–58, 2004.B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, R. Sears. Benchmarking 
cloud serving systems with YCSB, In: Proceedings of the 1st ACM Symposium 

on Cloud computing, June 10–11, 2010, Indianapolis, Indiana, USA,  

doi:10.1145/1807128.1807152.
 6.16 Exercises6.1 First, write down a list of your daily activities that you typically do on a 
weekday. For instance, you might get out of bed, take a shower, get dressed, eat 

breakfast, dry your hair, brush your teeth. Make sure to break down your list so you 

have a minimum of 10 activities.
6.1.1 [5] <§6.2> Now consider which of these activities is already exploiting some 
form of parallelism (e.g., brushing multiple teeth at the same time, versus one at 
a time, carrying one book at a time to school, versus loading them all into your 
 6.16 Exercises 563
564 Chapter 6 Parallel Processors from Client to Cloud
backpack and then carry them “in parallel”). For each of your activities, discuss if 
they are already working in parallel, but if not, why they are not.
6.1.2 [5] <§6.2> Next, consider which of the activities could be carried out 
concurrently (e.g., eating breakfast and listening to the news). For each of your 
activities, describe which other activity could be paired with this activity.
6.1.3 [5] <§6.2> For 6.1.2, what could we change about current systems (e.g., 
showers, clothes, TVs, cars) so that we could perform more tasks in parallel?

6.1.4 [5] <§6.2> Estimate how much shorter time it would take to carry out these 
activities if you tried to carry out as many tasks in parallel as possible.
6.2 You are trying to bake 3 blueberry pound cakes. Cake ingredients are as 
follows:
1 cup butter, so
 ened1 cup sugar
4 large eggs

1 teaspoon vanilla extract

1/2 teaspoon salt

1/4 teaspoon nutmeg

1 1/2 cu
 our
1 cup blueberries
 e recipe for a single cake is as follows:
Step 1: Preheat oven to 325°F (160°C). Grease an
 our your cake pan.
Step 2: In large bowl, beat together with a mixer butter and sugar at medium 
speed until light an
 y. Add eggs, vanilla, salt and nutmeg. Beat until 
thoroughly blended. Reduce mixer speed to low an
 our, 1/2 cup at a time, 
beating just until blended.
Step 3: Gently fold in blueberries. Spread evenly in prepared baking pan. Bake 
for 60 minutes.
6.2.1 [5] <§6.2> Your job is to cook 3 cak
  ciently as possible. Assuming 
that you only have one oven large enough to hold one cake, one large bowl, one 

cake pan, and one mixer, come up with a schedule to make three cakes as quickly 

as possible. Identify the bottlenecks in completing this task.
6.2.2 [5] <§6.2> Assume now that you have three bowls, 3 cake pans and 3 mixers. 
How much faster is the process now that you have additional resources?

6.2.3 [5] <§6.2> Assume now that you have two friends that will help you cook, 
and that you have a large oven that can accommodate all three cakes. How will this 
change the schedule you arrived at in Exercise 6.2.1 above?
6.2.4 [5] <§6.2> Compare the cake-making task to computing 3 iterations 
of a loop on a parallel computer. Identify data-level parallelism and task-level 
parallelism in the cake-making loop.
6.3 Many computer applications involve searching through a set of data and 
sorting the data. A number o
  cient searching and sorting algorithms have been 
devised in order to reduce the runtime of these tedious tasks. In this problem we 
will consider how best to parallelize these tasks.
6.3.1 [10] <§6.2> Consider the following binary search algorithm (a classic divide 
and conquer algorithm) that searches for a value 
X in a sorted N-element array A 
and returns the index of matched entry:
BinarySearch(A[0..N−1], X) {low = 0high = N −1
while (low <= high) {mid = (low + high) / 2
if (A[mid] >X) high = mid −1else if (A[mid] <X) low = mid + 1else return mid // found}
return −1 // not found}Assume that you have Y cores on a multi-core processor to run BinarySearch. 
Assuming that Y is much smaller than N, express the speedup factor you might 

expect to obtain for values of Y and N. Plot these on a graph.
6.3.2 [5] <§6.2> Next, assume that Y is equal to N. How would this a
 ect your 
conclusions in your previous answer? If you were tasked with obtaining the best 
speedup factor possible (i.e., strong scaling), explain how you might change this 

code to obtain it.
6.4 Consider the following piece of C code:
for (j=2;j<1000;j++)   D[j] = D[j−1]+D[j−2];
 6.16 Exercises 565
566 Chapter 6 Parallel Processors from Client to Cloud
 e MIPS code corresponding to the above fragment is:
          addiu   $s2,$zero,7992          addiu   $s1,$zero,16
   loop:  l.d     $f0, 16($s1)
          l.d     $f2, 8($s1)
          add.d   $f4, $f0, $f2
          s.d     $f4, 0($s1)
          addiu   $s1, $s1, 8
          bne     $s1, $s2, loopInstructions have the following associated latencies (in cycles):
add.dl.ds.daddiu
4612
6.4.1 [10] <§6.2> How many cycles does it take for all instructions in a single 
iteration of the above loop to execute?

6.4.2 [10] <§6.2> When an instruction in a later iteration of a loop depends upon 
a data value produced in an earlier iteration of the same loop, we say that there is 
a loop carried dependence
 between iterations of the loop. Identify the loop-carried 
dependences in the above code. Identify the dependent program variable and 

assembly-level registers. You can ignore the loop induction variable 
j.6.4.3 [10] <§6.2> Loop unrolling was described in Chapter 4. Apply loop 
unrolling to this loop and then consider running this code on a 2-node distributed 

memory message passing system. Assume that we are going to use message passing 

as described in Section 6.7, where we introduce a new operation send (x, y) that 

sends to node x the value y, and an operation receive( ) that waits for the value being 

sent to it. Assume that send operations take a cycle to issue (i.e., later instructions 

on the same node can proceed on the next cycle), but take 10 cycles be received 

on the receiving node. Receive instructions stall execution on the node where they 

are executed until they receive a message. Produce a schedule for the two nodes 

assuming an unroll factor of 4 for the loop body (i.e., the loop body will appear 

4 times). Compute the number of cycles it will take for the loop to run on the 

message passing system.
6.4.4  e latency of the interconnect network plays a large role in 
th
  ciency of message passing systems. How fast does the interconnect need to 
be in order to obtain any speedup from using the distributed system described in 
Exercise 6.4.3?
6.5 Consider the following recursive mergesort algorithm (another classic divide 
and conquer algorithm). Mergesor
 rst described by John Von Neumann in 
 e basic idea is to divide an unsorted list 
x of 
m elements into two sublists 
of about half the size of the original list. Re
peat this operation on each sublist, and 

continue until we have lists of size 1 in lengt
 en starting with sublists of length 
1, “merge” the two sublists into a single sorted list.
Mergesort(m)var list left, right, resultreturn melsevar middle = length(m) / 2for each x in m up to middleadd x to leftfor each x in m after middleadd x to rightleft = Mergesort(left)right = Mergesort(right)
result = Merge(left, right)
return result e merge step is carried out by the following code:
Merge(left,right)var list resultwhile length(left) >0 and length(right) > 0append first(left) to resultleft = rest(left)elseappend first(right) to result
right = rest(right)if length(left) >0append rest(left) to resultif length(right) >0append rest(right) to resultreturn result6.5.1 [10] <§6.2> Assume that you have Y cores on a multicore processor to run 
MergeSort. Assuming that Y is much smaller than length(m), express the speedup 
factor you might expect to obtain for values of Y and length(m). Plot these on a 

graph.
6.5.2 [10] <§6.2> Next, assume that Y is equal to length (m). How would this 
 ect your conclusions your previous answer? If you were tasked with obtaining 
the best speedup factor possible (i.e., strong scaling), explain how you might 
change this code to obtain it.
 6.16 Exercises 567
568 Chapter 6 Parallel Processors from Client to Cloud
6.6 Matrix multiplication plays an important role in a number of applications. 
Two matrices can only be multiplied if the number of columns of th
 rst matrix is 
equal to the number of rows in the second.
Let’s assume we have an 
m × n matrix 
A and we want to multiply it by an 
n × p matrix 
B. We can express their product as an 
m × p matrix denoted by 
AB (or 
A  B). If we assign 
C = AB, and 
ci,j denotes the entry in 
C at position (
i, j), then for each 
element 
i and 
j with
im an
jp. Now we want to see if we can parallelize 
the computation of C. Assume that matrices are laid out in memory sequentially as 
follows: a
1,1, a2,1, a3,1, a4,1, …, etc.
6.6.1 [10] <§6.5> Assume that we are going to compute 
C on both a single core 
shared memory machine and a 4-core shared-memory machine. Compute the 

speedup we would expect to obtain on the 4-core machine, ignoring any memory 

issues.6.6.2 [10] <§6.5> Repeat Exercise 6.6.1, assuming that updates to 
C incur a cache 
miss due to false sharing when consecutive elements are in a row (i.e., index 
i) are 
updated.

6.6.3 [10] <§6.5> How would yo
 x the false sharing issue that can occur?
6.7 Consider the following portions of tw
 erent programs running at the 
same time on four processors in a symmetric multicore processor (SMP). Assume 
that before this code is run, both x and y are 0.
Core 1: 
x = 2;Core 2: 
y = 2;Core 3: 
w = x + y + 1;Core 4: 
z = x + y;
6.7.1 [10] <§6.5> What are all the possible resulting values of w, x, y, and z? For 
each possible outcome, explain how we might arrive at those values. You will need 
to examine all possible interleavings of instructions.
6.7.2 [5] <§6.5> How could you make the execution more deterministic so that 
only one set of values is possible?
6.8  e dining philosopher’s problem is a classic problem of synchronization and 
concurrency.
 e general problem is stated as philosophers sitting at a round table 
doing one of two things: eating or thinking. When they are eating, they are not 
thinking, and when they are thinking, they are not eating
 ere is a bowl of pasta 
in the center. A fork is placed in between each philosopher
 e result is that each 
philosopher has one fork to  and one fork to her right. Given the nature of 

eating pasta, the philosopher needs two for
ks to eat, and can only use the forks on 
her immediat
  and righ
 e philosophers do not speak to one another.

6.8.1 [10] <§6.7> Describe the scenario where none of philosophers ever eats (i.e., 
starvation). What is the sequence of events that happen that lead up to this problem?
6.8.2 [10] <§6.7> Describe how we can solve this problem by introducing the 
concept of a priority? But can we guarantee that we will treat all the philosophers 
fairly? Explain.
Now assume we hire a waiter who is in charge of assigning forks to philosophers. 
Nobody can pick up a fork until the waiter says they ca
 e waiter has global 
knowledge of all forks. Further, if we impose the policy that philosophers will 

always request to pick up th
  fork before requesting to pick up their right 
fork, then we can guarantee to avoid deadlock.
6.8.3 [10] <§6.7> We can implement requests to the waiter as either a queue of 
requests or as a periodic retry of a request. With a queue, requests are handled in 
the order they are received
 e problem with using the queue is that we may not 
always be able to service the philosopher whose request is at the head of the queue 

(due to the unavailability of resources). Describe a scenario with 5 philosophers 

where a queue is provided, but service is not granted even though there are forks 

available for another philosopher (whose request is deeper in the queue) to eat.
6.8.4 [10] <§6.7> If we implement requests to the waiter by periodically repeating 
our request until the resources become available, will this solve the problem 
described in Exercise 6.8.3? Explain.
6.9 Consider the following three CPU organizations:
CPU SS: A 2-core superscalar microprocessor that provides out-of-order issue 
capabilities on 2 function units (FUs). Only a single thread can run on each core 

at a time.
CPU MT
 ne-grained multithreaded processor that allows instructions from 2 
threads to be run concurrently (i.e., there are two functional units), though only 
instructions from a single thread can be issued on any cycle.
CPU SMT: An SMT processor that allows instructions from 2 threads to be run 
concurrently (i.e., there are two functional units), and instructions from either or 

both threads can be issued to run on any cycle.
Assume we have two threads X and Y to run on these CPUs that include the 
following operations:
Thread XThread YA1 – takes 3 cycles to execute
B1 – take 2 cycles to execute
A2 – no dependences icts for a functional unit with B1
 icts for a functional unit with A1B3 – depends on the result of B2
A4 – depends on the result of A3B4 – no dependences and takes 2 cycles to execute
 6.16 Exercises 569
570 Chapter 6 Parallel Processors from Client to Cloud
Assume all instructions take a single cycle to execute unless noted otherwise or 
they encounter a hazard.
6.9.1 [10] <§6.4> Assume that you have 1 SS CPU. How many cycles will it take to 
execute these two threads? How many issue slots are wasted due to hazards?

6.9.2 [10] <§6.4> Now assume you have 2 SS CPUs. How many cycles will it take 
to execute these two threads? How many issue slots are wasted due to hazards?

6.9.3 [10] <§6.4> Assume that you have 1 MT CPU. How many cycles will it take 
to execute these two threads? How many issue slots are wasted due to hazards?
6.10 Virtualization so
 ware is being aggressively deployed to reduce the costs of 
managing today’s high performance servers. Companies like VMWare, Microso
  and IBM have all developed a range of virtualization produc
 e general concept, 
described in Chapter 5, is that a hypervisor layer can be introduced between the 
hardware and the operating system to allow multiple operating systems to share 

the same physical hardware
 e hypervisor layer is then responsible for allocating 
CPU and memory resources, as well as handling services typically handled by the 

operating system (e.g., I/O).
Virtualization provides an abstract view of the underlying hardware to the hosted 
operating system and application so
 ware. 
 is will require us to rethink how 
multi-core and multiprocessor systems will be designed in the future to support 

the sharing of CPUs and memories by a number of operating systems concurrently.
6.10.1 [30] <§6.4> Select two hypervisors on the market today, and compare 
and contrast how they virtualize and manage the underlying hardware (CPUs and 
memory).
6.10.2 [15] <§6.4> Discuss what changes may be necessary in future multi-core 
CPU platforms in order to better match the resource demands placed on these 
systems. For instance, can multithreading play a
 ective role in alleviating the 
competition for computing resources?
6.11 We would like to execute the loop belo
  ciently as possible. We have 
tw
 erent machines, a MIMD machine and a SIMD machine.
for (i=0; i < 2000; i++)
for (j=0; j<3000; j++)X_array[i][j] = Y_array[j][i] + 200;6.11.1 [10] <§6.3> For a 4 CPU MIMD machine, show the sequence of MIPS 
instructions that you would execute on each CPU. What is the speedup for this 
MIMD machine?
6.11.2 [20] <§6.3> For an 8-wide SIMD machine (i.e., 8 parallel SIMD functional 
units), write an assembly program in using your own SIMD extensions to MIPS 
to execute the loop. Compare the number of instructions executed on the SIMD 

machine to the MIMD machine.

6.12 A systolic array is an example of an MISD machine. A systolic array is a 
pipeline network or “wavefront” of data 
processing elements. Each of these elements 
does not need a program counter since execution is triggered by the arrival of data. 
Clocked systolic arrays compute in “lock-step” with each processor undertaking 

alternate compute and communication phases.
6.12.1 [10] <§6.3> Consider proposed implementations of a systolic array (you 
ca
 nd these in on the Internet or in technical publication
 en attempt to 
program the loop provided in Exercise 6.11 using this MISD model. Discuss any 
  culties you encounter.
6.12.2 [10] <§6.3> Discuss the similarities an
 erences between an MISD and 
SIMD machine. Answer this question in terms of data-level parallelism.
6.13 Assume we want to execute the DAXPY loop show on page 511 in MIPS 
assembly on the NVIDIA 8800 GTX GPU described in this chapter. In this problem, 
we will assume that all math operations are performed on single-precisio
 oating-
point numbers (we will rename the loop SAXPY). Assume that instructions take 

the following number of cycles to execute.
LoadsStoresAdd.SMult.S5234
6.13.1 [20] <§6.6> Describe how you will constructs warps for the SAXPY loop 
to exploit the 8 cores provided in a single multiprocessor.
6.14 Download the CUDA Toolkit and SDK from 
http://www.nvidia.com/object/
cuda_get.html
. Make sure to use the “emurelease” (Emulation Mode) version of the 
code (you will not need actual NVIDIA hardware for this assignment). Build the 

example programs provided in the SDK, and co
 rm that they run on the emulator.
6.14.1 [90] <§6.6> Using the “template” SDK sample as a starting point, write a 
CUDA program to perform the following vector operations:
1) a − b (vector-vector subtraction)
2) a  b (vector dot product)
 e dot product of two vectors 
a = [a1, a2, … , an] and 
b = [b1, b2, … , bn ned as:
ab 1122
abababab
ininni1–Submit code for each program that demonstrates each operation and ver
 es the 
correctness of the results.

6.14.2 [90] <§6.6> If you have GPU hardware available, complete a performance 
analysis your program, examining the computation time for the GPU and a CPU 
version of your program for a range of vector sizes. Explain any results you see.
 6.16 Exercises 571
572 Chapter 6 Parallel Processors from Client to Cloud
6.15 AMD has recently announced that they will be integrating a graphics 
processing unit with their 
x86 cores in a single package, though wit
 erent 
clocks for each of the cor
 is is an example of a heterogeneous multiprocessor 
system which we expect to see produced commericially in the near future. One 
of the key design points will be to allow for fast data communication between 

the CPU and the GPU. Presently communications must be performed between 

discrete CPU and GPU chips. But this is changing in AMDs Fusion architecture. 

Presently the plan is to use multiple (at least 16) PCI express channels for facilitate 

intercommunication. Intel is also jumping into this arena with their Larrabee chip. 

Intel is considering to use their QuickPath interconnect technology.
6.15.1 [25] <§6.6> Compare the bandwidth and latency associated with these 
two interconnect technologies.
6.16 Refer to 
Figure 6.14b
, which shows an n-cube interconnect topology of order 
3 that interconnects 8 nodes. One attractive feature of an n-cube interconnection 
network topology is its ability to sustain broken links and still provide connectivity.
6.16.1 [10] <§6.8> Develop an equation that computes how many links in the 
n-cube (where n is the order of the cube) can fail and we can still guarantee an 
unbroken link will exist to connect any node in the n-cube.
6.16.2 [10] <§6.8> Compare the resiliency to failure of n-cube to a fully-
connected interconnection network. Plot a comparison of reliability as a function 
of the added number of links for the two topologies.
6.17 Benchmarking
 eld of study that involves identifying representative 
workloads to run on sp
 c computing platforms in order to be able to objectively 
compare performance of one system to another. In this exercise we will compare 
two classes of benchmarks: the Whetstone CPU benchmark and the PARSEC 

Benchmark suite. Select one program from
 PARSEC. All programs should be freely 
available on the Internet. Consider running multiple copies of Whetstone versus 

running the PARSEC Benchmark on any of systems described in Section 6.11.
6.17.1 [60] <§6.10> What is inherentl
 erent between these two classes of 
workload when run on these multi-core systems?

6.17.2 [60] <§6.10> In terms of the Roo
 ine Model, how dependent will the 
results you obtain when running these benchmarks be on the amount of sharing 
and synchronization present in the workload used?
6.18 When performing computations on sparse matrices, latency in the memory 
hierarchy becomes much more of a factor. Sparse matrices lack the spatial locality 
in the data stream typically found in matrix operations. As a result, new matrix 

representations have been proposed.
One the earliest sparse matrix representations is the Yale Sparse Matrix Format. It 
stores an initial sparse 
m × n matrix, 
M in row form using three one-dimensional 

arrays. Let 
R be the number of nonzero entries in 
M. We construct an array 
A of length 
R that contains all nonzero entries of 
M -to-right top-to-bottom 
order). We also construct a second array 
IA of length 
m + 1 (i.e., one entry per row, 
plus one). 
IA(i) contains the index in 
A of th
 rst nonzero element of row 
i. Row 
i of the original matrix extends from 
A(IA(i)) to 
A(IA(i e third array, 
JA, contains the column index of each element of 
A, so it also is of length 
R.6.18.1 [15] <§6.10> Consider the sparse matrix 
X below and write C code that 
would store this code in Yale Sparse Matrix Format.
   Row 1 [1, 2, 0, 0, 0, 0]   Row 2 [0, 0, 1, 1, 0, 0]
   Row 3 [0, 0, 0, 0, 9, 0]
   Row 4 [2, 0, 0, 0, 0, 2]
   Row 5 [0, 0, 3, 3, 0, 7]
   Row 6 [1, 3, 0, 0, 0, 1]6.18.2 [10] <§6.10> In terms of storage space, assuming that each element in 
matrix 
X is single precisio
 oating point, compute the amount of storage used to 
store the Matrix above in Yale Sparse Matrix Format.
6.18.3 [15] <§6.10> Perform matrix multiplication of Matrix X by Matrix Y 
shown below.
   [2, 4, 1, 99, 7, 2]Put this computation in a loop, and time its execution. Make sure to increase 
the number of times this loop is executed to get good resolution in your timing 

measurement. Compare the runtime of using a naïve representation of the matrix, 

and the Yale Sparse Matrix Format.
6.18.4 [15] <§6.10> Can yo
 nd a more
  cient sparse matrix representation 
(in terms of space and computational overhead)?
6.19 In future systems, we expect to see heterogeneous computing platforms 
constructed out of heterogeneous CPUs. We have begun to see some appear in the 
embedded processing market in systems that contain both
 oating point DSPs and 
a microcontroller CPUs in a multichip module package.
Assume that you have three classes of CPU:

CPU A—A moderate speed multi-core CPU (wit
 oating point unit) that can 
execute multiple instructions per cycle.

CPU B—A fast single-core integer CPU (i.e
 oating point unit) that can 
execute a single instruction per cycle.

CPU C—A slow vector CPU (wit
 oating point capability) that can execute 
multiple copies of the same instruction per cycle.
 6.16 Exercises 573
574 Chapter 6 Parallel Processors from Client to Cloud
Assume that our processors run at the following frequencies:
CPU ACPU BCPU C
1 GHz3 GHz250 MHz
CPU A can execute 2 instructions per cycle, CPU B can execute 1 instruction per 
cycle, and CPU C can execute 8 instructions (though the same instruction) per 

cycle. Assume all operations can complete execution in a single cycle of latency 

without any hazards.
All three CPUs have the ability to perform integer arithmetic, though CPU B cannot 
perfor
 oating point arithmetic. CPU A and B have an instruction set similar 
to a MIPS processor. CPU C can only perfor
 oating point add and subtract 
operations, as well as memory loads and stores. Assume all CPUs have access to 

shared memory and that synchronization has zero cost.
 e task at hand is to compare two matrices X and Y that each contain 1024 × 1024 
 oating point elemen
 e output should be a count of the number indices where 
the value in X was larger or equal to the value in Y.

6.19.1 [10] <§6.11> Describe how you would partition the problem on the 3 
 erent CPUs to obtain the best performance.
6.19.2 [10] <§6.11> What kind of instruction would you add to the vector CPU 
C to obtain better performance?
6.20 Assume a quad-core computer system can process database queries at a 
steady state rate of requests per second. Also assume that each transaction takes, 
on average
 xed amount of time to pro
 e following table shows pairs of 
transaction latency and processing rate.
Average Transaction LatencyMaximum transaction processing rate
1 ms5000/sec2 ms5000/sec1 ms10,000/sec2 ms10,000/secFor each of the pairs in the table, answer the following questions:
6.20.1 [10] <§6.11> On average, how many requests are being processed at any 
given instant?

6.20.2 [10] <§6.11> If move to an 8-core system, ideally, what will happen to the 
system throughput (i.e., how many queries/second will the computer process)?

6.20.3 [10] <§6.11> Discuss why we rarely obtain this kind of speedup by simply 
increasing the number of cores.

§6.1, page 504: False. Task-level parallelism can help sequential applications and 
sequential applications can be made to run on parallel hardware, although it is 

more challenging.

§6.2, page 509: False. 
Weak
 scaling can compensate for a serial portion of the 
program that would otherwise limit scalability, but not so for strong scaling.

§6.3, page 514: True, but they are missing useful vector features like gather-scatter 

and vector length registers that improve th
  ciency of vector architectures. 
(As an elaboration in this section mentions, the AVX2 SIMD extensions o
 ers 
indexed loads via a gather operation but 
not scatter for indexed stor
 e Haswell 
generation x86 microprocessor is th
 rst to support AVX2.)
§6.4, page 519: 1. True. 2. True.

§6.5, page 523: False. Since the shared address is a 
physical
 address, multiple 
tasks each in their own 
virtual
 address spaces can run well on a shared memory 
multiprocessor.

§6.6, page 531: False. Graphics DRAM chips are prized for their higher bandwidth.

§6.7, page 536: 1. False. Sending and receiving a message is an implicit 

synchronization, as well as a way to share data. 2. True.

§6.8, page 538: True.

§6.10, page 550: True. We likely need innovation at all levels of the hardware and 

so
 ware stack for parallel computing to succeed.
Answers to 
Check Yourself
 6.16 Exercises 575
AFear of serious injury 
cannot alone justify 

suppression of free 

speech and assembly.
Louis BrandeisWhitney v. California, 
1927Assemblers, Linkers, 
and the SPIM 
SimulatorJames R. Larus
Microso
  Research
Microso 
APPENDIX
A.1 Introduction A-3
A.2 Assemblers 
A-10
A.3 Linkers 
A-18
A.4 Loading A-19
A.5 Memory Usage 
A-20
A.6 Procedure Call Convention 
A-22
A.7 Exceptions and Interrupts 
A-33
A.8 Input and Output 
A-38
A.9 SPIM A-40
A.10 MIPS R2000 Assembly Language 
A-45
A.11 Concluding Remarks 
A-81
A.12 Exercises A-82
 A.1 IntroductionEncoding instructions as binary numbers is natural an
  cient for computers. 
Humans, however, have a great deal o
  culty understanding and manipulating 
these numbers. People read and write symbols (words) much better than long 
sequences of digits. Chapter 2 showed that we need not choose between numbers 

and words, because computer instructions can be represented in many ways. 

Humans can write and read symbols, and computers can execute the equivalent 

binary number
 is appendix describes the process by which a human-readable 
program is translated into a form that a computer can execute, provides a few hints 

about writing assembly programs, and explains how to run these programs on 

SPIM, a simulator that executes MIPS programs. UNIX, Windows, and Mac OS X 

versions of the SPIM simulator are available on the CD.
Assembly language
 is the symbolic representation of a computer’s binary 
encoding—the 
machine language
. Assembly language is more readable than 
machine language, because it uses symbols instead of bi
 e symbols in assembly 
language name commonly occurr in bit patterns, such as opcodes and register 

sp
 ers, so people can read and remember them. In addition, assembly language 
machine language
 Binary representation 
used for communication 
within a computer 

system.

A-4 Appendix A Assemblers, Linkers, and the SPIM Simulator
FIGURE A.1.1 The process that produces an executable ﬁ
 le. An assembler translat
 le of 
assembly language into an objec
 le, which is linked with ot
 les and libraries into an executab
 le. 
ObjectfileSourcefileAssembler
Linker
Assembler
Assembler
Program
library
ObjectfileObjectfileSourcefileSourcefileExecutable
filepermits programmers to use 
labels
 to identify and name particular memory words 
that hold instructions or data. 
A tool called an 
assembler
 translates assembly language into binary instructions. 
Assemblers provide a friendlier representation than a computer’s 0s and 1s, which 
sim p
 es writing and reading programs. Symbolic names for operations and loca-
tions are one facet of this representation. Another facet is programming facilities 
that increase a program’s clarity. For example, 
macros
, discussed in  Section A.2, 
enable a programmer to extend the assembly language b
 ning new operations.
An assembler reads a single assembly language 
source
 le and produces an 
obje
 le containing machine instructions and bookkeeping information that 
helps combine several objec
 les into a program. 
Figure A.1.1
 illustrates how a 
program is built. Most programs consist of sev
 les—also called 
modules
—that are written, compiled, and assembled independently. A program may also use 

prewritten routines supplied in a 
program library
. A module typically contains 
ref-
erences
 to subroutines and dat
 ned in other modules and in librar
 e code 
in a module cannot be executed when it contains 
unresolved references
 to labels 
in other objec les or libraries. Another tool, called a 
linker
, combines a collection 
of object and librar
 les into an 
executab
 le, which a computer can run.
To see the advantage of assembly language, consider the following sequence of 
 gures, all of which contain a short subroutine that computes and prints the sum of 
the squares of integers from 0 to 100. 
Figure A.1.2
 shows the machine language that 

a MIPS computer executes. With considerab
 ort, you could use the opcode and 
instruction format tables in Chapter 2 to translate the instructions into a symbolic 

program similar to that shown in 
Figure A.1.3
 is form of the routine is much 
easier to read, because operations and operands are written with symbols rather 
assembler
 A program 
that translates a symbolic 
version of instruction into 

the binary ver sion.
macro
 A pattern-
matching and replacement 

facility that pro vides a 

simple mechanism to name 

a frequently used sequence 

of instructions.
unresolved reference
 A  reference that requires 

more  information from 

an outside source to be 

complete.
linker
 Also called 
link editor
. A systems 
program that combines 

independently assembled 

machine  language 

programs and resolves all 

 ned labels into an 
executab
 le.

 A.1 Introduction A-
5than with bit patterns. However, this assembly language is s
  cult to follow, 
because memory locations are named by their address rather than by a symbolic 
label.
Figure A.1.4
 shows assembly language that labels memory addresses with mne-
monic names. Most programmers prefer to read and write this form. Names that 

begin with a period, for example 
.data and 
.globl, are 
assembler directives
 that tell the assembler how to translate a program but do not produce machine 
instructions. Names followed by a colon, such as 
str: or 
main:, are labels that 
name the next memory locatio
 is program is as readable as most assembly 
language programs (except for a glaring lack of comments), but it is s
  cult 
to follow, because many simple operations are required to accomplish simple tasks 

and because assembly language’s lack of contro
 ow constructs provides few hints 
about the program’s operation.
By contrast, the C routine in 
Figure A.1.5
 is both shorter and clearer, since vari-
ables have mnemonic names and the loop is explicit rather than constructed with 

branches. In fact, the C routine is the only one that we wrote
 e other forms of 
the program were produced by a C compiler and assembler.
In general, assembly language plays two roles (see 
Figure A.1.6
 e 
 rst role 
is the output language of compilers. A 
compiler
 translates a program written in a 
high-level language
 (such as C or Pascal) into an equivalent program in machine or 
assembler directive
 An operation that tells the 
assembler how to translate 

a program but does not 

produce machine instruc-

tions; always begins with 

a period.
0010011110111101111111111110000010101111101111110000000000010100
10101111101001000000000000100000
10101111101001010000000000100100
10101111101000000000000000011000
10101111101000000000000000011100
10001111101011100000000000011100
10001111101110000000000000011000
00000001110011100000000000011001
00100101110010000000000000000001
00101001000000010000000001100101
10101111101010000000000000011100
00000000000000000111100000010010
00000011000011111100100000100001
00010100001000001111111111110111
10101111101110010000000000011000
00111100000001000001000000000000
10001111101001010000000000011000
00001100000100000000000011101100
00100100100001000000010000110000
10001111101111110000000000010100
00100111101111010000000000100000
00000011111000000000000000001000
00000000000000000001000000100001FIGURE A.1.2 MIPS machine language code for a routine to compute and print the sum 
of the squares of integers between 0 and 100.
 
A-6 Appendix A Assemblers, Linkers, and the SPIM Simulator
assembly language
 e high-level language is called the 
source  language
, and the 
compiler’s output is its 
target language
.Assembly language’s other role is as a language in which to write program
 is role used to be the dominant one. Today, however, because of larger main memo-
ries and better compilers, most programmers write in a high-level language and 

rarely, if ever, see the instructions that a computer executes. Nevertheless, assembly 

language is still important to write programs in which speed or size is critical or to 

exploit hardware features that have no analogues in high-level  languages.
Although this appendix focuses on MIPS assembly language, assembly pro-
gramming on most other machines is very similar
 e additional instructions and 
address modes in CISC machines, such as the VAX, can make assembly pro grams 

shorter but do not change the process of assembling a program or provide assembly 

language with the advantages of high-level languages, such as type-checking and 

structured contro
 ow.
source language
 e high-level language 
in which a pro gram is 

originally written.
addiu $29, $29, -32
sw $31, 20($29)
sw $4, 32($29)
sw $5, 36($29)
sw $0, 24($29)
sw $0, 28($29)

lw $14, 28($29)
lw $24, 24($29)
multu $14, $14
addiu $8, $14, 1
slti $1, $8, 101

sw $8, 28($29)
mflo $15addu $25, $24, $15

bne $1, $0, -9

sw $25, 24($29)
lui $4, 4096
lw $5, 24($29)
jal 1048812
addiu $4, $4, 1072

lw $31, 20($29)
addiu $29, $29, 32
jr $31move $2, $0
FIGURE A.1.3 The same routine as in Figure A.1.2 written in assembly language. 
However, 
the code for the routine does not label registers or memory locations or include comments. 

 A.1 Introduction A-
7When to Use Assembly Language e primary reason to program in assembly language, as opposed to an available 
high-level language, is that the speed or size of a program is critically important. 
For example, consider a computer that controls a piece of machinery, such as a 

car’s brakes. A computer that is incorporated in another device, such as a car, is 

called an 
embedded computer
 is type of computer needs to respond rapidly 
and predictably to events in the outside world. Because a compiler introduces 
FIGURE A.1.4 The same routine as in Figure A.1.2 written in assembly language with 
labels, but no com 
ments.  e commands that start with periods are assembler directives (see pages 
A-47–49). 
.text indicates that succeeding lines contain instructions. 
.data indicates that they contain 
data. 
.align n indicates that the items on the succ
eeding lines should be
 aligned on a 2
n byte boundary. 
Hence, 
.align 2 means the next item should be on a word boundary. 
.globl main declares that 
main is a global symbol that should be visible to code stored in ot
 les. Finally, 
.asciiz stores a null-terminated 
string in memory. 

A-8 Appendix A Assemblers, Linkers, and the SPIM Simulator
uncertainty about the time cost of operations, programmers ma
 nd it
  cult 
to ensure that a high-level language program responds wit
 nite time 
interval—say, 1 millisecond a
 er a sensor detects that a tire is skidding. An 
assembly language programmer, on the other hand, has tight control over which 
instruc tions execute. In addition, in embedded applications, reducing a program’s 

size, so that i
 ts in fewer memory chips, reduces the cost of the embedded 
computer.
A hybrid approach, in which most of a program is written in a high-level lan-
guage and time-critical sections are written in assembly language, builds on the 

strengths of both languages. Programs typically spend most of their time execut ing 

a small fraction of the program’s source code
 is observation is just the prin ciple 
of locality that underlies caches (see Section 5.1 in Chapter 5).
Program pro
 ling measures where a program spends its time and ca
 nd the 
time-critical parts of a program. In many cases, this portion of the program can 

be made faster with better data structures or algorithms. Sometimes, however, sig-

 cant performance improvements only come from recoding a critical portion of 
a program in assembly language.
#include <stdio.h>intmain (int argc, char *argv[])
{
  int i;

  int sum = 0;
  for (i = 0; i <= 100; i = i + 1) sum = sum + i * i;
  printf (“The sum from 0 .. 100 is %d\n”, sum);

}FIGURE A.1.5 The routine in Figure A.1.2 written in the C programming language.
 FIGURE A.1.6 Assembly language either is written by a programmer or is the output of 
a compiler.
 Linker
CompilerAssembler
ComputerHigh-level language program
Assembly language program
Program

 A.1 Introduction A-
9 is improvement is not necessarily an indication that the high-level  language’s 
compiler has failed. Compilers typically are better than programmers at produc-
ing uniformly high-quality machine code across an entire program. Pro grammers, 

however, understand a program’s algorithms and behavior at a deeper level than 

a compiler and can expend considerab
 ort and ingenuity improving small 
sections of the program. In particular, programmers o
 en consider several proce-
dures simultaneously while writing their code. Compilers typically compile each 

procedure in isolation and must follow strict conventions governing the use of 

registers at procedure boundaries. By retaining commonly used values in regis-

ters, even across procedure boundaries, programmers can make a program run 

faster.
Another major advantage of assembly language is the ability to exploit special-
ized instructions—for example, string copy or pattern-matching instructions. 

Compilers, in most cases, cannot determine that a program loop can be replaced 

by a single instruction. However, the programmer who wrote the loop can replace 

it easily with a single instruction.
Currently, a programmer’s advantage over a compiler has beco
  cult to 
maintain as compilation techniques improve and  machines’ pipelines increase in 

complexity (Chapter 4).
 e 
 nal reason to use assembly language is that no high-level language is 
available on a particular computer. Many older or specialized computers do not 

have a compiler, so a programmer’s only alternative is assembly language.
Drawbacks of Assembly Language
Assembly language has many disadvantages that strongly argue against its wide-

spread use. Perhaps its major disadvantage is that programs written in assembly 

language are inherently machine-spe
 c and must be totally rewritten to run on 
another computer architecture
 e rapid evolution of computers discussed in 
Chapter 1 means that architectures become obsolete. An assembly language pro-

gram remains tightly bound to its original archi tecture, even a
 er the computer is 
eclipsed by new, faster, and more
 ective machines.
Another disadvantage is that assembly language programs are longer than the 
equivalent programs written in a high-level language. For example, the C program 

in Figure A.1.5
 is 11 lines long, while the assembly program in 
Figure A.1.4
 is 
31 lines long. In more complex programs, the ratio of assembly to high-level lan-

guage (its 
expansion factor
) can be much larger than the factor of three in this 
exam ple. Unfortunately, empirical studies have shown that programmers write 

roughly the same number of lines of code per day in assembly as in high-level 

languag
 is means that programmers are roughly 
x times more productive in a 
high-level language, where 
x is the assembly language expansion factor.

A-10 Appendix A Assemblers, Linkers, and the SPIM Simulator
To compound the problem, longer programs are more
  cult to read and 
understand, and they contain more bugs. Assembly language exacerbates the prob-
lem because of its complete lack of structure. Common programming idioms, 

such as if-then
 statements and loops, must be built from branches and jum
 e resulting programs are hard to read, because the reader must reconstruct every 

higher-level construct from its pieces and each instance of a statement may be 

slightly
 erent. For example, look at 
Figure A.1.4
 and answer these questions: 
What type of loop is used? What are its lower and upper bounds?
Elaboration: Compilers can produce machine language directly instead of relying on 
an assembler. These compilers typically execute much faster than those that invoke 
an assembler as part of compilation. However, a compiler that generates machine lan-

guage must perform many tasks that an assembler normally handles, such as resolv-

ing addresses and encoding instructions as binary numbers. The tradeoff is between 

compilation speed and compiler simplicity. 
Elaboration: Despite these considerations, some embedded applications are writ-
ten in a high-level language. Many of these applications are large and complex pro-

grams that must be extremely reliable. Assembly language programs are longer and 
 cult to write and read than high-level language programs. This greatly increases 

the cost of writing an assembly language program and makes it extremely dif 
 cult to 
verify the correctness of this type of program. In fact, these considerations led the US 

Department of Defense, which pays for many complex embedded systems, to develop 

Ada, a new high-level language for writing embedded systems.
 A.2 Assemblers
An assembler translat
 le of assembly language statements into
 le of binary 
machine instructions and binary dat
 e translation process has two major 
par
 e 
 rst step is to
 nd memory locations with labels so that the relationship 
between symbolic names and addresses is known when instructions are trans lated. 
 e second step is to translate each assembly statement by combining the numeric 
equivalents of opcodes, register sp
 ers, and labels into a legal instruc tion. As 
shown in 
Figure A.1.1
, the assembler produces an outpu
 le, called an 
obje
 le, which contains the machine instructions, data, and bookkeeping infor mation.
An ob
 le typically cannot be executed, because it references procedures or 
data in ot
 les. A 
label
 is external
 (also called 
global
) if the labeled object can 
external label
 Also called
global label
. A label 
referring to an object that 
can be referenced from 

 les other than the one in 
which i
 ned.

be referenced from
 les other than the one in which i
 ned. A label is 
local 
if the object can be used only within th
 le in which i
 ned. In most assem-
blers, labels are local by default and must be explicitly declared global. Subrou tines 
and global variables require external labels since they are referenced from many 

 les in a program. 
Local labels
 hide names that should not be visible to other 
modules—for example, static functions in C, which can only be called by other 
functions in the sa
 le. In addition, compiler-generated names—for example, a 
name for the instruction at the beginning of a loop—are local so that the compiler 

need not produce unique names in ever
 le.
Local and Global LabelsConsider the program in 
Figure A.1.4
 e subroutine has an external (global) 
label 
main. It also contains two local labels—
loop and 
str—that are only 
visible with this assembly language
 le. Finally, the routine also contains an 
unresolved reference to an external label 
printf, which is the library routine 
that prints values. Which labels in 
Figure A.1.4
 could be referenced from 

anot
 le?Only global labels are visible ou
 le, so the only label that could be 
referenced from anot
 le is 
main.Since the assembler processes each
 le in a program individually and in isola tion, 
it only knows the addresses of local labe
 e assembler depends on another tool, 
the linker, to combine a collection of objec
 les and libraries into an executable 
 le by resolving external labe
 e assembler assists the lin
ker by pro viding lists 
of labels and unresolved references.
However, even local labels present an interesting challenge to an assembler. 
Unlike names in most high-level languages, assembly labels may be used before 

they ar
 ned. In the example in 
Figure A.1.4
, the label 
str is used by the 
la instruction before i
 ned. 
 e possibility of a 
forward reference
, like this one, 
forces an assembler to translate a program in two st
 rst 
 nd all labels and then 
produce instructions. In the example, when the assembler sees the 
la instruction, 
it does not know where the word labeled 
str is located or even whether 
str labels 
an instruction or datum.
local label
 A label 
referring to an object that 
can be used only within 

th
 le in which it is 
 ned.
EXAMPLEANSWERforward reference
 A label that is used 
before it is  
 ned.
 A.2 Assemblers 
A-11
A-12 Appendix A Assemblers, Linkers, and the SPIM Simulator
An assembler’s
 rst pass reads each line of an assembly
 le and breaks it into its 
component p
 ese pieces, which are called 
lexemes
, are individual words, 
numbers, and punctuation characters. For example, the line 
 ble $t0, 100, loop
contains six lexemes: the opcode 
ble, the register sp
 er $t0, a comma, the 
number 
100, a comma, and the symbol 
loop.If a line begins with a label, the assembler records in its 
symbol table
 the name 
of the label and the address of the memory word that the instruction occupies. 
 e assembler then calculates how many words of memory the instruction on the 
current line will occupy. By keeping track of the instructions’ sizes, the assembler 
can determine where the next instruction goes. To compute the size of a variable-

length instruction, like those on the VAX, an assembler has to examine it in detail. 

However,
 xed-length instructions, like those on MIPS, require only a cursory 
examinatio
 e assembler performs a similar calculation to compute the space 
required for data statements. When the assembler reaches the end of an assembly 

 le, the symbol table records the location of each labe
 ned in th
 le.
 e assembler uses the information in the symbol table during a second pass 
over th
 le, which actually produces machine code
 e assembler again exam-
ines each line in th
 le. If the line contains an instruction, the assembler com-
bines the binary representations of its opcode and operands (register sp
 ers or 
memory address) into a legal instructio
 e process is similar to the one used in 
Section 2.5 in Chapter 2. Instructions and data words that reference an external 

symbol
 ned in anot
 le cannot be completely assembled (they are unre-
solved), since the symbol’s address is not in the symbol table. An assembler does 

not complain about unresolved references, since the corresponding label is likely 

to b
 ned in anot
 le.
Assembly language is a programming language. Its princi
 erence 
from high-level languages such as BASIC, Java, and C is that assembly lan-

guage provides only a few, simple types of data and contro
 ow. Assembly 
language programs do not specify the type of value held in a variable. 

Instead, a programmer must apply the appropriate operations (e.g., integer 

or
 oating-point addition) to a value. In addition, in assem bly language, 
programs must implement all contro
 ow with 
go to
s. Both factors make 
assembly language programming for any machine—MIPS or x86—more 

  cult and error-prone than writing in a high-level  language.
symbol table
 A table 
that matches names of 
labels to the addresses of 

the memory words that 

instructions  occupy.
The BIGPicture
Elaboration: If an assembler’s speed is important, this two-step process can be done 
in one pass o le with a technique known as 
backpatching. In its pass o le, the assembler builds a (possibly incomplete) binary representation 
of every instruction. If the instr ned, 
the assembler records the label and instr ned, the 

 nd all instructions that contain a forward reference to 

the label. The assembler goes back and corrects their binary representation to incorpo-

rate the address of the label. Backpatching speeds assembly because the assembler 
only reads its input once. However, it requires an assembler to hold the entire binary rep-

resentation of a program in memory so instructions can be backpatched. This require-

ment can limit the size of programs that can be assembled. The process is com plicated by machines with several types of branches that span different ranges of instructions. 

 rst sees an unresolved label in a branch instruction, it must either 

use the largest possible branch or risk having to go back and readjust many instructions 

to make room for a larger branch.
Object File Format
Assemblers produce objec
 les. An objec
 le on UNIX contains six distinct 
sections (see 
Figure A.2.1
):   e obje
 le header 
describes the size and position of the other pieces of 
th
 le.
  e text segment
 contains the machine language code for routines in the 
sour
 le. 
 ese routines may be unexecutable because of unresolved 
references.
  e data segment
 contains a binary representation of the data in the source 
 le. 
 e data also may be incomplete because of unresolved references to 
labels in ot
 les.  e relocation information
 iden
 es instructions and data words that 
depend on 
absolute addresses
 ese references must change if portions of 
the program are moved in memory.
  e symbol table 
associates addresses with external labels in the sour
 le and lists unresolved references. 
  e debugging information
 contains a concise description of the way the 
program was compiled, so a debugger ca
 nd which instruction addresses 
correspond to lines in a sour
 le and print the data structures in readable 
form.
 e assembler produces an objec
 le that contains a binary representation of 
the program and data and additional information to help link pieces of a  program. 
backpatching
 A method for translating 
from assembly lan guage 

to machine instructions 

in which the  assembler 

builds a (possibly 

incomplete) binary 

 representation of every 

instruc tion in one pass 

over a program and then 

returns t
 ll in previ-
ously  
 ned labels.
text segment
 e segment of a UNIX 

ob
 le that  contains 
the machine language 

code for rou tines in the 

sour
 le.
data segment
 e segment of a UNIX 

object or executab
 le that contains a binary 

represen tation of the 

 initialized data used by 

the program.
relocation information
  e segment of a UNIX 
ob
 le that iden
 es instructions and data 
words that  depend on 

absolute addresses.
absolute address
 A variable’s or routine’s 
actual  address in memory.
 A.2 Assemblers 
A-13
A-14 Appendix A Assemblers, Linkers, and the SPIM Simulator
 is relocation information is necessary because the assembler does not know 
which memory locations a procedure or piece of data will occupy a
 er it is linked 
with the rest of the program. Procedures and data from
 le are stored in a con-
tiguous piece of memory, but the assembler does not know where this mem ory will 
be located.
 e assembler also passes some symbol table entries to the linker. In 
particular, the assembler must record which external symbols ar
 
 le and what unresolved references occ
 le.
Elaboration: For convenience, assembler le starts at the same 
address (for example, location 0) with the expectation that the linker will 
relocate
 the code 
and data when they are assigned locations in memory. The assembler produces 
relocation 
information, which contains an entry describing each instr
 le 
that references an absolute address. On MIPS, only the subroutine call, load, and store 
instructions reference absolute addresses. Instructions that use PC- relative addressing, 

such as branches, need not be relocated.
Additional Facilities
Assemblers provide a variety of convenience features that help make assembler 
programs shorter and easier to write, but do not fundamentally change assembly 

language. For example, 
data layout directives
 allow a programmer to describe data 
in a more concise and natural manner than its binary representation.
In 
Figure A.1.4
, the directive 
 .asciiz “The sum from 0 .. 100 is %d\n”stores characters from the string in memory. Contrast this line with the alternative 

of writing each character as its ASCII value (Figure 2.15 in Chapter 2 describes the 

ASCII encoding for characters):
.byte 84, 104, 101, 32, 115, 117, 109, 32.byte 102, 114, 111, 109, 32, 48, 32, 46
.byte 46, 32, 49, 48, 48, 32, 105, 115
.byte 32, 37, 100, 10, 0 e .asciiz directive is easier to read because it represents characters as letters, 
not binary numbers. An assembler can translate characters to their binary repre-
sentation much faster and more accurately than a human can. Data layout directives 
FIGURE A.2.1 Object ﬁ
 le. A UNIX assembler produces an objec
 le with six distinct sections. 
Object fileheaderText
segmentDatasegmentRelocationinformation
Symboltable
Debugging
information

specify data in a human-readable form that the assembler translates to binary. Other 
layout directives are described in Section A.10.
String DirectiveDe
 ne the sequence of bytes produced by this directive: 
.asciiz “The quick brown fox jumps over the lazy dog”.byte 84,  104, 101, 32,    113, 117, 105, 99
.byte 107, 32,  98,  114,  111, 119, 110, 32

.byte 102, 111, 120, 32,  106, 117, 109, 112

.byte 115, 32,  111, 118, 101, 114,   32,  116

.byte 104, 101, 32,  108,   97, 122, 121, 32

.byte 100, 111, 103, 0Macro
 is a pattern-matching and replacement facility that provides a simple 
mechanism to name a frequently used sequence of instructions. Instead of repeat-
edly typing the same instructions every time they are used, a programmer invokes 

the macro and the assembler replaces the macro call with the corresponding 

sequence of instructions. Macros, like subroutines, permit a programmer to create 

and name a new abstraction for a common operation. Unlike subroutines, how-

ever, macros do not cause a subroutine call and return when the program runs, 

since a macro call is replaced by the macro’s body when the program is assembled. 

 er this replacement, the resulting assembly is indistinguishable from the equiv-
alent program written without macros.
MacrosAs an example, suppose that a programmer needs to print many number
 e library routine 
printf accepts a format string and one or more values to print 
as its arguments. A programmer could print the integer in register 
$7 with the 
following instructions: 
 .dataint_str: .asciiz“%d”
 .text
 la $a0, int_str # Load string address

 # into first argEXAMPLEANSWEREXAMPLE A.2 Assemblers 
A-15
A-16 Appendix A Assemblers, Linkers, and the SPIM Simulator
 mov $a1, $7 # Load value into
   # second arg

 jal printf # Call the printf routine
 e .data directive tells the assembler to store the string in the program’s data 
segment, and the 
.text directive tells the assembler to store the instruc tions 
in its text segment.
However, printing many numbers in this fashion is tedious and produces a 
verbose program tha
  cult to understand. An alternative is to introduce 
a macro, 
print_int, to print an integer: 
 .data
int_str:.asciiz “%d”
 .text
 .macro print_int($arg)

 la  $a0, int_str # Load string address into

               # first arg

 mov $a1, $arg    
 # Load macro’s parameter 
               # ($arg) into second arg

 jal printf       
 # Call the printf routine
 .end_macro
print_int($7) e macro has a 
formal parameter
, $arg, that names the argument to the 
macro. When the macro is expanded, the argument from a call is substituted 
for the formal parameter throughout the macro’s body
 en the assembler 
replaces the call with the macro’s newly expanded body. In th
 rst call on 
print_int, the argument is 
$7, so the macro expands to the code
la  $a0, int_strmov $a1, $7
jal printfIn a second call on 
print_int, say, 
print_int($t0), the argument is 
$t0, so the macro expands to
la  $a0, int_str 
mov $a1, $t0 
jal printfWhat does the call 
print_int($a0) expand to?
formal parameter
 A variable that is the 
argument to a proce dure 

or macro; it is replaced by 

that argument once the 

macro is expanded.

la  $a0, int_str mov $a1, $a0 
jal printf is example illustrates a drawback of macros. A programmer who uses 
this macro must be aware that 
print_int uses register 
$a0 and so cannot 
correctly print the value in that register.
Some assemblers also implement 
pseudoinstructions,
 which are instructions pro-
vided by an assembler but not implemented in hardware. Chapter 2 contains 
many examples of how the MIPS assembler synthesizes pseudoinstructions 

and addressing modes from the spartan MIPS hardware instruction set. For 

example, Section 2.7 in Chapter 2 describes how the assembler synthesizes the 

blt instruc tion from two other instructions: 
slt and 
bne. By extending the 
instruction set, the MIPS assembler makes assembly language programming 

easier without complicating the hardware. Many pseudoinstructions could also 

be simulated with macros, but the MIPS assembler can generate better code for 

these instructions because it can use a dedicated register (
$at) and is able to 
optimize the generated code.
Elaboration: Assemblers 
conditionally assemble pieces of code, which permits a 
programmer to include or exclude groups of instructions when a program is assembled. 
This feature is particularly useful when several versions of a program differ by a small 

amount. Rather than k les—which greatly complicates 

 xing bugs in the common code—programmers typically merge the versions into a sin-

 le. Code particular to one version is conditionally assembled, so it can be excluded 

when other versions of the program are assembled.
If macros and conditional assembly are useful, why do assemblers for UNIX systems 
rarely, if ever, provide them? One reason is that most programmers on these systems 

write programs in higher-level languages like C. Most of the assembly code is produced 

by compilers, nd it more con ne macros. 

Another reason is that other tools on UNIX—such as cpp, the C preprocessor, or 
m4, a 
general macro processor—can provide macros and conditional assembly for assembly 

language programs.ANSWERHardware/
Software

Interface A.2 Assemblers 
A-17
A-18 Appendix A Assemblers, Linkers, and the SPIM Simulator
 A.3 Linkers
Separate compilation
 permits a program to be split into pieces that are stored in 
 erent 
 les. Eac
 le contains a logically related collection of subroutines and 
data structures that form a 
module
 in a larger progra
 le can be compiled 
and assembled independently of ot
 les, so changes to one module do not 
require recompiling the entire program. As we discussed above, separate compila-
tion necessitates the additional step of linking to combine objec
 les from separate 
modules an
 xing their unresolved references.
 e tool that merges thes les is the 
linker
 (see 
Figure A.3.1
). It performs three 
tasks: 
 Searches the program libraries t
 nd library routines used by the program
 Determines the memory locations that code from each module will occupy 

and relocates its instructions by adjusting absolute references
 Resolves references amon
 lesA linker’
 rst task is to ensure that a program contains
 ned labe
 e linker matches the external symbols and unresolved references from a pro gram’s 

 les. An external symbol in on
 le resolves a reference from anot
 le if both 
refer to a label with the same name. Unmatched references mean a symbol was 

used but no
 ned anywhere in the program.
Unresolved references at this stage in the linking process do not necessarily 
mean a programmer made a mistake
 e program could have referenced a library 
routine whose code was not in the objec
 les passed to the linker
 er matching 
symbols in the program, the linker searches the system’s program librar ies to 

 nd pr
 ned subroutines and data structures that the program refer
 e basic libraries contain routines that read and write data, allocate and deallo cate 

memory, and perform numeric operations. Other libraries contain routines to 

access a database or manipulate terminal windows. A program that references an 

unresolved symbol that is not in any library is erroneous and cannot be linked. 

When the program uses a library routine, the linker extracts the routine’s code 

from the library and incorporates it into the program text segmen
 is new rou-
tine, in turn, may depend on other library routines, so the linker continues to 

fetch other library routines until no external references are unresolved or a rou tine 

cannot be found.
If all external references are resolved, the linker next determines the memory 
locations that each module will occupy. Since th
 les were assembled in isolation, 
separate compilation
  Split ting a program across 
many
 les, each of which 
can be com piled without 

knowledge of what is in 

the ot
 les.
the assembler could not know where a module’s instructions or data would be 
placed relative to other modules. When the linker places a module in memory, all 

abso lute references must be 
relocated
 to re
 ect its true location. Since the linker 
has relocation information that iden
 es all relocatable references, it ca
  ciently 
 nd and backpatch these references.
 e linker produces an executab
 le that can run on a computer. Typically, 
this
 le has the same format as an ob
 le, except that it contains no unresolved 
references or relocation information.
 A.4 LoadingA program that links without an error can be run. Before being run, the program 

r
 le on secondary storage, such as a disk. On UNIX systems, the  operating 
FIGURE A.3.1 The linker searches a collection of object ﬁ les and program libraries to 
ﬁ nd nonlocal routines used in a program, combines them into a single executable ﬁ
 le, and 
resolves references between routines in different ﬁ
 les.  A.4 Loading A-
19
A-20 Appendix A Assemblers, Linkers, and the SPIM Simulator
system kernel brings a program into memory and starts it running. To start a program, 
the operating system performs the following steps: 
1. It reads the executab
 le’s header to determine the size of the text and data 
segments.
2. It creates a new address space for the progra
 is address space is large 
enough to hold the text and data segments, along with a stack segment (see 

Section A.5).
3. It copies instructions and data from the executab
 le into the new address 
space.
4. It copies arguments passed to the program onto the stack.
5. It initializes the machine registers. In general, most registers are cleared, but 
the stack pointer must be assigned the address of th
 rst free stack location 
(see Section A.5).
6. It jumps to a start-up routine that copies the program’s arguments from the 
stack to registers and calls the program’s 
main routine. If the 
main routine 
returns, the start-up routine terminates the program with the exit system call. 
 A.5 Memory Usage
 e next few sections elaborate the description of the MIPS architecture presented 
earlier in the book. Earlier chapters focused primarily on hardware and its relationship 
with low-level so
 ware. 
 ese sections focus primarily on how assembly language 
programmers use MIPS hardware
 ese sections describe a set of conventions 
followed on many MIPS systems. For the most part, the hardware does not impose 

these conventions. Instead, they represent an agreement among programmers to 

follow the same set of rules so that so
 ware written by
 erent people can work 
together and mak
 ective use of MIPS hardware.
Systems based on MIPS processors typically divide memory into three parts 
(see 
Figure A.5.1
 e 
 rst part, near the bottom of the address space (starting 
at address 400000
hex), is the 
text segment
, which holds the program’s instructions.
 e second part, above the text segment, is the 
data segment
, which is further 
divided into two parts. 
Static data
 (starting at address 10000000
hex) contains 
objects whose size is known to the compiler and whose lifetime—the interval 

dur ing which a program can access them—is the program’s entire execution. For 

example, in C, global variables are statically allocated, since they can be referenced 
static data
 e portion 
of memory that contains 
data whose size is known 

to the com piler and whose 

lifetime is the program’s 

entire execution.

FIGURE A.5.1 Layout of memory.
 Dynamic dataStatic dataReserved
Stack segment
Data segmentText segment
7fffffffhex10000000hex400000hexBecause the data segment begins far above the program at address 10000000
hex, load and store instructions cannot directly reference data objects with their 16-bit 
 set 
 elds (see Section 2.5 in Chapter 2). For example, to load the word in the 
data segment at address 10010020
hex into register 
$v0 requires two instructions:
lui $s0, 0x1001 # 0x1001 means 1001 base 16 
lw $v0, 0x0020($s0) # 0x10010000 + 0x0020 = 0x10010020
 e 0x before a number means that it is a hexadecimal value. For example, 
0x8000 is 8000hex or 32,768
ten
.)To avoid repeating the 
lui instruction at every load and store, MIPS systems 
typically dedicate a register (
$gp) as a global pointer
 to the static data segmen
 is register contains address 10008000
hex, so load and store instructions can use their 
signed 16-bit o
 set 
 elds to access th
 rst 64 KB of the static data segment. With 
this global pointer, we can rewrite the example as a single instruction: 
lw $v0, 0x8020($gp)Of course, a global pointer register makes addressing locations 10000000
hex–10010000hex faster than other heap location
 e MIPS compiler usually stores 
global variables
 in this area, because these variables hav
 xed locations an
 t bet-
ter than other global data, such as arrays.
Hardware/
Software

Interface A.5 Memory Usage 
A-21anytime during a program’s executio
 e linker both assigns static objects to 
locations in the data segment and resolves references to these objects.
Immediately above static data is 
dynamic data
 is data, as its name implies, is 
allocated by the program as it executes. In C programs, the 
malloc library rou tine 

A-22 Appendix A Assemblers, Linkers, and the SPIM Simulator
 nds and returns a new block of memory. Since a compiler cannot predict how 
much memory a program will allocate, the operating system expands the dynamic 
data area to meet demand. As the upward arrow in th
 gure indicates, 
malloc expands the dynamic area with the 
sbrk system call, which causes the operating 
system to add more pages to the program’s virtual address space (see Section 5.7 in 

Chapter 5) immediately above the dynamic data segment.
 e third part, the program 
stack segment
, resides at the top of the virtual 
address space (starting at addr
 f
hex). Like dynamic data, the maximum size 
of a program’s stack is not known in advance. As the program pushes values on to 

the stack, the operating system expands the stack segment down toward the data 

segment.
 is three-part division of memory is not the only possible one. However, it has 
two important characteristics: the two dynamically expandable segments are as far 

apart as possible, and they can grow to use a program’s entire address space
. A.6 Procedure Call Convention
Conventions governing the use of registers are necessary when procedures in a 

program are compiled separately. To compile a particular procedure, a compiler 

must know which registers it may use and which registers are reserved for other 

procedures. Rules for using registers are called 
register use
 or 
procedure call 
conventions
. As the name implies, these rules are, for the most part, conventions 
fol lowed by so
 ware rather than rules enforced by hardware. However, most com-
pilers and programmers try very hard to follow these conventions because violat-

ing them causes insidious bugs.
 e calling convention described in this section is the one used by the gcc com-
piler
 e native MIPS compiler uses a more complex convention that is slightly 
faster.
 e MIPS CPU contains 32 general-purpose registers that are numbered  0–31. 
Register 
$0 always contains the hardwired value 0. 
 Registers 
$at (1), $k0 (26), and 
$k1 (27) are reserved for the assembler and 
operating system and should not be used by user programs or compilers.
 Registers 
$a0–$a3 (4–7) are used to pass th rst four arguments to rou tines 
(remaining arguments are passed on the stack). Registers 
$v0 and 
$v1 (2, 3) are used to return values from functions.
stack segment
 e portion of memory used 
by a  program to hold 

procedure call frames.
register use convention
 Also called 
procedure 
call  convention
. A so
 ware proto col 
governing the use of 

registers by procedures.

 Registers 
$t0–$t9 (8–15, 24, 25) are 
caller-saved registers
 that are used 
to hold temporary quantities that need not be preserved across calls (see 
Section 2.8 in Chapter 2).
 Registers 
$s0–$s7 (16–23) are 
callee-saved registers
 that hold long-lived 
values that should be preserved across calls.
 Register 
$gp (28) is a global pointer that points to the middle of a 64K block 
of memory in the static data segment.
 Register 
$sp (29) is the stack pointer, which points to the last location on 
the stack. Register 
$fp (30) is the frame pointer
 e jal instruction writes 
register 
$ra (31), the return address from a procedure call
 ese two regis-
ters are explained in the next section.
 e two-letter abbreviations and names for these registers—for example 
$sp for the stack pointer—re
 ect the registers’ intended uses in the procedure call 
convention. In describing this convention, we will use the names instead of regis ter 
numbers. 
Figure A.6.1
 lists the registers and describes their intended uses.
Procedure Calls is section describes the steps that occur when one procedure (the 
caller
) invokes 
another procedure (the 
callee). Programmers who write in a high-level language 
(like C or Pascal) never see the details of how one procedure calls another, because 

the compiler takes care of this low-level bookkeeping. However, assembly language 

programmers must explicitly implement every procedure call and return.
Most of the bookkeeping associated with a call is centered around a block 
of memory called a 
procedure call frame
 is memory is used for a variety of 
purposes: 
 To hold values passed to a procedure as arguments
 To save registers that a procedure may modify, but which the procedure’s 

caller does not want changed
 To provide space for variables local to a procedure
In most programming languages, procedure calls and returns follow a strict 
 rst-out (LIFO) order, so this memory can be allocated and deallocated on 
a stack, which is why these blocks of memory are sometimes called stack frames.
Figure A.6.2
 shows a typical stack frame
 e frame consists of the memory 
between the frame pointer (
$fp), which points to th
 rst word of the frame, 
and the stack pointer (
$sp), which points to the last word of the frame
 e stack 
grows down from higher memory addresses, so the frame pointer points above the 
caller-saved register
 A regis ter saved by the 
routine  being called.
callee-saved register
 A regis ter saved by 

the routine making a 

procedure call.
procedure call frame
 A block of memory that 

is used to hold values 

passed to a procedure 

as arguments, to save 

registers that a procedure 

may modify but that the 

procedure’s caller does not 

want changed, and to pro-

vide space for variables 

local to a procedure.
 A.6 Procedure Call Convention 
A-23
A-24 Appendix A Assemblers, Linkers, and the SPIM Simulator
stack pointer.
 e executing procedure uses the frame pointer to quickly access 
values in its stack frame. For example, an argument in the stack frame can be 
loaded into register 
$v0 with the instruction
lw $v0, 0($fp) Register nameNumber 
Usage$zero0constant 0
$at1reserved for assembler 
$v02expression evaluation and results of a function
$v13expression evaluation and results of a function
$a04argument 1 
$a15argument 2 
$a26argument 3 
$a37argument 4 
$t08temporary (not preserved across call) 
$t19temporary (not preserved across call) 
$t210temporary (not preserved across call) 
$t311temporary (not preserved across call) 
$t412temporary (not preserved across call) 
$t513temporary (not preserved across call) 
$t614temporary (not preserved across call) 
$t715temporary (not preserved across call) 
$s016saved temporary (preserved across call) 
$s117saved temporary (preserved across call) 
$s218saved temporary (preserved across call) 
$s319saved temporary (preserved across call) 
$s420saved temporary (preserved across call) 
$s521saved temporary (preserved across call) 
$s622saved temporary (preserved across call) 
$s723saved temporary (preserved across call) 
$t824temporary (not preserved across call) 
$t925temporary (not preserved across call) 
$k026reserved for OS kernel 
$k127reserved for OS kernel 
$gp28pointer to global area 
$sp29stack pointer 
$fp30frame pointer 
$ra31return address (used by function call) 
FIGURE A.6.1 MIPS registers and usage convention.
 
A stack frame may be built in man
 erent ways; however, the caller and 
callee must agree on the sequence of st
 e steps below describe the calling 
convention used on most MIPS mac
 is convention comes into play at three 
points during a procedure call: immediately before the caller invokes the callee, 
just as the callee starts executing, and immediately before the callee returns to the 

caller. In th
 rst part, the caller puts the procedure call arguments in stan dard 
places and invokes the callee to do the following:
1. Pass arguments. By convention, th
 rst four arguments are passed in regis-
ters 
$a0–$a3. Any remaining arguments are pushed on the stack and appear 
at the beginning of the called procedure’s stack frame.
2. Save caller-saved register
 e called procedure can use these registers 
($a0–$a3 and 
$t0–$t9) withou
 rst saving their value. If the caller expects 
to use one of these registers a
 er a call, it must save its value before the call.
3. Execute a 
jal instruction (see Section 2.8 of Chapter 2), which jumps to the 
callee’s
 rst instruction and saves the return address in register 
$ra.FIGURE A.6.2 Layout of a stack frame. 
 e frame pointer (
$fp) points to th rst word in the 
currently executing procedure’s stack frame
 e stack pointer (
$sp) points to the last word of the frame
 e  rst four arguments are passed in registers, so th
 h argument is th
 rst one stored on the stack. 
 A.6 Procedure Call Convention 
A-25Argument 6Argument 5Saved registers
Local variables
Higher memory addresses
Lower memory addresses
Stack
grows
$fp$sp
A-26 Appendix A Assemblers, Linkers, and the SPIM Simulator
Before a called routine starts running, it must take the following steps to set up 
its stack frame: 
1. Allocate memory for the frame by subtracting the frame’s size from the stack 
pointer.
2. Save callee-saved registers in the frame. A callee must save the values in 
these registers (
$s0–$s7, $fp, and 
$ra) before altering them, since the 
caller expects to
 nd these registers unchanged a
 er the call. Register 
$fp is saved by every procedure that allocates a new stack frame. However, register 
$ra only needs to be saved if the callee itself makes a call e other callee-
saved registers that are used also must be saved.
3. Establish the frame pointer by adding the stack frame’s size minus 4 to 
$sp and storing the sum in register 
$fp. e MIPS register use convention provides callee- and caller-saved registers, 
because both types of registers are advantageo
 erent circumstances. Callee-
saved registers are better used to hold long-lived values, such as variables from a 
user’s progra
 ese registers are only saved during a procedure call if the callee 
expects to use the register. On the other hand, caller-saved registers are bet ter used 

to hold short-lived quantities that do not persist across a call, such as immediate 

values in an address calculation. During a call, the callee can also use these registers 

for short-lived temporaries.
Finally, the callee returns to the caller by executing the following steps: 
1. If the callee is a function that returns a value, place the returned value in 
register 
$v0.2. Restore all callee-saved registers that were saved upon procedure entry.

3. Pop the stack frame by adding the frame size to 
$sp.4. Return by jumping to the address in register 
$ra.Elaboration: A programming language that does not permit 
recursive procedures
—procedures that call themselves either directly or indirectly through a chain of calls—need not allocate frames on a stack. In a nonrecursive language, each procedure’s frame 
may be statically allocated, since only one invocation of a procedure can be active at a 

time. Older versions of Fortran prohibited recursion, because statically allocated frames 

produced faster code on some older machines. However, on load store architec 
tures like 
MIPS, stack frames may be just as fast, because a frame pointer register points directly 
Hardware/
Software
Interfacerecursive procedures
Procedures that call 
themselves  either directly 

or indirectly through a 

chain of calls.

to the active stack frame, which permits a single load or store instruc 
tion to access values in the frame. In addition, recursion is a valuable programming technique.
Procedure Call ExampleAs an example, consider the C routine
main (){
 printf (“The factorial of 10 is %d\n”, fact (10));

}int fact (int n){
 if (n < 1)

  return (1);

 else
  return (n * fact (n - 1));

}which computes and prints 10! (the factorial of 10, 10! 
= 10 × 9 × . . . × 1). fact is a recursive routine that computes 
n! by multiplying 
n times (n - e assembly 
code for this routine illustrates how programs manipulate stack frames.
Upon entry, the routine 
main creates its stack frame and saves the two callee-
saved registers it will modify: 
$fp and 
$ra e frame is larger than required for 
these two register because the calling convention requires the minimum size of a 
stack frame to be 24 byt
 is minimum frame can hold four argument registers 
($a0–$a3) and the return address 
$ra, padded to a double-word boundary 
(24 bytes). Since 
main also needs to save 
$fp, its stack frame must be two words 
larger (remember: the stack pointer is kept doubleword aligned).
 .text .globl main

main:
 subu $sp,$sp,32 # Stack frame is 32 bytes long

 sw $ra,20($sp) # Save return address

 sw $fp,16($sp) # Save old frame pointer

 addiu $fp,$sp,28 # Set up frame pointer
 e routine 
main then calls the factorial routine and passes it the single argument 
 er fact returns, 
main calls the library routine 
printf and passes it both 
a format string and the result returned from 
fact: A.6 Procedure Call Convention 
A-27
A-28 Appendix A Assemblers, Linkers, and the SPIM Simulator
 li $a0,10 # Put argument (10) in $a0
 jal fact # Call factorial function
 la $a0,$LC # Put format string in $a0
 move $a1,$v0 # Move fact result to $a1

 jal printf # Call the print function
Finally, a
 er printing the factorial, 
main returns. Bu
 rst, it must restore the 
registers it saved and pop its stack frame:
 lw $ra,20($sp) # Restore return address

 lw $fp,16($sp) # Restore frame pointer

 addiu $sp,$sp,32 # Pop stack frame

 jr $ra 
# Return to caller .rdata
$LC:
 .ascii “The factorial of 10 is %d\n\000”
 e factorial routine is similar in structure to 
main. First, it creates a stack frame 
and saves the callee-saved registers it will use. In addition to saving 
$ra and 
$fp, fact also saves its argument (
$a0), which it will use for the recursive call:
    .text 

 fact:
  subu $sp,$sp,32 # Stack frame is 32 bytes long

  sw $ra,20($sp) # Save return address

  sw $fp,16($sp) # Save frame pointer

  addiu $fp,$sp,28 # Set up frame pointer

  sw $a0,0($fp) # Save argument (n)
 e heart of the 
fact routine performs the computation from the C program. 
It tests whether the argument is greater than 0. If not, the routine returns the 
value 1. If the argument is greater than 0, the routine recursively calls itself to 

compute 
fact(n–1) and multiplies that value times
 n: lw $v0,0($fp) # Load n
 bgtz $v0,$L2 # Branch if n > 0

 li $v0,1 # Return 1

 jr $L1 # Jump to code to return
$L2: lw $v1,0($fp) # Load n

 subu $v0,$v1,1 # Compute n - 1

 move $a0,$v0 # Move value to $a0

 jal fact 
# Call factorial function lw $v1,0($fp) # Load n
 mul $v0,$v0,$v1 # Compute fact(n-1) * n
Finally, the factorial routine restores the callee-saved registers and returns the 
value in register 
$v0: $L1:  
# Result is in $v0 lw $ra, 20($sp) # Restore $ra
 lw $fp, 16($sp) # Restore $fp

 addiu $sp, $sp, 32 # Pop stack

 jr $ra 
# Return to callerStack in Recursive Procedure
Figure A.6.3
 shows the stack at the call 
fact(7). main runs
 rst, so its frame 
is deepest on the stack. 
main calls 
fact(10), whose stack frame is next on the 
stack. Each invocation recursively invokes 
fact to compute the next-lowest 
factorial
 e stack frames parallel the LIFO order of these calls. What does the 
stack look like when the call to 
fact(10) returns?
EXAMPLE A.6 Procedure Call Convention 
A-29FIGURE A.6.3 Stack frames during the call of 
fact(7). mainfact (10)
fact (9)
fact (8)
fact (7)
Stack
Stack grows
Old $ra
Old $fpOld $a0Old $ra

Old $fpOld $a0Old $ra

Old $fpOld $a0Old $ra

Old $fpOld $a0Old $ra

Old $fp
A-30 Appendix A Assemblers, Linkers, and the SPIM Simulator
ANSWERElaboration: The difference between the MIPS compiler and the gcc compiler is that 
the MIPS compiler usually does not use a frame pointer, so this register is available as 
another callee-saved register, 
$s8. This change saves a couple of instructions in the 

procedure call and return sequence. However, it complicates code generation, because 

a procedure must access its stack frame with $sp, whose value can change during a 
procedure’s execution if values are pushed on the stack.
Another Procedure Call ExampleAs another example, consider the following routine that computes the 
tak func-
tion, which is a widely used benchmark created by Ikuo Takeuchi
 is function 
does not compute anything useful, but is a heavily recursive program that illustrates 
the MIPS calling convention.
int tak (int x, int y, int z){
 if (y < x)

  return 1+ tak (tak (x - 1, y, z),

   tak (y - 1, z, x), 

   tak (z - 1, x, y));

 else
 return z;

}int main (){
 tak(18, 12, 6);

} e assembly code for this program is shown below.
 e tak function
 rst saves 
its return address in its stack frame and its arguments in callee-saved regis ters, 
since the routine may make calls that need to use registers 
$a0–$a2 and 
$ra e function uses callee-saved registers, since they hold values that persist over the 
mainStack
Stack grows
Old $ra
Old $fp
lifetime of the function, which includes several calls that could potentially modify 
registers.
 .text .globl tak
tak: subu $sp, $sp, 40

 sw $ra, 32($sp)
 sw $s0, 16($sp) # x
 move $s0, $a0

 sw $s1, 20($sp) # y

 move $s1, $a1

 sw $s2, 24($sp) # z

 move $s2, $a2

 sw $s3, 28($sp) # temporary
 e routine then begins execution by testing if 
y < x. If not, it branches to label 
L1, which is shown below.
 bge $s1, $s0, L1 # if (y < x)
If 
y < x, then it executes the body of the routine, which contains four recursive 
 e 
 rst call uses almost the same arguments as its parent:
 addiu $a0, $s0, -1

 move $a1, $s1

 move $a2, $s2

 jal tak 
# tak (x - 1, y, z) move $s3, $v0
Note that the result from the
 rst recursive call is saved in register 
$s3, so that it 
can be used later.
 e function now prepares arguments for the second recursive call.
 addiu $a0, $s1, -1

 move $a1, $s2

 move $a2, $s0

 jal tak 
# tak (y - 1, z, x)In the instructions below, the result from this recursive call is saved in register 
$s0. But 
 rst we need to read, for the last time, the saved value of th
 rst argu-
ment from this register.
 A.6 Procedure Call Convention 
A-31
A-32 Appendix A Assemblers, Linkers, and the SPIM Simulator
 addiu $a0, $s2, -1
 move $a1, $s0

 move $a2, $s1

 move $s0, $v0

 jal tak 
# tak (z - 1, x, y) er the three inner recursive calls, we are ready for th
 nal recursive call
 er the call, the function’s result is in 
$v0 and control jumps to the function’s epilogue.
move $a0, $s3

move $a1, $s0

move $a2, $v0

jal tak # 
tak (tak(...), tak(...), tak(...))addiu $v0, $v0, 1

j L2 is code at label 
L1 is the consequent of the 
if-then-else
 statement. It just moves 
the value of argument 
z into the return register and falls into the function epilogue.
L1:
 move $v0, $s2
 e code below is the function epilogue, which restores the saved registers and 
returns the function’s result to its caller.
L2:
 lw $ra, 32($sp)

 lw $s0, 16($sp)

 lw $s1, 20($sp)

 lw $s2, 24($sp)

 lw $s3, 28($sp)

 addiu $sp, $sp, 40

 jr $ra
 e main routine calls the 
tak function with its initial arguments, then takes the 
computed result (7) and prints it using SPIM’s system call for printing integers.
 .globl main

main:
 subu $sp, $sp, 24

 sw $ra, 16($sp)
 li $a0, 18
 li $a1, 12

 li $a2, 6
 jal tak 
# tak(18, 12, 6) move $a0, $v0

 li $v0, 1 # print_int syscall

 syscall lw $ra, 16($sp)
 addiu $sp, $sp, 24

 jr $ra
 A.7 Exceptions and Interrupts
Section 4.9 of Chapter 4 describes the MIPS exception facility, which responds both 
to exceptions caused by errors during an instruction’s execution and to external 

interrupts caused by I/O de
 is section describes exception and 
interrupt 
handling
 in more detail.
1 In MIPS processors, a part of the CPU called 
coprocessor 0
 records the information the so
 ware needs to handle excep tions and interrupts. 
 e MIPS simulator SPIM does not implement all of copro cessor 0’s registers, since 
many are not useful in a simulator or are part of the memory system, which SPIM 

does not implement. However, SPIM does provide the following coprocessor 0 

registers:
RegisternameRegisternumberUsage
BadVAddr 8 memory address at which an offending memory reference occurred 
Count 9 timer 
Compare 11value compared against timer that causes interrupt when they match
Status 12interrupt mask and enable bits
Cause13exception type and pending interrupt bits 
EPC14address of instruction that caused exception
 g16 guration of machine
 is section discusses exceptions in the MIPS-32 architecture, which is what SPIM imple ments 
in Version 7.0 and later. Earlier versions of SPIM implemented the MIPS-1 architecture, which 
handled exceptions slightl
 erently. Converting programs from these versions to run on 
MIPS-32 should not be  cult, as the changes are limited to the Status and Cause regist
 elds and the  replacement of the 
rfe instruction by the 
eret instruction.
interrupt handler
 A piece of code that is run 
as a result of an exception 

or an interrupt.
 A.7 Exceptions and Interrupts 
A-33
A-34 Appendix A Assemblers, Linkers, and the SPIM Simulator
 ese seven registers are part of coprocessor 0’s register s
 ey are accessed 
by the 
mfc0 and 
mtc0 instruction
 er an exception, register EPC contains the 
address of the instruction that was executing when the exception occurred. If the 
exception was caused by an external interrupt, then the instruction will not have 

started executing. All other exceptions are caused by the execution of the instruc-

tion at EPC, except when the o
 ending instruction is in the delay slot of a branch 
or jump. In that case, EPC points to the branch or jump instruction and the BD bit 

is set in the Cause register. When that bit is set, the exception handler must look 

at EPC + 4 for the o ending instruction. However, in either case, an excep tion 

handler properly resumes the program by returning to the instruction at EPC.
If the instruction that caused the exception made a memory access, register 
BadVAddr contains the referenced memory location’s address.
 e Count register is a timer that increments a
 xed rate (by default, every 
10 milliseconds) while SPIM is running. When the value in the Count register 

equals the value in the Compare register, a hardware interrupt at priority level 5 

occurs.
Figure A.7.1
 shows the subset of the Status regist
 elds implemented by the 
MIPS simulator SP
 e interrupt mask eld contains a bit for each of the 
six hardware and two so
 ware interrupt levels. A mask bit that is 1 allows inter-
rupts at that level to interrupt the processor. A mask bit that is 0 disables inter-

rupts at that level. When an interrupt arrives, it sets its interrupt pending bit in the 

Cause register, even if the mask bit is disabled. When an interrupt is pending, it will 

interrupt the processor when its mask bit is subsequently enabled.
 e user mode bit is 0 if the processor is running in kernel mode and 1 if it is 
running in user mode. On SPIM, this bit
 xed at 1, since the SPIM processor 
does not implement kernel mode
 e exception level bit is normally 0, but is set to 
1 a er an exception occurs. When this bit is 1, interrupts are disabled and the EPC 

is not updated if another exception occur is bit prevents an exception handler 

from being disturbed by an interrupt or exception, but it should be reset when the 

ha
 nishes. If the 
interrupt enable bit is 1, interrupts are allowed. If it is 
0, they are disabled.
Figure A.7.2
 shows the subset of Cause regist
 elds that SPIM implements. 
 e branch delay bit is 1 if the last exception occurred in an instruction executed in 
the delay slot of a branc
 e interrupt pending bits become 1 when an inter rupt 

is raised at a given hardware or so ware level
 e exception code register describes 
the cause of an exception through the following codes:
NumberName
Cause of exception0Intinterrupt (hardware)
4AdELaddress error exception (load or instruction fetch) 
5AdESaddress error exception (store) 

6IBEbus error on instruction fetch 

7DBEbus error on data load or store 
8Syssyscall exception 
9Bpbreakpoint exception 
10RIreserved instruction exception
11CpUcoprocessor unimplemented

12Ovarithmetic o ow exception
13Trtrap
15FPE oating point
Exceptions and interrupts cause a MIPS processor to jump to a piece of code, 
at address 80000180
hex (in the kernel, not user address space), called an 
exception 
handler
 is code examines the exception’s cause and jumps to an appropriate point 
in the operating syst
 e operating system responds to an exception either by 
terminating the process that caused the exception or by performing some action. 
A process that causes an error, such as executing an unimplemented instruction, is 

killed by the operating system. On the other hand, other exceptions such as page 
FIGURE A.7.1 The Status register.
 158410
Interrupt
maskUsermodeExceptionlevel
Interrupt
enable
FIGURE A.7.2 The Cause register.
 1531862
Pending
interrupts
Branch
delay
Exceptioncode A.7 Exceptions and Interrupts 
A-35
A-36 Appendix A Assemblers, Linkers, and the SPIM Simulator
faults are requests from a process to the operating system to perform a service, 
such as bringing in a page fro
 e operating system processes these requests 
and resumes the pro
 e 
 nal type of exceptions are interrupts from external 
de
 ese generally cause the operating system to move data to or from an I/O 
device and resume the interrupted process. 
 e code in the example below is a simple exception handler, which invokes 
a routine to print a message at each exception (but not interrup
 is code is 
similar to the exception handler (
exceptions.s) used by the SPIM simulator.
Exception Handler e exception ha
 rst saves register 
$at, which is used in pseudo-
instructions in the handler code, then saves 
$a0 and 
$a1, which it later uses to 
pass argumen
 e exception handler cannot store the old values from these 
registers on the stack, as would an ordinary routine, because the cause of the 

exception might have been a memory reference that used a bad value (such 

as 0) in the stack pointer. Instead, the exception handler stores these registers 

in an exception handler register (
$k1, since it can’t access memory without 
using 
$at) and two memory locations (
save0 and 
save1). If the exception 
routine itself could be interrupted, two locations would not be enough since 

the second exception would overwrite values saved during th
 rst exception. 
However, this simple exception ha
 nishes running before it enables 
interrupts, so the problem does not arise.
.ktext 0x80000180mov $k1, $at    # Save $at register
sw  $a0, save0  # Handler is not re-entrant and can’t use
sw  $a1, save1  # stack to save $a0, $a1
                # Don’t need to save $k0/$k1 e exception handler then moves the Cause and EPC registers into CPU 
register
 e Cause and EPC registers are not part of the CPU register set. 
In stead, they are registers in coprocessor 0, which is the part of the CPU that 
han dles exception
 e instruction
 mfc0 $k0, $13 moves coprocessor 0’s 
register 13 (the Cause register) into CPU register 
$k0. Note that the exception 
handler need not save registers 
$k0 and 
$k1, because user programs are not 
supposed to use these register
 e exception handler uses the value from the 
Cause reg ister to test whether the exception was caused by an interrupt (see 

the preceding ta ble). If so, the exception is ignored. If the exception was not an 

interrupt, the handler calls 
print_excp to print a message.
EXAMPLE
mfc0  $k0, $13        # Move Cause into $k0
srl  $a0, $k0, 2     # Extract ExcCode field
andi  $a0, $a0, Oxf
bgtz  $a0, done       # Branch if ExcCode is Int (0)

mov  $a0, $k0        # Move Cause into $a0
mfco  $a1, $14        # Move EPC into $a1

jal  print_excp      # Print exception error message
Before returning, the exception handler clears the Cause register; resets 
the Status register to enable interrupts and clear the EXL bit, which allows 
subse quent exceptions to change the EPC register; and restores registers 
$a0, $a1, and 
$at. It then executes the 
eret (exception return) instruction, which 
returns to the instruction pointed to b
 is exception handler returns 
to the instruction following the one that caused the exception, so as to not 

re-execute the faulting instruction and cause the same exception again.
done:    mfc0    $k0, $14       # Bump EPC         addiu   $k0, $k0, 4    # Do not re-execute
                                # faulting instruction
         mtc0    $k0, $14       # EPC         mtc0    $0, $13        # Clear Cause register
         mfc0    $k0, $12       # Fix Status register         andi    $k0, Oxfffd    # Clear EXL bit
         ori     $k0, Ox1       # Enable interrupts
         mtc0    $k0, $12         lw      $a0, save0     # Restore registers         lw      $a1, save1
         mov     $at, $k1         eret                   # Return to EPC
         .kdatasave0:   .word 0

save1:   .word 0
 A.7 Exceptions and Interrupts 
A-37
A-38 Appendix A Assemblers, Linkers, and the SPIM Simulator
Elaboration: On real MIPS processors, the return from an exception handler is more 
complex. The exception handler cannot always jump to the instruction following 
EPC. For 
example, if the instruction that caused the exception was in a branch instruction’s delay 
slot (see Chapter 4), the next instruction to execute may not be the following instruction 

in memory.
 A.8 Input and Output
SPIM simulates one I/O device: a memory-mapped console on which a program 
can read and write characters. When a program is running, SPIM connects its 

own terminal (or a separate console window in the X-window version 
xspim or 
the Windows version 
PCSpim) to the processor. A MIPS program running on 
SPIM can read the characters that you type. In addition, if the MIPS program 

writes characters to the terminal, they appear on SPIM’s terminal or console win-

dow. One exception to this rule is control-C: this character is not passed to the 

program, but instead causes SPIM to stop and return to command mode. When 

the program stops running (for example, because you typed control-C or because 

the program hit a breakpoint), the terminal is reconnected to SPIM so you can type 

SPIM commands. 
To use memory-mapped I/O (see below), 
spim or 
xspim must be started 
with the 
-mapped_io ag. 
PCSpim can enable memory-mapped I/O through a 
comman
 ag or the “Settings” dialog.
 e terminal device consists of two independent units: a 
receiver
 and a 
trans-
mitter
 e receiver reads characters from the keyboard
 e transmitter displays 
characters on the console.
 e two units are completely independen
 is means, 
for example, that characters typed at the keyboard are not automatically echoed on 

the display. Instead, a program echoes a character by reading it from the receiver 

and writing it to the transmitter.
A program controls the terminal with four memory-mapped device registers, 
as shown in 
Figure A.8.1
. “Memory-mapped’’ means that each register  appears as 

a special memory locatio
 e Receiver Control register
 is at location
 0000hex. Only two of its bits are actually used. Bit 0 is called “ready’’: if it is 1, it means 

that a character has arrived from the keyboard but has not yet been read from the 

Receiver Data register
 e ready bit is read-only: writes to it are ignored
 e ready 
bit changes from 0 to 1 when a character is typed at the keyboard, and it changes 

from 1 to 0 when the character is read from the Receiver Data register.

Bit 1 of the Receiver Control register is the keyboard “interrupt enable.
 is bit may be both read and written by a progra
 e interrupt enable is initially 0. 
If it is set to 1 by a program, the terminal requests an interrupt at hardware level 1 
whenever a character is typed, and the ready bit becomes 1. However, for the inter-

rupt to a
 ect the processor, interrupts must also be enabled in the Status register 
(see Section A.7). All other bits of the Receiver Control register are unused.
 e second terminal device register is the 
Receiver Data register
 (at address 
 0004hex e low-order eight bits of this register contain the last character typed 
at the keyboard. All other bits conta
 is register is read-only and changes 
only when a new character is typed at the keyboard. Reading the Receiver Data 

register resets the ready bit in the Receiver Control register t
 e value in this 
regist
 ned if the Receiver Control register is 0.
 e third terminal device register is the 
Transmitter Control register
 (at address 
 0008hex). Only the low-order two bits of this register are used
 ey behave much 
like the corresponding bits of the Receiver Control register. Bit 0 is called “ready’’ 
FIGURE A.8.1 The terminal is controlled by four device registers, each of which appears 
as a memory location at the given address. 
Only a few bits of these registers are actually used
 e others always read as 0s and are ignored on writes. 
1Interrupt
enable
Ready1Unused
Receiver control
(0xffff0000)8Received byteUnusedReceiver data
(0xffff0004)1InterruptenableReady1UnusedTransmitter control
(0xffff0008)Transmitter data
(0xffff000c)8Transmitted byteUnused A.8 Input and Output 
A-39
A-40 Appendix A Assemblers, Linkers, and the SPIM Simulator
and is read-only. If this bit is 1, the transmitter is ready to accept a new character 
for output. If it is 0, the transmitter is still busy writing the previous character. 

Bit 1 is “interrupt enable’’ and is readable and writable. If this bit is set to 1, then 

the terminal requests an interrupt at hardware level 0 whenever the transmitter is 

ready for a new character, and the ready bit becomes 1.
 e 
 nal device register is the 
Transmitter Data register
 (at addr
 000chex). When a value is written into this location, its low-order eight bits (i.e., an ASCII 

character as in Figure 2.15 in Chapter 2) are sent to the console. When the Trans-

mitter Data register is written, the ready bit in the Transmitter Control register is 

reset t
 is bit stays 0 until enough time has elapsed to transmit the character 
to the terminal; then the ready bit becomes 1 aga
 e Trans mitter Data register 
should only be written when the ready bit of the Transmitter Control register is 1. 

If the transmitter is not ready, writes to the Transmitter Data register are ignored 

(the write appears to succeed but the character is not output).
Real computers require time to send characters to a console or terminal
 ese 
time lags are simulated by SPIM. For example, a
 er the transmitter starts to write a 
character, the transmitter’s ready bit becomes 0 for a while. SPIM measures time in 

instructions executed, not in real clock time
 is means that the transmitter does 
not become ready again until the processor execut
 xed number of instructions. 
If you stop the machine and look at the ready bit, it will not change. However, if you 

let the machine run, the bit eventually changes back to 1. 
 A.9 SPIMSPIM is a so
 ware simulator that runs assembly language programs written for 
processors that implement the MIPS-32 architecture, sp
 cally Release 1 of this 
architecture with
 xed memory mapping, no caches, and only coprocessors 0 
and 1.
2 SPIM’s name is just MIPS spelled backwards. SPIM can read and immedi-
ately execute assembly languag
 les. SPIM is a self-contained system for running 
2. Earlier versions of SPIM (before 7.0) implemented the MIPS-1 architecture used in the origi nal 
MIPS R2000 processor
 is architecture is almost a proper subset of the MIPS-32 architec ture, 
with th
 erence being the manner in which exceptions are handled. MIPS-32 also introduced 
approximately 60 new instructions, which are supported by SPIM. Programs that ran on the 

 earlier versions of SPIM and did not use exceptions should run unmo
 ed on newer ver sions of 
SPIM. Programs that used exceptions will require minor changes.

MIPS programs. It contains a debugger and provides a few operating system-like 
services. SPIM is much slower than a real computer (100 or more times). How ever, 

its low cost and wide availability cannot be matched by real hardware!
An obvious question is, “Why use a simulator when most people have PCs that 
contain processors that r
 cantly faster than SPIM?” One reason is that 
the processors in PCs are Intel 80
×86s, whose architecture is far less regular and 
far more complex to understand and program than MIPS processor
 e MIPS 
architecture may be the epitome of a simple, clean RISC machine.
In addition, simulators can provide a better environment for assembly pro-
gramming than an actual machine because they can detect more errors and  provide 

a better interface than can an actual computer. 
Finally, simulators are useful tools in studying computers and the programs that 
run on them. Because they are implemented in so
 ware, not silicon, simulators can 
be examined and easily mo
 ed to add new instructions, build new systems such 
as multiprocessors, or simply collect data.
Simulation of a Virtual Machine
 e basic MIPS architecture
  cult to program directly because of delayed 
branches, delayed loads, and restricted address mo
 is 
  culty is tolerable 
since these computers were designed to be programmed in high-level languages 

and present an interface designed for compilers rather than assembly language 

programmers. A good part of the programming complexity results from delayed 

instructions. A 
delayed branch
 requires two cycles to execute (see the 
Elabora tions
 on pages 284 and 322 of Chapter 4). In the second cycle, the instruction imme-

diately following the branch execut
 is instruction can perform useful work 
that normally would have been done before the branch. It can also be a 
nop (no operation) that does nothing. Similarly, 
delayed loads
 require two cycles to bring 
a value from memory, so the instruction immediately  following a load cannot use 

the value (see Section 4.2 of Chapter 4).
MIPS wisely chose to hide this complexity by having its assembler implement 
a virtual machine
. is virtual computer appears to have nondelayed branches 
and loads and a richer instruction set than the actual hardware
 e assembler 
reorga nizes
 (rearranges) instructions t
 ll the delay slo
 e virtual computer 
also provides 
pseudoinstructions
, which appear as real instructions in assembly 
lan guage program
 e hardware, however, knows nothing about pseudoinstruc-
tions, so the assembler must translate them into equivalent sequences of actual 

machine instructions. For example, the MIPS hardware only provides instructions 

to branch when a register is equal to or not equal to 0. Other conditional branches, 

such as one that branches when one register is greater than another, are synthesized 

by comparing the two registers and branching when the result of the comparison 

is true (nonzero).
virtual machine
 A virtual computer 
that appears to have 

nondelayed branches 

and loads and a richer 

 instruction set than the 

actual hardware.
 A.9 SPIM A-
41
A-42 Appendix A Assemblers, Linkers, and the SPIM Simulator
By default, SPIM simulates the richer virtual machine, since this is the machine 
that most programmer
 nd useful. However, SPIM can also simulate the 
delayed branches and loads in the actual hardware. Below, we describe the virtual 
machine and only mention in passing features that do not belong to the actual 

hardware. In doing so, we follow the convention of MIPS assembly language pro-

grammers (and compilers), who routinely use the extended machine as if it was 

implemented in silicon.
Getting Started with SPIM
 e rest of this appendix introduces SPIM and the MIPS R2000 Assembly lan-
guage. Many details should never concern you; however, the sheer volume of 

information can sometimes obscure the fact that SPIM is a simple, easy-to-use 

progra
 is section starts with a quick tutorial on using SPIM, which should 
enable you to load, debug, and run simple MIPS programs.
SPIM co
 erent versions fo
 erent types of computer system
 e one constant is the simplest version, called 
spim, which is a command-line-driven 
pro gram that runs in a console window. It operates like most programs of this type: 

you type a line of text, hit the 
return key, and 
spim executes your command. 
Despite its lack of a fancy interface, 
spim can do everything that its fancy cousins 
can do.
 ere are two fancy cousins to 
spim e version that runs in the X-windows 
environment of a UNIX or Linux system is called 
xspim. xspim is an easier pro-
gram to learn and use than 
spim, because its commands are always visible on the 
screen and because it continually displays the machine’s registers and memory. 

 e other fancy version is called 
PCspim and runs on Microso
  Windo
 e UNIX and Windows versions of 
SPIM
  are available online at the publisher’s 
companion Web site for this book. Tutorials on 
xspim, pcSpim, spim, and 
SPIM 
command-line options
  are also online.
If you are going to run SPIM on a PC running Microso
  Windows, you should 
 rst look at the tutorial on 
PCSpim
  on the companion Web site. If you are going 
to run SPIM on a computer 
running 
UNIX or Lin
ux, you should read the tutorial 
on 
xspim
 .Surprising Features
Although SPIM faithfully simulates the MIPS computer, SPIM is a simulator, and 

certain things are not identical to an actual computer
 e most obviou
 er-
ences are that instruction timing and the memory systems are not identical. 

SPIM does not simulate caches or memory latency, nor does it accurately r
 ect  oating-point operation or multiply and divide instruction delays. In addition, 
th
 oating-point instructions do not detect many error conditions, which would 
cause exceptions on a real machine.

Another surprise (which occurs on the real machine as well) is that a pseudo-
instruction expands to several machine 
instructions. When you single-step or 
exam ine memory, the instructions that you see ar
 erent from the source 
progra
 e correspondence between the two sets of instructions is fairly simple, 
since SPIM does not reorganize instructions t
 ll slots.
Byte OrderProcessors can number bytes within a word so the byte with the lowest number is 

either th
 most or rightmost one.
 e convention used by a machine is called 
its 
byte order
. MIPS processors can operate with either 
big-endian
 or 
 little-endian
 byte order. For example, in a big-endian machine, the directive 
.byte 0, 1, 2, 3 would result in a memory word containing
Byte #0123while in a little-endian machine, the word would contain
Byte #3210SPIM operates with both byte orders. SPIM’s byte order is the same as the byte 
order of the underlying machine that runs the simulator. For example, on an Intel 

80x86, SPIM is little-endian, while on a Macintosh or Sun SPARC, SPIM is big-

endian.
System Calls
SPIM provides a small set of operating system–like services through the system 

call (syscall) instruction. To request a service, a program loads the system call 
code (see 
Figure A.9.1
) into register 
$v0 and arguments into registers 
$a0–$a3 (or 
$f12 fo
 oating-point values). System calls that return values put their results 
in register 
$v0 (or 
$f0 fo
 oating-point results). For example, the follow ing code 
prints 
"the answer = 5":  .data
str:
  .asciiz “the answer = ”

  .text
 A.9 SPIM A-
43
A-44 Appendix A Assemblers, Linkers, and the SPIM Simulator
 li  $v0, 4    # system call code for print_str
 la $a0, str  # address of string to print 

 syscall 
 # print the string li $v0, 1    # system call code for print_int

 li $a0, 5    # integer to print

 syscall 
 # print it e print_int system call is passed an integer and prints it on the console. 
print_float prints a sing
 oating-point number; 
print_double prints 
a double precision number; and 
print_string is passed a pointer to a null- 
terminated string, which it writes to the console.
 e system calls 
read_int, read_float, and 
read_double to read an entire 
line of input up to and including the newline. Characters following the number 
are ignored. 
read_string has the same semantics as the UNIX library routine 
fgets. It reads up to 
n  1 characters into a b
 er and terminates the string with 
a null byte. If fewer than 
n  1 characters are on the current line, 
read_string reads up to and including the newline and again null-terminates the string. 
ServiceSystem call code
ArgumentsResultprint_int1$a0 = integerprint_float2$f12 oat
print_double3$f12 = doubleprint_string4$a0 = stringread_int5integer (in $v0) read_float6 oat (in 
$f0) read_double7double (in $f0) read_string8$a0 = buffer, 
$a1 = lengthsbrk9$a0 = amountaddress (in $v0) exit10print_char11$a0 = charread_char12char (in $v0)open13$a0 lename (string), 
$a1 ags, $a2 = mode
 le descriptor (in 
$a0)read14$a0 le descriptor, 

$a1 = buffer, $a2 = length
num chars read (in 

$a0)write15$a0 le descriptor, 

$a1 = buffer, $a2 = length
num chars written (in 

$a0)close16$a0 le descriptor
exit217$a0 = resultFIGURE A.9.1 System services.
 
Warning:
 Programs that use these syscalls to read from the terminal should not use 
memory-mapped I/O (see Section A.8).
sbrk returns a pointer to a block of memory containing 
n additional bytes. 
exit stops the program SPIM is running. 
exit2 terminates the SPIM pro gram, 
and the argument to 
exit2 becomes the value returned when the SPIM simulator 
itself terminates.
print_char and 
read_char write and read a single character. 
open, read, write, and 
close are the standard UNIX library calls.
  A.10
 MIPS R2000 Assembly Language
A MIPS processor consists of an integer processing unit (the CPU) and a collec-
tion of coprocessors that perform ancillary tasks or operate on other types of data, 

 oating-point numbers (see 
Figure A.10.1
). SPIM simulates two coproces-
sors. Coprocessor 0 handles exceptions and interrupts. Coprocessor 1 is the 

 oating-point unit. SPIM simulates most aspects of this unit.
Addressing ModesMIPS is a load store architecture, which means that only load and store instruc tions 

access memory. Computation instructions operate only on values in regis ter
 e bare machine provides only one memory-addressing mode: 
c(rx), which uses 
the sum of the immediate 
c and register 
rx as the addr
 e virtual machine 
provides the following addressing modes for load and store instructions:
Format
Address computation(register)contents of register immimmediate imm (register)immediate + contents of register labeladdress of label label ± immaddress of label + or – immediate label ± imm (register)address of label + or – (immediate + contents of register)
Most load and store instructions operate 
only on aligned data. A quantity is 
aligned
 if its memory address is a multiple of its size in byt
 erefore, a half word 
 A.10 MIPS R2000 Assembly Language 
A-45
A-46 Appendix A Assemblers, Linkers, and the SPIM Simulator
object must be stored at even addresses, and a full word object must be stored at 
addresses that are a multiple of four. However, MIPS provides some instructions to 

manipulate unaligned data 
(lwl,
 lwr,
 swl,
 and 
swr).Elaboration: The MIPS assembler (and SPIM) synthesizes the more complex address-ing modes by producing one or more instructions before the load or store to compute a 
complex address. For example, suppose that the label 
table referred to memory loca-
tion 0x10000004 and a program contained the instruction
ld $a0, table + 4($a1)The assembler would translate this instruction into the instructions
FIGURE A.10.1 MIPS R2000 CPU and FPU.
 CPURegisters$0$31Arithmetic
unitMultiplydivideLoHiCoprocessor 1 (FPU)Registers$0$31Arithmetic
unitRegistersBadVAddr
Coprocessor 0 (traps and memory)
StatusCauseEPCMemory

lui $at, 4096addu $at, $at, $a1
lw $a0, 8($at) rst instruction loads the upper bits of the label’s address into register 
$at, which 
is the register that the assembler reserves for its own use. The second instruction adds 
the contents of register $a1 to the label’s partial address. Finally, the load instruction 

uses the hardware address mode to add the sum of the lower bits of the label’s address 

and the offset from the original instruction to the value in register 
$at.Assembler SyntaxComments in assemb
 les begin with a sharp sign (
#). Everything from the 
sharp sign to the end of the line is ignored.
Iden
 ers are a sequence of alphanumeric characters, underbars (
_), and dots 
(.) that do not begin with a number. Instruction opcodes are reserved words that 
cannot
 be used as iden
 ers. Labels are declared by putting them at the beginning 
of a line followed by a colon, for example: 
 .dataitem: .word 1

 .text
 .globl main # Must be global

main: lw $t0, item
Numbers are base 10 by default. If they are preceded by
 0x, they are interpreted 
as hexadecimal. Hence, 256 and 0x100 denote the same value.
Strings are enclosed in double quotes (”). Special characters in strings follow the 
C convention: 
 newline 
\n tab 
\t quote 
\”SPIM supports a subset of the MIPS assembler directives:
.align n Align the next datum on a 2
n byte boundary. For 
 example, 
.align 2 aligns the next value on a word 
boundary. 
.align 0 turns
  automatic alignment 
of 
.half, .word, .float, and 
.double  directives 
until the next 
.data or 
.kdata directive.
.ascii str Store the string 
str in memory, but do not null-
terminate it.
 A.10 MIPS R2000 Assembly Language 
A-47
A-48 Appendix A Assemblers, Linkers, and the SPIM Simulator
.asciiz str  
Store the string 
str in memory and null- terminate it.
.byte b1,..., bn  Store the 
n values in successive bytes of memory.
.data <addr> Subsequent items are stored in the data segment. 
If the optional argument 
addr
 is present, subse-
quent items are stored starting at address 
addr
..double d1,..., dn  Store the 
n oating-point double preci-
sion  num-bers in successive memory locations.
.extern sym size  Declare that the datum stored at 
sym
 is size
 bytes 
large and is a global label.
 is directive enables 
the assembler to store the datum in a portion of 

the data segment tha
  ciently accessed via 
register 
$gp..float f1,..., fn  Store the 
n oating-point single precision num-
bers in successive memory locations.
.globl sym Declare that label 
sym
 is global and can be refer-
enced from ot
 les..half h1,..., hn Store the 
n 16-bit quantities in successive mem ory 

halfwords.
.kdata <addr> Subsequent data items are stored in the kernel 
data segment. If the optional argument 
addr
 is present, subsequent items are stored starting at 

address 
addr
..ktext <addr> Subsequent items are put in the kernel text seg-
ment. In SPIM, these items may only be instruc-

tions or words (see the 
.word directive below). If 
the optional argument 
addr
 is present, subse quent 
items are stored starting at address 
addr
..set noat and 
.set at  
 e 
 rst directive prevents SPIM from complain-
ing about subsequent instructions that use regis ter 
$at e second directive re-enables the warning. 
Since pseudoinstructions expand into code that 

uses register 
$at, programmers must be very care-
ful about leaving values in this register.
.space n Allocates 
n bytes of space in the current segment 
(which must be the data segment in SPIM).

.text <addr> Subsequent items are put in the user text seg ment. 
In SPIM, these items may only be instruc tions 

or words (see the 
.word directive below). If the 
 optional argument 
addr
 is present, subse quent 
items are stored starting at address 
addr
..word w1,..., wn  Store the 
n 32-bit quantities in successive mem ory 
words. 
SPIM does not distinguish various parts of the data segment (
.data, .rdata, and 
.sdata).Encoding MIPS Instructions
Figure A.10.2
 explains how a MIPS instruction is encoded in a binary number. 

Each column contains instruction encodings fo
 eld (a contiguous group of 
bits) from an instructio e numbers at th
  margin are values fo
 eld. For example, the 
j opcode has a value of 2 in the opco
 eld. 
 e text at the top 
of a column na
 eld and sp
 es which bits it occupies in an instruction. 
For example, the 
op eld is contained in bits 26–31 of an instructio
 is 
 eld encodes most instructions. However, some groups of instructions use additional 

 elds to distinguish related instructions. For example, th
 eren
 oating-point 
instructions are sp
 ed by bi
 e arrows from th
 rst column show which 
opcodes use these addition
 elds.Instruction Format
 e rest of this appendix describes both the instructions implemented by actual 
MIPS hardware and the pseudoinstructions provided by the MIPS assembler
 e two types of instructions are easily distinguished. Actual instructions depict the 

 elds in their binary representation. For example, in 
Addition (with overﬂ
 ow)add rd, rs, rt0rsrtrd00x20
655556
the 
add instruction consists o
 elds. Each
 eld’s size in bits is the small num ber 
below th
 eld. 
 is instruction begins with six bits of 0s. Register sp
 ers begin 
with an 
r, so th
 eld is a 5-bit register sp
 er called 
rs is is the same 
register that is the second argument in the symbolic assembly at th
  of this 
line. Another common
 eld is 
imm16, which is a 16-bit immediate number.
 A.10 MIPS R2000 Assembly Language 
A-49
A-50 Appendix A Assemblers, Linkers, and the SPIM Simulator
FIGURE A.10.2 MIPS opcode map.
 e values of each
 eld are shown to i
 . 
 e 
 rst column shows the values in base 10, and the 
second shows base 16 for the op
 eld (bits 31 to 26) in the third col
 is op
 eld completely sp
 es the MIPS operation except for six 
op values: 0, 1, 16, 17, 18, an
 ese operations are determined by ot
 elds, iden
 ed by pointer
 
 eld (funct) uses “
f ” to 
mean “s” if rs 
= 16 and op 
= 17 or “d” if rs 
= 17 and op 
= e seco
 eld (rs) uses “
z” to mean “0”, “1”, “2”, or “3” if op 
= 16, 17, 18, or 19, respectively. If rs 
= 16, the operation is sp
 ed elsewhere: if 
z = 0, the operations are sp
 ed in the fourth
 eld (bits 4 to 0); if 
z = 1, then the 
operations are in th
 eld with 
f = s. If rs 
= 17 and 
z = 1, then the operations are in th
 eld with 
f = d. 
10012
3
4
56
7
8910
11
121314151617181920
2122232425
26272829303132
3334353637
3839
4041
42
434445
46
4748495051
52535455565758
5960
6162
6310012
3
4
56
7
8910
11
121314151617181920
2122232425
26272829303132
3334353637
3839
4041
42
434445
46
4748495051
52535455565758
5960
6162
6310012
3
4
56
7
8910
11
121314
151617181920
2122232425
26272829303132
3334353637
3839
4041
42
434445
46
4748495051
52535455565758
5960
6162
630
12
34
5
67
8910111213141516
17
181920
21
22
232425
26
2728
2930
310
12
34
5
67
8910111213141516
17
181920
21
22
232425
26
2728
2930
310
12
34
5
67
8910111213141516
17
181920
21
22
232425
26
2728
2930
3116
000102
03
04
0506
07
08090a0b0c0d
0e0f1011121314
1516171819
1a1b1c1d1e1f20
2122232425
2627
2829
2a
2b2c2d
2e2f30
313233
3435363738393a
3b3c3d
3e3f    rs
(25:21)
mfczcfczmtczctczcopzcopz(17:16)bczfbcztbczflbcztltlbrtlbwitlbwrtlbperetderetrt(20:16)bltz
bgezbltzl

bgezltgeitgeiu
tltitltiutegitnei
bltzal
bgezal
bltzall
bgczallcvt.s.
fcvt.d.fcvt.w.
fc.f.
fc.un.fc.eq.fc.ueq.fc.olt.fc.ult.f
c.ole.
f
c.ule.
fc.sf.
fc.ngle.
fc.seq.f
c.ngl.fc.lt.fc.nge.
fc.le.
fc.ngt.ffunct(5:0)funct(5:0)sllsrlsra
sllvsrlv
srav

jrjalrmovz
movn
syscall
breaksyncmfhimthimflomtlomult
multu

divdivuaddaddu
subsubu

andor
xornorsltsltutgetgeu
tlt
tltuteqtneif z = 1,f = dif z = 1,f = sif z = 0if z = 1 or z = 20123funct(4:0)sub.
fadd.fmul.
fdiv.
fsqrt.
fabs.
fmov.
fneg.fround.w.ftrunc.w.fcell.w.ffloor.
w.fmovz.
fmovn.
fclzclofunct(5:0)maddmaddumul
msubmsubu
(16:16)movf

movt
0
1(16:16)
movf.
fmovt.
f01op(31:26)jjalbeq
bne
blez

bgtz
addiaddiusltisltiuandi
ori
xori
luiz = 0z = 1z = 2beqlbnelblezlbgtzllblh
lwllw
lbu
lhu
lwrsbsh
swl

swswr
cache
lllwc1lwc2prefldc1ldc2scswc1
swc2
sdc1sdc2
Pseudoinstructions follow roughly the same conventions, but omit instruction 
encoding information. For example:
Multiply (without overﬂ
 ow)mul rdest, rsrc1, src2pseudoinstruction
In pseudoinstructions, 
rdest and 
rsrc1 are registers and 
src2 is either a regis-
ter or an immediate value. In general, the assembler and SPIM translate a more 
general form of an instruction (e.g., 
add $v1, $a0, 0x55) to a specialized form 
(e.g., 
addi $v1, $a0, 0x55).Arithmetic and Logical Instructions
Absolute valueabs rdest, rsrcpseudoinstruction
Put the absolute value of register 
rsrc in register 
rdest.Addition (with overﬂ
 ow)add rd, rs, rt0rsrtrd00x20
655556
Addition (without overﬂ
 ow)addu rd, rs, rt0rsrtrd00x21
655556
Put the sum of registers 
rs and 
rt into register 
rd.Addition immediate (with overﬂ
 ow)addi rt, rs, imm8rsrtimm
6551
6Addition immediate (without overﬂ
 ow)addiu rt, rs, imm9rsrtimm
65516
Put the sum of register 
rs and the sign-extended immediate into register 
rt. A.10 MIPS R2000 Assembly Language 
A-51
A-52 Appendix A Assemblers, Linkers, and the SPIM Simulator
ANDand rd, rs, rt0rsrtrd00x24
655556
Put the logical AND of registers 
rs and 
rt into register 
rd.AND immediateandi rt, rs, imm0xcrsrtimm
65516
Put the logical AND of register 
rs and the zero-extended immediate into reg-
ister 
rt.Count leading onesclo rd, rs0x1crs0rd0
0x21655556
Count leading zerosclz rd, rs0x1crs0rd0
0x20655556
Count the number of leading ones (zeros) in the word in register 
rs and put 
the result into register 
rd. If a word is all ones (zeros), the result is 32.
Divide (with overﬂ
 ow)div rs, rt0rsrt0
0x1a655106
Divide (without overﬂ
 ow)divu rs, rt0rsrt0
0x1b655106
Divide register 
rs by register 
rt. Leave the quotient in register 
lo and the remain-
der in register 
hi. Note that if an operand is negative, the remainder is unsp
 ed by the MIPS architecture and depends on the convention of the machine on which 
SPIM is run.

Divide (with overﬂ
 ow)div rdest, rsrc1, src2pseudoinstruction
Divide (without overﬂ
 ow)divu rdest, rsrc1, src2pseudoinstruction
Put the quotient of register 
rsrc1 and 
src2 into register 
rdest.Multiplymult rs, rt0rsrt0
0x18655106
Unsigned multiplymultu rs, rt0rsrt0
0x19655106
Multiply registers 
rs and 
rt. Leave the low-order word of the product in register 
lo and the high-order word in register 
hi.Multiply (without overﬂ
 ow)mul rd, rs, rt0x1crsrtrd02
655556
Put the low-order 32 bits of the product of 
rs and 
rt into register 
rd.Multiply (with overﬂ
 ow)mulo rdest, rsrc1, src2pseudoinstruction
Unsigned multiply (with overﬂ
 ow)mulou rdest, rsrc1, src2pseudoinstruction
Put the low-order 32 bits of the product of register 
rsrc1 and 
src2 into register 
rdest. A.10 MIPS R2000 Assembly Language 
A-53
A-54 Appendix A Assemblers, Linkers, and the SPIM Simulator
Multiply addmadd rs, rt0x1crsrt00
655106
Unsigned multiply addmaddu rs, rt0x1crsrt01
655106
Multiply registers 
rs and 
rt and add the resulting 64-bit product to the 64-bit 
value in the concatenated registers 
lo and 
hi.Multiply subtractmsub rs, rt0x1crsrt04
655106
Unsigned multiply subtractmsub rs, rt0x1crsrt05
655106
Multiply registers 
rs and 
rt and subtract the resulting 64-bit product from the 64-
bit value in the concatenated registers 
lo and 
hi.Negate value (with overﬂ
 ow)neg rdest, rsrcpseudoinstruction
Negate value (without overﬂ
 ow)negu rdest, rsrcpseudoinstruction
Put the negative of register 
rsrc into register 
rdest.NORnor rd, rs, rt0rsrtrd00x27
655556
Put the logical NOR of registers 
rs and 
rt into register 
rd.
NOTnot rdest, rsrcpseudoinstruction
Put the bitwise logical negation of register 
rsrc into register 
rdest.ORor rd, rs, rt0rsrtrd00x25
655556
Put the logical OR of registers 
rs and 
rt into register 
rd.OR immediateori rt, rs, imm0xdrsrtimm
65516
Put the logical OR of register 
rs and the zero-extended immediate into register 
rt.Remainderrem rdest, rsrc1, rsrc2pseudoinstruction
Unsigned remainderremu rdest, rsrc1, rsrc2pseudoinstruction
Put the remainder of register 
rsrc1 divided by register 
rsrc2 into register  
rdest. Note that if an operand is negative, the remainder is unsp
 ed by the MIPS 
architecture and depends on the convention of the machine on which SPIM is run.
Shift left logicalsll rd, rt, shamt0rsrtrdshamt0
655556
Shift left logical variablesllv rd, rt, rs0rsrtrd04
655556
 A.10 MIPS R2000 Assembly Language 
A-55
A-56 Appendix A Assemblers, Linkers, and the SPIM Simulator
Shift right arithmeticsra rd, rt, shamt0rsrtrdshamt3
655556
Shift right arithmetic variablesrav rd, rt, rs0rsrtrd07
655556
Shift right logicalsrl rd, rt, shamt0rsrtrdshamt2
655556
Shift right logical variablesrlv rd, rt, rs0rsrtrd06
655556
S
  register 
rt  (right) by the distance indicated by immediate 
shamt or the 
register 
rs and put the result in register 
rd. Note that argument 
rs is ignored for 
sll, sra, and 
srl.Rotate leftrol rdest, rsrc1, rsrc2pseudoinstruction
Rotate rightror rdest, rsrc1, rsrc2pseudoinstruction
Rotate register 
rsrc1  (right) by the distance indicated by 
rsrc2 and put the 
result in register 
rdest.Subtract (with overﬂ
 ow)sub rd, rs, rt0rsrtrd00x22
655556

Subtract (without overﬂ
 ow)subu rd, rs, rt0rsrtrd00x23
655556
Put th
 erence of registers 
rs and 
rt into register 
rd.Exclusive ORxor rd, rs, rt0rsrtrd00x26
655556
Put the logical XOR of registers 
rs and 
rt into register 
rd.XOR immediatexori rt, rs, imm0xersrtImm
65516
Put the logical XOR of register 
rs and the zero-extended immediate into reg-
ister 
rt.Constant-Manipulating Instructions
Load upper immediatelui rt, imm0xfOrtimm
65516
Load the lower halfword of the immediate 
imm into the upper halfword of reg-
ister 
rt e lower bits of the register are set to 0.
Load immediateli rdest, immpseudoinstruction
Move the immediate 
imm into register 
rdest.Comparison Instructions
Set less thanslt rd, rs, rt0rsrtrd00x2a
655556
 A.10 MIPS R2000 Assembly Language 
A-57
A-58 Appendix A Assemblers, Linkers, and the SPIM Simulator
Set less than unsignedsltu rd, rs, rt0rsrtrd00x2b
655556
Set register 
rd to 1 if register 
rs is less than 
rt, and to 0 otherwise.
Set less than immediateslti rt, rs, imm0xarsrtimm
65516
Set less than unsigned immediatesltiu rt, rs, imm0xbrsrtimm
65516
Set register 
rt to 1 if register 
rs is less than the sign-extended immediate, and to 
0 otherwise.
Set equalseq rdest, rsrc1, rsrc2pseudoinstruction
Set register 
rdest to 1 if register 
rsrc1 equals 
rsrc2, and to 0 otherwise.
Set greater than equalsge rdest, rsrc1, rsrc2pseudoinstruction
Set greater than equal unsignedsgeu rdest, rsrc1, rsrc2pseudoinstruction
Set register 
rdest to 1 if register 
rsrc1 is greater than or equal to 
rsrc2, and to 
0 otherwise.
Set greater thansgt rdest, rsrc1, rsrc2pseudoinstruction

Set greater than unsignedsgtu rdest, rsrc1, rsrc2pseudoinstruction
Set register 
rdest to 1 if register 
rsrc1 is greater than 
rsrc2, and to 0 otherwise.
Set less than equalsle rdest, rsrc1, rsrc2pseudoinstruction
Set less than equal unsignedsleu rdest, rsrc1, rsrc2pseudoinstruction
Set register 
rdest to 1 if register 
rsrc1 is less than or equal to 
rsrc2, and to 0 
otherwise.
Set not equalsne rdest, rsrc1, rsrc2pseudoinstruction
Set register 
rdest to 1 if register 
rsrc1 is not equal to 
rsrc2, and to 0 otherwise.
Branch Instructions
Branch instructions use a signed 16-bit instruction 
 set
 eld; hence, they can 
jump 2
15  1 instructions
 (not bytes) forward or 2
15 instructions backward
 e jump instruction contains a 26-bit addr eld. In actual MIPS processors, branch 
instructions are delayed branches, which do not transfer control until the instruction 
following the branch (its “delay slot”) has executed (see Chapter 4). Delayed branches 

 ect the o
 set calculation, since it must be computed relative to the address of the 
delay slot instruction (PC + 4), which is when the branch occurs. SPIM does not 

simulate this delay slot, unless the 
-bare or 
-delayed_branch ags are sp
 ed.
In assembly code, o
 sets are not usually sp
 ed as numbers. Instead, an 
instructions branch to a label, and the assembler computes the distance between 

the branch and the target instructions.
In MIPS-32, all actual (not pseudo) conditional branch instructions have a 
“likely” variant (for example, 
beq’s likely variant is 
beql), which does 
not execute 
the instruction in the branch’s delay slot if the branch is not taken. Do not use 
 A.10 MIPS R2000 Assembly Language 
A-59
A-60 Appendix A Assemblers, Linkers, and the SPIM Simulator
these instructions; they may be removed in subsequent versions of the architec ture. 
SPIM implements these instructions, but they are not described further.
Branch instruction
b labelpseudoinstruction
Unconditionally branch to the instruction at the label.
Branch coprocessor falsebclf cc label0x118
cc0Offset
653216
Branch coprocessor true
bclt cc label0x118
cc1Offset
653216
Conditionally branch the number of instructions sp
 ed by the o
 set if the 
 oating-point coprocessor’s conditio
 ag numbered 
cc is false (true). If 
cc is omitted from the instruction, condition co
 ag 0 is assumed.
Branch on equalbeq rs, rt, label4rsrtOffset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs equals 
rt.Branch on greater than equal zerobgez rs, label1rs1Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is greater than or equal to 0.

Branch on greater than equal zero and linkbgezal rs, label1rs0x11Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is greater than or equal to 0. Save the address of the next instruction in reg-
ister 31.
Branch on greater than zerobgtz rs, label7rs0Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is greater than 0.
Branch on less than equal zeroblez rs, label6rs0Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is less than or equal to 0.
Branch on less than and linkbltzal rs, label1rs0x10Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is less than 0. Save the address of the next instruction in register 31.
Branch on less than zerobltz rs, label 1rs0Offset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is less than 0.
 A.10 MIPS R2000 Assembly Language 
A-61
A-62 Appendix A Assemblers, Linkers, and the SPIM Simulator
Branch on not equalbne rs, rt, label5rsrtOffset
65516
Conditionally branch the number of instructions sp
 ed by the o
 set if  register 
rs is not equal to 
rt.Branch on equal zerobeqz rsrc, labelpseudoinstruction
Conditionally branch to the instruction at the label if 
rsrc equals 0.
Branch on greater than equalbge rsrc1, rsrc2, labelpseudoinstruction
Branch on greater than equal unsignedbgeu rsrc1, rsrc2, labelpseudoinstruction
Conditionally branch to the instruction at the label if register 
rsrc1 is greater than 
or equal to 
rsrc2.Branch on greater thanbgt rsrc1, src2, labelpseudoinstruction
Branch on greater than unsignedbgtu rsrc1, src2, labelpseudoinstruction
Conditionally branch to the instruction at the label if register 
rsrc1 is greater than 
src2.Branch on less than equalble rsrc1, src2, labelpseudoinstruction

Branch on less than equal unsignedbleu rsrc1, src2, labelpseudoinstruction
Conditionally branch to the instruction at the label if register 
rsrc1 is less than or 
equal to 
src2.Branch on less thanblt rsrc1, rsrc2, labelpseudoinstruction
Branch on less than unsignedbltu rsrc1, rsrc2, labelpseudoinstruction
Conditionally branch to the instruction at the label if register 
rsrc1 is less than 
rsrc2.Branch on not equal zerobnez rsrc, labelpseudoinstruction
Conditionally branch to the instruction at the label if register 
rsrc is not equal to 0.
Jump Instructions
Jumpj target2target
626
Unconditionally jump to the instruction at target.
Jump and linkjal target3target
626
Unconditionally jump to the instruction at target. Save the address of the next 
instruction in register 
$ra. A.10 MIPS R2000 Assembly Language 
A-63
A-64 Appendix A Assemblers, Linkers, and the SPIM Simulator
Jump and link registerjalr rs, rd0rs0rd09
655556
Unconditionally jump to the instruction whose address is in register 
rs. Save the 
address of the next instruction in register 
rd (which defaults to 31).
Jump registerjr rs0rs0
86515
6Unconditionally jump to the instruction whose address is in register 
rs.Trap Instructions
Trap if equal
teq rs, rt0rsrt0
0x34655106
If register 
rs is equal to register 
rt, raise a Trap exception.
Trap if equal immediate
teqi rs, imm1rs0xcimm
65516
If register 
rs is equal to the sign-extended value 
imm, raise a Trap exception.
Trap if not equal
teq rs, rt0rsrt0
0x36655106
If register 
rs is not equal to register 
rt, raise a Trap exception.
Trap if not equal immediate
teqi rs, imm1rs0xeimm
65516
If register 
rs is not equal to the sign-extended value 
imm, raise a Trap exception.

Trap if greater equal
tge rs, rt0rsrt0
0x30655106
Unsigned trap if greater equaltgeu rs, rt0rsrt0
0x31655106
If register 
rs is greater than or equal to register 
rt, raise a Trap exception.
Trap if greater equal immediate
tgei rs, imm1rs8imm
65516
Unsigned trap if greater equal immediatetgeiu rs, imm1rs9imm
65516
If register 
rs is greater than or equal to the sign-extended value 
imm, raise a Trap 
exception.
Trap if less than
tlt rs, rt0rsrt0
0x32655106
Unsigned trap if less thantltu rs, rt0rsrt0
0x33655106
If register 
rs is less than register 
rt, raise a Trap exception.
Trap if less than immediate
tlti rs, imm1rsaimm
65516
 A.10 MIPS R2000 Assembly Language 
A-65
A-66 Appendix A Assemblers, Linkers, and the SPIM Simulator
Unsigned trap if less than immediatetltiu rs, imm1rsbimm
65516
If register 
rs is less than the sign-extended value 
imm, raise a Trap exception.
Load Instructions
Load addressla rdest, addresspseudoinstruction
Load computed 
address
—not the contents of the location—into register 
rdest.Load byte
lb rt, address0x20rsrtOffset
65516
Load unsigned byte
lbu rt, address0x24rsrtOffset
65516
Load the byte at 
address
 into register 
rt e byte is sign-extended by 
lb, but not 
by 
lbu.Load halfwordlh rt, address0x21rsrtOffset
65516
Load unsigned halfwordlhu rt, address0x25rsrtOffset
65516
Load the 16-bit quantity (halfword) at 
address
 into register 
rt e halfword is 
sign-extended by 
lh, but not by 
lhu.
Load wordlw rt, address0x23rsrtOffset
65516
Load the 32-bit quantity (word) at 
address
 into register 
rt.Load word coprocessor 1lwcl ft, address0x31rsrtOffset
65516
Load the word at 
address
 into register 
ft in th
 oating-point unit.
Load word leftlwl rt, address0x22rsrtOffset
65516
Load word rightlwr rt, address0x26rsrtOffset
65516
Load th  (right) bytes from the word at the possibly unaligned 
address
 into 
register 
rt.Load doubleword
ld rdest, addresspseudoinstruction
Load the 64-bit quantity at 
address
 into registers 
rdest and 
rdest + 1.Unaligned load halfwordulh rdest, addresspseudoinstruction
 A.10 MIPS R2000 Assembly Language 
A-67
A-68 Appendix A Assemblers, Linkers, and the SPIM Simulator
Unaligned load halfword unsignedulhu rdest, addresspseudoinstruction
Load the 16-bit quantity (halfword) at the possibly unaligned 
address
 into  register 
rdest e halfword is sign-extended by 
ulh, but not 
ulhu.Unaligned load wordulw rdest, addresspseudoinstruction
Load the 32-bit quantity (word) at the possibly unaligned 
address
 into register 
rdest.Load linkedll rt, address0x30rsrtOffset
65516
Load the 32-bit quantity (word) at 
address
 into register 
rt and start an atomic 
read-modify-write operatio
 is operation is completed by a store conditional 
(sc) instruction, which will fail if another processor writes into the block contain-
ing the loaded word. Since SPIM does not simulate multiple processors, the store 
conditional operation always succeeds.
Store Instructions
Store byte
sb rt, address0x28rsrtOffset
65516
Store the low byte from register 
rt at 
address
.Store halfwordsh rt, address0x29rsrtOffset
65516
Store the low halfword from register 
rt at 
address
.
Store wordsw rt, address0x2brsrtOffset
65516
Store the word from register 
rt at 
address
.Store word coprocessor 1swcl ft, address0x31rsftOffset
65516
Store th
 oating-point value in register 
ft of
 oating-point coprocessor at 
address
.Store double coprocessor 1sdcl ft, address0x3drsftOffset
65516
Store the doubleword
 oating-point value in registers 
ft and 
ft + l of
 oating-
point coprocessor at 
address
. Register 
ft must be even numbered.
Store word leftswl rt, address0x2arsrtOffset
65516
Store word rightswr rt, address0x2ersrtOffset
 65516
Store th
  (right) bytes from register 
rt at the possibly unaligned 
address
.Store doubleword
sd rsrc, addresspseudoinstruction
Store the 64-bit quantity in registers 
rsrc and 
rsrc + 1 at 
address
. A.10 MIPS R2000 Assembly Language 
A-69
A-70 Appendix A Assemblers, Linkers, and the SPIM Simulator
Unaligned store halfwordush rsrc, addresspseudoinstruction
Store the low halfword from register 
rsrc at the possibly unaligned 
address
.Unaligned store wordusw rsrc, addresspseudoinstruction
Store the word from register 
rsrc at the possibly unaligned 
address
.Store conditionalsc rt, address0x38rsrtOffset
65516
Store the 32-bit quantity (word) in register 
rt into memory at 
address
 and com plete 
an atomic read-modify-write operation. If this atomic operation is success ful, the 
memory word is mo
 ed and register 
rt is set to 1. If the atomic operation fails 
because another processor wrote to a location in the block contain ing the addressed 

word, this instruction does not modify memory and writes 0 into register 
rt. Since 
SPIM does not simulate multiple processors, the instruc tion always succeeds.
Data Movement Instructions
Move
move rdest, rsrcpseudoinstruction
Move register 
rsrc to 
rdest.Move from hi
mfhi rd00
rd00x10
610556

Move from lo
mflo rd00
rd00x12
610556
 e multiply and divide unit produces its result in two additional registers, 
hi and 
lo ese instructions move values to and from these register
 e multiply, 
divide, and remainder pseudoinstructions that make this unit appear to operate on 
the general registers move the result a
 er the computatio
 nishes.Move the 
hi (lo) register to register 
rd.Move to hi
mthi rs0rs0
0x116515
6Move to lo
mtlo rs0rs0
0x136515
6Move register 
rs to the 
hi (lo) register.
Move from coprocessor 0
mfc0 rt, rd0x100rtrd0
655511
Move from coprocessor 1
mfcl rt, fs0x110rtfs0
655511
Coprocessors have their own register s
 ese instructions move values between 
these registers and the CPU’s registers.
Move register 
rd in a coprocessor (register 
fs in the FPU) to CPU register 
rt e  oating-point unit is coprocessor 1.
 A.10 MIPS R2000 Assembly Language 
A-71
A-72 Appendix A Assemblers, Linkers, and the SPIM Simulator
Move double from coprocessor 1
mfc1.d rdest, frsrc1pseudoinstruction
Move
 oating-point registers 
frsrc1 and 
frsrc1 + 1 to CPU registers 
rdest and 
rdest + 1.Move to coprocessor 0
mtc0 rd, rt0x104rtrd0
655511
Move to coprocessor 1
mtc1 rd, fs0x114rtfs0
655511
Move CPU register 
rt to register 
rd in a coprocessor (register 
fs in the FPU).
Move conditional not zero
movn rd, rs, rt0rsrtrd0xb
655511
Move register 
rs to register 
rd if register 
rt is not 0.
Move conditional zero
movz rd, rs, rt0rsrtrd0xa
655511
Move register 
rs to register 
rd if register 
rt is 0.Move conditional on FP false
movf rd, rs, cc0rs
cc0rd01
6532556
Move CPU register 
rs to register 
rd if FPU condition co
 ag number 
cc is 0. If 
cc is omitted from the instruction, condition co
 ag 0 is assumed.

Move conditional on FP true
movt rd, rs, cc0rs
cc1rd01
6532556
Move CPU register 
rs to register 
rd if FPU condition co
 ag number 
cc is 1. If 
cc is omitted from the instruction, condition code bit 0 is assumed.
Floating-Point Instructions
 
 oating-point coprocessor (numbered 1) that operates on single 
precision (32-bit) and double precision (64-bi
 oating-point number
 is coprocessor has its own registers, which are numbered 
$f0–$f31. Because these 
registers are only 32 bits wide, two of them are required to hold doubles, so only 
 oating-point registers with even numbers can hold double precision val
 e  oating-point coprocessor also has eight condition code (
cc ags, numbered 0–7, 
which are set by compare instructions and tested by branch (
bclf or 
bclt) and 
conditional move instructions.
Values are moved in or out of these registers one word (32 bits) at a time by 
lwc1, swc1, mtc1, and 
mfc1 instructions or one double (64 bits) at a time by 
ldcl and 
sdcl, described above, or by the 
l.s, l.d, s.s, and 
s.d pseudoinstructions 
described below. 
In the actual instructions below, bits 21–26 are 0 for single precision and 1 
for double precision. In the pseudoinstructions below, 
fdest oating-point 
register (e.g., 
$f2).Floating-point absolute value doubleabs.d fd, fs0x1110fsfd5
655556
Floating-point absolute value singleabs.s fd, fs0x1100fsfd5
Compute the absolute value of th
 oating-point double (single) in register 
fs and 
put it in register 
fd.Floating-point addition doubleadd.d fd, fs, ft0x110x11ftfsfd0
655556
 A.10 MIPS R2000 Assembly Language 
A-73
A-74 Appendix A Assemblers, Linkers, and the SPIM Simulator
Floating-point addition singleadd.s fd, fs, ft0x110x10ftfsfd0
655556
Compute the sum of th
 oating-point doubles (singles) in registers 
fs and 
ft and 
put it in register 
fd.Floating-point ceiling to wordceil.w.d fd, fs0x110x110fsfd0xe
655556
ceil.w.s fd, fs0x110x100fsfd0xe
Compute the ceiling of th
 oating-point double (single) in register 
fs, convert to 
a 32-bi
 xed-point value, and put the resulting word in register 
fd.Compare equal doublec.eq.d cc fs, ft0x110x11ftfs
cc0FC2
65553224
Compare equal singlec.eq.s cc fs, ft0x110x10ftfs
cc0FC2
65553224
Compare th oating-point double (single) in register 
fs against the one in 
ft and set th
 oating-point conditio
 ag cc to 1 if they are equal. If 
cc is omitted, 
condition co
 ag 0 is assumed.
Compare less than equal doublec.le.d cc fs, ft0x110x11ftfs
cc0FC0xe
65553224
Compare less than equal singlec.le.s cc fs, ft0x110x10ftfs
cc0FC0xe
65553224

Compare th oating-point double (single) in register 
fs against the one in 
ft and 
set th
 oating-point conditio
 ag cc to 1 if th
 rst is less than or equal to the 
second. If 
cc is omitted, condition co
 ag 0 is assumed.
Compare less than doublec.lt.d cc fs, ft0x110x11ftfs
cc0FC0xc
65553224
Compare less than singlec.lt.s cc fs, ft0x110x10ftfs
cc0FC0xc
65553224
Compare th oating-point double (single) in register 
fs against the one in 
ft and set the conditio
 ag cc to 1 if th
 rst is less than the second. If 
cc is omitted, 
condition co
 ag 0 is assumed.
Convert single to double
cvt.d.s fd, fs0x110x100fsfd0x21
655556
Convert integer to double
cvt.d.w fd, fs0x110x140fsfd0x21
655556
Convert the single precisio
 oating-point number or integer in register 
fs to a 
double (single) precision number and put it in register 
fd.Convert double to single
cvt.s.d fd, fs0x110x110fsfd0x20
655556
Convert integer to single
cvt.s.w fd, fs0x110x140fsfd0x20
655556
Convert the double precisio
 oating-point number or integer in register 
fs to a 
single precision number and put it in register 
fd. A.10 MIPS R2000 Assembly Language 
A-75
A-76 Appendix A Assemblers, Linkers, and the SPIM Simulator
Convert double to integer
cvt.w.d fd, fs0x110x110fsfd0x24
655556
Convert single to integer
cvt.w.s fd, fs0x110x100fsfd0x24
655556
Convert the double or single precisio
 oating-point number in register 
fs to an 
integer and put it in register 
fd.Floating-point divide doublediv.d fd, fs, ft0x110x11ftfsfd3
655556
Floating-point divide singlediv.s fd, fs, ft0x110x10ftfsfd3
655556
Compute the quotient of th oating-point doubles (singles) in registers 
fs and 
ft and put it in register 
fd.Floating-point ﬂ oor to word
floor.w.d fd, fs0x110x110fsfd0xf
655556
floor.w.s fd, fs0x110x100fsfd0xf
Compute th
 oor of th
 oating-point double (single) in register 
fs and put the 
resulting word in register 
fd.Load ﬂ oating-point double
l.d fdest, addresspseudoinstruction

Load ﬂ oating-point single
l.s fdest, addresspseudoinstruction
Load th
 oating-point double (single) at 
address into register 
fdest.Move ﬂ
 oating-point double
mov.d fd, fs0x110x110fsfd6
655556
Move ﬂ
 oating-point single
mov.s fd, fs0x110x100fsfd6
655556
Move th oating-point double (single) from register 
fs to register 
fd.Move conditional ﬂ oating-point double false
movf.d fd, fs, cc0x110x11
cc0fsfd0x11
6532556
Move conditional ﬂ oating-point single false
movf.s fd, fs, cc0x110x10
cc0fsfd0x11
6532556
Move th oating-point double (single) from register 
fs to register 
fd if condi tion 
co
 ag cc is 0. If 
cc is omitted, condition co
 ag 0 is assumed.
Move conditional ﬂ oating-point double true
movt.d fd, fs, cc0x110x11
cc1fsfd0x11
6532556
Move conditional ﬂ oating-point single true
movt.s fd, fs, cc0x110x10
cc1fsfd0x11
6532556
 A.10 MIPS R2000 Assembly Language 
A-77
A-78 Appendix A Assemblers, Linkers, and the SPIM Simulator
Move th oating-point double (single) from register 
fs to register 
fd if condi tion 
co
 ag cc is 1. If 
cc is omitted, condition co
 ag 0 is assumed.
Move conditional ﬂ oating-point double not zero
movn.d fd, fs, rt0x110x11rtfsfd0x13
655556
Move conditional ﬂ oating-point single not zero
movn.s fd, fs, rt0x110x10rtfsfd0x13
655556
Move th oating-point double (single) from register 
fs to register 
fd if proces sor 
register 
rt is not 0.
Move conditional ﬂ oating-point double zero
movz.d fd, fs, rt0x110x11rtfsfd0x12
655556
Move conditional ﬂ oating-point single zero
movz.s fd, fs, rt0x110x10rtfsfd0x12
655556
Move th oating-point double (single) from register 
fs to register 
fd if proces sor 
register 
rt is 0.Floating-point multiply doublemul.d fd, fs, ft0x110x11ftfsfd2
655556
Floating-point multiply singlemul.s fd, fs, ft0x110x10ftfsfd2
655556
Compute the product of th oating-point doubles (singles) in registers 
fs and 
ft and put it in register 
fd.Negate doubleneg.d fd, fs0x110x110fsfd7
655556

Negate singleneg.s fd, fs0x110x100fsfd7
655556
Negate th oating-point double (single) in register 
fs and put it in register 
fd.Floating-point round to wordround.w.d fd, fs0x110x110fsfd0xc
655556
round.w.s fd, fs0x110x100fsfd0xc
Round th
 oating-point double (single) value in register 
fs, convert to a 32-bit 
 xed-point value, and put the resulting word in register 
fd.Square root doublesqrt.d fd, fs0x110x110fsfd4
655556
Square root singlesqrt.s fd, fs0x110x100fsfd4
655556
Compute the square root of th
 oating-point double (single) in register 
fs and 
put it in register 
fd.Store ﬂ oating-point double
s.d fdest, addresspseudoinstruction
Store ﬂ oating-point single
s.s fdest, addresspseudoinstruction
Store th oating-point double (single) in register 
fdest at 
address
.Floating-point subtract doublesub.d fd, fs, ft0x110x11ftfsfd1
655556
 A.10 MIPS R2000 Assembly Language 
A-79
A-80 Appendix A Assemblers, Linkers, and the SPIM Simulator
Floating-point subtract singlesub.s fd, fs, ft0x110x10ftfsfd1
655556
Compute th
 erence of th
 oating-point doubles (singles) in registers 
fs and 
ft and put it in register 
fd.Floating-point truncate to word
trunc.w.d fd, fs0x110x110fsfd0xd
655556
trunc.w.s fd, fs0x110x100fsfd0xd
Truncate th oating-point double (single) value in register 
fs, convert to a 32-bit 
 xed-point value, and put the resulting word in register 
fd.Exception and Interrupt Instructions
Exception return
eret0x101
00x1861
196Set the EXL bit in coprocessor 0’s Status register to 0 and return to the instruction 
pointed to by coprocessor 0’s EPC register.
System call
syscall000xc6206
Register 
$v0 contains the number of the system call (see Figure A.9.1) provided 
by SPIM.
Breakbreak code0code0xd6206
Cause exception
 code. Exception 1 is reserved for the debugger.
No operationnop000000
655556
Do nothing. 

  A.11
 Concluding Remarks
Programming in assembly language requires a programmer to trade helpful fea-
tures of high-level languages—such as data structures, type checking, and control 

constructs—for complete control over the instructions that a computer executes. 

External constraints on some applications, such as response time or program size, 

require a programmer to pay close attention to every instruction. However, the 

cost of this level of attention is assembly language programs that are longer, more 

time-consuming to write, and mor
  cult to maintain than high-level language 
programs.
Moreover, three trends are reducing the need to write programs in assembly 
language
 e 
 rst trend is toward the improvement of compilers. Modern com-
pilers produce code that is typically comparable to the best handwritten code—

and is sometimes better
 e second trend is the introduction of new processors 
that are not only faster, but in the case of processors that execute multiple instruc-

tions simultaneously, also mor
  cult to program by hand. In addition, the rapid 
evolution of the modern computer favors high-level language programs that are 

not tied to a single architecture. Finally,
 we witness a trend toward increasingly 
complex applications, characterized by complex graphic interfaces and many more 

features than their predecessors had. Large applications are written by teams of 

programmers and require the modularity and semantic checking features pro vided 

by high-level languages.
Further Reading
Aho, A., R. Sethi, and J. Ullman [1985]. 
Compilers: Principles, Techniques, and Tools
, Reading, MA: Addison-
Wesley.
Slightly dated and lacking in coverage of modern architectures, but still the standard reference on compilers.
Sweetman, D. [1999]. 
See MIPS Run
, San Francisco, CA: Morgan Kaufmann Publishers.
A complete, detailed, and engaging introduction to the MIPS instruction set and assembly language program-
ming on these machines.
Detailed documentation on the MIPS-32 architecture is available on the Web:
MIPS32™ Architecture for Programmers Volume I: Introduction to the MIPS32™ Architecture 
(http://mips.com/content/Documentation/MIPSDocumentation/ProcessorArchitecture/

ArchitectureProgrammingPublicationsforMIPS32/MD00082-2B-MIPS32INT-AFP-02.00.pdf/

getDownload
)MIPS32™ Architecture for Programmers Vol e MIPS32™ Instruction Set 
(http://mips.com/content/Documentation/MIPSDocumentation/ProcessorArchitecture/

ArchitectureProgrammingPublic
ationsforMIPS32/MD00086-2B-MIPS32BIS-AFP-02.00.pdf/getDownload
)MIPS32™ Architecture for Programmers Vol
 e MIPS32™ Privileged Resource Architecture
(http://mips.com/content/Documentation/MIPSDocumentation/ProcessorArchitecture/

ArchitectureProgrammingPublicat
ionsforMIPS32/MD00090-2B-MIPS32PRA-AFP-02.00.pdf/getDownload
) A.11 Concluding Remarks 
A-81
A-82 Appendix A Assemblers, Linkers, and the SPIM Simulator
  A.12
 ExercisesA.1 [5] <§A.5> Section A.5 described how memory is partitioned on most MIPS 
systems. Propose another way of dividing memory that meets the same goals.
A.2 [20] <§A.6> Rewrite the code for 
fact to use fewer instructions.
A.3 [5] <§A.7> Is it ever safe for a user program to use registers 
$k0 or 
$k1?A.4 [25] <§A.7> Section A.7 contains code for a very simple exception handler. 
One serious problem with this handler is that it disables interrupts for a long 
time
 is means that interrupts from a fast I/O device may be lost. Write a better 
exception handler that is interruptable and enables interrupts as quickly as possible.
A.5  e simple exception handler always jumps back to the instruc-
tion following the exceptio
 is wo
 ne unless the instruction that causes the 
exception is in the delay slot of a branch. In that case, the next instruction is the 

target of the branch. Write a better handler that uses the EPC register to determine 

which instruction should be executed a er the exception.
A.6 [5] <§A.9> Using SPIM, write and test an adding machine program that 
repeatedly reads in integers and adds them into a runnin
 e program 
should stop when it gets an input that is 0, printing out the sum at that point. Use 

the SPIM system calls described on pages A-43 and A-45.
A.7 [5] <§A.9> Using SPIM, write and test a program that reads in three integers 
and prints out the sum of the largest two of the three. Use the SPIM system calls 

described on pages A-43 and A-45. You can break ties arbitrarily.
A.8 [5] <§A.9> Using SPIM, write and test a program that reads in a positive inte-
ger using the SPIM system calls. If the integer is not positive, the program should 

terminate with the message “Invalid Entry”; otherwise the program should print 

out the names of the digits of the integers, delimited by exactly one space. For 

example, if the user entered “728,” the output would be “Seven Two Eight.”
A.9 [25] <§A.9> Write and test a MIPS assembly language program to compute 
and print th
 rst 100 prime numbers. A number 
n is prime if no numbers except 
1 and 
n divide it evenly. You should implement two routines: 
 test_prime (n)   Return 1 if 
n is prime and 0 if 
n is not prime.
 main ()   Iterate over the integers, testing if each is prime. Print th
 rst 
100 numbers that are prime.
Test your programs by running them on SPIM.

A.10 [10] <§§A.6, A.9> Using SPIM, write and test a recursive program for solv ing 
the classic mathematical recreation, the Towers of Hanoi puzzle
 is will require 
the use of stack frames to support recursio
 e puzzle consists of three pegs 
(1, 2, and 3) and 
n disks (the number 
n can vary; typical values might be in the 
range from 1 to 8). Disk 1 is smaller than disk 2, which is in turn smaller than disk 
3, and so forth, with disk 
n being the largest. Initially, all the disks are on peg 1, 
starting with disk 
n on the bottom, disk 
n  1 on top of that, and so forth, up to 
disk 1 on the top.
 e goal is to move all the disks to peg 2. You may only move one 
disk at a time, that is, the top disk from any of the three pegs onto the top of either 

of the other two pegs. Moreover, there is a constraint: You must not place a larger 

disk on top of a smaller disk.
 e C program below can be used to help write your assembly language program.
/* move n smallest disks from start to finish using extra */void hanoi(int n, int start, int finish, int extra){ if(n != 0){
  hanoi(n-1, start, extra, finish);
  print_string(“Move disk”);

  print_int(n);

  print_string(“from peg”);

  print_int(start);

  print_string(“to peg”);

  print_int(finish);

  print_string(“.\n”);

  hanoi(n-1, extra, finish, start);
 }}
main(){
 int n;

 print_string(“Enter number of disks>“);

 n = read_int();

 hanoi(n, 1, 2, 3);

 return 0;

} A.12 Exercises A-83
I always loved that 
word, 
Boolean.
Claude ShannonIEEE Spectrum, 
April 1992
 
(Shannon’s master’s thesis showed 

that the algebra invented by George 

Boole in the 1800s could represent the 

workings of electrical switches.)
The Basics of Logic DesignB.1 Introduction B-3B.2 Gates, Truth Tables, and Logic 
Equations B-4B.3 Combinational Logic 
B-9B.4 Using a Hardware Description 
Language B-20B.5 Constructing a Basic Arithmetic Logic 
Unit B-26B.6 Faster Addition: Carry Lookahead 
B-38B.7 Clocks B-48BAPPENDIXComputer Organization and Design. DOI: © 2013 Elsevier Inc. All rights reserved.http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-12013
B.8 Memory Elements: Flip-Flops, Latches, and Registers 
B-50B.9 Memory Elements: SRAMs and DRAMs 
B-58B.10 Finite-State Machines 
B-67B.11 Timing Methodologies 
B-72B.12 Field Programmable Devices 
B-78B.13 Concluding Remarks 
B-79B.14 Exercises B-80 B.1 Introduction is appendix provides a brief discussion of the basics of logic design. It does not 
replace a course in logic design, nor w
ill it enable you to
 cant working 
logic systems. If you have little or no exposure to logic design, however, this 
appendix will prov
  cient background to understand all the material in this 
book. In addition, if you are looking to understand some of the motivation behind 

how computers are implemented, this material will serve as a useful introduction. 

If your curiosity is aroused but not sated by this appendix, the references at the end 

provide several additional sources of information.
Section B.2 introduces the basic building blocks of logic, namely, 
gates
. Section 
B.3 uses these building blocks to construct simple 
combinational
 logic systems, 
which contain no memory. If you have had some exposure to logic or digital 

systems, you will probably be familiar with the material in thes
 rst two sections. 
Section B.5 shows how to use the concepts of Sections B.2 and B.3 to design an 

ALU for the MIPS processor. Section B.6 shows how to make a fast adder, and 

B-4 Appendix B The Basics of Logic Design
may be safely skipped if you are not interested in this topic. Section B.7 is a short 
introduction to the topic of clocking, which is necessary to discuss how memory 

elements work. Section B.8 introduces memory elements, and Section B.9 extends 

it to focus on random access memories; it describes both the characteristics that 

are important to understanding how they are used, as discussed in Chapter 4, and 

the background that motivates many of the aspects of memory hierarchy design 

discussed in Chapter 5. Section B.10 describes the design and use o
 nite-state 
machines, which are sequential logic blocks. If you intend to read 
 Appendix D
, you should thoroughly understand the material in Sections B.2 through B.10. If 
you intend to read only the material on control in Chapter 4, you can skim the 

appendices; however, you should have some familiarity with all the material except 

Section B.11. Section B.11 is intended for those who want a deeper understanding 

of clocking methodologies and timing. It explains the basics of how edge-triggered 

clocking works, introduces another clocking scheme, and br
 y describes the 
problem of synchronizing asynchronous inputs.
 roughout this appendix, where it is appropriate, we also include segments 
to demonstrate how logic can be represented in Verilog, which we introduce in 

Section B.4. A more extensive and complete Verilog tutorial appears elsewhere on 

the CD.
 B.2 Gates, Truth Tables, and Logic Equations
 e electronics inside a modern computer are 
digital
. Digital electronics operate 
with only two voltage levels of interest: a high voltage and a low voltage. All other 

voltage values are temporary and occur while transitioning between the values. 

(As we discuss later in this section, a possible pitfall in digital design is sampling 

a signal when it not clearly either high or low
 e fact that computers are digital 
is also a key reason they use binary numbers, since a binary system matches the 

underlying abstraction inherent in the electronics. In various logic families, the 

values and relationships between the two voltage val
 er. 
 us, rather than 
refer to the voltage levels, we talk about signals that are (logically) true, or 1, or are 
asserted
; or signals that are (logically) false, or 0, or are 
deasserted
 e values 0 
and 1 are called 
complements
 or 
inverses
 of one another.
Logic blocks are categorized as one of two types, depending on whether they 
contain memory. Blocks without memory are called 
combinational
; the output of 
a combinational block depends only on the current input. In blocks with memory, 
the outputs can depend on both the inputs and the value stored in memory, which 

is called the 
state
 of the logic block. In this section and the next, we will focus 
asserted signal
 A signal 
that is (logically) true, 
or 1.
deasserted signal
 A signal that is (logically) 

false, or 0.

 B.2 Gates, Truth Tables, and Logical Equations 
B-5only on 
combinational logic
 er introducin
 erent memory elements in 
Section B.8, we will describe how 
sequential logic
, which is logic including state, 
is designed.Truth Tables
Because a combinational logic block contains no memory, it can be completely 
sp
 ed b
 ning the values of the outputs for each possible set of input values. 
Such a description is normally given as a 
truth table
. For a logic block with 
n inputs, there are 2
n entries in the truth table, since there are that many possible 
combinations of input values. Each entry sp
 es the value of all the outputs for 
that particular input combination.
Truth Tables
Consider a logic function with three inputs, 
A, B, and 
C, and three outputs, 
D, E, and 
F e functio
 ned as follows: 
D is true if at least one input is true, 
E is true if exactly two inputs are true, and 
F is true only if all three inputs are 
true. Show the truth table for this function.
 e truth table will contain 2
3  8 entries. Here it is:
InpuInputsOutputsABCDEF
000000
001100

010100

011110

100100

101110

110110

111101
Truth tables can completely describe any combinational logic function; however, 
they grow in size quickly and may not be easy to understand. Sometimes we want 
to construct a logic function that will be 0 for many input combinations, and we 

use a shorthand of specifying only the truth table entries for the nonzero outputs. 

 is approach is used in Chapter 4 and 
 Appendix D
.combinational logic
 A logic system whose 
blocks do not contain 

memory and hence 

compute the same output 

given the same input.
sequential logic
 A group of logic elements 

that contain memory 

and hence whose value 

depends on the inputs 

as well as the current 

contents of the memory.
EXAMPLEANSWER
B-6 Appendix B The Basics of Logic Design
Boolean AlgebraAnother approach is to express the logic function with logic equation
 is is done with the use of 
Boolean algebra
 (named
 er Boole, a 19th-century 
mathematician). In Boolean algebra, all the variables have the values 0 or 1 and, in 
typical formulations, there are three operators:
 e OR operator is written as 
, as in A  B e result of an OR operator is 
1 if either of the variab
 e OR operation is also called a 
logical sum
, since its result is 1 if either operand is 1.
 e AND operator is written as 
 , as in A  B e result of an AND operator 
is 1 only if both inputs ar
 e AND operator is also called 
logical product
, since its result is 1 only if both operands are 1.
 e unary operator NOT is written as 
A e result of a NOT operator is 1 only if 
the input is 0. Applying the operator NOT to a logical value results in an inversion 

or negation of the value (i.e., if the input is 0 the output is 1, and vice versa).
 ere are several laws of Boolean algebra that are helpful in manipulating logic 
equations.
 Identity law: 
A  0  A and 
A  1  A Zero and One laws: 
A  1  1 and 
A  0  0 Inverse laws: 
AA1 and 
AA0 Commutative laws: 
A  B  B  A and 
A  B  B  A Associative laws: 
A  (B  C)  (A  B)  C and 
A  (B  C)  (A  B)  C Distributive laws: 
A  (B  C)  (A  B)  (A  C) and 
A  (B  C)  (A  B)  (A  C)In addition, there are two other useful theorems, called DeMorgan’s laws, that are 

discussed in more depth in the exercises.
Any set of logic functions can be written as a series of equations with an output 
on th
 -hand side of each equation and a formula consisting of variables and the 
three operators above on the right-hand side.

 B.2 Gates, Truth Tables, and Logical Equations 
B-7Logic EquationsShow the logic equations for the logic functions, 
D, E, and 
F, described in the 
previous example.
Here’s the equation for 
D:DABC
F is equally simple:
FABC
E is a little tricky
 ink of it in two parts: what must be true for 
E to be true 
(two of the three inputs must be true), and what cannot be true (all three 
cannot be tr
 us we can write 
E asEABACBCABC
(()()())()
We can also derive 
E by realizing that 
E is true only if exactly two of the inputs 
are true
 en we can write 
E as an OR of the three possible terms that have 
two true inputs and one false input:
EABCACBBCA
()()()
Proving that these two expressions are equivalent is explored in the exercises.
In Verilog, we describe combinational logic whenever possible using the assign 
statement, which is described beginning on page B-23. We can writ
 nition 
for 
E using the Verilog exclusive-OR operator as 
assign E  (A ^ B ^ C) * (A + B + C) * (A * B * C), which is yet another way to describe this function. 
D and 
F have even simpler representations, which are just like the corresponding C 
code: 
D  A | B | C and 
F  A & B & C.EXAMPLEANSWER
B-8 Appendix B The Basics of Logic Design
GatesLogic blocks are built from 
gates
 that implement basic logic functions. For example, 
an AND gate implements the AND function, and an OR gate implements the OR 
function. Since both AND and OR are commutative and associative, an AND or an 
OR gate can have multiple inputs, with the output equal to the AND or OR of all 

the inpu
 e logical function NOT is implemented with an inverter that always 
has a single inpu
 e standard representation of these three logic building blocks 
is shown in 
Figure B.2.1
.Rather than draw inverters explicitly, a common practice is to add “bubbles” 
to the inputs or outputs of a gate to cause the logic value on that input line or 

output line to be inverted. For example, 
Figure B.2.2
 shows the logic diagram for 

the function 
AB, using explicit inverters on th
  and bubbled inputs and 
outputs on the right.
Any logical function can be constructed using AND gates, OR gates, and 
inversion; several of the exercises give you the opportunity to try implementing 

some common logic functions with gates. In the next section, we’ll see how an 

implementation of any logic function can be constructed using this knowledge.
In fact, all logic functions can be constructed with only a single gate type, if that 
gate is invertin
 e two common inverting gates are called 
NOR and 
NAND
 and 
correspond to inverted OR and AND gates, respectively. NOR and NAND gates are 
called universal
, since any logic function can be built using this one gate type
 e exercises explore this concept further.
Are the following two logical expressions equivalent? If no
 nd a setting of the 
variables to show they are not:
 ()()()
ABCACBBCA
 BACCA
()gate
 A device that 
implements basic logic 
functions, such as AND 

or OR.
NOR gate
 An inverted 
OR gate.
NAND gate
 An inverted 
AND gate.
Check Yourself
FIGURE B.2.1 Standard drawing for an AND gate, OR gate, and an inverter, shown from 
left to right. e signals to th  of each symbol are the inputs, while the output appears on the righ
 e AND and OR gates both have two inputs. Inverters have a single input.
ABABFIGURE B.2.2 Logic gate implementation of AB using explicit inverts on the left and 
bubbled inputs and outputs on the right. is logic function can be simp
 ed to 
ABor in Verilog, 
A & ~ B.
 B.3 Combinational Logic B-9 B.3 Combinational Logic
In this section, we look at a couple of larger logic building blocks that we use 
heavily, and we discuss the design of structured logic that can be automatically 

implemented from a logic equation or truth table by a translation program. Last, 

we discuss the notion of an array of logic blocks.
Decoders
One logic block that we will use in building larger components is a 
decoder
 e most common type of decoder has an 
n-bit input and 2
n outputs, where only one 
output is asserted for each input combinatio
 is decoder translates the 
n-bit 
input into a signal that corresponds to the binary value of the 
n-bit inpu
 e outputs are thus usually numbered, say, Out0, Out1, … , Out2
n  1. If the value of 
the input is 
i, then Out
i will be true and all other outputs will be false. 
Figure B.3.1
 shows a 3-bit decoder and the truth table
 is decoder is called a 
3-to-8 decoder
 since there are 3 inputs and 8 (2
3) outpu
 ere is also a logic element called 
an 
encoder that performs the inverse function of a decoder, taking 2
n inputs and 
producing an 
n-bit output.
decoder
 A logic block 
that has an 
n-bit input 
and 2
n outputs, where 
only one output is 

asserted for each input 

combination.
stuptuO
stupnI
121110Out7Out6Out5Out4Out3Out2Out1Out0
0000000000100100000010
01000000100
01100001000
10000010000
10100100000
11001000000
11110000000b. The truth table for a 3-bit decoder
Decoder3Out0Out1Out2Out3Out4Out5Out6Out7a. A 3-bit decoderFIGURE B.3.1 A 3-bit decoder has 3 inputs, called 12, 11, and 10, and 2
3 = 8 outputs, called Out0 to Out7.
 Only the 
output corresponding to the binary value of the input is true, as shown in the truth table
 e label 3 on the input to the decoder says that the 
input signal is 3 bits wide.

B-10 Appendix B The Basics of Logic Design
Multiplexors
One basic logic function that we use quite o
 en in Chapter 4 is the 
multiplexor
. A multiplexor might more properly be called a 
selector
, since its output is one of 
the inputs that is selected by a control. Consider the two-input multiplexor
 e   side of 
Figure B.3.2
 shows this multiplexor has three inputs: two data values 
and a 
selector 
(or control
) value
 e selector value determines which of the 
inputs becomes the output. We can represent the logic function computed by a 
two-input multiplexor, shown in gate form on the right side of 
Figure B.3.2
, as CASBS
()()
.Multiplexors can be created with an arbitrary number of data inputs. When 
there are only two inputs, the selector is a single signal that selects one of the inputs 

if it is true (1) and the other if it is false (0). If there are 
n data inputs, there will 
need to be 
log
2n
 selector inputs. In this case, the multiplexor basically consists 
of three parts:
1. A decoder that generates 
n signals, each indicatin
 erent input value
2. An array of 
n AND gates, each combining one of the inputs with a signal 
from the decoder
3. A single large OR gate that incorporates the outputs of the AND gates
To associate the inputs with selector values, we o
 en label the data inputs numerically 
(i.e., 0, 1, 2, 3, …, 
n  1) and interpret the data selector inputs as a binary number. 
Sometimes, we make use of a multiplex
or with undecoded selector signals.
Multiplexors are easily represented combinationally in Verilog by using 
if expressions. For larger multiplexors, 
case
 statements are more convenient, but care 
must be taken to synthesize combinational logic.
selector value
 Also 
called control value
 e control signal that is used 
to select one of the input 

values of a multiplexor 

as the output of the 

multiplexor.
Mux10CSBAABSCFIGURE B.3.2 A two-input multiplexor on the left and its implementation with gates on the right. e multiplexor has two data inputs (
A and 
B), which are labeled 
0 and 
1, and one selector input 
(S), as well as an output 
C. Implementing multiplexors in Verilog requires a little more work, especially when 
they are wider than two inputs. We show how to do this beginning on page B-23.

 B.3 Combinational Logic B-11Two-Level Logic and PLAs
As pointed out in the previous section, any logic function can be implemented with 
only AND, OR, and NOT functions. In fact, a much stronger result is true. Any logic 

function can be written in a canonical form, where every input is either a true or 

complemented variable and there are only two levels of gates—one being AND and 

the other OR—with a possible inversion on th
 nal output. Such a representation 
is called a 
two-level representation
, and there are two forms, called 
sum of products
 and product of sums
. A sum-of-products representation is a logical sum (OR) of 
products (terms using the AND operator); a product of sums is just the opposite. 

In our earlier example, we had two equations for the output 
E:EABACBCABC
(()()())()
andEABCACBBCA
()()()
 is second equation is in a sum-of-products form: it has two levels of logic and the 
only inversions are on individual variab
 e 
 rst equation has three levels of logic.
Elaboration: We can also write 
E as a product of sums:EABCACBBCA
()()()
To derive this form, you need to use 
DeMorgan’s theorems
, which are discussed in the 
exercises.In this text, we use the sum-of-products form. It is easy to see that any logic 
function can be represented as a sum of products by constructing such a 

representation from the truth table for the function. Each truth table entry for 

which the function is true corresponds to a product ter
 e product term 
consists of a logical product of all the inputs or the complements of the inputs, 

depending on whether the entry in the truth table has a 0 or 1 corresponding to 

this variable
 e logic function is the logical sum of the product terms where the 
function is true
 is is more easily seen with an example.
sum of products
 A form 
of logical representation 
that employs a logical sum 

(OR) of products (terms 

joined using the AND 

operator).

B-12 Appendix B The Basics of Logic Design
Sum of ProductsShow the sum-of-products representation for the following truth table for 
D.InputsOutputsABCD
00000011
0101
0110
1001
1010
1100
1111 ere are four product terms, since the function is true (1) for fo
 erent 
input combination
 ese are:
ABCABCABCABC us, we can write the function for 
D as the sum of these terms:
DABCABCABCABC
()()()()
Note that only those truth table entries for which the function is true generate 
terms in the equation.
We can use this relationship between a truth table and a two-level representation 
to generate a gate-level implementation of any set of logic functions. A set of logic 

functions corresponds to a truth table with multiple output columns, as we saw in 

the example on page B-5. Each output column represen
 erent logic function, 
which may be directly constructed from the truth table.
 e sum-of-products representation corresponds to a common structured-logic 
implementation called a 
programmable logic array (PLA)
. A PLA has a set of 
inputs and corresponding input complements (which can be implemented with a 
set of inverters), and two stages o
 e 
 rst stage is an array of AND gates that 
form a set of 
product terms
 (sometimes called 
minterms
); each product term can 
consist of any of the inputs or their complemen
 e second stage is an array of 
OR gates, each of which forms a logical sum of any number of the product terms. 

Figure B.3.3
 shows the basic form of a PLA.
EXAMPLEANSWERprogrammable logic 
array (PLA)
 A structured-logic 
element composed 

of a set of inputs and 

corresponding input 

complements and two 

stages of logic: th
 rst 
generates product terms 

of the inputs and input 

complements, and the 

second generates sum 

terms of the product 

terms. Hence, PLAs 

implement logic functions 

as a sum of products.
minterms
 Also called 
product terms
. A set 
of logic inputs joined 

by conjunction (AND 

operations); the product 

terms form the
 rst logic 
stage of the 
programmable 

logic array
 (PLA).

 B.3 Combinational Logic B-13A PLA can directly implement the truth table of a set of logic functions with 
multiple inputs and outputs. Since each entry where the output is true requires 
a product term, there will be a corresponding row in the PLA. Each output 

corresponds to a potential row of OR gates in the second stage.
 e number of OR 
gates corresponds to the number of truth table entries for which the output is true. 

 e total size of a PLA, such as that shown in 
Figure B.3.3
, is equal to the sum of the 
size of the AND gate array (called the 
AND plane
) and the size of the OR gate array 
(called the 
OR plane
). Looking at 
Figure B.3.3
, we can see that the size of the AND 
gate array is equal to the number of inputs times the number o
 erent product 
terms, and the size of the OR gate array is the number of outputs times the number 

of product terms.
A PLA has two characteristics that help make it a
  cient way to implement a 
set of logic functions. First, only the truth table entries that produce a true value for 

at least one output have any logic gates associated with them. Second, eac
 erent 
product term will have only one entry in the PLA, even if the product term is used 

in multiple outputs. Let’s look at an example.
PLAsConsider the set of logic function
 ned in the example on page B-5. Show 
a PLA implementation of this example for 
D, E, and 
F.EXAMPLEAND gatesOR gatesProduct terms
OutputsInputsFIGURE B.3.3 The basic form of a PLA consists of an array of AND gates followed by an 
array of OR gates.
 Each entry in the AND gate array is a product term consisting of any number of inputs or 
inverted inputs. Each entry in the OR gate array is a sum term consisting of any number of these product terms.

B-14 Appendix B The Basics of Logic Design
Here is the truth table we constructed earlier:
InputsOutputsABCDEF
000000
001100

010100

011110

100100

101110

110110

111101
Since there are seven unique product terms with at least one true value in the 
output section, there will be seven columns in the AND plane.
 e number of 
rows in the AND plane is three (since there are three inputs), and there are also 

three rows in the OR plane (since there are three outputs). 
Figure B.3.4
 shows 

the resulting PLA, with the product terms corresponding to the truth table 

entries from top to bottom.
Rather than drawing all the gates, as we do in 
Figure B.3.4
, designers o
 en show 
just the position of AND gates and OR gates. Dots are used on the intersection of a 

product term signal line and an input line 
or an output line when a corresponding 
AND gate or OR gate is required. 
Figure B.3.5
 shows how the PLA of 
Figure B.3.4
 
would look when drawn in this way
 e contents of a PLA ar
 xed when the PLA 
is created, although there are also forms of PLA-like structures, called 
PALs
, that 
can be programmed electronically when a designer is ready to use them.
ROMs
Another form of structured logic that can be used to implement a set of logic 

functions is a 
read-only memory (ROM)
. A ROM is called a memory because it 
has a set of locations that can be read; however, the contents of these locations are 
 xed, usually at the time the ROM is manufactured
 ere are also 
programmable 
ROMs (PROMs)
 that can be programmed electronically, when a designer knows 
their conten
 ere are also erasable PROMs; these devices require a slow erasure 
process using ultraviolet light, and thus are used as read-only memories, except 

during the design and debugging process.
A ROM has a set of input address lines and a set of outpu
 e number of 
addressable entries in the ROM determines the number of address lines: if the 
ANSWERread-only memory 
(ROM)
 A memory 
whose contents are 
designated at creation 

time, a
 er which the 
contents can only be read. 

ROM is used as structured 

logic to implement a 

set of logic functions by 

using the terms in the 

logic functions as address 

inputs and the outputs as 

bits in each word of the 

memory.
programmable ROM 
(PROM)
 A form of 
read-only memory that 
can be pro grammed 

when a designer knows its 

contents.

 B.3 Combinational Logic B-15ROM contains 2
m addressable entries, called the 
height
, then there are 
m input 
 e number of bits in each addressable entry is equal to the number of output 
bits and is sometimes called the 
width
 of the RO
 e total number of bits in the 
ROM is equal to the height times the widt
 e height and width are sometimes 
collectively referred to as the 
shape
 of the ROM.
ABCEFOutputsDInputsFIGURE B.3.4 The PLA for implementing the logic function described in the example.A ROM can encode a collection of logic functions directly from the truth table. 
For example, if there are 
n functions with 
m inputs, we need a ROM with 
m address 
lines (and 2
m entries), with each entry being 
n bits wide
 e entries in the input 
portion of the truth table represent the addresses of the entries in the ROM, while 
the contents of the output portion of the truth table constitute the contents of the 

ROM. If the truth table is organized so that the sequence of entries in the input 

portion constitutes a sequence of binary numbers (as have all the truth tables 

we have shown so far), then the output portion gives the ROM contents in order 

as well. In the example starting on page B-13, there were three inputs and three 

outpu
 is leads to a ROM with 2
3  8 entries, each 3 bits wide
 e contents of 
those entries in increasing order by address are directly given by the output portion 

of the truth table that appears on page B-14.
ROMs and PLAs are closely related. A ROM is fully decoded: it contains a full 
output word for every possible input combination. A PLA is only partially decoded. 

 is means that a ROM will always contain more entries. For the earlier truth table 
on page B-14, the ROM contains entries for all eight possible inputs, whereas the 

PLA contains only the seven active product terms. As the number of inputs grows, 

B-16 Appendix B The Basics of Logic Design
the number of entries in the ROM grows exponentially. In contrast, for most real 
logic functions, the number of product terms grows much more slowly (see the 

examples in 
 Appendix D
 is 
 erence makes PLAs generally mor
  cient 
for implementing combinational logic functions. ROMs have the advantage of 

being able to implement any logic function with the matching number of inputs 

and outpu
 is advantage makes it easier to change the ROM contents if the logic 
function changes, since the size of the ROM need not change.
In addition to ROMs and PLAs, modern logic synthesis systems will also 
translate small blocks of combinational logic into a collection of gates that can 

be placed and wired automatically. Although some small collections of gates are 

usually not are
  cient, for small logic functions they have less overhead than the 
rigid structure of a ROM and PLA and so are preferred.
For designing logic outside of a custom or semicustom integrated circuit, a common 
cho
 eld programming device; we describe these devices in Section B.12.
ABCInputsAND planeOR planeDEFOutputsFIGURE B.3.5 A PLA drawn using dots to indicate the components of the product terms 
and sum terms in the array.
 Rather than use inverters on the gates, usually all the inputs are run the 
width of the AND plane in both true and complement forms. A dot in the AND plane indicates that the 

input, or its inverse, occurs in the product term. A dot in the OR plane indicates that the corresponding 

product term appears in the corresponding output.

 B.3 Combinational Logic B-17Don’t Cares
 en in implementing some combinational logic, there are situations where we do 
not care what the value of some output is, either because another output is true or 
because a subset of the input combinations determines the values of the outputs. 

Such situations are referred to as 
don’t cares
. Don’t cares are important because they 
make it easier to optimize the implementation of a logic function.
 ere are two types of don’t cares: output don’t cares and input don’t cares, both 
of which can be represented in a truth table. 
Output don’t cares
 arise when we don’t 
care about the value of an output for some input combinatio
 ey appear as Xs in 
the output portion of a truth table. When an output is a don’t care for some input 

combination, the designer or logic optimization program is free to make the output 

true or false for that input combination. 
Input don’t cares
 arise when an output 
depends on only some of the inputs, and they are also shown as Xs, though in the 

input portion of the truth table.
Don’t Cares
Consider a logic function with inputs 
A, B, and 
C ned as follows:
 If 
A or 
C is true, then output 
D is true, whatever the value of 
B. If 
A or 
B is true, then output 
E is true, whatever the value of 
C. Output 
F is true if exactly one of the inputs is true, although we don’t care 
about the value of 
F, whenever 
D and 
E are both true.
Show the full truth table for this function and the truth table using don’t cares. 

How many product terms are required in a PLA for each of these?
Here’s the full truth table, without don’t cares:
InputsOutputsABCDEF
000000
001101

010011

011110

100111

101110

110110

111110
EXAMPLEANSWER
B-18 Appendix B The Basics of Logic Design
 is requires seven product terms without optimizatio
 e truth table 
written with output don’t cares looks like this:
InputsOutputsABCDEF
000000
001101

010011

01111X

10011X

10111X

11011X

11111X
If we also use the input don’t cares, this truth table can be further simp
 ed to yield the following:
InputsOutputsABCDEF
000000

001101

010011
X1111X
1XX11X
 is simp
 ed truth table requires a PLA with four minterms, or it can be 
implemented in discrete gates with one two-input AND gate and three OR gates 
(two with three inputs and one with two inpu
 is compares to the original 
truth table that had seven minterms and would have required four AND gates.
Logic minimization is critical to achieving
  cient implementations. One tool 
useful for hand minimization of random logic is 
Karnaugh maps
. Karnaugh maps 
represent the truth table graphically, so that product terms that may be combined 

are easily seen. Nevertheless, hand optimization o
 cant logic functions 
using Karnaugh maps is impractical, both because of the size of the maps and their 

complexity. Fortunately, the process of logic minimization is highly mechanical and 

can be performed by design tools. In the process of minimization, the tools take 

advantage of the don’t cares, so specifying them is importan
 e text book references 
at the end of this appendix provide further discussion on logic minimization, 

Karnaugh maps, and the theory behind such minimization algorithms.
Arrays of Logic Elements
Many of the combinational operations to be performed on data have to be done 

to an entire word (32 bits) of dat
 us we o
 en want to build an array of logic 

 B.4 Using a Hardware Description Language 
B-19elements, which we can represent simply by showing that a given operation will 
happen to an entire collection of inputs. Inside a machine, much of the time we 

want to select between a pair of 
buses
. A bus
 is a collection of data lines that is 
treated together as a single logical signal.
 e term 
bus
 is also used to indicate a 
shared collection of lines with multiple sources and uses.)
For example, in the MIPS instruction set, the result of an instruction that is written 
into a register can come from one of two sources. A multiplexor is used to choose 

which of the two buses (each 32 bits wide) will be written into the Result register. 

 e 1-bit multiplexor, which we showed earlier, will need to be replicated 32 times.
We indicate that a signal is a bus rather than a single 1-bit line by showing it with 
a thick
 gure. Most buses are 32 bits wide; those that are not are explicitly 
labeled with their width. When we show a logic unit whose inputs and outputs are 

buses, this means that the unit must be replicat
  cient number of times to 
accommodate the width of the input. 
Figure B.3.6
 shows how we draw a multiplexor 

that selects between a pair of 32-bit buses and how this expands in terms of 1-bit-

wide multiplexors. Sometimes we need to construct an array of logic elements 

where the inputs for some elements in the array are outputs from earlier elements. 

For example, this is how a multibit-wide ALU is constructed. In such cases, we must 

explicitly show how to create wider arrays, since the individual elements of the array 

are no longer independent, as they are in the case of a 32-bit-wide multiplexor.
bus
 In logic design, a 
collection of data lines 
that is treated together 

as a single logical signal; 

also, a shared collection 

of lines with multiple 

sources and uses.
MuxCSelect323232BAMuxSelectB31A31C31MuxB30A30C30MuxB0A0C0..
..
.
.a. A 32-bit wide 2-to-1 multiplexor b. The 32-bit wide multiplexor is actually 
an array of 32 1-bit multiplexorsFIGURE B.3.6 A multiplexor is arrayed 32 times to perform a selection between two 32-
bit inputs. Note that there is still only one data selection signal used for all 32 1-bit multiplexors.

B-20 Appendix B The Basics of Logic Design
Parity is a function in which the output depends on the number of 1s in the input. 
For an even parity function, the output is 1 if the input has an even number of ones. 

Suppose a ROM is used to implement an even parity function with a 4-bit input. 

Which of A, B, C, or D represents the contents of the ROM?
AddressABCD00101
10110

20101

30110

40101

50110

60101

70110

81001

91010
101001
111010
121001
131010
141001
151010
 B.4 Using a Hardware Description Language
Today most digital design of processors and related hardware systems is done 
using a 
hardware description language
. Such a language serves two purposes. 
First, it provides an abstract description of the hardware to simulate and debug the 
design. Second, with the use of logic synthesis and hardware compilation tools, this 

description can be compiled into the hardware implementation.
In this section, we introduce the hardware description language Verilog and 
show how it can be used for combinational design. In the rest of the appendix, 

we expand the use of Verilog to include desig
n of sequential logic. In the optional 
sections of Chapter 4 that appear online, we use Verilog to describe processor 

implementations. In the optional section from Chapter 5 that appears online, we 

use system Verilog to describe cache controller implementations. System Verilog 

adds structures and some other useful features to Verilog.
Verilog
 is one of the two primary hardware description languages; the other 
is VHDL
. Verilog is somewhat more heavily used in industry and is based on C, 
as opposed to VHDL, which is based on Ad
 e reader generally familiar with 
 nd the basics of Verilog, which we use in this appendix, easy to follow. 
Check Yourself
hardware description 
language
 A programming language 
for describing hardware, 

used for generating 

simulations of a hardware 

design and also as input 

to synthesis tools that can 

generate actual hardware.
Verilog
 One of the two 
most common hardware 

description languages.
VHDL
 One of the two 
most common hardware 

description languages.

 B.4 Using a Hardware Description Language 
B-21Readers already familiar with VHDL sho
 nd the concepts simple, provided 
they have been exposed to the syntax of C.
Verilog can specify both a behavioral and a struct
 nition of a digital 
system. A 
behavioral spe
 cation
 describes how a digital system functionally 
operates. A 
structural spe
 cation
 describes the detailed organization of a digital 
system, usually using a hierarchical description. A structural sp
 cation can be 
used to describe a hardware system in terms of a hierarchy of basic elements such 
as gates and switch
 us, we could use Verilog to describe the exact contents of 
the truth tables and datapath of the last section.
With the arrival of 
hardware synthesis
 tools, most designers now use Verilog 
or VHDL to structurally describe only the datapath, relying on logic synthesis to 
generate the control from a behavioral description. In addition, most CAD systems 

provide extensive libraries of standardized parts, such as ALUs, multiplexors, 

regist
 les, memories, and programmable logic blocks, as well as basic gates.
Obtaining an acceptable result using libraries and logic synthesis requires that 
the sp
 cation be written with an eye toward the eventual synthesis and the 
desired outcome. For our simple designs, this primarily means making clear what 

we expect to be implemented in combinational logic and what we expect to require 

sequential logic. In most of the examples we use in this section and the remainder 

of this appendix, we have written the Verilog with the eventual synthesis in mind.
Datatypes and Operators in Verilog
 ere are two primary datatypes in Verilog:
1. A wire
 sp
 es a combinational signal.
2. A reg
 (register) holds a value, which can vary with time. A reg need not 
necessarily correspond to an actual register in an implementation, although 
it
 en will.
A register or wire, named X, that is 32 bits wide is declared as an array: 
reg [31:0] X or 
wire [31:0] X, which also sets the index of 0 to designate the 
le
 cant bit of the register. Because we o
 en want to access a sub
 eld of a 
register or wire, we can refer to a contiguous set of bits of a register or wire with the 

notation [
starting bit: ending bit], where both indices must be constant 
values.
An array of registers is used for a structure like a regist
 le or memory
 us, the declaration
reg [31:0] registerfile[0:31]sp
 es a variable register
 le that is equivalent to a MIPS register
 le, where 
register 0 is th
 rst. When accessing an array, we can refer to a single element, as 
in C, using the notation 
registerfile[regnum].behavioral 
spe
 cation
 Describes 
how a digital system 
operates functionally.
structural 
spe
 cation
 Describes 
how a digital system is 
organized in terms of a 

hierarchical connection of 

elements.
hardware synthesis 
tools
 Computer-aided 
design so
 ware that 
can generate a gate-
level design based on 

behavioral descriptions of 

a digital system.
wire
 In Verilog, sp
 es a combinational signal.
reg
 In Verilog, a register.

B-22 Appendix B The Basics of Logic Design
 e possible values for a register or wire in Verilog are
 0 or 1, representing logical false or true
 X, representing unknown, the initial value given to all registers and to any 
wire not connected to something
 Z, representing the high-impedance state for tristate gates, which we will not 
discuss in this appendix
Constant values can be sp
 ed as decimal numbers as well as binary, octal, or 
hexadecimal. We o
 en want to say exactly how large a constan
 eld is in bi
 is is done by pr
 xing the value with a decimal number specifying its size in bits. For 
example:
 4’b0100 sp
 es a 4-bit binary constant with the value 4, as does 
4’d4. - 8 ‘h4 sp
 es an 8-bit constant with the value 
4 (in two’s complement 
representation)
Values can also be concatenated by placing them within 
{ } separated by commas. 
 e notation 
{x{bitfield}}
 replicates 
bit field x times. For example:
 {16{2’b01}} creates a 32-bit value with the pattern 0101 … 01.
 {A[31:16],B[15:0]} creates a value whose upper 16 bits come from 
A and whose lower 16 bits come from 
B.Verilog provides the full set of unary and binary operators from C, including the 
arithmetic operators (
, , *. /), the logical operators (&, |, 
), the comparison 
operators (
 , !, , ,  ,  ), th
  operators (
, ), and C’s 
conditional operator (?, which is used in the form 
condition ? expr1 :expr2 and returns 
expr1 if the condition is true and 
expr2 if it is false). Verilog adds 
a set of unary logic reduction operators (&, |, 
^) that yield a single bit by applying 
the logical operator to all the bits of an operand. For example, 
&A returns the value 
obtained by ANDing all the bits of 
A together, and 
^A returns the reduction obtained 
by using exclusive OR on all the bits of 
A.Which of the followin
 ne exactly the same value?
l. 8’bimoooo2. 8’hF0
3. 8’d240
4. {{4{1’b1}},{4{1’b0}}}
5. {4’b1,4’b0)Check Yourself

 B.4 Using a Hardware Description Language 
B-23Structure of a Verilog Program
A Verilog program is structured as a set of modules, which may represent anything 
from a collection of logic gates to a complete system. Modules are similar to classes 

in C, although not nearly as powerful. A module sp
 es its input and output 
ports, which describe the incoming and outgoing connections of a module. A 

module may also declare additional variab
 e body of a module consists of:
 initial constructs, which can initialize 
reg variables
 Continuous assignments, whic
 ne only combinational logic
 always constructs, which ca
 ne either sequential or combinational 
logic Instances of other modules, which are used to implement the module being 
 ned
Representing Complex Combinational Logic in Verilog
A continuous assignment, which is indicated with the keyword 
assign, acts like 
a combinational logic function: the output is continuously assigned the value, and 

a change in the input values is r
 ected immediately in the output value. Wires 
may only be assigned values with cont
inuous assignments. Using continuous 
assignments, we ca
 ne a module that implements a half-adder, as 
Figure B.4.1
 shows.
Assign statements are one sure way to write Verilog that generates combinational 
logic. For more complex structures, however, assign statements may be awkward or 

tedious to use. It is also possible to use the 
always block of a module to describe 
a combinational logic element, although care must be taken. Using an 
always block allows the inclusion of Verilog control constructs, such as 
if-then-else, case
 statements, 
for statements, and 
repeat
 statements, to be used
 ese statements are 
similar to those in C with small changes.
An always block sp
 es an optional list of signals on which the block is 
sensitive (in a list starting wit
 e always block is re-evaluated if any of the 
FIGURE B.4.1 A Verilog module that deﬁ nes a half-adder using continuous assignments.

B-24 Appendix B The Basics of Logic Design
listed signals changes value; if the list is omitted, the 
always block is constantly re-
evaluated. When an 
always block is specifying combinational logic, the 
sensitivity 
list should include all the input signals. If there are multiple Verilog statements to 
be executed in an 
always block, they are surrounded by the keywords 
begin and 
end, which take the place of the { and } in C. An 
always block thus looks like this:
always @(list of signals that cause reevaluation) beginVerilog statements including assignments and other control statements endReg variables may only be assigned inside an 
always block, using a procedural 
assignment statement (as distinguished 
from continuous assignment we saw 
ea
 ere are, however, tw
 erent types of procedural assignmen
 e assignment operator 
 executes as it does in C; the right-hand side is evaluated, 
and th
 -hand side is assigned the value. Furthermore, it executes like the 
normal C assignment statement: that is, it is completed before the next statement is 

executed. Hence, the assignment operator 
 has the name 
blocking assignment
.  is blocking can be useful in the generation of sequential logic, and we will return 
to it shortly
 e other form of assignment (
nonblocking
) is indicated by 
<=. In 
nonblocking assignment, all right-hand sides of the assignments in an 
always group are evaluated and the assignments are done simultaneously. A
 rst 
example of combinational logic implemented using an 
always block, 
Figure B.4.2
 shows the implementation of a 4-to-1 multiplexor, which uses a 
case construct to 
make it easy to write.
 e case construct looks like a C 
switch statement. 
Figure
 B.4.3
 sho
 nition of a MIPS ALU, which also uses a 
case statement.
Since only reg variables may be assigned inside 
always blocks, when we want to 
describe combinational logic using an 
always block, care must be taken to ensure 
that the reg does not synthesize into a register. A variety of pitfalls are described in 

the elaboration below.
Elaboration: Continuous assignment statements always yield combinational logic, 
but other Verilog structures, even when in 
always blocks, can yield unexpected results 
during logic synthesis. The most common problem is creating sequential logic by 
implying the existence of a latch or register, which results in an implementation that is 

both slower and more costly than perhaps intended. To ensure that the logic that you 

intend to be combinational is synthesized that way, make sure you do the following:
1. Place all combinational logic in a continuous assignment or an 
always block.2. Make sure that all the signals used as inputs appear in the sensitivity list of an 
always block.3. Ensure that every path through an 
always block assigns a value to the exact same set of bits.The last of these is the easiest to overlook; read through the example in 
Figure
 B.5.15 to convince yourself that this property is adhered to.
sensitivity list
 e list of 
signals that sp
 es when 
an 
always block should 
be re-evaluated.
blocking assignment
 In Verilog, an assignment 
that completes before 

the execution of the next 

statement.
nonblocking 
assignment
 An assignment that continues 
 er evaluating the right-
hand side, assigning the 

 -hand side the value 
only
 er all right-hand 
sides are evaluated.

 B.5 Constructing a Basic Arithmetic Logic Unit 
B-25FIGURE B.4.3 A Verilog behavioral deﬁ nition of a MIPS ALU.
 is could be synthesized using a module library containing basic 
arithmetic and logical operations.
FIGURE B.4.2 A Verilog deﬁ
 nition of a 4-to-1 multiplexor with 32-bit inputs, using a 
case statement e case statement acts like a C 
switch statement, except that in Verilog only the code 
associated with the selected case is executed (as if each case state had a break at the end) and there is no fall-
through to the next statement.

B-26 Appendix B The Basics of Logic Design
Assuming all values are initially zero, what are the values of 
A and 
B er executing 
this Verilog code inside an 
always block?
C=1;A <= C;
B = C; B.5 Constructing a Basic Arithmetic Logic 
Unit e arithmetic logic unit
 (ALU)
 is the brawn of the computer, the device that per-
forms the arithmetic operations like addition and subtraction or logical operations 
like AND and OR
 is section constructs an ALU from four hardware building 
blocks (AND and OR gates, inverters, and multiplexors) and illustrates how 
combinational logic works. In the next section, we will see how addition can be 

sped up through more clever designs.
Because the MIPS word is 32 bits wide, we need a 32-bit-wide ALU. Let’s assume 
that we will connect 32 1-bit ALUs to create the desired ALU. We’ll therefore start 

by constructing a 1-bit ALU.
A 1-Bit ALU e logical operations are easiest, because they map directly onto the hardware 
components in 
Figure B.2.1
. e 1-bit logical unit for AND and OR looks like 
Figure B.5.1
 e multiplexor 
on the right then selects 
a AND b or 
a OR 
b, depending on whether the value 
of 
Operation
 is 0 o
 e line that controls the multiplexor is shown in color 
to distinguish it from the lines containing data. Notice that we have renamed the 

control and output lines of the multiplexor to give them names that r
 ect the 
function of the ALU.
 e next function to include is addition. An adder must have two inputs for the 
operands and a single-bit output for th
 ere must be a second output to 
pass on the carry, called 
CarryOut
. Since the CarryOut from the neighbor adder 
must be included as an input, we need a third inpu
 is input is called 
CarryIn
. Figure B.5.2
 shows the inputs and the outputs of a 1-bit adder. Since we know what 

addition is supposed to do, we can specify the outputs of this “black box” based on 

its inputs, as 
Figure B.5.3
 demonstrates.
We can express the output functions CarryOut and Sum as logical equations, 
and these equations can in turn be implemented with logic gates. Let’s do CarryOut. 

Figure B.5.4
 shows the values of the inputs when CarryOut is a 1.
We can turn this truth table into a logical equation:
CarryOutbCarryInaCarryInababCarryIn
()()()()
Check Yourself
ALU n. [
Arthritic 

Logic 
Unit or (rare) 

Arithmetic 
Logic 
Unit] 
A random-number 

generator supplied 

as standard with all 

computer systems.
Stan Kelly-Bootle, 
 e Devil’s DP Dictionary,
 1981
 B.1 Introduction B-
27Operation
10ResultabFIGURE B.5.1 The 1-bit logical unit for AND and OR.CarryIn
SumCarryOut
ab+FIGURE B.5.2 A 1-bit adder.
 is adder is called a full adder; it is also called a (3,2) adder because it has 
3 inputs and 2 outputs. An adder with only the a and b inputs is called a (2,2) adder or half-adder.
stuptuO
stupnI
CommentsabCarryInCarryOutSum
000000 + 0 + 0 = 00two001010 + 0 + 1 = 01two010010 + 1 + 0 = 01two011100 + 1 + 1 = 10two100011 + 0 + 0 = 01two101101 + 0 + 1 = 10two110101 + 1 + 0 = 10two111111 + 1 + 1 = 11twoFIGURE B.5.3 Input and output speciﬁ cation for a 1-bit adder.

B-28 Appendix B The Basics of Logic Design
If a 
 b  CarryIn is true, then all of the other three terms must also be true, so we 
can leave out this last term corresponding to the fourth line of the table. We can 

thus simplify the equation to
CarryOutbCarryInaCarryInab
()()()
Figure B.5.5
 shows that the hardware within the adder black box for CarryOut 

consists of three AND gates and one OR gate
 e three AND gates correspond 
exactly to the three parenthesized terms of the formula above for CarryOut, and 

the OR gate sums the three terms.
InputsabCarryIn
011101
110
111FIGURE B.5.4 Values of the inputs when CarryOut is a 1.
abCarryIn
CarryOut
FIGURE B.5.5 Adder hardware for the CarryOut signal.
 e rest of the adder hardware is the logic 
for the Sum output given in the equation on this page.
 e Sum bit is set when exactly one input is 1 or when all three inputs ar
 e Sum results in a complex Boolean equation (recall that 
a means NOT a):
SumabCarryInabCarryInabCarryInabCarryIn
()()()()
) e drawing of the logic for the Sum bit in the adder black bo
  as an exercise 
for the reader.

 B.5 Constructing a Basic Arithmetic Logic Unit 
B-29abCarryIn
CarryOut
Operation
102ResultFIGURE B.5.6 A 1-bit ALU that performs AND, OR, and addition (see 
Figure B.5.5
).Figure B.5.6
 shows a 1-bit ALU derived by combining the adder with the earlier 
components. Sometimes designers also want the ALU to perform a few more 
simple operations, such as generatin e easiest way to add an operation is to 

expand the multiplexor controlled by the Operation line and, for this example, to 

connect 0 directly to the new input of that expanded multiplexor.
A 32-Bit ALUNow that we have completed the 1-bit ALU, the full 32-bit ALU is created by 

connecting adjacent “black boxes.” Using 
xi to mean the 
ith bit of 
x, Figure B.5.7
 shows a 32-bit ALU. Just as a single stone can cause ripples to radiate to the shores 

of a quiet lake, a single carry out of the le
 cant bit (Result0) can ripple all 
the way through the adder, causing a carry out of th
 cant bit (Result31). 
Hence, the adder created by directly linking the carries of 1-bit adders is called a 

ripple carry
 adder. We’ll see a faster way to connect the 1-bit adders starting on 
page B-38.
Subtraction is the same as adding the negative version of an operand, and this 
is how adders perform subtraction. Recall that the shortcut for negating a two’s 

complement number is to invert each bit (sometimes called the 
one’s complement
) and then add 1. To invert each bit, we simply add a 2:1 multiplexor that chooses 

between b and 
b, as Figure B.5.8
 shows.
Suppose we connect 32 of these 1-bit ALUs, as we did in 
Figure B.5.7
 e added 
multiplexor gives the option of b or its inverted value, depending on Binvert, but 

B-30 Appendix B The Basics of Logic Design
this is only one step in negating a two’s complement number. Notice that the least 
 cant bit still has a CarryIn signal, even though it’s unnecessary for addition. 
What happens if we set this CarryIn to 1 instead of
 e adder will then calculate 
a  b  1. By selecting the inverted version of b, we get exactly what we want:
ababab)ab
11()(
 e simplicity of the hardware design of a two’s complement adder helps explain 
why two’s complement representation has become the universal standard for 

integer computer arithmetic.
a0Operation
CarryIn
ALU0CarryOut
b0CarryIn
a1CarryIn
ALU1CarryOut
b1Result0Result1a2CarryIn
ALU2CarryOut
b2a31CarryIn
ALU31b31Result2Result31..
..
.
..
.
.FIGURE B.5.7 A 32-bit ALU constructed from 32 1-bit ALUs.
 CarryOut of th
 cant bit is 
connected to the CarryIn of the mor
 cant bi
 is organization is called ripple carry.

 B.5 Constructing a Basic Arithmetic Logic Unit 
B-31Binvert
abCarryIn
CarryOut
Operation
102Result10FIGURE B.5.8 A 1-bit ALU that performs AND, OR, and addition on a and b or a and b.
 By 
selecting 
b (Binvert 
 1) and setting CarryIn to 1 in the le
 cant bit of the ALU, we get two’s comple-
ment subtraction of b from a instead of addition of b to a.
A MIPS ALU also needs a NOR function. Instead of adding a separate gate 
for NOR, we can reuse much of the hardware already in the ALU, like we did for 
subtrac
 e insight comes from the following truth about NOR:
()abab
 at is, NOT (a OR b) is equivalent to NOT a AND NOT b
 is fact is called 
DeMorgan’s theorem and is explored in the exercises in more depth.
Since we have AND and NOT b, we only need to add NOT a to the ALU. 
Figure
 B.5.9
 shows that change.
Tailoring the 32-Bit ALU to MIPS
 ese four operations—add, subtract, AND, OR—are found in the ALU of almost 
every computer, and the operations of most MIPS instructions can be performed 

by this ALU. But the design of the ALU is incomplete.
One instruction that still needs support is the set on less than instruction (
slt). 
Recall that the operation produces 1 if rs 
 rt, and 0 otherwise. Consequently, 
slt will set all but the le
 cant bit to 0, with the le cant bit set according to 
the comparison. For the ALU to perform 
slt, w
 rst need to expand the three-input 

B-32 Appendix B The Basics of Logic Design
multiplexor in 
Figure B.5.8
 to add an input for the 
slt result. We call that new input 
Less
 and use it only for 
slt. e top drawing of 
Figure B.5.10
 shows the new 1-bit ALU with the expanded 
multiplexor. From the description of 
slt above, we must connect 0 to the Less 
input for the upper 31 bits of the ALU, since those bits are always set to 0. What 
remains to consider is how to compare and set the 
least signi
 cant bit
 for set on less 
than instructions.
What happens if we subtract b from a? If th
 erence is negative, then a 
 b since()(())()
ababbb
ab00We want the le
 cant bit of a set on less than operation to be a 1 if a 
 b; that is, a 1 if a 
 b is negative and a 0 if it’s positive
 is desired result corresponds 
exactly to the sign bit values: 1 means negative and 0 means positive. Following this 
line of argument, we need only connect the sign bit from the adder output to the 

le
 cant bit to get set on less than.
Unfortunately, the Result output from th
 cant ALU bit in the top of 
Figure B.5.10
 for the 
slt operation is 
not the output of the adder; the ALU output 
for the 
slt operation is obviously the input value Less.
BinvertabCarryInCarryOutOperation102Result10Ainvert10FIGURE B.5.9 A 1-bit ALU that performs AND, OR, and addition on a and b or a and b.
 By 
selecting 
a (Ainvert 
 1) and 
b (Binvert 
 1), we get a NOR b instead of a AND b.

Binvert
abCarryIn
CarryOut
Operation
102Result10Ainvert
1
03LessBinvert
abCarryIn
Operation
102Result103LessOverflow
detectionSetOverflow
Ainvert
1
0FIGURE B.5.10 (Top) A 1-bit ALU that performs AND, OR, and addition on a and b or b , and 
(bottom) a 1-bit ALU for the most signiﬁ cant bit.
 e top drawing includes a direct input that is 
connected to perform the set on less than operation (see 
Figure B.5.11
); the bottom has a direct output from 
the adder for the less than comparison called Set. (See Exercise B.24 at the end of this appendix to see how to 

calculate over
 ow with fewer inputs.)

B-34 Appendix B The Basics of Logic Design
 us, we need a new 1-bit ALU for th
 cant bit that has an extra 
output bit: the adder outpu
 e bottom drawing of 
Figure B.5.10
 shows the 
design, with this new adder output line called 
Set
, and used only for 
slt. As long 
as we need a special ALU for th
 cant bit, we added the over
 ow detec-
tion logic since it is also associated with that bit.
..
.a0Operation
CarryIn
ALU0LessCarryOut
b0CarryIn
a1CarryIn
ALU1LessCarryOut
b1Result0Result1a2CarryIn
ALU2LessCarryOut
b2a31CarryIn
ALU31Lessb31Result2Result31..
..
.
..
.
.Binvert
.
.
.Ainvert
000Overflow
.
.
.SetCarryIn
FIGURE B.5.11 A 32-bit ALU constructed from the 31 copies of the 1-bit ALU in the top 
of Figure B.5.10
 and one 1-bit ALU in the bottom of that ﬁ gure. e Less inputs are connected 
to 0 except for the le
 cant bit, which is connected to the Set output of th
 cant bit. If the 
ALU performs a 
 b and we select the input 3 in the multiplexor in 
Figure B.5.10
, then Result 
 0 … 001 if a  b, and Result 
 0 … 000 otherwise.

 B.1 Introduction B-
35Alas, the test of less than is a little more complicated than just described because 
of over
 ow, as we explore in the exercises. 
Figure B.5.11
 shows the 32-bit ALU.
Notice that every time we want the ALU to subtract, we set both CarryIn and 
Binvert to 1. For adds or logical operations, we want both control lines to be 0. We 
can therefore simplify control of the ALU by combining the CarryIn and Binvert to 

a single control line called 
Bnegate
.To further tailor the ALU to the MIPS instruction set, we must support 
conditional branch instruction ese instructions branch either if two registers 

are equal or if they are unequal.
 e easiest way to test equality with the ALU is to 
subtract b from a and then test to see if the result is 0, since
()abab
0 us, if we add hardware to test if the result is 0, we can test for equality
 e simplest way is to OR all the outputs together and then send that signal through 

an inverter:
ZeroRe
sultResultResultResultResult
()3130
210
–Figure B.5.12
 shows the revised 32-bit ALU. We can think of the combination of 
the 1-bit Ainvert line, the 1-bit Binvert line, and the 2-bit Operation lines as 4-bit 

control lines for the ALU, telling it to perform add, subtract, AND, OR, or set on 

less than. 
Figure B.5.13
 shows the ALU control lines and the corresponding ALU 

operation.
Finally, now that we have seen what is inside a 32-bit ALU, we will use the 
universal symbol for a complete ALU, as shown in 
Figure B.5.14
.Deﬁ ning the MIPS ALU in Verilog
Figure B.5.15
 shows how a combinational MIPS ALU might be sp
 ed in Verilog; 
such a sp
 cation would probably be compiled using a standard parts library that 
provided an adder, which could be instantiated. For completeness, we show the 

ALU control for MIPS in 
Figure B.5.16
, which is used in Chapter 4, where we build 

a Verilog version of the MIPS datapath.
 e next question is, “How quickly can this ALU add two 32-bit operands?” 
We can determine the a and b inputs, but the CarryIn input depends on the 

operation in the adjacent 1-bit adder. If we trace all the way through the chain of 

dependencies, we connect th
 cant bit to the le
 cant bit, so 
th cant bit of the sum must wait for the 
sequential
 evaluation of all 32 
1-bit adder
 is sequential chain reaction is too slow to be used in time-critical 
hardware
 e next section explores how to speed-up additio
 is topic is not 
crucial to understanding the rest of the appendix and may be skipped.

B-36 Appendix B The Basics of Logic Design
..
.a0Operation
CarryIn
ALU0LessCarryOut
b0a1CarryIn
ALU1LessCarryOut
b1Result0Result1a2CarryIn
ALU2LessCarryOut
b2a31CarryIn
ALU31Lessb31Result2Result31..
..
.
..
.
.Bnegate.
.
.Ainvert
000Overflow
.
.
.SetCarryIn
.
.
..
.
.ZeroFIGURE B.5.12 The ﬁ nal 32-bit ALU.
 is adds a Zero detector to 
Figure B.5.11
.ALU control linesFunction
0000AND0001OR0010add0110subtract
0111set on less than

1100NOR
FIGURE B.5.13 The values of the three ALU control lines, Bnegate, and Operation, and the 
corresponding ALU operations.

 B.5 Constructing a Basic Arithmetic Logic Unit 
B-37ALUaALU operation
bCarryOut
ZeroResultOverflow
FIGURE B.5.14 The symbol commonly used to represent an ALU, as shown in 
Figure
 B.5.12. is symbol is also used to represent an adder, so it is normally labeled either with ALU or Adder.
FIGURE B.5.15 A Verilog behavioral deﬁ nition of a MIPS ALU.

B-38 Appendix B The Basics of Logic Design
Suppose you wanted to add the operation NOT (a AND b), called NAND. How 
could the ALU change to support it?
1. No change. You can calculate NAND quickly using the current ALU since 
()abab
 and we already have NOT a, NOT b, and OR.
2. You must expand the big multiplexor to add another input, and then add 
new logic to calculate NAND.
 B.6 Faster Addition: Carry Lookahead
 e key to speeding up addition is determining the carry in to the high-order bits 
sooner
 ere are a variety of schemes to anticipate the carry so that the worst-
case scenario is a function of the log
2 of the number of bits in the adder
 ese 
anticipatory signals are faster because they go through fewer gates in sequence, but 

it takes many more gates to anticipate the proper carry.
A key to understanding fast-carry schemes is to remember that, unlike so
  ware, hardware executes in parallel whenever inputs change.
Fast Carry Using “Inﬁ
 nite” Hardware
As we mentioned earlier, any equation can be represented in two levels of logic. 

Since the only external inputs are the two operands and the CarryIn to the least 
Check Yourself
FIGURE B.5.16 The MIPS ALU control: a simple piece of combinational control logic.
 B.6 Faster Addition: Carry Lookahead 
B-39 cant bit of the adder, in theory we could calculate the CarryIn values to all 
the remaining bits of the adder in just two levels of logic.
For example, the CarryIn for bit 2 of the adder is exactly the CarryOut of bit 1, 
so the formula is
CarryInbCarryInaCarryInab1
211111
()()()
Similarly, CarryI
 ned as
CarryInbCarryInaCarryInab
1000000
()()()
Using the shorter and more traditional abbreviation of c
i for CarryIn
i, we can 
rewrite the formulas as
cbcacab
cbcacab
2111111
1000000
()()()
()()()
Substituting th
 nition of c1 for th
 rst equation results in this formula:
caabaacabc
babbac
2100100100
100100
()()()
()()(
()()
bbcab
10011
You can imagine how the equation expands as we get to higher bits in the adder; 
it grows rapidly with the number of bi
 is complexity is r
 ected in the cost of 
the hardware for fast carry, making this simple scheme prohibitively expensive for 

wide adders.
Fast Carry Using the First Level of Abstraction: Propagate 
and GenerateMost fast-carry schemes limit the complexity of the equations to simplify the 
hardware, while still making substantial speed improvements over ripple carry. 

One such scheme is a 
carry-lookahead adder
. In Chapter 1, we said computer 
systems cope with complexity by using levels of abstraction. A carry-lookahead 

adder relies on levels of abstraction in its implementation.
Let’s factor our original equation
 rst step:
c1bcacab
ababc
iiiiiii
iiiii
()()()
()()
=If we were to rewrite the equation for c2 using this formula, we would see some 

repeated patterns:
cababababc
2111100000
()()(()())
Note the repeated appearance of (a
i  bi) and (a
i  bi) in the formula above
 ese 
two important factors are traditionally called 
generate
 (gi) and 
propagate
 (pi):
B-40 Appendix B The Basics of Logic Design
gab
pab
iii
iii
Using them t
 ne c
i  1, we get
c1gpc
iiii
To see where the signals get their names, suppose g
i enc1gpc1pc1
iiiiii
 at is, the adder 
generates
 a CarryOut (c
i  1) independent of the value of Car-
ryIn (c
i). Now suppose that g
i is 0 and p
i encgpc1cc
iiiiii
10 at is, the adder 
propagate
s CarryIn to a CarryOut. Putting the two together, 
CarryIn
i  1 is a 1 if either g
i is 1 or both p
i is 1 and CarryIn
i is 1.As an analogy, imagine a row of dominoes set on edge
 e end domino can be 
tipped over by pushing one far away, provided there are no gaps between the two. 

Similarly, a carry out can be made true by a generate far away, provided all the 

propagates between them are true.
Relying on th
 nitions of propagate and generate as o
 rst level of 
abstraction, we can express the CarryIn signals more economically. Let’s show it 

for 4 bits:
cgpc
cgpgppc
cgpgpp
1000
2110100
322121
()()()
()(g
gpppc
cgpgppgpppg
02100
43323213210
)()
()()()
(pp3p2p1pc 
00)
 ese equations just represent common sense: CarryIn
i is a 1 if some earlier adder 
generates a carry and all intermediary adders propagate a carry. 
Figure B.6.1
 uses 
plumbing to try to explain carry lookahead.
Even this simp
 ed form leads to large equations and, hence, considerable logic 
even for a 16-bit adder. Let’s try moving to two levels of abstraction.
Fast Carry Using the Second Level of Abstraction
First, we consider this 4-bit adder with its carry-lookahead logic as a single building 

block. If we connect them in ripple carry fashion to form a 16-bit adder, the add 

will be faster than the original with a little more hardware.

 B.6 Faster Addition: Carry Lookahead 
B-41To go faster, we’ll need carry lookahead at a higher level. To perform carry look 
ahead for 4-bit adders, we need to propagate and generate signals at this higher 
level. Here they are for the four 4-bit adder blocks:
Ppppp
Ppppp
Ppppp
Pppp
03210
17654
2111098
3151413
p12
 at is, the “super” propagate signal for the 4-bit abstraction (P
i) is true only if each 
of the bits in the group will propagate a carry.
For the “super” generate signal (G
i), we care only if there is a carry out of the 
 cant bit of the 4-bit group.
 is obviously occurs if generate is true 
for tha
 cant bit; it also occurs if an earlier generate is true 
and
 all the 
intermediate propagates, including that of th
 cant bit, are also true:
Ggpgppgpppg
Ggpgpp
03323213210
177676
()()()
()(
gpppg
Ggpgppgpp
57654
2111110111091110
)()
()()(p
pgGgpgppgpppg
98315151415141315141312
)()()()
Figure B.6.2
 updates our plumbing analogy to show P0 and G0.
 en the equations at this higher level of abstraction for the carry in for each 
4-bit group of the 16-bit adder (C1, C2, C3, C4 in 
Figure B.6.3
) are very similar to 

the carry out equations for each bit of the 4-bit adder (c1, c2, c3, c4) on page B-40:
CGPc
CGPGPPc
CGPGPP
1000
2110100
322121
()()()
()(G
GPPPc
CGPGPPGPPPG 
02100
43323213210
)()
()()()
(()PPPPc
32100
Figure B.6.3
 shows 4-bit adders connected with such a carry-lookahead unit. 
 e exercises explore the sp
 erences between these carry sc
 erent 
notations for multibit propagate and generate signals, and the design of a 64-bit 
adder.

B-42 Appendix B The Basics of Logic Design
c4p3p2p1p0g3g2g1g0c0c2p1p0g1g0c0c1p0g0c0FIGURE B.6.1 A plumbing analogy for carry lookahead for 1 bit, 2 bits, and 4 bits using 
water pipes and valves.
 e wrenches are turned to open and close valves. Water is shown in color
 e output of the pipe (c
i  1) will be full if either the nearest generate value (g
i) is turned on or if the 
i propagate 
value (p
i) is on and there is water further upstream, either from an earlier generate or a propagate with water 
behind it. CarryIn (c0) can result in a carry out without the help of any generates, but with the help of 
all
 propagates.

 B.6 Faster Addition: Carry Lookahead 
B-43G0p3p2p1g3g2g1g0P0p3p2p1p0FIGURE B.6.2 A plumbing analogy for the next-level carry-lookahead signals P0 and G0.
 P0 is open only if all four propagates (p
i) are open, while wat
 ows in G0 only if at least one generate (g
i) is open and all the propagates downstream from that generate are open.

B-44 Appendix B The Basics of Logic Design
Both Levels of the Propagate and Generate
Determine the g
i, pi, Pi, and G
i values of these two 16-bit numbers:
a:    0001 1010 0011 0011twob:    1110 0101 1110 1011twoAlso, what is CarryOut15 (C4)?
Aligning the bits makes it easy to see the values of generate g
i (ai  bi) and 
propagate p
i (ai  bi):a:    0001 1010 0011 0011b:    1110 0101 1110 1011
gi:   0000 0000 0010 0011

pi:   1111 1111 1111 1011
where the bits are numbered 15 to 0 fro
  to right. Next, the “super” 
propagates (P3, P2, P1, P0) are simply the AND of the lower-level propagates:
P11111
P11111
P11111
P111
321000
 e “super” generates are more complex, so use the following equations:
Ggpgppgpppg
03323213210
0101011
()()()
()()(
=001100000
17767657654
0)()()()
Ggpgppgpppg
(
()()()
()(
10111111000101
2111110111
Ggpgpp0
09111098
010110111000000
gpppg
)()
()()()
G
Ggpgppgpppg
315151415141315141312
010
()()()
()(
()()
110111000000
Finally, CarryOut15 is
CGPGPPGPPPG
PPPPc
43323213210
32100
0()()()
()(
()()()()
1111111111
11000
00000
Hence, there 
is a carry out when adding these two 16-bit numbers.
EXAMPLEANSWER
 B.6 Faster Addition: Carry Lookahead 
B-45a4CarryIn
ALU1  P1  G1b4a5b5a6b6a7b7a0CarryIn
ALU0  P0
  G0b0Carry-lookahead unit
a1b1a2b2a3b3CarryIn
Result0Œ3pi
gici + 1pi + 1gi + 1C1Result4Œ7a8CarryIn
ALU2  P2
  G2b8a9b9a10b10a11b11ci + 2pi + 2gi + 2C2Result8Œ11a12CarryIn
ALU3  P3
  G3b12a13b13a14b14a15b15ci + 3pi + 3gi + 3C3Result12Œ15ci + 4C4CarryOut
FIGURE B.6.3 Four 4-bit ALUs using carry lookahead to form a 16-bit adder.
 Note that the 
carries come from the carry-lookahead unit, not from the 4-bit ALUs.

B-46 Appendix B The Basics of Logic Design
 e reason carry lookahead can make carries faster is that all logic begins 
evaluating the moment the clock cycle begins, and the result will not change once 
the output of each gate stops changing. By taking the shortcut of going through 

fewer gates to send the carry in signal, the output of the gates will stop changing 

sooner, and hence the time for the adder can be less.
To appreciate the importance of carry lookahead, we need to calculate the 
relative performance between it and ripple carry adders.
Speed of Ripple Carry versus Carry Lookahead
One simple way to model time for logic is to assume each AND or OR gate 

takes the same time for a signal to pass through it. Time is estimated by simply 

counting the number of gates along the path through a piece of logic. Compare 

the number of 
gate delays
 for paths of two 16-bit adders, one using ripple carry 
and one using two-level carry lookahead.
Figure B.5.5
 on page B-28 shows that the carry out signal takes two gate 
delays per bi
 en the number of gate delays between a carry in to the least 
 cant bit and the carry out of th cant is 16 
 2  32.For carry lookahead, the carry out of th
 cant bit is just C4, 
 ned in the example. It takes two levels of logic to specify C4 in terms of 
Pi and G
i (the OR of several AND terms). P
i is sp
 ed in one level of logic 
(AND) using p
i, and G
i is sp
 ed in two levels using p
i and g
i, so the worst 
case for this next level of abstraction is two levels of logic. p
i and g
i are each 
one level o
 ned in terms of a
i and b
i. If we assume one gate delay 
for each level of logic in these equations, the worst case is 2 
 2  1  5 gate 
delays.
Hence, for the path from carry in to carry out, the 16-bit addition by a 
carry-lookahead adder is six times faster, using this very simple estimate of 

hardware speed.
Summary
Carry lookahead o ers a faster path than waiting for the carries to ripple through 

all 32 1-bit adder
 is faster path is paved by two signals, generate and propagate. 
EXAMPLEANSWER
 B.6 Faster Addition: Carry Lookahead 
B-47 e former creates a carry regardless of the carry input, and the latter passes a carry 
along. Carry lookahead also gives another example of how abstraction is important 
in computer design to cope with complexity.
Using the simple estimate of hardware speed above with gate delays, what is the 
relative performance of a ripple carry 8-bit add versus a 64-bit add using carry-

lookahead logic?
1. A 64-bit carry-lookahead adder is three times faster: 8-bit adds are 16 gate 
delays and 64-bit adds are 7 gate delays.
 ey are about the same speed, since 64-bit adds need more levels of logic in 
the 16-bit adder.
3. 8-bit adds are faster than 64 bits, even with carry lookahead.
Elaboration: We have now accounted for all but one of the arithmetic and logical 
operations for the core MIPS instruction set: the ALU in 
Figure B.5.14
 omits support of 
shift instructions. It would be possible to widen the ALU multiplexor to include a left shift 

by 1 bit or a right shift by 1 bit. But hardware designers have created a circuit called a 

barrel shifter,
 which can shift from 1 to 31 bits in no more time than it takes to add two 

32-bit numbers, so shifting is normally done outside the ALU.
Elaboration: The logic equation for the Sum output of the full adder on page B-28 can 
be expressed more simply by using a more powerful gate than AND and OR. An 
exclusive 
OR gate is true if the two operands disagree; that is,
xy and xy
10In some technologies, cient than two levels of AND and OR 
gates. Using the symbol  to represent exclusive OR, here is the new equation:
SumabCarryIn
Also, we have drawn the ALU the traditional way, using gates. Computers are designed 
today in CMOS transistors, which are basically switches. CMOS ALU and barrel shifters 

take advantage of these switches and have many fewer multiplexors than shown in our 

designs, but the design principles are similar.
Elaboration: Using lowercase and uppercase to distinguish the hierarchy of generate 
and propagate symbols breaks down when you have more than two levels. An alternate 

notation that scales is gi..j and pi..j for the generate and propagate signals for bits i to j. Thus, g
1..1 is generated for bit 1, g
4..1 is for bits 4 to 1, and g
16..1 is for bits 16 to 1.Check Yourself

B-48 Appendix B The Basics of Logic Design
 B.7 ClocksBefore we discuss memory elements and se
quential logic, it is useful to discuss 
br
 y the topic of cloc
 is short section introduces the topic and is similar 
to the discussion found in Section 4.2. More details on clocking and timing 
methodologies are presented in Section B.11.
Clocks
 are needed in sequential logic to decide when an element that contains 
state should be updated. A clock is simply a free-running signal wit
 xed 
cycle 
time; the 
clock frequency
 is simply the inverse of the cycle time. As shown in 
Figure
 B.7.1
, the 
clock cycle time
 or 
clock period
 is divided into two portions: when the 
clock is high and when the clock is low. In this text, we use only 
edge-triggered 
clocking
 is means that all state changes occur on a clock edge. We use an edge-
triggered methodology because it is simpler to explain. Depending on the tech-

nology, it may or may not be the best choice for a 
clocking methodology
.edge-triggered 
clocking
 A clocking 
scheme in which all state 
changes occur on a clock 

edge.
clocking methodology
  e approach used to 
determine when data is 

valid and stable relative to 

the clock.
Clock period
Rising edgeFalling edge
FIGURE B.7.1 A clock signal oscillates between high and low values.
 e clock period is the 
time for one full cycle. In an edge-triggered design, either the rising or falling edge of the clock is active and 
causes state to be changed.
In an edge-triggered methodology, either the rising edge or the falling edge of 
the clock is 
active
 and causes state changes to occur. As we will see in the next 
section, the 
state elements
 in an edge-triggered design are implemented so that the 
contents of the state elements only change on the active clock edge
 e choice of 
which edge is active
 uenced by the implementation technology and does not 
 ect the concepts involved in designing the logic.
 e clock edge acts as a sampling signal, causing the value of the data input to a 
state element to be sampled and stored in the state element. Using an edge trigger 
means that the sampling process is essentially instantaneous, eliminating problems 

that could occur if signals were sampled at slightl
 erent times.
 e major constraint in a clocked system, also called a 
synchronous system
, is that the signals that are written into state elements must be 
valid
 when the active 
state element
 A memory element.
synchronous system
 A memory system that 
employs clocks and where 

data signals are read only 

when the clock indicates 

that the signal values are 

stable.

 B.7 Clocks B-49clock edge occurs. A signal is valid if it is
 stable (i.e., not changing), and the value 
will not change again until the inputs change. Since combinational circuits cannot 
have feedback, if the inputs to a combinational logic unit are not changed, the 

outputs will eventually become valid.
Figure B.7.2
 shows the relationship among the state elements and the 
combinational logic blocks in a sync
hronous, sequen
 e state 
elements, whose outputs change only a
 er the clock edge, provide valid inputs 
to the combinational logic block. To ensure that the values written into the state 

elements on the active clock edge are valid, the clock must have a long enough 

period so that all the signals in the comb
inational logic block stabilize, and then the 
clock edge samples those values for storage in the state elemen
 is constraint 
sets a lower bound on the length of the clock period, which must be long enough 

for all state element inputs to be valid.
In the rest of this appendix, as well as in Chapter 4, we usually omit the clock 
signal, since we are assuming that all state elements are updated on the same clock 

edge. Some state elements will be written on every clock edge, while others will be 

written only under certain conditions (such as a register being updated). In such 

cases, we will have an explicit write signal for that state elemen
 e write signal 
must still be gated with the clock so that 
the update occurs only on the clock edge if 
the write signal is active. We will see how this is done and used in the next section.
One other advantage of an edge-triggered methodology is that it is possible 
to have a state element that is used as both an input and output to the same 

combinational logic block, as shown in 
Figure B.7.3
. In practice, care must be 

taken to prevent races in such situations and to ensure that the clock period is long 

enough; this topic is discussed further in Section B.11.
Now that we have discussed how clocking is used to update state elements, we 
can discuss how to construct the state elements.
Stateelement1Stateelement2Combinational logicClock cycle
FIGURE B.7.2 The inputs to a combinational logic block come from a state element, and 
the outputs are written into a state element. e clock edge determines when the contents of the 
state elements are updated.

B-50 Appendix B The Basics of Logic Design
Elaboration: Occasionally, designer nd it useful to have a small number of state 
elements that change on the opposite clock edge from the majority of the state elements. Doing so requires extreme care, because such an approach has effects on both the 

inputs and the outputs of the state element. Why then would designers ever do this? 

Consider the case where the amount of combinational logic before and after a state 
element is small enough so that each could operate in one-half clock cycle, rather than 

the more usual full clock cycle. Then the state element can be written on the clock edge 
corresponding to a half clock cycle, since the inputs and outputs will both be usable 

after one-half clock cycle. One common place where this technique is used is in register ﬁ les, le can often be done in half the normal 
clock cycle. Chapter 4 makes use of this idea to reduce the pipelining overhead.
 B.8 Memory Elements: Flip-Flops, Latches, 
and Registers
In this section and the next, we discuss the basic principles behind memory 
elements, starting wit
 i
 ops and latches, moving on to regist
 les, and 
 nishing with memories. All memory elements store state: the output from any 
memory element depends both on the inputs and on the value that has been stored 

inside the memory elemen
 us all logic blocks containing a memory element 
contain state and are sequential.
register
 le A state 
element that consists 
of a set of registers that 

can be read and written 

by supplying a register 

number to be accessed.
StateelementCombinational logicFIGURE B.7.3 An edge-triggered methodology allows a state element to be read and 
written in the same clock cycle without creating a race that could lead to undetermined 
data values. Of course, the clock cycle must still be long enough so that the input values are stable when 

the active clock edge occurs.
RSQQFIGURE B.8.1 A pair of cross-coupled NOR gates can store an internal value.
 e value 
stored on the output 
Q is recycled by inverting it to obtain 
Q and then inverting 
Q to obtain 
Q. If either 
R or 
Q is asserted, 
Q will be deasserted and vice versa.

 B.8 Memory Elements: Flip-Flops, Latches, and Registers 
B-51 e simplest type of memory elements are 
unclocked
; that is, they do not 
have any clock input. Although we only use clocked memory elements in this 
text, an unclocked latch is the simplest memory element, so let’s look at this 

circui
 rst. 
Figure B.8.1
 shows an 
S-R latch
 (set-reset latch), built from a pair of 
NOR gates (OR gates with inverted outpu
 e outputs 
Q and 
Q represent the 
value of the stored state and its complement. When neither 
S nor 
R are asserted, 
the cross-coupled NOR gates act as inverters and store the previous values of 

Q and 
Q.For example, if the output, 
Q, is true, then the bottom inverter produces a false 
output (which is 
Q), which becomes the input to the top inverter, which produces 
a true output, which is 
Q, and so on. If 
S is asserted, then the output 
Q will be asserted and 
Q will be deasserted, while if 
R is asserted, then the output 
Q will be asserted and 
Q will be deasserted. When 
S and 
R are both deasserted, the last values 
of 
Q and 
Q will continue to be stored in the cross-coupled structure. Asserting 
S and 
R simultaneously can lead to incorrect operation: depending on how 
S and 
R are deasserted, the latch may oscillate or become metastable (this is described in 

more detail in Section B.11).
 is cross-coupled structure is the basis for more complex memory elements 
that allow us to store dat
 ese elements contain additional gates used to 
store signal values and to cause the state to be updated only in conjunction with a 

cloc
 e next section shows how these elements are built.
Flip-Flops and Latches ops
 and 
latches
 are the simplest memory elements. In both
 i ops and 
latches, the output is equal to the value of the stored state inside the element. 

Furthermore, unlike the S-R latch described above, all the latches an
 i ops we 
will use from this point on are clocked, which means that they have a clock input 

and the change of state is triggered by that cloc
 e 
 erence betw
 ip-
 op and a latch is the point at which the clock causes the state to actually change. 
In a clocked latch, the state is changed whenever the appropriate inputs change 

and the clock is asserted, wherea
 i
 op, the state is changed only on a clock 
edge. Since throughout this text we use an edge-triggered timing methodology 

where state is only updated on clock edges, we need only us
 i ops. Fli
 ops 
are
 en built from latches, so we start by describing the operation of a simple 
clocked latch and then discuss the operation o
 i
 op constructed from that 
latch.
For computer applications, the function of bot
 i
 ops and latches is to 
store a signal. A 
D latch
 or 
 op stores the value of its data input signal in 
the internal memory. Although there are many other types of latch an
 i
 op, 
the D type is the only basic building block that we will need. A D latch has two 

inputs and two outpu
 e inputs are the data value to be stored (called 
D) and 
a clock signal (called 
C) that indicates when the latch should read the value on 
the 
D input and store i
 e outputs are simply the value of the internal state (
Q) latch
 A memory element 
in which the output is 
equal to the value of the 

stored state inside the 

element and the state is 

changed whenever the 

appropriate inputs change 

and the clock is asserted.
 op A memory 
element for which the 

output is equal to the 

value of the stored state 

inside the element and for 

which the internal state is 

changed only on a clock 

edge.
 op A 
 i
 op 
with one data input 

that stores the value of 

that input signal in the 

internal memory when 

the clock edge occurs.

B-52 Appendix B The Basics of Logic Design
and its complement (
Q). When the clock input 
C is asserted, the latch is said to 
be 
open
, and the value of the output (
Q) becomes the value of the input 
D. When 
the clock input 
C is deasserted, the latch is said to be 
closed
, and the value of the 
output (
Q) is whatever value was stored the last time the latch was open.
Figure B.8.2
 shows how a D latch can be implemented with two additional gates 
added to the cross-coupled NOR gates. Since when the latch is open the value of 
Q changes as 
D changes, this structure is sometimes called a 
transparent latch
. Figure
 B.8.3
 shows how this D latch works, assuming that the output 
Q is initially false and 
that 
D chang
 rst.
As mentioned earlier, we us
 i
 ops as the basic building block, rather than 
latches. Fli
 ops are not transparent: their outputs change 
only on the clock edge. 
 i
 op can be built so that it triggers on either the rising (positive) or falling 
(negative) clock edge; for our designs we can use either type. 
Figure B.8.4
 shows 
how a falling-edg
 i
 op is constructed from a pair of D latches. In
 ip-
 op, the output is stored when the clock edge occurs. 
Figure B.8.5
 shows how this 
 i op operates.
CDQQFIGURE B.8.2 A D latch implemented with NOR gates. A NOR gate acts as an inverter if the other 
inpu
 us, the cross-coupled pair of NOR gates acts to store the state value unless the clock input, 
C, is asserted, in which case the value of input 
D replaces the value of 
Q and is stored
 e value of input 
D must 
be stable when the clock signal 
C changes from asserted to deasserted.
DCQFIGURE B.8.3 Operation of a D latch, assuming the output is initially deasserted.
 When 
the clock, 
C, is asserted, the latch is open and the 
Q output immediately assumes the value of the 
D input.

 B.8 Memory Elements: Flip-Flops, Latches, and Registers 
B-53DCDlatchDCQDlatchDCQQQQFIGURE B.8.4 A D ﬂ ip-ﬂ op with a falling-edge trigger.
 e 
 rst latch, called the master, is open 
and follows the input 
D when the clock input, 
C, is asserted. When the clock input, 
C, falls, th
 rst latch is 
closed, but the second latch, called the slave, is open and gets its input from the output of the master latch.
Here is a Verilog description of a module for a rising-edg
 i
 op, assuming 
that C is the clock input and D is the data input:
module DFF(clock,D,Q,Qbar); input clock, D;
 output reg Q; // Q is a reg since it is assigned in an 
always block output Qbar;
 assign Qbar = ~ Q; // Qbar is always just the inverse 

of Q
 always @(posedge clock) // perform actions whenever the 

clock rises Q = D;
endmoduleBecause the 
D input is sampled on the clock edge, it must be valid for a period 
of time immediately before and immediately a
 er the clock edge
 e minimum 
time that the input must be valid before the clock edge is called the 
setup time
; the 
DCQFIGURE B.8.5 Operation of a D ﬂ ip-ﬂ op with a falling-edge trigger, assuming the output is 
initially deasserted.
 When the clock input (
C) changes from asserted to deasserted, the 
Q output stores 
the value of the 
D input. Compare this behavior to that of the clocked D latch shown in 
Figure B.8.3
. In a 
clocked latch, the stored value and the output, 
Q, both change whenever 
C is high, as opposed to only when 
C transitions.
setup time
 e minimum time that the 
input to a memory device 

must be valid before the 

clock edge.

B-54 Appendix B The Basics of Logic Design
minimum time during which it must be valid a
 er the clock edge is called the 
hold 
time us the inputs to an
 i
 op (or anything built usin
 i
 ops) must be valid 
during a window that begins at time 
tsetup
 before the clock edge and ends at 
thold er 
the clock edge, as shown in 
Figure B.8.6
. Section B.11 talks about clocking and timing 
constraints, including the propagation delay throug
 i
 op, in more detail.
We can use an array o
 i
 ops to build a register that can hold a multibit 
datum, such as a byte or word. We used registers throughout our datapaths in 

Chapter 4.
Register Files
One structure that is central to our datapath is a 
regist
 le. A regist
 le consists 
of a set of registers that can be read and written by supplying a register number 

to be accessed. A regist
 le can be implemented with a decoder for each read 
or write port and an array of registers built fro
 i
 ops. Because reading a 
register does not change any state, we need only supply a register number as an 

input, and the only output will be the data contained in that register. For writing a 

register we will need three inputs: a register number, the data to write, and a clock 

that controls the writing into the register. In Chapter 4, we used a regist
 le that 
has two read ports and one write por
 is regist
 le is drawn as shown in 
Figure
 B.8.7
 e read ports can be implemented with a pair of multiplexors, each of which 
is as wide as the number of bits in each register of the regist
 le. 
Figure B.8.8
 shows the implementation of two register read ports for a 32-bit-wide regist
 le.
Implementing the write port is slightly more complex, since we can only change 
the contents of the designated register. We can do this by using a decoder to generate 

a signal that can be used to determine which register to write. 
Figure B.8.9
 shows 

how to implement the write port for a regist le. It is important to remember that 

th
 i
 op changes state only on the clock edge. In Chapter 4, we hooked up write 
signals for the regist
 le explicitly and assumed the clock shown in 
Figure B.8.9
 is attached implicitly.
What happens if the same register is read and written during a clock cycle? 
Because the write of the regist
 le occurs on the clock edge, the register will be 
DCSetup timeHold timeFIGURE B.8.6 Setup and hold time requirements for a D ﬂ ip-ﬂ op with a falling-edge trigger.
  e input must be stable for a period of time before the clock edge, as well as a
 er the clock edge
 e minimum time the signal must be stable before the clock edge is called the setup time, while the minimum 
time the signal must be stable a
 er the clock edge is called the hold time. Failure to meet these minimum 
requirements can result in a situation where the output of th
 i
 op may not be predictable, as described 
in Section B.11. Hold times are usually either 0 or very small and thus not a cause of worry.
hold time
 e minimum 
time during which the 
input must be valid a
 er the clock edge.

 B.8 Memory Elements: Flip-Flops, Latches, and Registers 
B-55Read registernumber 1
Read data 1Read register
number 2
Read data 2Write

registerWrite
Write

dataRegister fileFIGURE B.8.7 A register ﬁ le with two read ports and one write port has ﬁ ve inputs and 
two outputs. e control input Write is shown in color.
Read registernumber 1
Register 0Register 1. . .Register n Œ 2Register n Œ 1MuxRead registernumber 2
MuxRead data 1Read data 2FIGURE B.8.8 The implementation of two read ports for a register ﬁ
 le with 
n registers 
can be done with a pair of n-to-1 multiplexors, each 32 bits wide.
 e register read number 
signal is used as the multiplexor
 selector signal. 
Figure B.8.9
 shows how the write port is implemented.

B-56 Appendix B The Basics of Logic Design
valid during the time it is read, as we saw earlier in 
Figure B.7.2
 e value returned 
will be the value written in an earlier clock cycle. If we want a read to return the 
value currently being written, additional logic in the regist
 le or outside of it is 
needed. Chapter 4 makes extensive use of such logic.
Specifying Sequential Logic in Verilog
To specify sequential logic in Verilog, we must understand how to generate a 

clock, how to describe when a value is written into a register, and how to specify 

sequential control. Let us start by specifying a clock. A clock is not a pr
 ned 
object in Verilog; instead, we generate a clock by using the Verilog notation 
#n before a statement; this causes a delay of 
n simulation time steps before the execu-
tion of the statement. In most Verilog simulators, it is also possible to generate 

a clock as an external input, allowing the user to specify at simulation time the 

number of clock cycles during which to run a simulation.
 e code in 
Figure B.8.10
 implements a simple clock that is high or low for one 
simulation unit and then switches state. We use the delay capability and blocking 

assignment to implement the clock.
Write
01n-to-2ndecodern Œ 2n Œ 1Register 0CDRegister 1CDRegister n Œ 2CDRegister n Œ 1CD..
.Register number
.
.
.Register dataFIGURE B.8.9 The write port for a register ﬁ le is implemented with a decoder that is 
used with the write signal to generate the C input to the registers
. All three inputs (the register 
number, the data, and the write signal) will have setup and hold-time constraints that ensure that the correct 

data is written into the regist
 le.

 B.8 Memory Elements: Flip-Flops, Latches, and Registers 
B-57Next, we must be able to specify the operation of an edge-triggered register. In 
Verilog, this is done by using the sensitivity list on an 
always block and specifying 
as a trigger either the positive or negative edge of a binary variable with the 
notation 
posedge or 
negedge, respectively. Hence, the following Verilog code 
causes register 
A to be written with the value b at the positive edge clock:
FIGURE B.8.10 A speciﬁ cation of a clock.
FIGURE B.8.11 A MIPS register ﬁ le written in behavioral Verilog.
 is regist
 le writes on 
the rising clock edge.
 roughout this chapter and the Verilog sections of Chapter 4, we will assume 
a positive edge-triggered design. 
Figure B.8.11
 shows a Verilog sp
 cation of a 
MIPS regist
 le that assumes two reads and one write, with only the write being 
clocked.

B-58 Appendix B The Basics of Logic Design
In the Verilog for the regist
 le in 
Figure B.8.11
, the output ports corresponding to 
the registers being read are assigned using 
a continuous assignment, but the register 
being written is assigned in an 
always block. Which of the following is the reason?
 ere is no special reason. It was simply convenient.
b. Because Data1 and Data2 are output ports and WriteData is an input port.
c. Because reading is a combinational event, while writing is a sequential event.
 B.9 Memory Elements: SRAMs and DRAMs
Registers and regist
 les provide the basic building blocks for small memories, 
but larger amounts of memory are built using either 
SRAMs (static random 
access memories)
 or 
DRAMs
 (dynamic random access memories). We
 rst discuss 
SRAMs, which are somewhat simpler, and then turn to DRAMs.
SRAMsSRAMs are simply integrated circuits that are memory arrays with (usually) a single 
access port that can provide either a read or a write. SRAMs hav
 xed access 
time to any datum, though the read and write access characteristics o
 en 
 er. 
An SRAM chip has a sp
 c co
 guration in terms of the number of addressable 
locations, as well as the width of each addressable location. For example, a 4M 
 8 SRAM provides 4M entries, each of which is 8 bits wide
 us it will have 22 address 
lines (since 4M  222), an 8-bit data output line, and an 8-bit single data input line. 
As with ROMs, the number of addressable locations is o
 en called the 
height
, with 
the number of bits per unit called the 
width
. For a variety of technical reasons, the 
newest and fastest SRAMs are typically available in narrow co
 gurations: 
 1 and 
 4. Figure B.9.1
 shows the input and output signals for a 2M 
 16 SRAM.
Check Yourself
static random access 
memory (SRAM)
 A memory where data 
is stored statically (as 

 i ops) rather 
than dynamically (as 

in DRAM). SRAMs are 

faster than DRAMs, 

but less dense and more 

expensive per bit.
SRAM2M  16Dout[15Œ0]Address21Din[15Œ0]16Chip selectOutput enable
Write enable
16FIGURE B.9.1 A 32K  8 SRAM showing the 21 address lines (32K  215) and 16 data inputs, the 3 control lines, and the 16 data outputs.

 B.9 Memory Elements: SRAMs and DRAMs 
B-59To initiate a read or write access, the Chip select signal must be made active. 
For reads, we must also activate the Output enable signal that controls whether or 
not the datum selected by the address is actually driven on the pin
 e Output 
enable is useful for connecting multiple memories to a single-output bus and using 

Output enable to determine which memory drives the bu
 e SRAM read access 
time is usually sp
 ed as the delay from the time that Output enable is true and 
the address lines are valid until the time that the data is on the output lines. Typical 

read access times for SRAMs in 2004 varied from about 2–4 ns for the fastest CMOS 

parts, which tend to be somewhat smaller and narrower, to 8–20 ns for the typical 

largest parts, which in 2004 had more than 32 million bits of dat
 e demand for 
low-power SRAMs for consumer products and digital appliances has grown greatly 

in th
 ve years; these SRAMs have much lower stand-by and access power, 
but usually are 5–10 times slower. Most recently, synchronous SRAMs—similar to 

the synchronous DRAMs, which we discuss in the next section—have also been 

developed.
For writes, we must supply the data to be written and the address, as well as 
signals to cause the write to occur. When both the Write enable and Chip select are 

true, the data on the data input lines is written into the cell sp
 ed by the address. 
 ere are setup-time and hold-time requirements for the address and data lines, 
just as there were fo
 i
 ops and latches. In addition, the Write enable signal 
is not a clock edge but a pulse with a minimum width requiremen
 e time to 
complete a write is sp ed by the combination of the setup times, the hold times, 

and the Write enable pulse width.
Large SRAMs cannot be built in the same way we build a regist
 le because, 
unlike a regist
 le where a 32-to-1 multiplexor might be practical, the 64K-to-
1 multiplexor that would be needed for a 64K 
 1 SRAM is totally impractical. 
Rather than use a giant multiplexor, large memories are implemented with a shared 

output line, called a 
bit line
, which multiple memory cells in the memory array can 
assert. To allow multiple sources to drive a single line, a 
three-state
 er (or 
tristate 
 er) is used. A three-state b
 er has two inputs—a data signal and an Output 
enable—and a single output, which is in one of three states: asserted, deasserted, 

or high impedance
 e output of a tristate b
 er is equal to the data input signal, 
either asserted or deasserted, if the Output enable is asserted, and is otherwise in a 

high-impedance state
 that allows another three-state b
 er whose Output enable is 
asserted to determine the value of a shared output.
Figure B.9.2
 shows a set of three-state bu
 ers wired to form a multiplexor with a 
decoded input. It is critical that the Output enable of at most one of the three-state 

bu
 ers be asserted; otherwise, the three-state bu
 ers may try to set the output line 
 erently. By using three-state bu
 ers in the individual cells of the SRAM, each 
cell that corresponds to a particular output can share the same output line
 e use 
of a set of distributed three-state bu
 ers is a more
  cient implementation than a 
large centralized multiplexor
 e three-state bu
 ers are incorporated into th
 ip-
 ops that form the basic cells of the SRAM. 
Figure B.9.3
 shows how a small 4 
 2 SRAM might be built, using D latches with an input called Enable that controls the 

three-state output.

B-60 Appendix B The Basics of Logic Design
 e design in 
Figure B.9.3
 eliminates the need for an enormous multiplexor; 
however, it still requires a very large decoder and a correspondingly large number 
of word lines. For example, in a 4M 
 8 SRAM, we would need a 22-to-4M decoder 
and 4M word lines (which are the lines used to enable the individu
 i ops)! 
To circumvent this problem, large memories are organized as rectangular arrays 

and use a two-step decoding process. 
Figure B.9.4
 shows how a 4M 
 8 SRAM 
might be organized internally using a two-step decode. As we will see, the two-level 

decoding process is quite important in understanding how DRAMs operate.
Recently we have seen the development of both synchronous SRAMs (SSRAMs) 
and synchronous DRAMs (SDR
 e key capability provided by synchronous 
RAMs is the ability to transfer a 
burst
 of data from a series of sequential addresses 
within an array or row.
 e burs
 ned by a starting address, supplied in the 
usual fashion, and a burst lengt
 e speed advantage of synchronous RAMs 
comes from the ability to transfer the bits in the burst without having to specify 

additional address bits. Instead, a clock is used to transfer the successive bits in the 

burs e elimination of the need to specify the address for the transfers within 

the burs
 cantly improves the rate for transferring the block of data. Because 
of this capability, synchronous SRAMs and DRAMs are rapidly becoming the 

RAMs of choice for building memory systems in computers. We discuss the use of 

synchronous DRAMs in a memory system in more detail in the next section and 

in Chapter 5.
Select 0Data 0Enable
OutInSelect 1Data 1Enable
OutInSelect 2Data 2Enable
OutInSelect 3Data 3Enable
OutInOutputFIGURE B.9.2 Four three-state buffers are used to form a multiplexor.
 Only one of the four 
Select inputs can be asserted. A three-state b
 er with a deasserted Output enable has a high-impedance 
output that allows a three-state b
 er whose Output enable is asserted to drive the shared output line.

 B.9 Memory Elements: SRAMs and DRAMs 
B-61latchDCEnable
QD02-to-4decoderWrite enable
Din[1]latchDCEnable
QDDin[1]Dout[1]Dout[0]latchDCEnable
QD1latchDCEnable
QDlatchDCEnable
QD2latchDCEnable
QDlatchDCEnable
QD3latchDCEnable
QDAddressFIGURE B.9.3 The basic structure of a 4 
 2 SRAM consists of a decoder that selects which pair of cells to activate.  e activated cells use a three-state output connected to 
the vertical bit lines that supply the requested dat
 e address that selects the cell is 
sent on one of a set of horizontal address lines, called word 
lines. For simplicity, the Output enable and Chip select signals 
have been omitted, 
but they could easily be added with a few AND gates.

B-62 Appendix B The Basics of Logic Design
12to4096decoderAddress[21Œ10]40964K 1024SRAM4K 1024SRAM4K 1024SRAM4K 1024SRAM4K 1024SRAM4K 1024SRAM4K 1024SRAM4K 1024SRAMMuxDout7MuxDout6MuxDout5MuxDout4MuxDout3MuxDout2MuxDout1MuxDout01024Address[9Œ0]FIGURE B.9.4 Typical organization of a 4M 
 8 SRAM as an array of 4K 
 1024 arrays.
 e 
 rst decoder generates the 
addresses for eight 4K 
 1024 arrays; then a set of multiplexors is used to select 1 bit from each 1024-bit-wide array
 is is a much easier 
design than a single-level decode that would need either an enormo
us decoder or a gigantic multiplexor. In practice, a modern S
RAM of this 
size would probably use an even larger number of blocks, each somewhat smaller.

 B.9 Memory Elements: SRAMs and DRAMs 
B-63DRAMsIn a static RAM (SRAM), the value stored in a cell is kept on a pair of inverting gates, 
and as long as power is applied, the value can be kep
 nitely. In a dynamic 
RAM (DRAM), the value kept in a cell is stored as a charge in a capacitor. A single 

transistor is then used to access this stored charge, either to read the value or to 

overwrite the charge stored there. Because DRAMs use only a single transistor per 

bit of storage, they are much denser and cheaper per bit. By comparison, SRAMs 

require four to six transistors per bit. Because DRAMs store the charge on a 

capacitor, it cannot be kept
 nitely and must periodically be 
refreshed
 at is 
why this memory structure is called 
dynamic
, as opposed to the static storage in a 
SRAM cell.
To refresh the cell, we merely read its contents and write it bac
 e charge can 
be kept for several milliseconds, which might correspond to close to a million clock 

cycles. Today, single-chip memory controllers o
 en handle the refresh function 
independently of the processor. If every bit had to be read out of the DRAM and 

then written back individually, with large DRAMs containing multiple megabytes, 

we would constantly be refreshing the DRAM, leaving no time for accessing it. 

Fortunately, DRAMs also use a two-level decoding structure, and this allows us 

to refresh an entire row (which shares a word line) with a read cycle followed 

immediately by a write cycle. Typically, refresh operations consume 1% to 2% of 

the active cycles of the DRAM, leaving the remaining 98% to 99% of the cycles 

available for reading and writing data.
Elaboration: How does a DRAM read and write the signal stored in a cell? The 
transistor inside the cell is a switch, called a 
pass transistor, that allows the value stored 
on the capacitor to be accessed for either reading or writing. 
Figure B.9.5
 shows how 
the single-transistor cell looks. The pass transistor acts like a switch: when the signal 

on the word line is asserted, the switch is closed, connecting the capacitor to the bit 

line. If the operation is a write, then the value to be written is placed on the bit line. If 

the value is a 1, the capacitor will be charged. If the value is a 0, then the capacitor will 

be discharged. Reading is slightly more complex, since the DRAM must detect a very 

small charge stored in the capacitor. Before activating the word line for a read, the bit 

line is charged to the voltage that is halfway between the low and high voltage. Then, by 

activating the word line, the charge on the capacitor is read out onto the bit line. This 

causes the bit line to move slightly toward the high or low direction, and this change is 

 er, which can detect small changes in voltage.

B-64 Appendix B The Basics of Logic Design
Word line
Pass transistor
CapacitorBit lineFIGURE B.9.5 A single-transistor DRAM cell contains a capacitor that stores the cell contents and a transistor used to access the cell.Address[10Œ0]Row
decoder11-to-20482048  2048array
Column latchesMuxDoutFIGURE B.9.6 A 4M  1 DRAM is built with a 2048  2048 array.
 e row access uses 11 bits to 
select a row, which is then latched in 2048 1-bit latches. A multiplexor chooses the output bit from these 2048 
latch
 e RAS and CAS signals control whether the address lines are sent to the row decoder or column 
multiplexor.

 B.9 Memory Elements: SRAMs and DRAMs 
B-65DRAMs use a two-level decoder consisting of a 
row access
 followed by a 
column 
access, as shown in 
Figure B.9.6
 e row access chooses one of a number of rows 
and activates the corresponding word line
 e contents of all the columns in the 
active row are then stored in a set of latch
 e column access then selects the 
data from the column latches. To save pins and reduce the package cost, the same 
address lines are used for both the row and column address; a pair of signals called 

RAS (Row Access Strobe
) and CAS (
Column Access Strobe
) are used to signal the 
DRAM that either a row or column address is being supplied. Refresh is performed 

by simply reading the columns into the column latches and then writing the same 

values bac
 us, an entire row is refreshed in one cycle
 e two-level addressing 
scheme, combined with the internal circuitry, makes DRAM access times much 

longer (by a factor of 5–10) than SRAM access times. In 2004, typical DRAM access 

times ranged from 45 to 65 ns; 256 Mbit DRAMs are in full production, and the 

 rst customer samples of 1 GB DRAMs became available in th
 rst quarter of 
 e much lower cost per bit makes DRAM the choice for main memory, 
while the faster access time makes SRAM the choice for caches.
You might observe that a 64M 
 4 DRAM actually accesses 8K bits on every 
row access and then throws away all but 4 of those during a column access. DRAM 

designers have used the internal structure of the DRAM as a way to provide 

higher bandwidth out of a DR
 is is done by allowing the column address to 
change without changing the row address, resulting in an access to other bits in the 

column latches. To make this process faster and more precise, the address inputs 

were clocked, leading to the dominant form of DRAM in use today: synchronous 

DRAM or SDRAM.
Since about 1999, SDRAMs have been the memory chip of choice for most 
cache-based main memory systems. SDRAMs provide fast access to a series of bits 

within a row by sequentially transferring all the bits in a burst under the control 

of a clock signal. In 2004, DDRRAMs (Double Data Rate RAMs), which are called 

double data rate because they transfer data on both the rising and falling edge of 

an externally supplied clock, were the most heavily used form of SDRAMs. As we 

discuss in Chapter 5, these high-speed transfers can be used to boost the bandwidth 

available out of main memory to match the needs of the processor and caches.
Error Correction
Because of the potential for data corruption in large memories, most computer 

systems use some sort of error-checking code to detect possible corruption of data. 

One simple code that is heavily used is a 
parity code
. In a parity code the number 
of 1s in a word is counted; the word has odd parity if the number of 1s is odd and 

B-66 Appendix B The Basics of Logic Design
even otherwise. When a word is written into memory, the parity bit is also written 
(1 for odd, 0 for ev
 en, when the word is read out, the parity bit is read and 
checked. If the parity of the memory word and the stored parity bit do not match, 

an error has occurred.
A 1-bit parity scheme can detect at most 1 bit of error in a data item; if there 
are 2 bits of error, then a 1-bit parity scheme will not detect any errors, since the 

parity will match the data with two errors. (Actually, a 1-bit parity scheme can 

detect any odd number of errors; however, the probability of having three errors is 

much lower than the probability of having two, so, in practice, a 1-bit parity code is 

limited to detecting a single bit of error.) Of course, a parity code cannot tell which 

bit in a data item is in error.
A 1-bit parity scheme is an 
error detection code
; there are also 
error correction 
codes (ECC) that will detect and allow correction of an error. For large main 
memories, many systems use a code that allows the detection of up to 2 bits of error 

and the correction of a single bit of error.
 ese codes work by using more bits to 
encode the data; for example, the typical codes used for main memories require 7 

or 8 bits for every 128 bits of data.
Elaboration: A 1-bit parity code is a 
distance-2 code, which means that if we look 
at the data plus the parity bit, cient to generate another legal 
combination of the data plus parity. For example, if we change a bit in the data, the parity 

will be wrong, and vice versa. Of course, if we change 2 bits (any 2 data bits or 1 data 

bit and the parity bit), the parity will match the data and the error cannot be detected. 

Hence, there is a distance of two between legal combinations of parity and data.
To detect more than one error or correct an error, we need a 
distance-3 code, which 
has the property that any legal combination of the bits in the error correction code and 

the data has at least 3 bits differing from any other combination. Suppose we have such 

a code and we have one error in the data. In that case, the code plus data will be one bit 

away from a legal combination, and we can correct the data to that legal combination. 

If we have two errors, we can recognize that there is an error, but we cannot correct 

the errors. Let’s look at an example. Here are the data words and a distance-3 error 

correction code for a 4-bit data item.
Data Word
Code bitsDataCode bits0000000100011100010111001100001010110100100011110101100101001101100001010110111010100110011111010001110001111111error detection code
 A code that enables the 
detection of an error in 

data, but not the precise 

location and, hence, 

correction of the error.

 B.10 Finite-State 
Machines B-67To see how this works, let’s choose a data word, say 0110, whose error correction 
code is 011. Here are the four 1-bit error possibilities for this data: 1110, 0010, 0100, 
and 0111. Now look at the data item with the same code (011), which is the entry with 

the value 0001. If the error correction decoder received one of the four possible data 

words with an error, it would have to choose between correcting to 0110 or 0001. While 

these four words with error have only one bit changed from the correct pattern of 0110, 

they each have two bits that are different from the alternate correction of 0001. Hence, 

the error correction mechanism can easily choose to correct to 0110, since a single 

error is a much higher probability. To see that two errors can be detected, simply notice 

that all the combinations with two bits changed have a different code. The one reuse of 

the same code is with three bits different, but if we correct a 2-bit error, we will correct 

to the wrong value, since the decoder will assume that only a single error has occurred. 

If we want to correct 1-bit errors and detect, but not erroneously correct, 2-bit errors, we 

need a distance-4 code.Although we distinguished between the code and data in our explanation, in truth, 
an error correction code treats the combination of code and data as a single word in 

a larger code (7 bits in this example). Thus, it deals with errors in the code bits in the 

same fashion as errors in the data bits.
While the above example requires 
n  1 bits for n bits of data, the number of bits 
required grows slowly, so that for a distance-3 code, a 64-bit word needs 7 bits and a 

128-bit word needs 8. This type of code is called a Hamming code, after R. Hamming, 

who described a method for creating such codes. B.10 Finite-State Machines
As we saw earlier, digital logic systems can be cl ed as combinational or 
sequential. Sequential systems contain state stored in memory elements internal to 

the syst
 eir behavior depends both on the set of inputs supplied and on the 
contents of the internal memory, or state of the syst
 us, a sequential system 
cannot be described with a truth table. Instead, a sequential system is described as 

a  nite-state machine
 (or o
 en just 
state machine
 nite-state machine has a set 
of states and two functions, called the 
next-state function
 and the 
output function
.  e set of states corresponds to all the possible values of the internal storage. 
 us, if there are 
n bits of storage, there are 2
n stat
 e next-state function is a 
combinational function that, given the inputs and the current state, determines the 

next state of the syst
 e output function produces a set of outputs from the 
current state and the inputs. 
Figure B.10.1
 shows this diagrammatically.
 e state machines we discuss here and in Chapter 4 are 
synchronous
 is means 
that the state changes together with the clock cycle, and a new state is computed 

once every cloc
 us, the state elements are updated only on the clock edge. We 
use this methodology in this section and throughout Chapter 4, and we do not 
 nite-state machine
 A sequential logic 
function consisting of a 

set of inputs and out puts, 

a next-state function that 

maps the current state and 

the inputs to a new state, 

and an output function 

that maps the current 

state and possibly the 

inputs to a set of asserted 

outputs.
next-state function
 A combinational function 

that, given the inputs 

and the current state, 

determines the next state 

o
 nite-state machine.

B-68 Appendix B The Basics of Logic Design
usually show the clock explicitly. We use state machines throughout Chapter 4 to 
control the execution of the processor and the actions of the datapath.
To illustrate ho
 nite-state machine operates and is designed, let’s look at a 
simple and classic example: controlling a tra
  c light. (Chapters 4 and 5 contain more 
detailed examples of usin
 nite-state machines to control processor execution.) When 
 nite-state machine is used as a controller, the output function is o
 en restricted to 
depend on just the current state. Such
 nite-state machine is called a 
Moore machine
.  is is the type of
 nite-state machine we use throughout this book. If the output 
function can depend on both the current state and the current input, the machine 

is called a 
Mealy machine
 ese two machines are equivalent in their capabilities, 
and one can be turned into the other mechanically.
 e basic advantage of a Moore 
machine is that it can be faster, while a Mealy machine may be smaller, since it may 

need fewer states than a Moore machine. In Chapter 5, we discuss th
 erences in 
more detail and show a Verilog version of
 nite-state control using a Mealy machine.
Our example concerns the control of a tra
  c light at an intersection of a north-
south route and an east-west route. For simplicity, we will consider only the green 

and red lights; adding the yellow ligh
  for an exercise. We want the lights to 
cycle no faster than 30 seconds in each direction, so we will use a 0.033 Hz clock 

so that the machine cycles between states at no faster than once every 30 seconds. 

 ere are two output signals:
InputsCurrent stateOutputsClock
Next-state
functionOutputfunctionNext
stateFIGURE B.10.1 A state machine consists of internal storage that contains the state and 
two combinational functions: the next-state function and the output function. en, the 
output function is restricted to take only the current state as its input; this does not change the capability of 
a sequential machine, but does a
 ect its internals.

 B.10 Finite-State 
Machines B-69 NSlite:
 When this signal is asserted, the light on the north-south road is 
green; when this signal is deasserted, the light on the north-south road is red.
 EWlite:
 When this signal is asserted, the light on the east-west road is green; 
when this signal is deasserted, the light on the east-west road is red.
In addition, there are two inputs:
 NScar:
 Indicates that a car is over the detector placed in the roadbed in front 

of the light on the north-south road (going north or south).
 EWcar:
 Indicates that a car is over the detector placed in the roadbed in front 
of the light on the east-west road (going east or west).
 e tra
  c light should change from one direction to the other only if a car is 
waiting to go in the other direction; otherwise, the light should continue to show 

green in the same direction as the last car that crossed the intersection.
To implement this simple tra
  c light we need two states:
 NSgreen:
 e tra
  c light is green in the north-south direction.
 EWgreen:
 e tra
  c light is green in the east-west direction.
We also need to create the next-state function, which can be sp
 ed with a table:
 InputsNScarEWcarNext state
NSgreen00NSgreenNSgreen01EWgreenNSgreen10NSgreenNSgreen11EWgreenEWgreen00EWgreenEWgreen01EWgreenEWgreen10NSgreenEWgreen11NSgreenNotice that we didn’t specify in the algorithm what happens when a car 
approaches from both directions. In this case, the next-state function given above 

changes the state to ensure that a steady stream of cars from one direction cannot 

lock out a car in the other direction.
 e 
 nite-state machine is completed by specifying the output function.
Before we examine how to implement th
 nite-state machine, let’s look at a 
graphical representation, which is o
 en used fo
 nite-state machines. In this 
representation, nodes are used to indicate states. Inside the node we place a list of 

the outputs that are active for that state. Directed arcs are used to show the next-state

B-70 Appendix B The Basics of Logic Design
wOutputsNSliteEWlite
NSgreen10EWgreen01
function, with labels on the arcs specifying the input condition as logic functions. 
Figure B.10.2
 shows the graphical representation for t
 nite-state machine.
NSliteEWliteNScarNSgreen
EWgreen
EWcarEWcarNScarFIGURE B.10.2 The graphical representation of the two-state trafﬁ c light controller.
 We 
simp
 ed the logic functions on the state transitions. For example, the transition from NSgreen to EWgreen 
in the next-state table is 
()()
NScarEWcarNScarEWcar
, which is equivalent to EWcar.
 nite-state machine can be implemented with a register to hold the current 
state and a block of combinational logic that computes the next-state function and 

the output function. 
Figure B.10.3
 shows ho
 nite-state machine with 4 bits of 
state, and thus up to 16 states, might look. To implement th
 nite-state machine 
in this way, we mu
 rst assign state numbers to the stat
 is process is called 
state assignment
. For example, we could assign NSgreen to state 0 and EWgreen to 
stat
 e state register would contain a single bi
 e next-state function would 
be given as
NextStateCurrentStateEWca
rCurrentStateNScar
()()

 B.11 Timing 
Methodologies B-71where CurrentState is the contents of the state register (0 or 1) and NextState is the 
output of the next-state function that will be written into the state register at the 

end of the clock cycle
 e output function is also simple:
NSliteCurrentState
EWlite
CurrentState
 e combinational logic block is o
 en implemented using structured logic, 
such as a PLA. A PLA can be constructed automatically from the next-state and 
output function tables. In fact, there are 
computer-aided design
 (CAD) programs 
Combinational logicOutputsState registerInputsNext state
FIGURE B.10.3 A ﬁ nite-state machine is implemented with a state register that holds 
the current state and a combinational logic block to compute the next state and output 
functions. e latter two functions are o
 en split apart and implemented with two separate blocks of logic, 
which may require fewer gates.
that take either a graphical or textual representation of
 nite-state machine and 
produce an optimized implementation automatically. In Chapters 4 an
 nite-
state machines were used to control processor execution. 
 Appendix D
 discusses 
the detailed implementation of these controllers with both PLAs and ROMs.
To show how we might write the control in Verilog, 
Figure B.10.4
 shows a 
Verilog version designed for synthesis. Note that for this simple control function, 
a Mealy machine is not useful, but this style of sp
 cation is used in Chapter 5 to 
implement a control function that is a Mealy machine and has fewer states than the 

Moore machine controller.

B-72 Appendix B The Basics of Logic Design
What is the smallest number of states in a Moore machine for which a Mealy 
machine could have fewer states?
a. Two, since there could be a one-state Mealy machine that might do the same 
thing.
b.
 ree, since there could be a simple Moore machine that went to one of two 
 erent states and always returned to the original state a
 er that. For such a 
simple machine, a two-state Mealy machine is possible.
c. You need at least four states to exploit the advantages of a Mealy machine 
over a Moore machine.
 B.11 Timing Methodologies
 roughout this appendix and in the rest of the text, we use an edge-triggered 
timing methodology
 is timing methodology has an advantage in that it is 
simpler to explain and understand than a level-triggered methodology. In this 

section, we explain this timing methodology in a little more detail and also 

introduce level-sensitive clocking. We conclude this section by br
 y discussing 
Check Yourself
FIGURE B.10.4 A Verilog version of the trafﬁ c light controller.

 B.11 Timing 
Methodologies B-73the issue of asynchronous signals and synchronizers, an important problem for 
digital designers.
 e purpose of this section is to introduce the major concepts in clocking 
methodology.
 e section makes some important simplifying assumptions; if you 
are interested in understanding timing methodology in more detail, consult one of 

the references listed at the end of this appendix.
We use an edge-triggered timing methodology because it is simpler to explain 
and has fewer rules required for correctness. In particular, if we assume that all 

clocks arrive at the same time, we are guaranteed that a system with edge-triggered 

registers between blocks of combinational logic can operate correctly without races 

if we simply make the clock long enough. A 
race
 occurs when the contents of a 
state element depend on the relative speed o
 erent logic elements. In an edge-
triggered design, the clock cycle must be long enough to accommodate the path 

from one
 i
 op through the combinational logic to anot
 i
 op where it 
must satisfy the setup-time requirement. 
Figure B.11.1
 shows this requirement for 

a system using rising edge-trigger
 i
 ops. In such a system the clock period 
(or cycle time) must be at least as large as
ttt
propcombinationalsetup
for the worst-case values of these three delays, which ar
 ned as follows:
 tprop is the time for a signal to propagate throug
 i
 op; it is also sometimes 
called clock-to-
Q. tcombinational
 is the longest delay for any combinational logic (which b
 nition 
is surrounded by tw
 i ops).
 tsetup
 is the time before the rising clock edge that the input t
 i
 op must 
be valid.
Flip-flopDCQCombinationallogic block
Flip-flopDCQtproptcombinationaltsetupFIGURE B.11.1 In an edge-triggered design, the clock must be long enough to allow 
signals to be valid for the required setup time before the next clock edge. e time for a 
 i
 op input to propagate to th
 i
 ip outputs is 
tprop; the signal then takes 
tcombinational
 to travel through the 
combinational logic and must be valid 
tsetup
 before the next clock edge.

B-74 Appendix B The Basics of Logic Design
We make one simplifying assumption: the hold-time requirements are sa
 ed, 
which is almost never an issue with modern logic.
One additional complication that must be considered in edge-triggered designs 
is clock skew
. Clock skew is th
 erence in absolute time between when two state 
elements see a clock edge. Clock skew arises because the clock signal will o
 en use tw
 erent paths, with slightly
 erent delays, to reach tw
 erent state 
elements. If the clock skew is large enough, it may be possible for a state element to 
change and cause the input to anot
 i
 op to change before the clock edge is 
seen by the seco
 i
 op.
Figure B.11.2
 illustrates this problem, ignoring setup time an
 i
 op 
propagation delay. To avoid incorrect operation, the clock period is increased to 

allow for the maximum clock skew
 us, the clock period must be longer than
tttt
propcombinationalsetupskew

With this constraint on the clock period, the two clocks can also arrive in the 

opposite order, with the second clock arriving 
tskew
 earlier, and the circuit will work 
clock skew
 e  erence in absolute time 
between the times when 
two state elements see a 

clock edge.
Flip-flopDCQCombinationallogic block with
delay time of 
Flip-flopDCQClock arrives
at time tClock arrives
after t + FIGURE B.11.2 Illustration of how clock skew can cause a race, leading to incorrect operation.
 Because of th
 erence 
in when the tw
 i
 ops see the clock, the signal that is stored into th
 rst 
 i
 op can race forward and change the input to the seco
 ip-
 op before the clock arrives at the seco
 i
 op.
correctly. Designers reduce clock-skew problems by carefully routing the clock 
signal to minimize th
 erence in arrival times. In addition, smart designers also 
provide some margin by making the clock a little longer than the minimum; this 

allows for variation in components as well as in the power supply. Since clock skew 

can also a
 ect the hold-time requirements, minimizing the size of the clock skew 
is important.
Edge-triggered designs have two drawbacks: they require extra logic and they 
may sometimes be slower. Just looking at th
 i
 op versus the level-sensitive 
latch that we used to construct th
 i
 op shows that edge-triggered design 
requires more logic. An alternative is to use 
level-sensitive clocking
. Because state 
changes in a level-sensitive methodology are not instantaneous, a level-sensitive 
scheme is slightly more complex and requires additional care to make it operate 

correctly.
level-sensitive 
clocking
 A timing 
methodology in which 
state changes occur 

at either high or low 

clock levels but are not 

instantaneous as such 

changes are in edge-

triggered designs.

 B.11 Timing 
Methodologies B-75Level-Sensitive Timing
In level-sensitive timing, the state changes occur at either high or low levels, but 
they are not instantaneous as they are in an edge-triggered methodology. Because of 

the noninstantaneous change in state, races can easily occur. To ensure that a level-

sensitive design will also work correctly if the clock is slow enough, designers use 
two-phase clocking
. Two-phase clocking is a scheme that makes use of two nonoverlapping 
clock signals. Since the two clocks, typically called 
1 and 
2, are nonoverlapping, at 
most one of the clock signals is high at any given time, as 
Figure B.11.3
 shows. We 

can use these two clocks to build a system that contains level-sensitive latches but is 

free from any race conditions, just as the edge-triggered designs were.
Nonoverlapping
periods
12FIGURE B.11.3 A two-phase clocking scheme showing the cycle of each clock and the nonoverlapping periods.
LatchDCQCombinationallogic block
1LatchDCQCombinationallogic block
2LatchDC1FIGURE B.11.4 A two-phase timing scheme with alternating latches showing how the system operates on both clock 
phases. e output of a latch is stable on the opposite phase from its C inpu
 us, th
 rst block of combinational inputs has a stable input 
during 
2, and its output is latched by 
2 e second (rightmost) combinational block operates in just the opposite fashion, with stable inputs 
during 
1 us, the delays through the combinational blocks determine the minimum time that the respective clocks must be asserted
 e size of the nonoverlapping period is determined by the maximum clock skew and the minimum delay of any logic block.
One simple way to design such a system is to alternate the use of latches that are 
open on 
1 with latches that are open on 
2. Because both clocks are not asserted 
at the same time, a race cannot occur. If the input to a combinational block is a 
1 clock, then its output is latched by a 
2 clock, which is open only during 
2 when 
the input latch is closed and hence has a valid output. 
Figure B.11.4
 shows how 

a system with two-phase timing and alternating latches operates. As in an edge-

triggered design, we must pay attention to clock skew, particularly between the two 

B-76 Appendix B The Basics of Logic Design
clock phases. By increasing the amount of nonoverlap between the two phases, we 
can reduce the potential margin of error
 us, the system is guaranteed to operate 
correctly if each phase is long enough and if there is large enough nonoverlap 

between the phases.
Asynchronous Inputs and Synchronizers
By using a single clock or a two-phase clock, we can eliminate race conditions 

if clock-skew problems are avoided. Unfortunately, it is impractical to make an 

entire system function with a single clock and still keep the clock skew small. 

While the CPU may use a single clock, I/O devices will probably have their own 

clock. An asynchronous device may communicate with the CPU through a series 

of handshaking steps. To translate the asyn
chronous input to a synchronous signal 
that can be used to change the state of a system, we need to use a 
synchronizer
, whose inputs are the asynchronous signal an
d a clock and whose output is a signal 
synchronous with the input clock.
 rst attempt to build a synchronizer uses an edge-trigger
 i
 op, 
whose 
D input is the asynchronous signal, as 
Figure B.11.5
 shows. Because we 
communicate with a handshaking protocol, it does not matter whether we detect 

the asserted state of the asynchronous signal on one clock or the next, since the 

signal will be held asserted until it is acknowledged
 us, you might think that this 
simple structure is enough to sample the signal accurately, which would be the case 

except for one small problem.
Flip-flopDCQClock
Asynchronous inputSynchronous outputFIGURE B.11.5 A synchronizer built from a D ﬂ ip-ﬂ op is used to sample an asynchronous signal to produce an output that is synchronous with the clock. is “synchronizer” will 
not work properly!
 e problem is a situation called 
metastability
. Suppose the asynchronous 
signal is transitioning between high and low when the clock edge arrives. Clearly, 
it is not possible to know whether the signal will be latched as high or low
 at 
problem we could live with. Unfortunately, the situation is worse: when the signal 

that is sampled is not stable for the required setup and hold times, th
 i
 op may 
go into a 
metastable
 state. In such a state, the output will not have a legitimate high 
or low value, but will be in the indeterminate region between them. Furthermore, 
metastability
 A situation that occurs if 
a signal is sampled when 

it is not stable for the 

required setup and hold 

times, possibly causing 

the sampled value to 

fall in the indeterminate 

region between a high and 

low value.

 B.13 Concluding Remarks 
B-77th
 i
 op is not guaranteed to exit this state in any bounded amount of time. 
Some logic blocks that look at the output of th
 i
 op may see its output as 0, 
while others may see i
 is situation is called a 
synchronizer failure
.In a purely synchronous system, synchronizer failure can be avoided by ensuring 
that the setup and hold times fo
 i
 op or latch are always met, but this is 
impossible when the input is asynchronous. Instead, the only solution possible is to 
wait long enough before looking at the output of th
 i
 op to ensure that its output 
is stable, and that it has exited the metastable state, if it ever entered it. How long is 

long enough? Well, the probability that th
 i
 op will stay in the metastable state 
decreases exponentially, so a
 er a very short time the probability that th
 i
 op 
is in the metastable state is very low; however, the probability never reaches 0! So 

designers wait long enough such that the probability of a synchronizer failure is very 

low, and the time between such failures will be years or even thousands of years.
Fo
 i
 op designs, waiting for a period that is several times longer than 
the setup time makes the probability of synchronization failure very low. If the 

clock rate is longer than the potential metastability period (which is likely), then a 

safe synchronizer can be built with tw
 i
 ops, as 
Figure B.11.6
 shows. If you 
are interested in reading more about these problems, look into the references.
synchronizer failure
 A situation in which 
 i
 op enters a 
metastable state and 

where some logic blocks 

reading the output of the 

 i
 op see a 0 while 
others see a 1.
Flip-flopDCQClock
Asynchronous inputFlip-flopDCQSynchronous outputFIGURE B.11.6 This synchronizer will work correctly if the period of metastability that 
we wish to guard against is less than the clock period.
 Although the output of the
 rst 
 i
 op 
may be metastable, it will not be seen by any other logic element until the second clock, when the second D 
 i
 op samples the signal, which by that time should no longer be in a metastable state.
Suppose we have a design with very large clock skew—longer than the register 
propagation time
. Is it always possible for such a design to slow the clock down 
enough to guarantee that the logic operates properly?
a. Yes, if the clock is slow enough the signals can always propagate and the 
design will work, even if the skew is very large.
b. No, since it is possible that two registers see the same clock edge far enough 
apart that a register is triggered, and its outputs propagated and seen by a 

second register with the same clock edge.
Check Yourself
propagation time
 e time required for an input 

t
 i
 op to propagate 
to the outputs of th
 ip-
 op.

B-78 Appendix B The Basics of Logic Design
 B.12 Field Programmable Devices
Within a custom or semicustom chip, designers can make use of th
 exibility of the 
underlying structure to easily implemen
t combinational or sequential logic. How 
can a designer who does not want to use a custom or semicustom IC implement 
a complex piece of logic taking advantage of the very high levels of integration 

availab
 e most popular component used for sequential and combinational 
logic design outside of a custom or semicustom IC is a 
 eld programmable 
device (FPD)
. An FPD is an integrated circuit containing combinational logic, and 
possibly memory devices, that are co gurable by the end user.
FPDs generally fall into two camps: 
programmable logic devices
 (PLDs)
, which are purely combinational, and 
 eld programmable gate arrays (FPGAs)
, which provide both combinational logic an
 i
 ops. PLDs consist of two forms: 
simple PLDs (SPLDs)
, which are usually either a PLA or a 
programmable array 
logic (PAL)
, and complex PLDs, which allow more than one logic block as well as 
co gurable interconnections among blocks. When we speak of a PLA in a PLD, 
we mean a PLA with user programmable and-plane and or-plane. A PAL is like a 

PLA, except that the or-pla
 xed.
Before we discuss FPGAs, it is useful to talk about how FPDs are co
 gured. 
Con
 guration is essentially a question of where to make or break connections. 
Gate and register structures are static, but the connections can be co
 gured. 
Notice that by co
 guring the connections, a user determines what logic functions 
are implemented. Consider a co
 gurable PLA: by determining where the 
connections are in the and-plane and the or-plane, the user dictates what logical 

functions are computed in the PLA. Connections in FPDs are either permanent 

or reco
 gurable. Permanent connections involve the creation or destruction of 
a connection between two wires. Current FPLDs all use an 
antifuse
 technology, 
which allows a connection to be built at programming time that is then permanent. 
 e other way to co gure CMOS FPLDs is through a SR
 e SRAM is 
downloaded at power-on, and the contents control the setting of switches, which 

in turn determines which metal lines are connected
 e use of SRAM control 
has the advantage in that the FPD can be reco
 gured by changing the contents 
of the SR
 e disadvantages of the SRAM-based control are two fold: the 
co
 guration is volatile and must be reloaded on power-on, and the use of active 
transistors for switches slightly increases the resistance of such connections.
FPGAs include both logic and memory devices, usually structured in a two-
dimensional array with the corridors dividing the rows and columns used for 
 eld programmable 
devices (FPD)
 An integrated circuit 
containing combinational 

logic, and possibly 

memory devices, that are 

co
 gurable by the end 
user.
programmable logic 
device (PLD)
 An integrated circuit 
containing combinational 

logic whose function is 

co
 gured by the end 
user.
 eld programmable 
gate array (FPGA)
 A co
 gurable integrated 
circuit containing both 

combinational logic 

blocks an
 i ops.
simple programmable 
logic device 

(SPLD)
 Programmable 
logic device, usually 
containing either a single 

PAL or PLA.
programmable array 
logic (PAL)
 Contains a 
programmable and-plane 
followed by
 xed or-
plane.
antifuse
 A structure in 
an integrated circuit that 

when programmed makes 

a permanent connection 

between two wires.

 B.14 Exercises B-79global interconnect between the cells of th
e array. Each cell is a combination of 
gates an
 i
 ops that can be programmed to perform some sp
 c function. 
Because they are basically small, programmable RAMs, they are also called 
lookup 
tables
 (LUTs)
. Newer FPGAs contain more sophisticated building blocks such as 
pieces of adders and RAM blocks that can be used to build regist
 les. A few large 
FPGAs even contain 32-bit RISC cores!
In addition to programming each cell to perform a sp
 c function, the 
interconnections between cells are also programmable, allowing modern FPGAs 
with hundreds of blocks and hundreds of thousands of gates to be used for complex 

logic functions. Interconnect is a major challenge in custom chips, and this is even 

more true for FPGAs, because cells do not represent natural units of decomposition 

for structured design. In many FPGAs, 90% of the area is reserved for interconnect 

and only 10% is for logic and memory blocks.
Just as you cannot design a custom or semicustom chip without CAD tools, you 
also need them for FPDs. Logic synthesis tools have been developed that target 

FPGAs, allowing the generation of a system using FPGAs from structural and 

behavioral Verilog.
 B.13 Concluding Remarks
 is appendix introduces the basics of logic design. If you have digested the 
material in this appendix, you are ready to tackle the material in Chapters 4 and 5, 

both of which use the concepts discussed in this appendix extensively.
lookup tables (LUTs)
 I
 eld programmable 
device, the name given 
to the cells because they 

consist of a small amount 

of logic and RAM.
Further Reading
 ere are a number of good texts on logic design. Here are some you might like to 
look into.
Ciletti, M. D. [2002]. 
Advanced Digital Design with the Verilog HDL,
 Englewood
 
 s, NJ: Prentice Hall.
A thorough book on logic design using Verilog.
Katz, R. H. [2004]. 
Modern Logic Design
, 2nd ed., Reading, MA: Addison-Wesley.
A general text on logic design.

Wakerly, J. F. [2000]. 
Digital Design: Principles and Practices,
 3rd ed., Englewood
 
 s, NJ: Prentice Hall.
A general text on logic design.

B-80 Appendix B The Basics of Logic Design
 B.14 ExercisesB.1 [10] §B.2
 In addition to the basic laws we discussed in this section, there 
are two important theorems, called DeMorgan’s theorems:
ABAB and ABAB
Prove DeMorgan’s theorems with a truth table of the form
ABABA + BA ˙ BA ˙ BA + B00111111
01100011
10010011

11000000
B.2 [15] §B.2
 Prove that the two equations for E in the example starting on 
page B-7 are equivalent by using DeMorgan’s theorems and the axioms shown on 
page B-7.
B.3 [10] §B.2
 Show that there are 2
n entries in a truth table for a function with 
n inputs.

B.4 [10] §B.2
 One logic function that is used for a variety of purposes 
(including within adders and to compute parity) is 
exclusive OR
 e output of a 
two-input exclusive OR function is true only if exactly one of the inputs is true. 
Show the truth table for a two-input exclusive OR function and implement this 

function using AND gates, OR gates, and inverters.
B.5 [15] §B.2
 Prove that the NOR gate is universal by showing how to build 
the AND, OR, and NOT functions using a two-input NOR gate.

B.6 [15] §B.2
 Prove that the NAND gate is universal by showing how to build 
the AND, OR, and NOT functions using a two-input NAND gate.

B.7 [10] §§B.2, B.3
 Construct the truth table for a four-input odd-parity 
function (see page B-65 for a description of parity).

B.8 [10] §§B.2, B.3
 Implement the four-input odd-parity function with AND 
and OR gates using bubbled inputs and outputs.

B.9 [10] §§B.2, B.3
 Implement the four-input odd-parity function with a PLA.

 B.14 Exercises B-81B.10 [15] §§B.2, B.3
 Prove that a two-input multiplexor is also universal by 
showing how to build the NAND (or NOR) gate using a multiplexor.
B.11 [5] §§4.2, B.2, B.3
 Assume that X consists of 3 bits, x2 x1 x0. Write four 
logic functions that are true if and only if
 X contains only one 0
 X contains an even number of 0s
 X when interpreted as an unsigned binary number is less than 4
 X when interpreted as a signed (two’s complement) number is negative
B.12 [5] §§4.2, B.2, B.3
 Implement the four functions described in Exercise 
B.11 using a PLA.

B.13 [5] §§4.2, B.2, B.3
 Assume that X consists of 3 bits, x2 x1 x0, and Y 
consists of 3 bits, y2 y1 y0. Write logic functions that are true if and only if
 X  Y, where X and Y are thought of as unsigned binary numbers
 X  Y, where X and Y are thought of as signed (two’s complement) numbers
 X  YUse a hierarchical approach that can be extended to larger numbers of bits. Show 
how can you extend it to 6-bit comparison.
B.14 [5] §§B.2, B.3
 Implement a switching network that has two data inputs 
(A and 
B), two data outputs (
C and 
D), and a control input (
S). If 
S equals 1, the 
network is in pass-through mode, and 
C should equal 
A, and 
D should equal 
B. If 
S equals 0, the network is in crossing mode, and 
C should equal 
B, and 
D should 
equal 
A.B.15 [15] §§B.2, B.3
 Derive the product-of-sums representation for 
E shown 
on page B-11 starting with the sum-of-products representation. You will need to 
use DeMorgan’s theorems.
B.16 [30] §§B.2, B.3
 Give an algorithm for constructing the sum-of- products 
representation for an arbitrary logic equation consisting of AND, OR, and NOT. 
 e algorithm should be recursive and should not construct the truth table in the 
process.
B.17 [5] §§B.2, B.3
 Show a truth table for a multiplexor (inputs 
A, B, and 
S; output 
C ), using don’t cares to simplify the table where possible.

B-82 Appendix B The Basics of Logic Design
B.18 [5] §B.3
 What is the function implemented by the following Verilog 
modules:
module FUNC1 (I0, I1, S, out); input I0, I1;

 input S;

 output out;

 out = S? I1: I0;

endmodulemodule FUNC2 (out,ctl,clk,reset); output [7:0] out;

 input ctl, clk, reset;

 reg [7:0] out;

 always @(posedge clk)

 if (reset) begin

  
out <= 8’b0 ; end
 else if (ctl) begin

  
out <= out + 1; end
 else begin

  
out <= out - 1; end
endmoduleB.19 [5] §B.4
 e Verilog code on page B-53 is fo
 i
 op. Show the 
Verilog code for a D latch.
B.20 [10] §§B.3, B.4
 Write down a Verilog module implementation of a 2-to-4 
decoder (and/or encoder).

B.21 [10] §§B.3, B.4
 Given the following logic diagram for an accumulator, 
write down the Verilog module implementation of it. Assume a positive edge-
triggered register and asynchronous Rst.

 B.14 Exercises B-83InOutLoad16AdderRegisterClkRstLoad16B.22 [20] §§B3, B.4, B.5
 Section 3.3 presents basic operation and possible 
implementations of multipliers. A basic unit of such implementation
 -and-add unit. Show a Verilog implementation for this unit. Show how can you use 
this unit to build a 32-bit multiplier.
B.23 [20] §§B3, B.4, B.5
 Repeat Exercise B.22, but for an unsigned divider 
rather than a multiplier.

B.24 [15] §B.5
 e ALU supported set on less than (
slt) using just the sign 
bit of the adder. Let’s try a set on less than operation using the values 
7ten
 and 6
ten
. To make it simpler to follow the example, let’s limit the binary representations to 4 
bits: 1001
two
 and 0110
two
.1001two – 0110two = 1001two + 1010two = 0011two is result would suggest that 
7  6, which is clearly wrong. Hence, we must 
factor in over ow in the decision. Modify the 1-bit ALU in 
Figure B.5.10
 on page 

B-33 to handle 
slt correctly. Make your changes on a photocopy of t
 gure to 
save time.
B.25 [20] §B.6
 A simple check for over
 ow during addition is to see if the 
CarryIn to th
 cant bit is not the same as the CarryOut of the most 
 cant bit. Prove that this check is the same as in Figure 3.2.
B.26 [5] §B.6
 Rewrite the equations on page B-44 for a carry-lookahead logic 
for a 16-bit adder using a new notation. First, use the names for the CarryIn signals 
of the individual bits of the adder
 at is, use c4, c8, c12, … instead of C1, C2, 
C3, …. In addition, let P
i,j; mean a propagate signal for bits 
i to 
j, and G
i,j; mean a 
generate signal for bits 
i to 
j. For example, the equation
CGPGPPc
2110100
()()

B-84 Appendix B The Basics of Logic Design
can be rewritten as
cGPGPPc
807474307430
,,,,,
()()
 is more general notation is useful in creating wider adders.
B.27 [15] §B.6
 Write the equations for the carry-lookahead logic for a 64-
bit adder using the new notation from Exercise B.26 and using 16-bit adders as 
building blocks. Include a drawing similar to 
Figure B.6.3
 in your solution.
B.28 [10] §B.6
 Now calculate the relative performance of adders. Assume that 
hardware corresponding to any equation containing only OR or AND terms, such 
as the equations for p
i and g
i on page B-40, takes one time unit T. Equations that 
consist of the OR of several AND terms, such as the equations for c1, c2, c3, and 

c4 on page B-40, would thus take two time units, 2T
 e reason is it would take T 
to produce the AND terms and then an additional T to produce the result of the 

OR. Calculate the numbers and performance ratio for 4-bit adders for both ripple 

carry and carry lookahead. If the terms in equations are furt
 ned by other 
equations, then add the appropriate delays for those intermediate equations, and 

continue recursively until the actual input bits of the adder are used in an equation. 

Include a drawing of each adder labeled with the calculated delays and the path of 

the worst-case delay highlighted.
B.29 [15] §B.6
 is exercise is similar to Exercise B.28, but this time calculate 
the relative speeds of a 16-bit adder using ripple carry only, ripple carry of 4-bit 
groups that use carry lookahead, and the carry-lookahead scheme on page B-39.
B.30 [15] §B.6
 is exercise is similar to Exercises B.28 and B.29, but this 
time calculate the relative speeds of a 64-bit adder using ripple carry only, ripple 
carry of 4-bit groups that use carry lookahead, ripple carry of 16-bit groups that use 

carry lookahead, and the carry-lookahead scheme from Exercise B.27.
B.31 [10] §B.6
 Instead of thinking of an adder as a device that adds two 
numbers and then links the carries together, we can think of the adder as a hardware 
device that can add three inputs together (a
i, bi, ci) and produce two outputs 
(s, ci  1). When adding two numbers together, there is little we can do with this 
observation. When we are adding more than two operands, it is possible to reduce 

the cost of the carry
 e idea is to form two independent sums, called S
 (sum bits) 
and C
 (carry bits). At the end of the process, we need to add C
 and S
 together 

using a normal adder
 is technique of delaying carry propagation until the end 
of a sum of numbers is called 
carry save addition
 e block drawing on the lower 
right of 
Figure B.14.1
 (see below) shows the organization, with two levels of carry 

save adders connected by a single normal adder.
Calculate the delays to add four 16-bit numbers using full carry-lookahead adders 
versus carry save with a carry-lookahead adder forming th
 
 e time 
unit T in Exercise B.28 is the same.)

 B.14 Exercises B-85B.32 [20] §B.6
 Perhaps the most likely case of adding many numbers at once 
in a computer would be when trying to multiply more quickly by using many 
adders to add many numbers in a single clock cycle. Compared to the multiply 

algorithm in Chapter 3, a carry save scheme with many adders could multiply more 

than 10 times faster
 is exercise estimates the cost and speed of a combinational 
multiplier to multiply two positive 16-bit numbers. Assume that you have 16 

intermediate terms M15, M14, …, M0, called 
partial products
, that contain the 
multiplicand ANDed with multiplier bi
 e idea is to use 
carry save adders to reduce the 
n operands into 2
n/3 in parallel groups of three, 
and do this repeatedly until you get two large numbers to add together with a 

traditional adder.
FIGURE B.14.1 Traditional ripple carry and carry save addition of four 4-bit numbers.
 e details are shown on th
 , with the individual signals in lowercase, and the corresponding higher-level 
blocks are on the right, with collective signals in upper case. Note that the sum of four 
n-bit numbers can 
take 
n + 2 bits.
s4s3s2s1s0f0e0b0f1e1b1
f2e2b2
f3e3b3
a0a1a2a3s5c'3s'3s'4c'2s'2c'1s'1c'0s'0
Carry save adder
EFBACarry save adder
Traditional adder
SC'S'
s5s0b0a0
e0f0s1b1a1
e1f1s2b2a2
e2f2s3b3a3
e3f3s4EFSBA
Traditional adder
Traditional adder
Traditional adder

B-86 Appendix B The Basics of Logic Design
First, show the block organization of the 16-bit carry save adders to add these 16 
terms, as shown on the right in 
Figure B.14.1
 en calculate the delays to add these 
16 numbers. Compare this time to the iterative multiplication scheme in Chapter 

3 but only assume 16 iterations using a 16-bit adder that has full carry lookahead 

whose speed was calculated in Exercise B.29.
B.33 [10] §B.6
 ere are times when we want to add a collection of numbers 
together. Suppose you wanted to add four 4-bit numbers (A, B, E, F) using 1-bit 
full adders. Let’s ignore carry lookahead for now. You would likely connect the 

1-bit adders in the organization at the top of 
Figure B.14.1
. Below the traditional 

organization is a novel organization of full adders. Try adding four numbers using 

both organizations to convince yourself that you get the same answer.
B.34 [5] §B.6
 First, show the block organization of the 16-bit carry save 
adders to add these 16 terms, as shown in 
Figure B.14.1.
 Assume that the time delay 
through each 1-bit adder is 2T. Calculate the time of adding four 4-bit numbers to 

the organization at the top versus the organization at the bottom of 
Figure B.14.1
.B.35 [5] §B.8
 Quite
 en, you would expect that given a timing diagram 
containing a description of changes that take place on a data input 
D and a clock 
input 
C (as in Figures B.8.3 and B.8.6
 on pages B-52 and B-54, respectively), there 
would b erences between the output waveforms (
Q) for a D latch an
 ip-
 op. In a sentence or two, describe the circumstances (e.g., the nature of the inputs) 
for which there would not be an
 erence between the two output waveforms.
B.36 [5] §B.8
 Figure B.8.8
 on page B-55 illustrates the implementation of the 
regist
 le for the MIPS datapath. Pretend that a new regist
 le is to be built, 
but that there are only two registers and only one read port, and that each register 
has only 2 bits of data. Redraw 
Figure B.8.8
 so that every wire in your diagram 

corresponds to only 1 bit of data (unlike the diagram in 
Figure B.8.8
, in which 

some wires are 5 bits and some wires are 32 bits). Redraw the registers usin
 ip-
 ops. You do not need to show how to implemen
 i
 op or a multiplexor.
B.37 [10] §B.10
 A friend would like you to build an “electronic eye” for use 
as a fake security device
 e device consists of three lights lined up in a row, 
controlled by the outputs Le
 , Middle, and Right, which, if asserted, indicate that 
a light should be on. Only one light is on at a time, and the light “moves” from 

  to right and then from right t
 , thus scaring away thieves who believe that 
the device is monitoring their activity. Draw the graphical representation for the 

 nite-state machine used to specify the electronic eye. Note that the rate of the eye’s 
movement will be controlled by the clock speed (which should not be too great) 

and that there are essentially no inputs.
B.38 [10] §B.10
 Assign state numbers to the states of th
 nite-state machine 
you constructed for Exercise B.37 and write a set of logic equations for each of the 
outputs, including the next-state bits.

 B.14 Exercises B-87B.39 [15] §§B.2, B.8, B.10
 Construct a 3-bit counter using thre
 ip-
 ops and a selection of gat
 e inputs should consist of a signal that resets the 
counter to 0, called 
reset
, and a signal to increment the counter, called 
inc
 e outputs should be the value of the counter. When the counter has value 7 and is 
incremented, it should wrap around and become 0.
B.40 [20] §B.10
 A Gray code
 is a sequence of binary numbers with the property 
that no more than 1 bit changes in going from one element of the sequence to 
another. For example, here is a 3-bit binary Gray code: 000, 001, 011, 010, 110, 

111, 101, and 100. Using thre
 i
 ops and a PLA, construct a 3-bit Gray code 
counter that has two inputs: 
reset
, which sets the counter to 000, and 
inc
, which 
makes the counter go to the next value in the sequence. Note that the code is cyclic, 

so that the value a
 er 100 in the sequence is 000.
B.41 [25] §B.10
 We wish to add a yellow light to our tra
  c light example on 
page B-68. We will do this by changing the clock to run at 0.25 Hz (a 4-second clock 

cycle time), which is the duration of a yellow light. To prevent the green and red lights 

from cycling too fast, we add a 30-second timer
 e timer has a single input, called 
TimerReset
, which restarts the timer, and a single output, called 
TimerSignal
, which 
indicates that the 30-second period has expired. Also, we must re
 ne the tr
  c 
signals to include yellow. We do this b
 ning two out put signals for each light: 
green and yellow. If the output NSgreen is asserted, the green light is on; if the output 

NSyellow is asserted, the yellow light is on. If both signals are o
 , the red light is on. Do 
not assert both the green and yellow signals at the same time, since American drivers 
will certainly be confused, even if European drivers understand what this means! Draw 

the graphical representation for th
 nite-state machine for this improved controller. 
Choose names for the states that are 
 erent
 from the names of the outputs.
B.42 [15] §B.10
 Write down the next-state and output-function tables for the 
tra
  c light controller described in Exercise B.41.
B.43 [15] §§B.2, B.10
 Assign state numbers to the states in the tra
 c light 
example of Exercise B.41 and use the tables of Exercise B.42 to write a set of logic 

equations for each of the outputs, including the next-state outputs.
B.44 [15] §§B.3, B.10
 Implement the logic equations of Exercise B.43 as a 
PLA.
§B.2, page B-8: No. If 
A  1, C  1, B  0, th rst is true, but the second is false.
§B.3, page B-20: C.
§B.4, pag
 ey are all exactly the same.
§B.4, page B-26: 
A  0, B  1.§B.5, page B-38: 2.
§B.6, page B-47: 1.

§B.8, page B-58: c.

§B.10, page B-72: b.

§B.11, page B-77: b.
Answers to 
Check Yourself

Imagination is more 
important than 

knowledge.
Albert Einstein
On Science, 
1930sGraphics and Computing GPUsJohn Nickolls
Director of Architecture

NVIDIA
David Kirk
Chief Scientist

NVIDIA
CAPPENDIX
C.1 Introduction C-3C.2 GPU System Architectures 
C-7C.3 Programming GPUs 
C-12C.4 Multithreaded Multiprocessor Architecture 
C-24C.5 Parallel Memory System 
C-36C.6 Floating-point Arithmetic 
C-41C.7 Real Stuff: The NVIDIA GeForce 8800 
C-45C.8 Real Stuff: Mapping Applications to GPUs 
C-54C.9 Fallacies and Pitfalls 
C-70C.10 Concluding Remarks 
C-74C.11 Historical Perspective and Further Reading 
C-75 C.1 Introduction is appendix focuses on the 
GPU—the ubiquitous 
graphics processing unit
 in every PC, laptop, desktop computer, and workstation. In its most basic form, 
the GPU generates 2D and 3D graphics, images, and video that enable window-

based operating systems, graphical user interfaces, video games, visual imaging 

applications, and video.
 e modern GPU that we describe here is a highly parallel, 
highly multithreaded multiprocessor optimized for 
visual computing
. To provide 
real-time visual interaction with computed objects via graphics, images, and video, 

th
 ed graphics and computing architecture that serves as both a 
programmable graphics processor and a scalable parallel computing platform. PCs 

and game consoles combine a GPU with a CPU to form 
heterogeneous systems
.A Brief History of GPU Evolution
 een years ago, there was no such thing as a GPU. Graphics on a PC were 
performed by a 
video graphics array
 (VGA) controller. A VGA controller was 
simply a memory controller and display generator connected to some DRAM. In 

the 1990s, semiconductor technology adva
  ciently that more functions 
could be added to the VGA controller. 
By 1997, VGA controllers were beginning 
to incorporate some 
three-dimensional
 (3D) acceleration functions, including 
graphics processing 
unit (GPU) A processor 
optimized for 2D and 3D 
graphics, video, visual 

computing, and display.
visual computing
 A mix of graphics 

processing and computing 

that lets you visually 

interact with computed 

objects via graphics, 

images, and video.
heterogeneous 
system
 A system 
combinin
 erent 
processor types. A PC is a 
heterogeneous CPU–GPU 

system.

C-4 Appendix C Graphics and Computing GPUs
hardware for triangle setup and rasterization (dicing triangles into individual 
pixels) and texture mapping and shading (applying “decals” or patterns to pixels 

and blending colors).
In 2000, the single chip graphics processor incorporated almost every detail of 
the traditional high-end workstation graphics pipeline and, therefore, deserved a 

new name beyond VGA controller
 e term GPU was coined to denote that the 
graphics device had become a processor.
Over time, GPUs became more programmable, as programmable processors 
repl
 xed function dedicated logic while maintaining the basic 3D graphics 
pipeline organization. In addition, computations became more precise over time, 

progressing from indexed arithmetic, to integer an
 xed point, to single precision 
 oating-point, and recently to double precisio
 oating-point. GPUs have become 
massively parallel programmable processors with hundreds of cores and thousands 

of threads.
Recently, processor instructions and memory hardware were added to support 
general purpose programming languages, and a programming environment was 

created to allow GPUs to be programmed using familiar languages, including C 

and C
 is innovation makes a GPU a fully general-purpose, programmable, 
manycore processor, albeit still with some special bene
 ts and limitations.
GPU Graphics Trends
GPUs and their associated drivers implement the OpenGL and DirectX 

models of graphics processing. OpenGL is an open standard for 3D graphics 

programming available for most computers. DirectX is a series of Microso
  multimedia programming interfaces, including Direct3D for 3D graphics. Since 

these 
application programming interfaces (APIs)
 have we
 ned behavior, 
it is possible to bu
 ective hardware acceleration of the graphics processing 
function
 ned by the API
 is is one of the reasons (in addition to increasing 
device density) why new GPUs are being developed every 12 to 18 months that 

double the performance of the previous generation on existing applications.
Frequent doubling of GPU performance enables new applications that were 
not previously possible
 e intersection of graphics processing and parallel 
computing invites a new paradigm for graphics, known as visual computing. It 

replaces large sections of the traditional sequential hardware graphics pipeline 

model with programmable elements for geometry, vertex, and pixel programs. 

Visual computing in a modern GPU combines graphics processing and parallel 

computing in novel ways that permit new graphics algorithms to be implemented, 

and opens the door to entirely new parallel processing applications on pervasive 

high-performance GPUs.
Heterogeneous System
Although the GPU is arguably the most parallel and most powerful processor in 

a typical PC, it is certainly not the only processor
 e CPU, now multicore and 
application 
programming interface 

(API) A set of function 
and data structure 
 nitions providing an 
interface to a library of 

functions.

 C.1 Introduction C-
5soon to be manycore, is a complementary, primarily serial processor companion 
to the massively parallel manycore GPU. Together, these two types of processors 

comprise a heterogeneous multiprocessor system.
 e best performance for many applications comes from using both the CPU 
and the GPU
 is appendix will help you understand how and when to best split 
the work between these two increasingly parallel processors.
GPU Evolves into Scalable Parallel Processor
GPUs have evolved functionally from hardwired, limited capability VGA controllers 

to programmable parallel processor
 is evolution has proceeded by changing 
the logical (API-based) graphics pipeline 
to incorporate programmable elements 
and also by making the underlying hard
ware pipeline stages less specialized and 
more programmable. Eventually, it made sense to merge disparate programmable 

pipeline elements into on
 ed array of many programmable processors.
In the GeForce 8-series generation of GPUs, the geometry, vertex, and pixel 
processing all run on the same type of processor
 is 
 cation allows for 
dramatic scalability. More programmable processor cores increase the total system 

throughput. Unifying the processors also delivers ver
 ective load balancing, 
since any processing function can use the whole processor array. At the other end 

of the spectrum, a processor array can now be built with very few processors, since 

all of the functions can be run on the same processors.
Why CUDA and GPU Computing?
 is uniform and scalable array of processors invites a new model of programming 
for the GPU
 e large amount of
 oating-point processing power in the GPU 
processor array is very attractive for solving nongraphics problems. Given the large 

degree of parallelism and the range of scalability of the processor array for graphics 

applications, the programming model for more general computing must express 

the massive parallelism directly, but allow for scalable execution.
GPU computing
 is the term coined for using the GPU for computing via a 
parallel programming language and API, 
without using the traditional graphics 
API and graphics pipeline model.
 is is in contrast to the earlier 
General Purpose 
computation on GPU (GPGPU)
 approach, which involves programming the GPU 
using a graphics API and graphics pipeline to perform nongraphics tasks.
Compute Unifed Device Architecture (CUDA)
 is a scalable parallel programming 
model and so
 ware platform for the GPU and other parallel processors that allows 
the programmer to bypass the graphics API and graphics interfaces of the GPU 

and simply program in C or C
 e CUDA programming model has an SPMD 
(single-program multiple data) so
 ware style, in which a programmer writes a 
program for one thread that is instanced and executed by many threads in parallel 

on the multiple processors of the GPU. In fact, CUDA also provides a facility for 

programming multiple CPU cores as well, so CUDA is an environment for writing 

parallel programs for the entire heterogeneous computer system.
GPU computing
 Using 
a GPU for computing via 
a parallel programming 

language and API.
GPGPU Using a GPU 
for general-purpose 

computation via a 

traditional graphics API 

and graphics pipeline.
CUDA
 A scalable 
parallel programming 

model and language based 

on C/C
. It is a parallel 
programming platform 

for GPUs and multicore 

CPUs.

C-6 Appendix C Graphics and Computing GPUs
GPU Unifes Graphics and ComputingWith the addition of CUDA and GPU computing to the capabilities of the GPU, 
it is now possible to use the GPU as both a graphics processor and a computing 

processor at the same time, and to combine these uses in visual computing 

application
 e underlying processor architecture of the GPU is exposed in two 
wa
 rst, as implementing the programmable graphics APIs, and second, as a 
massively parallel processor array programmable in C/C
 with CUDA.
Although the underlying processors of the GPU ar
 ed, it is not necessary 
that all of the SPMD thread programs are the same
 e GPU can run graphics 
shader programs for the graphics aspect of the GPU, processing geometry, vertices, 

and pixels, and also run thread programs in CUDA.
 e GPU is truly a versatile multiprocessor architecture, supporting a variety of 
processing tasks. GPUs are excellent at graphics and visual computing as they were 

sp
 cally designed for these applications. GPU
s are also excellent at many general-
purpose throughput applications that are “
 rst cousins” of graphics, in that they 
perform a lot of parallel work, as well as having a lot of regular problem structure. 

In general, they are a good match to data-parallel problems (see Chapter 6), 

particularly large problems, but less so for less regular, smaller problems.
GPU Visual Computing Applications
Visual computing includes the traditional types of graphics applications plus many 

new application
 e original purview of a GPU was “anything with pixels,” but it 
now includes many problems without pixels but with regular computation and/or 

data structure. GPUs ar
 ective at 2D and 3D graphics, since that is the purpose 
for which they are designed. Failure to deliver this application performance would 

be fatal. 2D and 3D graphics use the GPU in its “graphics mode,” accessing the 

processing power of the GPU through the graphics APIs, OpenGL™, and DirectX™. 

Games are built on the 3D graphics processing capability.
Beyond 2D and 3D graphics, image processing and video are important 
applications for GPU
 ese can be implemented using the graphics APIs or as 
computational programs, using CUDA to program the GPU in computing mode. 

Using CUDA, image processing is simply another data-parallel array program. To 

the extent that the data access is regular and there is good locality, the program 

  cient. In practice, image processing is a very good application for GPUs. 
Video processing, especially encode and 
decode (compression and decompression 
according to some standard algorithms), is quit
  cient.
 e greatest opportunity for visual computing applications on GPUs is to “break 
the graphics pipeline.” Early GPUs implemented only sp
 c graphics APIs, albeit 
at very high performance.
 is was wonderful if the API supported the operations 
that you wanted to do. If not, the GPU could not accelerate your task, because early 

GPU functionality was immutable. Now, with the advent of GPU computing and 

CUDA, these GPUs can be programmed to implemen
 erent virtual pipeline 
by simply writing a CUDA program to describe the computation and dat
 ow that 

 C.2 GPU System Architectures C-
7is desired. So, all applications are now possible, which will stimulate new visual 
computing approaches.
 C.2 GPU System Architectures
In this section, we survey GPU system architectures in common use today. We 

discuss system co
 gurations, GPU functions and services, standard programming 
interfaces, and a basic GPU internal architecture.
Heterogeneous CPU–GPU System Architecture
A heterogeneous computer system architecture using a GPU and a CPU can be 

described at a high level by two primary characteri
 rst, how many functional 
subsystems and/or chips are used and what are their interconnection technologies 

and topology; and second, what memory subsystems are available to these 

functional subsystems. See Chapter 6 for background on the PC I/O systems and 

chip sets.
The Historical PC (circa 1990)Figure C.2.1 shows a high-level block diagram of a legacy PC, cir
 e north 
bridge (see Chapter 6) contains high-bandwidth interfaces, connecting the CPU, 

memory, and PCI bu
 e south bridge contains legacy interfaces and devices: 
ISA bus (audio, LAN), interrupt controller; DMA controller; time/counter. In 

this system, the display was driven by a simple frameb
 er subsystem known 
CPUNorthBridgeSouthBridgeFront Side BusPCI BusFramebufferMemoryVGAControllerMemoryUARTLANVGADisplayFIGURE C.2.1 Historical PC. VGA controller drives graphics display from frameb
 er memory.

C-8 Appendix C Graphics and Computing GPUs
as a VGA (video graphics array) which was attached to the PCI bus. Graphics 
subsystems with built-in processing 
elements (GPUs) did not exist in the PC 
landscape of 1990.
Figure C.2.2 illustrates two confgurations in common use today
 ese are 
characterized by a separate GPU (discrete GPU) and CPU with respective memory 

subsystems. In Figure C.2.2a, with an Intel CPU, we see the GPU attached via a 

16-lane 
PCI-Express
 2.0 link to provide a peak 16 GB/s transfer rate, (peak of 8 
GB/s in each direction). Similarly, in Figure C.2.2b, with an AMD CPU, the GPU 
PCI-Express (PCIe)
 A standard system I/O 
interconnect that uses 

point-to-point links. 

Links have a co
 gurable 
number of lanes and 

bandwidth.
Front Side BusGPUMemorySouthBridgeNorthBridgeIntelCPUDDR2Memoryx16 PCI-Express Linkx4 PCI-Express Linkderivative128-bit667 MT/sdisplayGPU128-bit667 MT/sinternal bus
GPUMemoryDDR2Memoryx16 PCI-Express LinkChipsetCPUcoreAMDCPUGPUNorthBridgeHyperTransport 1.03display(a)(b)FIGURE C.2.2 Contemporary PCs with Intel and AMD CPUs.
 See Chapter 6 for an explanation of 
the components and interconnects in t
 gure.

 C.2 GPU System Architectures C-
9is attached to the chipset, also via PCI-Express with the same available bandwidth. 
In both cases, the GPUs and CPUs may access each other’s memory, albeit with less 

available bandwidth than their access to the more directly attached memories. In 

the case of the AMD system, the north bridge or memory controller is integrated 

into the same die as the CPU.
A low-cost variation on these systems, a 
 ed memory architecture 
(UMA)
 system, uses only CPU system memory, omitting GPU memory from 
the syst
 ese systems have relatively low performance GPUs, since their 
achieved performance is limited by the available system memory bandwidth and 

increased latency of memory access, whereas dedicated GPU memory provides 

high bandwidth and low latency.
A high performance system variation uses multiple attached GPUs, typically 
two to four working in parallel, with their displays daisy-chained. An example is 

the NVIDIA SLI (scalable link interc
onnect) multi-GPU system, designed for high 
performance gaming and workstations.
 e next system category integrates the GPU with the north bridge (Intel) or 
chipset (AMD) with and without dedicated graphics memory.
Chapter 5 explains how caches maintain coherence in a shared address space. 
With CPUs and GPUs, there are multiple address spaces. GPUs can access their 

own physical local memory and the CPU system’s physical memory using virtual 

addresses that are translated by an MMU on the GPU
 e operating system kernel 
manages the GPU’s page tables. A system ph
ysical page can be accessed using either 
coherent or noncoherent PCI-Express transactions, determined by an attribute in 

the GPU’s page table
 e CPU can access GPU’s local memory through an address 
range (also called aperture) in the PCI-Express address space.
Game ConsolesConsole systems such as the Sony PlayStation 3 and the Microso
  Xbox 360 
resemble the PC system architectures previously described. Console systems are 

designed to be shipped with identical performance and functionality over a lifespan 

that can last
 ve years or more. During this time, a system may be reimplemented 
many times to exploit more advanced silicon manufacturing processes and thereby 

to provide constant capability at ever lower costs. Console systems do not need 

to have their subsystems expanded and up
graded the way PC systems do, so the 
major internal system buses tend to be customized rather than standardized.
GPU Interfaces and Drivers
In a PC today, GPUs are attached to a CPU via PCI-Express. Earlier generations 

used 
AGP
. Graphics applications call OpenGL [Segal and Akeley, 2006] or Direct3D 
[Microso
  DirectX Specifcation] API functions that use the GPU as a coprocessor. 
 e APIs send commands, programs, and data to the GPU via a graphics device 
driver optimized for the particular GPU.
 ed memory 
architecture (UMA)
 A system architecture in 
which the CPU and GPU 

share a common system 

memory.
AGP
 An extended 
version of the original PCI 

I/O bus, which provided 

up to eight times the 

bandwidth of the original 

PCI bus to a single card 

slot. Its primary purpose 

was to connect graphics 

subsystems into PC 

systems.

C-10 Appendix C Graphics and Computing GPUs
Mapping Graphics Pipeline to Uniﬁ ed GPU Processors
Figure C.2.4 shows how the logical pipeline comprising separate independent 
programmable stages is mapped onto a physical distributed array of processors.
Basic Unifed GPU ArchitectureU
 ed GPU architectures are based on a parallel array of many programmable 
processor
 ey unify vertex, geometry, and pixel shader processing and parallel 
computing on the same processors, unlike earlier GPUs which had separate 

processors dedicated to each processing type
 e programmable processor array is 
tightly integrated wi
 xed function processors for textur
 ltering, rasterization, 
raster operations, anti-aliasing, compre
ssion, decompression, display, video 
decoding, and hig
 nition video processing. Although th
 xed-function 
processor
 cantly outperform more general programmable processors in 
terms of absolute performance constrained by an area, cost, or power budget, we 

will focus on the programmable processors here.
Compared with multicore CPUs, manycore GPUs hav
 erent architectural 
design point, one focused on executing many parallel thre
  ciently on many 
InputAssemblerVertexShaderGeometryShaderSetup &RasterizerPixelShaderRaster Operations/Output MergerFIGURE C.2.3 Graphics logical pipeline. Programmable graphics shader stages are blue, an
 xed-function blocks are white.
Unified ProcessorArray InputAssemblerVertexShaderSetup &Rasterizer Raster Operations/Output Merger GeometryShaderPixelShaderFIGURE C.2.4 Logical pipeline mapped to physical processors.
 e programmable shader 
stages execute on the array o
 ed processors, and the logical graphics pipeline data
 ow recirculates 
through the processors.
Graphics Logical Pipeline e graphics logical pipeline is described in Section C.3. Figure C.2.3 illustrates 
the major processing stages, and highlights the important programmable stages 

(vertex, geometry, and pixel shader stages).

 C.2 GPU System Architectures C-
11processor cores. By using many simpler cores and optimizing for data-parallel 
behavior among groups of threads, more of the per-chip transistor budget is 

devoted to computation, and less to on-chip caches and overhead.
Processor Array
 ed GPU processor array contains many processor cores, typically organized 
into multithreaded multiprocessors. Figure C.2.5 shows a GPU with an array 

of 112 
streaming processor
 (SP) cores, organized as 14 multithreaded 
streaming 
multiprocessors
 (SMs). Each SP core is highly multithreaded, managing 96 
concurrent threads and their state in hardware
 e processors connect with 
four 64-bit-wide DRAM partitions via an interconnection network. Each SM 

has eight SP cores, two 
special function units
 (SFUs), instruction and constant 
caches, a multithreaded instruction unit, and a shared memory
 is is the basic 
Tesla architecture implemented by the NVIDIA GeForce 8800. I
 ed architecture in which the traditional graphics programs for vertex, geometry, and 

pixel shading run on th
 ed SMs and their SP cores, and computing programs 
run on the same processors.
GPUHost CPUSystem MemoryDRAMROPL2DRAMROPL2DRAMROPL2DRAMROPL2TPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPTPCTexture UnitTex L1SMSPSPSPSPSPSPSPSPSMSPSPSPSPSPSPSPSPVertex WorkDistribution Input AssemblerHost InterfaceBridgePixel WorkDistribution Viewport/Clip/
Setup/Raster/ZCullCompute WorkDistributionSPSharedMemorySPSPSPSPSPSPSMSPI-CacheMT IssueC-CacheSFUSFUInterconnection NetworkDisplay InterfaceDisplayHigh-DefinitionVideo ProcessorsSharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemorySharedMemoryFIGURE C.2.5 Basic uniﬁ ed GPU architecture.
 Example GPU with 112 streaming processor (SP) cores organized in 14 streaming 
multiprocessors (SMs); the cores are highly multithreaded. It has the basic Tesla architecture of an NVIDIA GeFor
 e processors 
connect with four 64-bit-wide DRAM partitions via an interconnection network. Each SM has eight SP cores, two special function 
units 
(SFUs), instruction and constant caches, a multithreaded instruction unit, and a shared memory.

C-12 Appendix C Graphics and Computing GPUs
 e processor array architecture is scalable to smaller and larger GPU 
co
 gurations by scaling the number of multiprocessors and the number of 
memory partitions. Figure C.2.5 shows seven clusters of two SMs sharing a texture 
unit and a texture L1 cache
 e texture unit deliver
 ltered results to the SM 
given a set of coordinates into a texture map. Becaus
 lter regions of support 
 en overlap for successive texture reques
ts, a small streaming L1 texture cache is 
 ective to reduce the number of requests to the memory syst
 e processor 
array connects with 
raster operation
 processors 
(ROPs), L2 texture caches, external 
DRAM memories, and system memory via a GPU-wide interconnection network. 

 e number of processors and number of memories can scale to design balanced 
GPU systems fo
 erent performance and market segments.
 C.3 Programming GPUs
Programming multiprocessor GPUs is qualitativel
 erent than programming 
other multiprocessors like multicore CPUs. GPUs provide two to three orders of 

magnitude more thread and data parallelism than CPUs, scaling to hundreds of 

processor cores and tens of thousands of concurrent threads. GPUs continue 

to increase their parallelism, doubling it about every 12 to 18 months, enabled 

by Moore’s law [1965] of increasing integrated circuit density and by improving 

architect
  ciency. To span the wide price and performance range o
 erent 
market segmen
 erent GPU products implement widely varying numbers of 
processors and threads. Yet users expect games, graphics, imaging, and computing 

applications to work on any GPU, regardless of how many parallel threads it 

executes or how many parallel processor cores it has, and they expect more 

expensive GPUs (with more threads and cores) to run applications faster. As a 

result, GPU programming models and applic
ation programs are designed to scale 
transparently to a wide range of parallelism.
 e driving force behind the large number of parallel threads and cores in a 
GPU is real-time graphics performance—the need to render complex 3D scenes 

with high resolution at interactive frame rates, at least 60 frames per second. 

Correspondingly, the scalable programming models of graphics shading languages 

such as Cg (C for graphics) and HLSL (high-level shading language) are designed 

to exploit large degrees of parallelism via many independent parallel threads and to 

scale to any number of processor cor
 e CUDA scalable parallel programming 
model similarly enables general parallel computing applications to leverage large 

numbers of parallel threads and scale to any number of parallel processor cores, 

transparently to the application.
In these scalable programming models, the programmer writes code for a single 
thread, and the GPU runs myriad thread instances in parallel. Programs thus scale 

transparently over a wide range of hardware pa
 is simple paradigm 
arose from graphics APIs and shading languages that describe how to shade one 

 C.3 Programming GPUs C-
13vertex or one pixel. It has remained an
 ective paradigm as GPUs have rapidly 
increased their parallelism and performance since the late 1990s.
 is section br
 y describes programming GPUs for real-time graphics 
applications using graphics APIs and programming languages. It then describes 
programming GPUs for visual computing and general parallel computing 

applications using the C language and the CUDA programming model.
Programming Real-Time Graphics
APIs have played an important role in the rapid, successful development of GPUs 

and processor
 ere are two primary standard graphics APIs: 
OpenGL
 and 
Direct3D
, one of the Microso
  DirectX multimedia programming interfaces. 
OpenGL, an open standard, was originally proposed an
 ned by Silicon 
Graphics Incorporated
 e ongoing development and extension of the OpenGL 
standard [Segal and Akeley, 2006], [Kessenich, 2006] is managed by Khronos, 

an industry consortium. Direct3D [Blythe, 2006], a de facto standard
 ned 
and evolved forward by Microso
  and partners. OpenGL and Direct3D are 
similarly structured, and continue to evolve rapidly with GPU hardware advances. 

 ey 
 ne a logical graphics processing pipeline that is mapped onto the GPU 
hardware and processors, along with programming models and languages for the 

programmable pipeline stages.
Logical Graphics PipelineFigure C.3.1 illustrates the Direct3D 10 logical graphics pipeline. OpenGL has a 

similar graphics pipeline structure
 e API and logical pipeline provide a streaming 
data
 ow infrastructure and plumbing for the programmable shader stages, shown in 
blue
 e 3D application sends the GPU a sequence of vertices grouped into geometric 
primitives—points, lines, triangles, and polygon
 e input assembler collects 
vertices and primitiv
 e vertex shader program executes per-vertex processing, 
OpenGL
 An open-
standard graphics API.
Direct3D
 A graphics 
 ned by Microso
  and partners.
InputAssemblerVertexShaderGeometryShaderSetup &RasterizerPixelShaderRaster Operations/Output MergerVertexBufferTextureTextureTextureRenderTargetSamplerSamplerSamplerConstantDepthZ-BufferConstantConstantStreamBufferStreamOutIndex BufferMemoryStencilGPUFIGURE C.3.1 Direct3D 10 graphics pipeline. Each logical pipeline stage maps to GPU hardware or to a GPU processor. Programmable 
shader stages are blue
 xed-function blocks are white, and memory objects are gray. Each stage processes a vertex, geometric primitive, or pixel 
in a streaming data
 ow fashion.

C-14 Appendix C Graphics and Computing GPUs
including transforming the vertex 3D position into a screen position and lighting the 
vertex to determine its color
 e geometry shader program executes per-primitive 
processing and can add or drop primitiv
 e setup and rasterizer unit generates 
pixel fragments (fragments are potential contributions to pixels) that are covered by 

a geometric primitive
 e pixel shader program performs per-fragment processing, 
including interpolating per-fragment parameters, texturing, and coloring. Pixel 

shaders make extensive use of sampled an
 ltered lookups into large 1D, 2D, or 
3D arrays called 
textures
, using interpolat
 oating-point coordinates. Shaders use 
texture accesses for maps, functions, decals, images, and dat
 e raster operations 
processing (or output merger) stage performs Z-b
 er depth testing and stencil 
testing, which may discard a hidden pixel fragment or replace the pixel’s depth with 

the fragment’s depth, and performs a color blending operation that combines the 

fragment color with the pixel color and writes the pixel with the blended color.
 e graphics API and graphics pipeline provide input, output, memory objects, 
and infrastructure for the shader programs that process each vertex, primitive, and 

pixel fragment.
Graphics Shader ProgramsReal-time graphics applications use man
 erent 
shader
 programs to model 
how light interacts wit
 erent materials and to render complex lighting and 
shadows. 
Shading languages
 are based on a data
 ow or streaming programming 
model that corresponds with the logical
 graphics pipeline. Vertex shader programs 
map the position of triangle vertices onto the screen, altering their position, color, 

or orientation. Typically a vertex shader thread inpu
 oating-point (x, y, z, w) 
vertex position and comput
 oating-point (x, y, z) screen position. Geometry 
shader programs operate on geometric primitives (such as lines and triangles) 

 ned by multiple vertices, changing them or generating additional primitives. 
Pixel fragment shaders each “shade” one pixel, computin
 oating-point red, 
green, blue, alpha (RGBA) color contribution to the rendered image at its pixel 

sample (x, y) image position. Shaders (and GPUs) us
 oating-point arithmetic 
for all pixel color calculations to eliminate visible artifacts while computing the 

extreme range of pixel contribution values encountered while rendering scenes with 

complex lighting, shadows, and high dynamic range. For all three types of graphics 

shaders, many program instances can be run in parallel, as independent parallel 

threads, because each works on indepen
dent data, produces independent results, 
an
 ects. Independent vertices, primitives, and pixels further enable 
the same graphics program to run o
 erently sized GPUs that pro
 erent 
numbers of vertices, primitives, and pixels in parallel. Graphics programs thus scale 

transparently to GPUs with
 erent amounts of parallelism and performance.
Users program all three logical graphics threads with a common targeted high-
level language. HLSL (high-level shading language) and Cg (C for graphics) are 

commonly used
 ey have C-like syntax and a rich set of library functions for 
matrix operations, trigonometry, interpolation, and texture access an
 ltering, 
but are far from general computing languages: they currently lack general memory 
texture
 A 1D, 2D, or 
3D array that supports 
sampled an
 ltered 
lookups with interpolated 

coordinates.
shader
 A program that 
operates on graphics data 

such as a vertex or a pixel 

fragment.
shading language
 A graphics rendering 

language, usually having 

a data
 ow or streaming 
programming model.

 C.3 Programming GPUs C-
15access, pointer
 le I/O, and recursion. HLSL and Cg assume that programs live 
within a logical graphics pipeline, and thus I/O is implicit. For example, a pixel 
fragment shader may expect the geometric normal and multiple texture coordinates 

to have been interpolated from vertex values by upstrea
 xed-function stages and 
can simply assign a value to the COLOR output parameter to pass it downstream to 

be blended with a pixel at an implied (x, y) position.
 e GPU hardware creates a new independent thread to execute a vertex, 
geometry, or pixel shader program for every vertex, every primitive, and every 

pixel fragment. In video games, the bulk of threads execute pixel shader programs, 

as there are typically 10 to 20 times or more pixel fragments than vertices, and 

complex lighting and shadows require even larger ratios of pixel to vertex shader 

thre
 e graphics shader programming model drove the GPU architecture to 
  ciently execute thousands of independen
 ne-grained threads on many parallel 
processor cores.
Pixel Shader ExampleConsider the following Cg pixel shader program that implements the “environment 

mapping” rendering technique. For each pixel thread, this shader is pass
 ve 
parameters, includin
 oating-point texture image coordinates needed to 
sample the surface color, an
 oating-point vector giving the refection of 
the view direction o
  the surface
 e other three “uniform” parameters do not 
vary from one pixel instance (thread) to th
 e shader looks up color in 
two texture images: a 2D texture access for the surface color, and a 3D texture 

access into a cube map (six images corresponding to the faces of a cube) to obtain 

the external world color corresponding to the refection directio
 en th
 nal 
four-component (red, green, blue, alph
 oating-point color is computed using a 
weighted average called a “lerp” or linear interpolation function.
void refection( float2 texCoord : TEXCOORD0,

 float3 refection_dir : TEXCOORD1,

 out float4 color : COLOR,

 uniform float shiny,

 uniform sampler2D surfaceMap,

 uniform samplerCUBE envMap)

{
// Fetch the surface color from a texture
 float4 surfaceColor = tex2D(surfaceMap, texCoord);
// Fetch reflected color by sampling a cube map  float4 reflectedColor = texCUBE(environmentMap, refection_dir);// Output is weighted average of the two colors
 color = lerp(surfaceColor, refectedColor, shiny);

}
C-16 Appendix C Graphics and Computing GPUs
Although this shader program is only three lines long, it activates a lot of GPU 
hardware. For each texture fetch, the GPU texture subsystem makes multiple 
memory accesses to sample image colors in the vicinity of the sampling coordinates, 

and then interpolates th
 nal result with
 oating-point 
 ltering arit
 e multithreaded GPU executes thousands of these lightweight Cg pixel shader threads 

in parallel, deeply interleaving them to hide texture fetch and memory latency.
Cg focuses the programmer’s view to a single vertex or primitive or pixel, 
which the GPU implements as a single thread; the shader program transparently 

scales to exploit thread parallelism on the available processors. Being application-

sp
 c, Cg provides a rich set of useful data types, library functions, and language 
constructs to express diverse rendering techniques.
Figure C.3.2 shows skin rendered by a fragment pixel shader. Real skin appears 
quit
 erent fro
 esh-color paint because light bounces around a lot before 
re-emerging. In this complex shader, three separate skin layers, each with unique 

subsurface scattering behavior, are modeled to give the skin a visual depth and 

translucency. Scattering can be modeled by a blurring convolution in a fattened 

“texture” space, with red being blurred more than green, and blue blurre
 e FIGURE C.3.2 GPU-rendered image. To give the skin visual depth and translucency, the pixel shader 
program models three separate skin layers, each with unique subsurface scattering behavior. It executes 1400 
instructions to render the red, green, blue, and alpha color components of each skin pixel fragment.

 C.3 Programming GPUs C-
17compiled Cg shader executes 1400 instructions to compute the color of one skin 
pixel.
As GPUs have evolved superior
 oating-point performance and very high 
streaming memory bandwidth for real-time graphics, they have attracted highly 

parallel applications beyond traditional graphics. At
 rst, access to this power 
was available only by couching an application as a graphics-rendering algorithm, 

but this GPGPU approach was o
 en awkward and limiting. More recently, the 
CUDA programming model has provided a far easier way to exploit the scalable 

high-performan
 oating-point and memory bandwidth of GPUs with the C 
programming language.
Programming Parallel Computing Applications
CUDA, Brook, and CAL are programming interfaces for GPUs that are focused 

on data parallel computation rather than on graphics. CAL (Compute Abstraction 

Layer) is a low-level assembler language interface for AMD GPUs. Brook is a 

streaming language adapted for GPUs by Buck et al. [2004]. CUDA, developed 

by NVIDIA [2007], is an extension to the C and C
 languages for scalable 
parallel programming of manycore GPUs and multicore CPU
 e CUDA 
programming model is described below, adapted from an article by Nickolls et al. 

[2008].With the new model the GPU excels in data parallel and throughput computing, 
executing high performance computing applications as well as graphics applications.
Data Parallel Problem Decomposition
To map large computing problem
 ectively to a highly parallel processing 
architecture, the programmer or compiler decomposes the problem into many 

small problems that can be solved in parallel. For example, the programmer 

partitions a large result data array into blocks and further partitions each block into 

elements, such that the result blocks can be computed independently in parallel, 

and the elements within each block are computed in parallel. Figure C.3.3 shows 

a decomposition of a result data array into a 3 
 2 grid of blocks, where each 
block is further decomposed into a 5 
 3 array of elemen
 e two-level parallel 
decomposition maps naturally to the GPU architecture: parallel multiprocessors 

compute result blocks, and parallel threads compute result elements.
 e programmer writes a program that computes a sequence of result data 
grids, partitioning each result grid into coarse-grained result blocks that can be 

computed independently in parallel
 e program computes each result block with 
an array o
 ne-grained parallel threads, partitioning the work among threads so 
that each computes one or more result elements.
Scalable Parallel Programming with CUDA
 e CUDA scalable parallel programming model extends the C and C
 languages to exploit large degrees of parallelism for general applications on highly 

parallel multiprocessors, particularly GPUs. Early experience with CUDA shows 

C-18 Appendix C Graphics and Computing GPUs
that many sophisticated programs can be readily expressed with a few easily 
understood abstractions. Since NVIDIA released CUDA in 2007, developers have 

rapidly developed scalable parallel programs for a wide range of applications, 

including seismic data processing, computational chemistry, linear algebra, sparse 

matrix solvers, sorting, searching, physics models, and visual computin
 ese 
applications scale transparently to hundreds of processor cores and thousands of 

concurrent threads. NVIDIA GPUs with the T
 ed graphics and computing 
architecture (described in Sections C.4 and C.7) run CUDA C programs, and are 

widely available in laptops, PCs, workstations, and server
 e CUDA model is 
also applicable to other shared memory parallel processing architectures, including 

multicore CPUs.
CUDA provides three key abstractions—a 
hierarchy of thread groups, shared 
memories
, and 
barrier synchronization
—that provide a clear parallel structure to 
conventional C code for one thread of the hierarchy. Multiple levels of threads, 

memory, and synchronization prov
 ne-grained data parallelism and thread 
parallelism, nested within coarse-grained 
data parallelism and task pa
 e abstractions guide the programmer to partition the problem into coarse subproblems 

that can be solved independently in parallel, and then int
 ner pieces that can be 
solved in parallel
 e programming model scales transparently to large numbers of 
processor cores: a compiled CUDA program executes on any number of processors, 

and only the runtime system needs to
 know the physical processor count.
Step 1:SequenceBlo(0, Blo(0, Step 2:Result Data Grid 1 ck0) Block(1, 0) Block(1, 1) Block(2, 0) Block(2, 1) Block (1, 1)Elem(0, 0)Elem(1, 0)Elem(2, 0)Elem(3, 0)Elem(4, 0)Elem(0, 1)Elem(1, 1)Elem(2, 1)Elem(3, 1)Elem(4, 1)Elem(0, 2)Elem(1, 2)Elem(2, 2)Elem(3, 2)Elem(4, 2)ck1) Result Data Grid 2 FIGURE C.3.3 Decomposing result data into a grid of blocks of elements to be computed in parallel.
 C.3 Programming GPUs C-
19The CUDA Paradigm
CUDA is a minimal extension of the C and C
 programming languag
 e programmer writes a serial program that calls parallel 
kernels, which may be simple 
functions or full programs. A kernel executes in parallel across a set of parallel 
thre
 e programmer organizes these threads into a hierarchy of thread blocks 
and grids of thread blocks. A 
thread block
 is a set of concurrent threads that can 
cooperate among themselves through barrier synchronization and through shared 

access to a memory space private to the block. A 
grid
 is a set of thread blocks that 
may each be executed independently and thus may execute in parallel.
When invoking a kernel, the programmer sp
 es the number of threads per 
block and the number of blocks comprising the grid. Each thread is given a unique 

thread ID
 number 
threadIdx within its thread block, numbered 
0, 1, 2, ..., blockDim-1, and each thread block is given a unique 
block ID
 number 
blockIdx within its grid. CUDA supports thread blocks containing up to 512 threads. For 

convenience, thread blocks and grids may have 1, 2, or 3 dimensions, accessed via 
.x, .y, and 
.z elds.As a very simple example of parallel programming, suppose that we are given 
two vectors 
x and 
y of 
n oating-point numbers each and that we wish to compute 
the result of 
y  ax  y for some scalar value 
a is is the so-called 
SAXPY kernel 
 ned by the BLAS linear algebra library. Figure C.3.4 shows C code for performing 
this computation on both a serial processor and in parallel using CUDA.
 e __global__ declaration sp
 er indicates that the procedure is a kernel 
entry point. CUDA programs launch parallel kernels with the extended function 

call syntax:
kernel<<<dimGrid, dimBlock>>>(... parameter list ...);where 
dimGrid and 
dimBlock are three-element vectors of type 
dim3 that specify 
the dimensions of the grid in blocks and the dimensions of the blocks in threads, 

respectively. Unsp
 ed dimensions default to one.
In Figure C.3.4, we launch a grid of 
n threads that assigns one thread to each 
element of the vectors and puts 256 threads in each block. Each individual thread 

computes an element index from its thread and block IDs and then performs the 

desired calculation on the corresponding vector elements. Comparing the serial and 

parallel versions of this code, we see that they are strikingly similar.
 is represents 
a fairly common patter
 e serial code consists of a loop where each iteration is 
independent of all the others. Such loops can be mechanically transformed into 

parallel kernels: each loop iteration becomes an independent thread. By assigning 

a single thread to each output element, we avoid the need for any synchronization 

among threads when writing results to memory.
 e text of a CUDA kernel is simply a C function for one sequential thread. 
 us, it is generally straightforward to write and is typically simpler than writing 
parallel code for vector operations. Parallelism is determined clearly and explicitly 

by specifying the dimensions of a grid and its thread blocks when launching a 

kernel.
kernel A program or 
function for one thread, 
designed to be executed 

by many threads.
thread block
 A set 
of concurrent threads 

that execute the same 

thread program and may 

cooperate to compute a 

result.
grid
 A set of thread 
blocks that execute the 

same kernel program.

C-20 Appendix C Graphics and Computing GPUs
Parallel execution and thread management is automatic. All thread creation, 
scheduling, and termination is handle
d for the programmer by the underlying 
system. Indeed, a Tesla architecture GPU performs all thread management directly 
in hardware
 e threads of a block execute concurrently and may synchronize 
at a 
synchronization barrier
 by calling the 
__syncthreads() intrin
 is guarantees that no thread in the block can proceed until all threads in the block 

have reached the barrier
 er passing the barrier, these threads are also guaranteed 
to see all writes to memory performed by threads in the block before the barrier. 

 us, threads in a block may communicate with each other by writing and reading 
per-block shared memory at a synchronization barrier.
Since threads in a block may share memory and synchronize via barriers, they 
will reside together on the same physical processor or multiprocessor
 e number 
of thread blocks can, however, greatly exceed the number of processor
 e CUDA 
thread programming model virtualizes the processors and gives the programmer the 

 exibility to parallelize at whatever granularity is most convenient. Virtualization 
synchronization 
barrier
 reads wait at 
a synchronization barrier 
until all threads in the 

thread block arrive at the 

barrier.
FIGURE C.3.4 Sequential code (top) in C versus parallel code (bottom) in CUDA for SAXPY 
(see Chapter 6). CUDA parallel threads replace the C serial loop—each thread computes the same result 
as one loop iteratio
 e parallel code computes 
n results with 
n threads organized in blocks of 256 threads.
Computing y = ax + y with a serial loop:void saxpy_serial(int n, float alpha, float *x, float *y){
 for(int i = 0; i<n; ++i)

  y[i] = alpha*x[i] + y[i];

}
// Invoke serial SAXPY kernel
saxpy_serial(n, 2.0, x, y);Computing y = ax + y in parallel using CUDA:
__global__void saxpy_parallel(int n, float alpha, float *x, float *y)
{
 int i = blockIdx.x*blockDim.x + threadIdx.x;
 if( i<n ) y[i] = alpha*x[i] + y[i];
}// Invoke parallel SAXPY kernel (256 threads per block)int nblocks = (n + 255) / 256;
saxpy_parallel<<<nblocks, 256>>>(n, 2.0, x, y);
 C.3 Programming GPUs C-
21into threads and thread blocks allows intuitive problem decompositions, as the 
number of blocks can be dictated by the size of the data being processed rather than 

by the number of processors in the system. It also allows the same CUDA program 

to scale to widely varying numbers of processor cores.
To manage this processing element virtualization and provide scalability, CUDA 
requires that thread blocks be able to execute independently. It must be possible to 

execute blocks in any order, in parallel or in ser
 erent blocks have no means of 
direct communication, although they may 
coordinate
 their activities using 
atomic 
memory operations
 on the global memory visible to all threads—by atomically 
incrementing queue pointers, for example
 is independence requirement allows 
thread blocks to be scheduled in any order across any number of cores, making 

the CUDA model scalable across an arbitrary number of cores as well as across a 

variety of parallel architectures. It also helps to avoid the possibility of deadlock. 

An application may execute multiple grids either independently or dependently. 

Independent grids may execute concurrently, giv
  cient hardware resources. 
Dependent grids execute sequentially, with an implicit interkernel barrier between 

them, thus guaranteeing that all blocks of th
 rst grid complete before any block 
of the second, dependent grid begins.
 reads may access data from multiple memory spaces during their execution. 
Each thread has a private 
local memory
. CUDA uses local memory for thread-
private variables that do no
 t in the thread’s registers, as well as for stack frames 
and register spilling. Each thread block has a 
shared memory
, visible to all threads 
of the block, which has the same lifetime as the block. Finally, all threads have 

access to the same 
global memory
. Programs declare variables in shared and 
global memory with the 
__shared__ and 
__device__ type qualifers. On a 
Tesla architecture GPU, these memory spaces correspond to physically separate 

memories: per-block shared memory is a low-latency on-chip RAM, while global 

memory resides in the fast DRAM on the graphics board.
Shared memory is expected to be a low-latency memory near each processor, 
much like an L1 cache. It can therefore provide high-performance communication 

and data sharing among the threads of a thread block. Since it has the same lifetime 

as its corresponding thread block, kernel code will typically initialize data in shared 

variables, compute using shared variables, and copy shared memory results to 

global memory
 read blocks of sequentially dependent grids communicate via 
global memory, using it to read input and write results.
Figure C.3.5 shows diagrams of the nested levels of threads, thread blocks, 
and grids of thread blocks. It further shows the corresponding levels of memory 

sharing: local, shared, and global memories for per-thread, per-thread-block, and 

per-application data sharing.
A program manages the global memory 
space visible to kernels through calls 
to the CUDA runtime, such as 
cudaMalloc() and 
cudaFree(). Kernels may 
execute on a physically separate device, as is the case when running kernels on 

the GPU. Consequently, the application must use 
cudaMemcpy() to copy data 
between the allocated space and the host system memory.
atomic memory 
operation
 A memory 
read, modify, write 
operation sequence that 

completes without any 

intervening access.
local memory
 Per-
thread local memory 

private to the thread.
shared memory
 Per-
block memory shared by 

all threads of the block.
global memory
 Per-
application memory 

shared by all threads.

C-22 Appendix C Graphics and Computing GPUs
 e CUDA programming model is simi
lar in style to the familiar 
single- 
program multiple data (SPMD)
 model—it expresses parallelism explicitly, and 
each kernel executes o
 xed number of threads. However, CUDA is more
 exible 
than most realizations of SPMD, because each kernel call dynamically creates a 
new grid with the right number of thread blocks and threads for that application 

step
 e programmer can use a convenient degree of parallelism for each kernel, 
rather than having to design all phases of the computation to use the same number 

of threads. Figure C.3.6 shows an example of an SPMD-like CUDA code sequence. 

It
 rst instantiates 
kernelF on a 2D grid of 3 
 2 blocks where each 2D thread 
block consists of 5 
 3 threads. It then instantiates 
kernelG on a 1D grid of four 
1D thread blocks with six threads each. Because 
kernelG depends on the results 
of 
kernelF, they are separated by an interkernel synchronization barrier.
 e concurrent threads of a thread block expre
 ne-grained data parallelism 
and thread pa
 e independent thread blocks of a grid express coarse-
single-program 
multiple data 

(SPMD)
 A style of 
parallel programming 
model in which all 

threads execute the same 

program. SPMD threads 

typically coordinate with 

barrier synchronization.
Threadper-Thread Local MemoryThread Blockper-BlockShared MemoryGrid 0 . . . Grid 1 . . . Global MemorySequenceInter-Grid SynchronizationFIGURE C.3.5 Nested granularity levels—thread, thread block, and grid—have 
corresponding memory sharing levels—local, shared, and global.
 Per-thread local memory is 
private to the thread. Per-block shared memory is shared by all threads of the block. Per-application global 
memory is shared by all threads.

 C.3 Programming GPUs C-
23grained data parallelism. Independent grids 
express coarse-grained task parallelism. 
A kernel is simply C code for one thread of the hierarchy.
RestrictionsFo
  ciency, and to simplify its implementation, the CUDA programming model 
has some restriction
 reads and thread blocks may only be created by invoking 
a parallel kernel, not from within a parallel kernel. Together with the required 
independence of thread blocks, this makes it possible to execute CUDA programs 
FIGURE C.3.6 Sequence of kernel
 F instantiated on a 2D grid of 2D thread blocks, an interkernel 
synchronization barrier, followed by kernel 
G on a 1D grid of 1D thread blocks.
kernelG 1D Grid is 4 thread blocks; each block is 6 threadsSequenceInterkernel Synchronization Barrier  
Block 2Thread 5Thread 0 Thread 1 Thread 2Thread 3Thread 4kernelF<<<(3, 2), (5, 3)>>>(params);kernelF 2D Grid is 3  2 thread blocks; each block is 5  3 threadsBlock 1, 1Thread 0, 0Thread 1, 0Thread 2, 0Thread 3, 0Thread 4, 0Thread 0, 1Thread 1, 1Thread 2, 1Thread 3, 1Thread 4, 1Thread 0, 2Thread 1, 2Thread 2, 2Thread 3, 2Thread 4, 2Block 0, 1Block 2, 1Block 1, 1Block 0, 0Block 2, 0Block 1, 0kernelG<<<4, 6>>>(params);Block 0Block 2Block 1Block 3
C-24 Appendix C Graphics and Computing GPUs
with a simple scheduler that introduces minimal runtime overhead. In fact, the 
Tesla GPU architecture implements 
hardware
 management and scheduling of 
threads and thread blocks.
Task parallelism can be expressed at the thread block level bu
  cult to 
express within a thread block because thread synchronization barriers operate on 

all the threads of the block. To enable CUDA programs to run on any number of 

processors, dependencies among thread blocks within the same kernel grid are not 

allowed—blocks must execute independently. Since CUDA requires that thread 

blocks be independent and allows blocks to be executed in any order, combining 

results generated by multiple blocks must in general be done by launching a second 

kernel on a new grid of thread blocks (although thread blocks may 
coordinate
 their 
activities using atomic memory operations on the global memory visible to all 

threads—by atomically incrementing queue pointers, for example).
Recursive function calls are not currently allowed in CUDA kernels. Recursion 
is unattractive in a massively parallel kernel, because providing stack space for the 

tens of thousands of threads that may be active would require substantial amounts 

of memory. Serial algorithms that are normally expressed using recursion, such as 

quicksort, are typically best implemented using nested data parallelism rather than 

explicit recursion.
To support a heterogeneous system architecture combining a CPU and a 
GPU, each with its own memory system, CUDA programs must copy data and 

results between host memory and device memory
 e overhead of CPU–GPU 
interaction and data transfers is minimized by using DMA block transfer engines 

and fast interconnects. Compute-intensive problems large enough to need a GPU 

performance boost amortize the overhead better than small problems.
Implications for Architecture e parallel programming models for graphics and computing have driven 
GPU architecture to be
 erent than CPU architecture
 e key aspects of GPU 
programs driving GPU processor architecture are:
 Extensive use of
 ne-grained data parallelism:
 Shader programs describe how 
to process a single pixel or vertex, and CUDA programs describe how to 

compute an individual result.
 Highly threaded programming model:
 A shader thread program processes a 
single pixel or vertex, and a CUDA thread program may generate a single 

result. A GPU must create and execute millions of such thread programs per 

frame, at 60 frames per second.
 Scalability:
 A program must automatically increase its performance when 
provided with additional processors, without recompiling.
 Intensiv
 oating-point (or integer) computation
. Support of high throughput computations
.
 C.4 Multithreaded Multiprocessor Architecture C-
25 C.4 Multithreaded Multiprocessor 
ArchitectureTo addr
 erent market segments, GPUs implement scalable numbers of multi-
processors—in fact, GPUs are multiprocessors composed of multiprocessors. 
Furthermore, each multiprocessor is highly multithreaded to execute man
 ne-grained vertex and pixel shader thre
  ciently. A quality basic GPU has two to 
four multiprocessors, while a gaming enthusiast’s GPU or computing platform has 

dozens of th
 is section looks at the architecture of one such multithreaded 
multiprocessor, a simp
 ed version of the NVIDIA Tesla 
streaming multiprocessor 
(SM) described in Section C.7.
Why use a multiprocessor, rather than several independent processor
 e parallelism within each multiprocessor provides localized high performance and 

supports extensive multithreading for th
 ne-grained parallel programming 
models described in Sectio
 e individual threads of a thread block execute 
together within a multiprocessor to share dat
 e multithreaded multiprocessor 
design we describe here has eight scalar processor cores in a tightly coupled 

architecture, and executes up to 512 threads (the SM described in Section C.7 

executes up to 768 threads). For area and pow
  ciency, the multiprocessor shares 
large complex units among the eight processor cores, including the instruction 

cache, the multithreaded instruction unit, and the shared memory RAM.
Massive MultithreadingGPU processors are highly multithreaded to achieve several goals:
 Cover the latency of memory loads and texture fetches from DRAM
 Support 
 ne-grained parallel graphics shader programming models
 Support 
 ne-grained parallel computing programming models
 Virtualize the physical processors as threads and thread blocks to provide 
transparent scalability
 Simplify the parallel programming model to writing a serial program for one 
thread
Memory and texture fetch latency can require hundreds of processor clocks, 
because GPUs typically have small streaming caches rather than large working-set 

caches like CPUs. A fetch request generally requires a full DRAM access latency 

plus interconnect and b
 ering latency. Multithreading helps cover the latency with 
useful computing—while one thread is waiting for a load or texture fetch to complete, 

the processor can execute another thread
 e 
 ne-grained parallel programming 
models provide literally thousands of independent threads that can keep many 

processors busy despite the long memory latency seen by individual threads.

C-26 Appendix C Graphics and Computing GPUs
A graphics vertex or pixel shader program is a program for a single thread that 
processes a vertex or a pixel. Similarly, a CUDA program is a C program for a 
single thread that computes a result. Graphics and computing programs instantiate 

many parallel threads to render complex images and compute large result arrays. 

To dynamically balan
 ing vertex and pixel shader thread workloads, each 
multiprocessor concurrently executes multip
 erent thread programs and 
 erent types of shader programs.
To support the independent vertex, primitive, and pixel programming model of 
graphics shading languages and the single-thread programming model of CUDA 

C/C, each GPU thread has its own private registers, private per-thread memory, 
program counter, and thread execution state, and can execute an independent code 

path. To
  ciently execute hundreds of concurrent lightweight threads, the GPU 
multiprocessor is hardware multithreaded—it manages and executes hundreds 

of concurrent threads in hardware without scheduling overhead. Concurrent 

threads within thread blocks can synchronize at a barrier with a single instruction. 

Lightweight thread creation, zero-overhead thread scheduling, and fast barrier 

synchronizatio
  ciently support very
 ne-grained parallelism.
Multiprocessor Architecture ed graphics and computing multiprocessor executes vertex, geometry, and 
pixel fragment shader programs, and parallel computing programs. As Figure C.4.1 

shows, the example multiprocessor consists of eight 
scalar processor
 (SP) cores each 
with a large multithreaded 
regist
 le (RF), two 
special function units
 (SFUs), a 
multithreaded instruction unit, an instruction cache, a read-only constant cache, 

and a shared memory.
 e 16 KB shared memory holds graphics data b
 ers and shared computing 
data. CUDA variables declared as 
__shared__ reside in the shared memory. To 
map the logical graphics pipeline workload through the multiprocessor multiple 

times, as shown in Section C.2, vertex, geometry, and pixel threads have independent 

input and output b
 ers, and workloads arrive and depart independently of thread 
execution.
Each SP core contains scalar integer an
 oating-point arithmetic units that 
execute most instruction
 e SP is hardware multithreaded, supporting up to 
64 threads. Each pipelined SP core executes one scalar instruction per thread per 

clock, which ranges from 1.2 GHz to 1.6 GH
 erent GPU products. Each SP 
core has a large RF of 1024 general-purpose 32-bit registers, partitioned among its 

assigned threads. Programs declare their re
gister demand, typically 16 to 64 scalar 
32-bit registers per thread
 e SP can concurrently run many threads that use 
a few registers or fewer threads that use more register
 e compiler optimizes 
register allocation to balance the cost of spilling registers versus the cost of fewer 

threads. Pixel shader programs o
 en use 16 or fewer registers, enabling each SP to 
run up to 64 pixel shader threads to cover long-latency texture fetches. Compiled 

CUDA programs o
 en need 32 registers per thread, limiting each SP to 32 threads, 
which limits such a kernel program to 256 threads per thread block on this example 

multiprocessor, rather than its maximum of 512 threads.

 C.4 Multithreaded Multiprocessor Architecture C-
27 e pipelined SFUs execute thread instructions that compute special functions 
and interpolate pixel attributes from primitive vertex attribut
 ese instructions 
can execute concurrently with instructions on the SP
 e SFU is described later.
 e multiprocessor executes texture fetch instructions on the texture unit via the 
texture interface, and uses the memory interface for external memory load, store, 
and atomic access instruction
 ese instructions can execute concurrently with 
instructions on the SPs. Shared memory access uses a low-latency interconnection 

network between the SP processors and the shared memory banks.
Single-Instruction Multiple-Thread (SIMT)
To manage and execute hundreds of threads running sev
 erent programs 
  ciently, the multiprocessor employs a 
single-instruction multiple-thread 
(SIMT)
 architecture. It creates, manages, schedules, and executes concurrent threads 
in groups of parallel threads called 
warps
 e term 
warp
 originates from weaving, 
the
 rst parallel thread technology
 e photograph in Figure C.4.2 shows a warp of 
parallel threads emerging from a loo
 is example multiprocessor uses a SIMT 
warp size of 32 threads, executing four threads in each of the eight SP cores over four 
single-instruction 
multiple-thread 

(SIMT)
 A processor 
architecture that applies 
one instruction to 

multiple independent 

threads in parallel.
warp
 e set of parallel 
threads that execute the 

same instruction together 

in a SIMT architecture.
Instruction CacheMultithreaded Instruction UnitMultithreaded MultiprocessorConstant CacheSFUSFUSPRFSPRFSPRFSPRFSPRFSPRFSPRFSPRFShared MemoryTextureInterfaceMemoryInterfaceMultiprocessorControllerOutputInterfaceInterconnection NetworkInputInterfaceWork InterfaceFIGURE C.4.1 Multithreaded multiprocessor with eight scalar processor (SP) cores. e eight SP cores each have a large multithreaded regist
 le (RF) and share an instruction cache, multithreaded 
instruction issue unit, constant cache, two special function units (SFUs), interconnection network, and a 
multibank shared memory.

C-28 Appendix C Graphics and Computing GPUs
cloc
 e Tesla SM multiprocessor described in Section C.7 also uses a warp size 
of 32 parallel threads, executing four threads per SP core fo
  ciency on plentiful 
pixel threads and computing thre
 read blocks consist of one or more warps.
 is example SIMT multiprocessor manages a pool of 16 warps, a total of 512 
threads. Individual parallel threads composing a warp are the same type and start 
together at the same program address, but are otherwise free to branch and execute 

independently. At each instruction issue time, the SIMT multithreaded instruction 

unit selects a warp that is ready to execute its next instruction, and then issues that 

instruction to the active threads of that warp. A SIMT instruction is broadcast 

synchronously to the active parallel threads of a warp; individual threads may be 

inactive due to independent branching or predication. In this multiprocessor, each 

SP scalar processor core executes an instruction for four individual threads of a 

warp using four clocks, r
 ecting the 4:1 ratio of warp threads to cores.
SIMT processor architecture is akin to 
single-instruction multiple data
 (SIMD) 
design, which applies one instruction to multiple data lanes, bu
 ers in that 
SIMT applies one instruction to multiple independent threads in parallel, not just 
warp 8 instruction 11warp 1 instruction 42warp 3 instruction 95warp 8 instruction 12timeSIMT multithreadedinstruction schedulerwarp 1 instruction 43warp 3 instruction 96Photo: Judy SchoonmakerFIGURE C.4.2 SIMT multithreaded warp scheduling.
 e scheduler selects a ready warp and issues 
an instruction synchronously to the parallel threads composing the warp. Because warps are independent, 
the scheduler may selec
 erent warp each time.

 C.4 Multithreaded Multiprocessor Architecture C-
29to multiple data lanes. An instruction for a SIMD processor controls a vector of 
multiple data lanes together, whereas an instruction for a SIMT processor controls 

an individual thread, and the SIMT instruction unit issues an instruction to a warp 

of independent parallel threads fo
  ciency. 
 e SIMT processo
 nds data-level 
parallelism among threads at runtime, analogous to the way a superscalar processor 

 nds instruction-level parallelism among instructions at runtime.
A SIMT processor realizes f
  ciency and performance when all threads 
of a warp take the same execution path. If threads of a warp diverge via a data-

dependent conditional branch, execution serializes for each branch path taken, and 

when all paths complete, the threads converge to the same execution path. For equal 

length paths, a divergent if-else code bloc
  cient. 
 e multiprocessor 
uses a branch synchronization stack to manage independent threads that diverge 

and converge
 erent warps execute independently at full speed regardless of 
whether they are executing common or disjoint code paths. As a result, SIMT 

GPUs are dramatically mor
  cient and 
 exible on branching code than earlier 
GPUs, as their warps are much narrower than the SIMD width of prior GPUs.
In contrast with SIMD vector architectures, SIMT enables programmers 
to write thread-level parallel code for individual independent threads, as well 

as data-parallel code for many coordinated threads. For program correctness, 

the programmer can essentially ignore the SIMT execution attributes of warps; 

however, substantial performance improvements can be realized by taking care that 

the code seldom requires threads in a warp to diverge. In practice, this is analogous 

to the role of cache lines in traditional codes: cache line size can be safely ignored 

when designing for correctness but must be considered in the code structure when 

designing for peak performance.
SIMT Warp Execution and Divergence
 e SIMT approach of scheduling independent warps is mor
 exible than the 
scheduling of previous GPU architectures. A warp comprises parallel threads of 

the same type: vertex, geometry, pixel, or compute
 e basic unit of pixel fragment 
shader processing is the 2-by-2 pixel quad implemented as four pixel shader threads. 

 e multiprocessor controller packs the pixel quads into a warp. It similarly groups 
vertices and primitives into warps, and packs computing threads into a warp. A 

thread block comprises one or more war
 e SIMT design shares the instruction 
fetch and issue uni
  ciently across parallel threads of a warp, but requires a full 
warp of active threads to get full performa
  ciency.
 is 
 ed multiprocessor schedules and executes multiple warp types 
concurrently, allowing it to concurrently execute vertex and pixel warps. Its warp 

scheduler operates at less than the processor clock rate, because there are four thread 

lanes per processor core. During each scheduling cycle, it selects a warp to execute 

a SIMT warp instruction, as shown in Figure C.4.2. An issued warp-instruction 

executes as four sets of eight threads over four processor cycles of throughpu
 e processor pipeline uses several clocks of latency to complete each instruction. If the 

number of active warps times the clocks per warp exceeds the pipeline latency, the 

C-30 Appendix C Graphics and Computing GPUs
programmer can ignore the pipeline latency. For this multiprocessor, a round-robin 
schedule of eight warps has a period of 32 cycles between successive instructions 

for the same warp. If the program can keep 256 threads active per multiprocessor, 

instruction latencies up to 32 cycles can be hidden from an individual sequential 

thread. However, with few active warps, the processor pipeline depth becomes 

visible and may cause processors to stall.
A challenging design problem is implementing zero-overhead warp scheduling 
for a dynamic mix o
 erent warp programs and program typ
 e instruction 
scheduler must select a warp every four clocks to issue one instruction per clock 

per thread, equivalent to an IPC of 1.0 per processor core. Because warps are 

independent, the only dependences are among sequential instructions from the 

same warp
 e scheduler uses a register dependency scoreboard to qualify warps 
whose active threads are ready to execute an instruction. It prioritizes all such ready 

warps and selects the highest priority one for issue. Prioritization must consider 

warp type, instruction type, and the desire to be fair to all active warps.
Managing Threads and Thread Blocks e multiprocessor controller and instruction unit manage threads and thread 
bloc
 e controller accepts work requests and input data and arbitrates access 
to shared resources, including the texture unit, memory access path, and I/O 

paths. For graphics workloads, it creates and manages three types of graphics 

threads concurrently: vertex, geometry, and pixel. Each of the graphics work 

types has independent input and output paths. It accumulates and packs each of 

these input work types into SIMT warps of parallel threads executing the same 

thread program. It allocates a free warp, allocates registers for the warp threads, 

and starts warp execution in the multiprocessor. Every program declares its per-

thread register demand; the controller starts a warp only when it can allocate the 

requested register count for the warp threads. When all the threads of the warp 

exit, the controller unpacks the results and frees the warp registers and resources.
 e controller creates 
cooperative thread arrays (CTAs)
 which implement 
CUDA thread blocks as one or more warps of parallel threads. It creates a CTA 

when it can create all CTA warps and allocate all CTA resources. In addition to 

threads and registers, a CTA requires allocating shared memory and barriers. 

 e program declares the required capacit
ies, and the controller waits until it can 
allocate those amounts before launching the CT
 en it creates CTA warps at the 
warp scheduling rate, so that a CTA program starts executing immediately at full 

multiprocessor performance.
 e controller monitors when all threads of a CTA 
have exited, and frees the CTA shared resources and its warp resources.
Thread Instructions
 e SP thread processors execute scalar instructions for individual threads, unlike 
earlier GPU vector instruction architectures, which executed four-component 

vector instructions for each vertex or pixel shader program. Vertex programs 
cooperative thread 
array (CTA)
 A set 
of concurrent threads 
that executes the same 

thread program and may 

cooperate to compute 

a result. A GPU CTA 

implements a CUDA 

thread block.

 C.4 Multithreaded Multiprocessor Architecture C-
31generally compute (x, y, z, w) position vectors, while pixel shader programs 
compute (red, green, blue, alpha) color vectors. However, shader programs are 

becoming longer and more scalar, and it is increasingl
  cult to fully occupy 
even two components of a legacy GPU four-component vector architecture. In 

 ect, the SIMT architecture paralle
lizes across 32 independent pixel threads, 
rather than parallelizing the four vector components within a pixel. CUDA C/C
 programs have predominantly scalar code per thread. Previous GPUs employed 

vector packing (e.g., combining subvectors of work to ga
  ciency) but that 
complicated the scheduling hardware as well as the compiler. Scalar instructions 

are simpler and compiler friendly. Texture instructions remain vector based, taking 

a source coordinate vector and returnin
 ltered color vector.
To support multiple GPUs wit
 erent binary microinstruction formats, high-
level graphics and computing language compilers generate intermediate assembler-

level instructions (e.g., Direct3D vector instructions or PTX scalar instructions), 

which are then optimized and translated to binary GPU microinstructions. 

 e NVIDIA PTX (parallel thread execution) instruction s
 nition [2007] 
provides a stable target ISA for compilers, and provides compatibility over several 

generations of GPUs with evolving binary microinstruction-set architectur
 e optimizer readily expands Direct3D vector instructions to multiple scalar binary 

microinstructions. PTX scalar instructions translate nearly one to one with scalar 

binary microinstructions, although some PTX instructions expand to multiple 

binary microinstructions, and multiple PTX instructions may fold into one binary 

microinstruction. Because the intermediate assembler-level instructions use virtual 

registers, the optimizer analyzes data dependencies and allocates real register
 e optimizer eliminates dead code, folds instructions together when feasible, and 

optimizes SIMT branch diverge and converge points.
Instruction Set Architecture (ISA)
 e thread ISA described here is a simp
 ed version of the Tesla architecture 
PTX ISA, a register-based scalar instruction set comprisin
 oating-point, integer, 
logical, conversion, special function
 ow control, memory access, and texture 
operations. Figure C.4.3 lists the basic PTX GPU thread instructions; see the 

NVIDIA PTX sp
 cation [2007] for detai
 e instruction format is:
opcode.type d, a, b, c;where 
d is the destination operand, 
a, b, c are source operands, and 
.type is one of:
Type.type Specifer
Untyped bits 8, 16, 32, and 64 bits.b8, .b16, .b32, .b64
Unsigned integer 8, 16, 32, and 64 bits.u8, .u16, .u32, .u64

Signed integer 8, 16, 32, and 64 bits.s8, .s16, .s32, .s64

Floating-point 16, 32, and 64 bits.f16, .f32, .f64

C-32 Appendix C Graphics and Computing GPUs
FIGURE C.4.3 Basic PTX GPU thread instructions.
Basic PTX GPU Thread Instructions
GroupInstructionExampleMeaningComments
Arithmeticarithmetic .type = .s32, .u32, .f32, .s64, .u64, .f64add.typeadd.f32 d, a, bd = a + b;
sub.typesub.f32 d, a, bd = a – b;
mul.typemul.f32 d, a, bd = a * b;
mad.typemad.f32 d, a, b, cd = a * b + c;
multiply-adddiv.
typediv.f32 d, a, bd = a / b;
multiple microinstructions
rem.typerem.u32 d, a, bd = a % b;
integer remainderabs.typeabs.f32 d, ad = |a|;
neg.typeneg.f32 d, ad = 0 - a;
min.typemin.f32 d, a, bd = (a < b)? a:b;
ßoating selects non-NaNmax.typemax.f32 d, a, bd = (a > b)? a:b;
ßoating selects non-NaNsetp.cmp.typesetp.lt.f32 p, a, bp = (a < b);
compare and set predicatenumeric .cmp =eq, ne, lt, le, gt, ge; unordered cmp = equ, neu, ltu, leu, gtu, geu, num, nanmov.
typemov.b32 d, ad = a;
move
selp.typeselp.f32 d, a, b, pd = p? a: b;
select with predicatecvt.dtype.atypecvt.f32.s32 d, ad = convert(a);
convert atype to dtype
Special Functionspecial .type = .f32 (some .f64)rcp.typercp.f32 d, ad = 1/a;
reciprocalsqrt
.typesqrt.f32 d, ad = sqrt(a);
square rootrsqrt
.typersqrt.f32 d, ad = 1/sqrt(a);
reciprocal square rootsin.typesin.f32 d, ad = sin(a);
sinecos.typecos.f32 d, ad = cos(a);
cosinelg2.typelg2.f32 d, ad = log(a)/log(2)
binary logarithm
ex2.typeex2.f32 d, ad = 2 ** a;
binary exponential
Logicallogic. type = .pred, .b32, .b64and.typeand.b32 d, a, bd = a & b;
or.
typeor.b32 d, a, bd = a | b;
xor.
typexor.b32 d, a, bd = a ^ b;
not.typenot.b32 d, a, bd = ~a;
oneÕs complement
cnot.typecnot.b32 d, a, bd = (a==0)? 1:0;
C logical notshl.typeshl.b32 d, a, bd = a << b;
shift leftshr.
typeshr.s32 d, a, bd = a >> b;
shift rightMemory

Accessmemory 
.space = .global, .shared, .local, .const; .type = .b8, .u8, .s8, .b16, .b32, .b64ld.space.typeld.global.b32 d, [a+off]d = *(a+off);
load from memory 
spacest.space.typest.shared.b32 [d+off], a*(d+off) = a;
store to memory 
spacetex.nd.dtyp.btypetex.2d.v4.f32.f32 d, a, bd = tex2d(a, b);
texture lookupatom.spc.op.typeatom.global.add.u32 d,[a], b atom.global.cas.b32 d,[a], b, catomic { d = *a;   *a = op(*a, b); }
atomic read-modify-write  
operationatom .op =and, or, xor, add, min, max, exch, cas;.spc = .global; .type = .b32Control
Flowbranch@p bra targetif (p) goto 
target;conditional branchcallcall (ret), func, (params)ret = func(params);
call functionretretreturn;
return from function call
bar.sync
bar.sync dwait for threads
barrier synchronization
exitexitexit;
terminate thread execution

 C.4 Multithreaded Multiprocessor Architecture C-
33Source operands are scalar 32-bit or 64-bit values in registers, an immediate 
value, or a constant; predicate operands 
are 1-bit Boolean values. Destinations are 
registers, except for store to memory. Instructions are predicated by pr
 xing them 
with 
@p or 
@!p, where 
p is a predicate register. Memory and texture instructions 
transfer scalars or vectors of two to four components, up to 128 bits in total. PTX 
instructions specify the behavior of one thread.
 e PTX arithmetic instructions operate on 32-bit and 64-bi
 oating-point, 
signed integer, and unsigned integer 
types. Recent GPUs support 64-bit double 
precisio
 oating-point; see Section C.6. On current GPUs, PTX 64-bit integer 
and logical instructions are translated to two or more binary microinstructions 

that perform 32-bit operation
 e GPU special function instructions are limited 
to 32-bi
 oating-point. 
 e thread contro
 ow instructions are conditional 
branch, function 
call and 
return, thread 
exit, and 
bar.sync (barrier 
synchronizatio
 e conditional branch instruction 
@p bra target uses a 
predicate register 
p (or 
!p) previously set by a compare and set predicate 
setp instruction to determine whether the thread takes the branch or not. Other 

instructions can also be predicated on
 a predicate register being true or false.
Memory Access Instructions
 e tex instruction fetches an
 lters texture samples from 1D, 2D, and 3D 
texture arrays in memory via the texture subsystem. Texture fetches generally use 

interpolat
 oating-point coordinates to address a texture. Once a graphics pixel 
shader thread computes its pixel fragment color, the raster operations processor 

blends it with the pixel color at its assigned (x, y) pixel position and writes th
 nal 
color to memory.
To support computing and C/C
 language needs, the Tesla PTX ISA 
implements memory load/store instructions. It uses integer byte addressing with 

register plus o
 set address arithmetic to facilitate conventional compiler code 
optimizations. Memory load/store instructions are common in processors, but are 

 cant new capability in the Tesla architecture GPUs, as prior GPUs provided 
only the texture and pixel accesses required by the graphics APIs.
For computing, the load/store instructions access three read/write memory 
spaces that implement the corresponding CUDA memory spaces in Section C.3:
 Local memory for per-thread private addressable temporary data 
(implemented in external DRAM)
 Shared memory for low-latency access to data shared by cooperating threads 
in the same CTA/thread block (implemented in on-chip SRAM)
 Global memory for large data sets shared by all threads of a computing 
application (implemented in external DRAM)
 e memory load/store instructions 
ld.global, st.global, ld.shared, st.shared, ld.local, and 
st.local access the global, shared, and local 
memory spaces. Computing programs use the fast barrier synchronization 

instruction 
bar.sync to synchronize threads within a CTA/thread block that 
communicate with each other via shared and global memory.

C-34 Appendix C Graphics and Computing GPUs
To improve memory bandwidth and reduce overhead, the local and global load/
store instructions coalesce individual parallel thread requests from the same SIMT 
warp together into a single memory block request when the addresses fall in the 

same block and meet alignment criteri
a. Coalescing memory requests provides a 
 cant performance boost over separate requests from individual thre
 e multiprocessor’s large thread count, together with support for many outstanding 

load requests, helps cover load-to-use latency for local and global memory 

implemented in external DRAM.
 e latest Tesla architecture GPUs also prov
  cient atomic memory operations 
on memory with the 
atom.op.u32 instructions, including integer operations 
add, min, max, and, or, xor, exchange, and cas (compare-and-swap) operations, 
facilitating parallel reductions and parallel data structure management.
Barrier Synchronization for Thread Communication
Fast barrier synchronization permits CUDA programs to communicate frequently 

via shared memory and global memory by simply calling 
__syncthreads(); as 
part of each interthread communication step
 e synchronization intrinsic function 
generates a single 
bar.sync instruction. However, implementing fast barrier 
synchronization among up to 512 threads per CUDA thread block is a challenge.
Grouping threads into SIMT warps of 32 threads reduces the synchronization 
  culty by a factor o
 reads wait at a barrier in the SIMT thread scheduler so 
they do not consume any processor cycles while waiting. When a thread executes 

a bar.sync instruction, it increments the barrier’s thread arrival counter and the 
scheduler marks the thread as waiting at the barrier. Once all the CTA threads 

arrive, the barrier counter matches the expected terminal count, and the scheduler 

releases all the threads waiting at the barrier and resumes executing threads.
Streaming Processor (SP) e multithreaded streaming processor (SP) core is the primary thread instruction 
processor in the multiprocessor. Its regist
 le (RF) provides 1024 scalar 32-
bit registers for up to 64 threads. It executes all the fundamenta
 oating-point 
operations, including 
add.f32, mul.f32, mad.f32 oating multiply-add), 
min.f32, max.f32, and 
setp.f32 oating compare and set predicat
 e 
 oating-
point add and multiply operations are compatible with the IEEE 754 standard 

for single precision FP numbers, including not-a-number (NaN) an
 nity 
val
 e SP core also implements all of the 32-bit and 64-bit integer arithmetic, 
comparison, conversion, and logical PTX instructions shown in Figure C.4.3.
 e 
 oating-point 
add and 
mul operations employ IEEE round-to-nearest-even 
as the default rounding mode
 e mad.f32 oating-point multiply-add operation 
performs a multiplication with truncation, followed by an addition with round-

to-nearest-ev
 e S
 ushes input denormal operands to sign-preserved-zero. 
Results that under
 ow the target output exponent range ar
 ushed to sign-
preserved-zero a
 er rounding.

 C.4 Multithreaded Multiprocessor Architecture C-
35Special Function Unit (SFU)Certain thread instructions can execute on the SFUs, concurrently with other 
thread instructions executing on the SP
 e SFU implements the special function 
instructions of Figure C.4.3, which compute 32-bi
 oating-point approximations 
to reciprocal, reciprocal square root, and key transcendental functions. It also 

implements 32-bi
 oating-point planar attribute interpolation for pixel shaders, 
providing accurate interpolation of attributes such as color, depth, and texture 

coordinates.
Each pipelined SFU generates one 32-bi
 oating-point special function result 
per cycle; the two SFUs per multiprocessor execute special function instructions 

at a quarter the simple instruction rate of the eight SP
 e SFUs also execute the 
mul.f32 multiply instruction concurrently with the eight SPs, increasing the peak 
computation rate up to 50% for threads with a suitable instruction mixture.
For functional evaluation, the Tesla architecture SFU employs quadratic 
interpolation based on enhanced minimax approximations for approximating the 

reciprocal, reciprocal square-root, log
2x, 2x, and sin/cos function
 e accuracy of 
the function estimates ranges from 22 to 24 mantissa bits. See Section C.6 for more 

details on SFU arithmetic.
Comparing with Other Multiprocessors
Compared with SIMD vector architectures such as x86 SSE, the SIMT multiprocessor 

can execute individual threads independently, rather than always executing them 

together in synchronous groups. SIMT hardwar
 nds data parallelism among 
independent threads, whereas SIMD hardware requires the so
 ware to express 
data parallelism explicitly in each vector instruction. A SIMT machine executes a 

warp of 32 threads synchronously when the threads take the same execution path, 

yet can execute each thread independently when they diverge
 e advantage is 
 cant because SIMT programs and instructions simply describe the behavior 
of a single independent thread, rather than a SIMD data vector of four or more 

data lanes. Yet the SIMT multiprocessor has SIMD-lik
  ciency, spreading the 
area and cost of one instruction unit across the 32 threads of a warp and across the 

eight streaming processor cores. SIMT provides the performance of SIMD together 

with the productivity of multithreading, avoiding the need to explicitly code SIMD 

vectors for edge conditions and partial divergence.
 e SIMT multiprocessor imposes little overhead because it is hardware 
multithreaded with hardware barrier synchronizatio
 at allows graphics 
shaders and CUDA threads to express ver
 ne-grained parallelism. Graphics and 
CUDA programs use threads to expr
 ne-grained data parallelism in a per-
thread program, rather than forcing the programmer to express it as SIMD vector 

instructions. It is simpler and more productive to develop scalar single-thread code 

than vector code, and the SIMT multiprocessor executes the code with SIMD-like 

  ciency.

C-36 Appendix C Graphics and Computing GPUs
Coupling eight streaming processor cores together closely into a multiprocessor 
and then implementing a scalable number of such multiprocessors makes a two-
level multiprocessor composed of multiprocessor
 e CUDA programming model 
exploits the two-level hierarchy by providing individual threads fo
 ne-grained 
parallel computations, and by providing grids of thread blocks for coarse-grained 

parallel operation
 e same thread program can provide bot
 ne-grained and 
coarse-grained operations. In contrast, CPUs with SIMD vector instructions must 

use tw
 erent programming models to prov
 ne-grained and coarse-grained 
operations: coarse-grained parallel threads on
 erent cores, and SIMD vector 
instructions fo
 ne-grained data parallelism.
Multithreaded Multiprocessor Conclusion e example GPU multiprocessor based on the Tesla architecture is highly 
multithreaded, executing a total of up to 512 lightweight threads concurrently to 

suppor
 ne-grained pixel shaders and CUDA threads. It uses a variation on SIMD 
architecture and multithreading called SIMT (single-instruction multiple-thread) 

to e
  ciently broadcast one instruction to a warp of 32 parallel threads, while 
permitting each thread to branch and execute independently. Each thread executes 

its instruction stream on one of the eight 
streaming processor
 (SP) cores, which are 
multithreaded up to 64 threads.
 e PTX ISA is a register-based load/store scalar ISA that describes the execution 
of a single thread. Because PTX instructions are optimized and translated to binary 

microinstructions for a sp
 c GPU, the hardware instructions can evolve rapidly 
without disrupting compilers and so
 ware tools that generate PTX instructions.
 C.5 Parallel Memory System
Outside of the GPU itself, the memory subsystem is the most important 

determiner of the performance of a graphics system. Graphics workloads demand 

very high transfer rates to and from memory. Pixel write and blend (read-modify-

write) operations, depth bu
 er reads and writes, and texture map reads, as well 
as command and object vertex and attribute data reads, comprise the majority of 

memory tr
  c.
Modern GPUs are highly parallel, as shown in Figure C.2.5. For example, the 
GeForce 8800 can process 32 pixels per clock, at 600 MHz. Each pixel typically 

requires a color read and write and a depth read and write of a 4-byte pixel. Usually 

an average of two or three texels of four bytes each are read to generate the pixel’s 

color. So for a typical case, there is a demand of 28 bytes times 32 pixels 
 896 bytes 
per clock. Clearly the bandwidth demand on the memory system is enormous.

 C.5 Parallel Memory System C-
37To supply these requirements, GPU memory systems have the following 
characteristics:
 ey are wide, meaning there are a large number of pins to convey data 
between the GPU and its memory devices, and the memory array itself 
comprises many DRAM chips to provide the full total data bus width.
 ey are fast, meaning aggressive signaling techniques are used to maximize 
the data rate (bits/second) per pin.
 GPUs seek to use every available cycle to transfer data to or from the memory 
array. To achieve this, GPUs sp
 cally do not aim to minimize latency to the 
memory system. High throughput (utilizatio
  ciency) and short latency 
are fundamentally in co
 ict.
 Compression techniques are used, both lossy, of which the programmer must 
be aware, and lossless, which is invisi
ble to the application and opportunistic.
 Caches and work coalescing structures are used to reduce the amount of o
 -chip tr
  c needed and to ensure that cycles spent moving data are used as 
fully as possible.
DRAM ConsiderationsGPUs must take into account the unique characteristics of DRAM. DRAM chips 

are internally arranged as multiple (typically four to eight) banks, where each bank 

includes a power-of-2 number of rows (typically around 16,384), and each row 

contains a power-of-2 number of bits (typically 8192). DRAMs impose a variety of 

timing requirements on their controlling processor. For example, dozens of cycles 

are required to activate one row, but once activated, the bits within that row are 

randomly accessible with a new column address every four clocks. Double-data 

rate (DDR) synchronous DRAMs transfer data on both rising and falling edges 

of the interface clock (see Chapter 5). So a 1 GHz clocked DDR DRAM transfers 

data at 2 gigabits per second per data pin. Graphics DDR DRAMs usually have 32 

bidirectional data pins, so eight bytes can be read or written from the DRAM per 

clock.
GPUs internally have a large number of generators of memory tra
  c. 
 erent 
stages of the logical graphics pipeline each have their own request streams: 

command and vertex attribute fetch, shader texture fetch and load/store, and 

pixel depth and color read-write. At each logical stage, there are o
 en multiple 
independent units to deliver the parallel throughpu
 ese are each independent 
memory requestors. When viewed at the memory system, there are an enormous 

number of uncorrelated req
 ight. 
 is is a natural mismatch to the reference 
pattern preferred by the DRAMs. A solution is for the GPU’s memory controller to 

maintain separate heaps of tra
  c bound fo
 erent DRAM banks, and wait until 

C-38 Appendix C Graphics and Computing GPUs
enough tra
  c for a particular DRAM row is pending before activating that row 
and transferring all the tra
  c at once. Note that accumulating pending requests, 
while good for DRAM row locality and th
  cient use of the data bus, leads to 
longer average latency as seen by the requestors whose requests spend time waiting 
for other
 e design must take care that no particular request waits too long, 
otherwise some processing units can starve waiting for data and ultimately cause 

neighboring processors to become idle.
GPU memory subsystems are arranged as multiple 
memory partitions
, each of 
which comprises a fully independent memory controller and one or two DRAM 

devices that are fully and exclusively owned by that partition. To achieve the best 

load balance and therefore approach the theoretical performance of 
n partitions, 
addresses ar
 nely interleaved evenly across all memory partition
 e partition 
interleaving stride is typically a block of a few hundred byt
 e number of 
memory partitions is designed to balance the number of processors and other 

memory requesters.
CachesGPU workloads typically have very large working sets—on the order of hundreds 

of megabytes to generate a single graphics frame. Unlike with CPUs, it is not 

practical to construct caches on chips large enough to hold anything close to the 

full working set of a graphics application. Whereas CPUs can assume very high 

cache hit rates (99.9% or more), GPUs experience hit rates closer to 90% and must 

therefore cope with many miss
 ight. While a CPU can reasonably be designed 
to halt while waiting for a rare cache miss, a GPU needs to proceed with misses and 

hits intermingled. We call this a 
streaming cache architecture
.GPU caches must deliver very high-bandwidth to their clients. Consider the case 
of a texture cache. A typical texture unit may evaluate two bilinear interpolations for 

each of four pixels per clock cycle, and a GPU may have many such texture units all 

operating independently. Each bilinear interpolation requires four separate texels, 

and each texel might be a 64-bit value. Four 16-bit components are typical.
 us, total bandwidth is 2 
 4  4  64  2048 bits per clock. Each separate 64-bit texel 
is independently addressed, so the cach
e needs to handle 32 unique addresses per 
cloc
 is naturally favors a multibank and/or multiport arrangement of SRAM 
arrays.
MMUModern GPUs are capable of translating virtual addresses to physical addresses. 

On the GeForce 8800, all processing units generate memory addresses in a 

40-bit virtual address space. For computing, load and store thread instructions use 

32-bit byte addresses, which are extended to a 40-bit virtual address by adding a 

40-bit
 set. A memory management unit performs virtual to physical address 

 C.5 Parallel Memory System C-
39translation; hardware reads the page tables from local memory to respond to 
misses on behalf of a hierarchy of translation lookaside b
 ers spread out among 
the processors and rendering engines. In
 addition to physical page bits, GPU page 
table entries specify the compression algorithm for each page. Page sizes range 

from 4 to 128 kilobytes.
Memory Spaces
As introduced in Section C.3, CUDA expos
 erent memory spaces to allow the 
programmer to store data values in the most performance-optimal way. For the 

following discussion, NVIDIA Tesla architecture GPUs are assumed.
Global memory
Global memory is stored in external DRAM; it is not local to any one physical 

streaming multiprocessor
 (SM) because it is meant for communication among 
 erent CTAs (thread bloc
 erent grids. In fact, the many CTAs that 
reference a location in global memory may not be executing in the GPU at the 

same time; by design, in CUDA a programmer does not know the relative order 

in which CTAs are executed. Because the address space is evenly distributed 

among all memory partitions, there must be a read/write path from any streaming 

multiprocessor to any DRAM partition.
Access to global memory by
 erent threads (a
 erent processors) is not 
guaranteed to have sequential consistency
 read programs see a relaxed memory 
ordering model. Within a thread, the order of memory reads and writes to the same 

address is preserved, but the order of accesses t
 erent addresses may not be 
preserved. Memory reads and writes requested by
 erent threads are unordered. 
Within a CTA, the barrier synchronization instruction 
bar.sync can be used 
to obtain strict memory ordering among the threads of the CT
 e membar thread instruction provides a memory barrier/fence operation that commits prior 

memory accesses and makes them visible to other threads before proceeding. 

 reads can also use the atomic memory operations described in Section C.4 to 
coordinate work on memory they share.
Shared memory
Per-CTA shared memory is only visible to the threads that belong to that CTA, 

and shared memory only occupies storage from the time a CTA is created to the 

time it terminates. Shared memory can therefore reside on-chip
 is approach has 
many be
 ts. First, shared memory tra
 c does not need to compete with limited 
 -chip bandwidth needed for global memory references. Second, it is practical to 
build very high-bandwidth memory structures on-chip to support the read/write 

demands of each streaming multiprocessor. In fact, the shared memory is closely 

coupled to the streaming multiprocessor.

C-40 Appendix C Graphics and Computing GPUs
Each streaming multiprocessor contains eight physical thread processors. During 
one shared memory clock cycle, each thread processor can process two threads’ 
worth of instructions, so 16 threads’ worth of shared memory requests must be 

handled in each clock. Because each thread can generate its own addresses, and the 

addresses are typically unique, the shared memory is built using 16 independently 

addressable SRAM banks. For common access patterns, 16 banks ar
  cient 
to maintain throughput, but pathological cases are possible; for example, all 16 

threads might happen to
 erent address on one SRAM bank. It must be 
possible to route a request from any thread lane to any bank of SRAM, so a 16-by-

16 interconnection network is required.
Local Memory
Per-thread local memory is private memory visible only to a single thread. Local 

memory is architecturally larger than the thread’s regist
 le, and a program 
can compute addresses into local memory. To support large allocations of local 

memory (recall the total allocation is the per-thread allocation times the number 

of active threads), local memory is allocated in external DRAM.
Although global and per-thread local memory reside o
 -chip, they are well-
suited to being cached on-chip.
Constant Memory
Constant memory is read-only to a program running on the SM (it can be written 

via commands to the GPU). It is stored in external DRAM and cached in the SM. 

Because commonly most or all threads in a SIMT warp read from the same address 

in constant memory, a single address lookup per cloc
  cient. 
 e constant 
cache is designed to broadcast scalar
 values to threads in each warp.
Texture Memory
Texture memory holds large read-only arrays of data. Textures for computing have 

the same attributes and capabilities as textures used with 3D graphics. Although 

textures are commonly two-dimensional images (2D arrays of pixel values), 1D 

(linear) and 3D (volume) textures are also available.
A compute program references a texture using a 
tex instruction. Operands 
include an iden
 er to name the texture, and 1, 2, or 3 coordinates based on the 
texture dimensionality
 e 
 oating-point coordinates include a fractional portion 
that sp
 es a sample location, o
 en in between texel locations. Noninteger 
coordinates invoke a bilinear weighted interpolation of the four closest values (for 

a 2D texture) before the result is returned to the program.
Texture fetches are cached in a streaming cache hierarchy designed to optimize 
throughput of texture fetches from thousands of concurrent threads. Some 

programs use texture fetches as a way to cache global memory.

 C.6 Floating-point Arithmetic C-
41SurfacesSurface
 is a generic term for a one-dimensional, two-dimensional, or three-
dimensional array of pixel values and an associated format. A variety of formats 
ar
 ned; for example, a pixel may b
 ned as four 8-bit RGBA integer 
components, or four 16-bi
 oating-point components. A program kernel does 
not need to know the surface type. A 
tex instruction recasts its result values as 
 oating-point, depending on the surface format.
Load/Store AccessLoad/store instructions with integer byte addressing enable the writing and 

compiling of programs in conventional languages like C and C
. CUDA 
programs use load/store instructions to access memory.
To improve memory bandwidth and reduce overhead, the local and global load/
store instructions coalesce individual parallel thread requests from the same warp 

together into a single memory block request when the addresses fall in the same 

block and meet alignment criteria. Coales
cing individual small memory requests 
into large block requests prov
 cant performance boost over separate 
req
 e large thread count, together with support for many outstanding load 
requests, helps cover load-to-use latency for local and global memory implemented 

in external DRAM.
ROP
As shown in Figure C.2.5, NVIDIA Tesla architecture GPUs comprise a scalable 

streaming processor array (SPA), which performs all of the GPU’s programmable 

calculations, and a scalable memory system, which comprises external DRAM 

control an
 xed function 
Raster Operation Processors
 (ROPs) that perform color 
and depth frameb
 er operations directly on memory. Each ROP unit is paired 
with a sp
 c memory partition. ROP partitions are fed from the SMs via an 
interconnection network. Each ROP is responsible for depth and stencil tests and 

updates, as well as color blendin
 e ROP and memory controllers cooperate 
to implement lossless color and depth co
mpression (up to 8:1) to reduce external 
bandwidth demand. ROP units also perform atomic operations on memory.
 C.6 Floating-point Arithmetic
GPUs today perform most arithmetic operations in the programmable processor 

cores using IEEE 754-compatible single precision 32-bi
 oating-point operations 
(see Chapt
 e 
 xed-point arithmetic of early GPUs was succeeded by 16-
bit, 24-bit, and 32-bi
 oating-point, then IEEE 754-compatible 32-bi
 oating-

C-42 Appendix C Graphics and Computing GPUs
point. So
 xed-function logic within a GPU, such as textur
 ltering hardware, 
continues to use proprietary numeric formats. Recent GPUs also provide IEEE 754-
compatible double precision 64-bi
 oating-point instructions.
Supported Formats
 e IEEE 754 standard fo
 oating-point arithmetic sp
 es basic and storage 
formats. GPUs use two of the basic formats for computation, 32-bit and 64-bit 

binary
 oating-point, commonly called single precision and double precisio
 e standard also spe
 es a 16-bit binary storag
 oating-point format, 
half precision
. GPUs and the Cg shading language employ the narrow 16-bit half data format for 

  cient data storage and movement, while maintaining high dynamic range. GPUs 
perform many textur
 ltering and pixel blending computations at half precision 
within the textur
 ltering unit and the raster operations uni
 e OpenEXR high 
dynamic-range imag
 le format developed by Industrial Light and Magic [2003] 
uses the identical half format for color component values in computer imaging and 

motion picture applications.
Basic ArithmeticCommon single precisio
 oating-point operations in GPU programmable cores 
include addition, multiplication, 
multiply-add
, minimum, maximum, compare, 
set predicate, and conversions between integer an
 oating-point numbers. 
Floating-point instructions o
 en provide source operand mo
 ers for negation 
and absolute value.
 e 
 oating-point addition and multiplication operations of most GPUs today are 
compatible with the IEEE 754 standard for single precision FP numbers, including not-

a-number (NaN) an
 nity val
 e FP addition and multiplication operations 
use IEEE round-to-nearest-even as the default rounding mode. To increas
 oating-
point instruction throughput, GPUs o
 en use a compound multiply-add instruction 
(mad e multiply-add operation performs FP multiplication with truncation, 
followed by FP addition with round-to-nearest-even. It provides tw
 oating-point 
operations in one issuing cycle, without requiring the instruction scheduler to 

dispatch two separate instructions, but the computation is not fused and truncates 

the product before the additio
 is makes it
 erent from the fused multiply-add 
instruction discussed in Chapter 3 and later in this section. GPUs typicall
 ush 
denormalized source operands to sign-preserved zero, and the
 ush results that 
under
 ow the target output exponent range to sign-preserved zero a
 er rounding.
Specialized ArithmeticGPUs provide hardware to accelerate special function computation, attribute 

interpolation, and textur
 ltering. Special function instructions include cosine, 
half precision
 A 16-bit 
binary
 oating-point 
format, with 1 sign bit, 
5-bit exponent, 10-bit 

fraction, and an implied 

integer bit.
multiply-add (MAD)
 A sing
 oating-point 
instruction that performs 

a compound operation: 

multiplication followed by 

addition.

 C.6 Floating-point Arithmetic C-
43sine, binary exponential, binary logarithm, reciprocal, and reciprocal square root. 
Attribute interpolation instructions prov
  cient generation of pixel attributes, 
derived from plane equation evaluatio
 e special function unit (SFU)
 introduced in Section C.4 computes special functions and interpolates planar 

attributes [Oberman and Siu, 2005].
Several methods exist for evaluating special functions in hardware. It has been 
shown that quadratic interpolation based on Enhanced Minimax Approximations 

is a ver
  cient method for approximating functions in hardware, including 
reciprocal, reciprocal square-root, log
2x, 2x, sin, and cos.
We can summarize the method of SFU quadratic interpolation. For a binary 
input operand X with 
n-bit
 cand, th
 cand is divided into two parts: 
Xu is the upper part containing 
m bits, and X
l is the lower part containing 
n-m bits. 
 e upper 
m bits X
u are used to consult a set of three lookup tables to return three 
 nite-word co
  cients C
0, C1, and C
2. Each function to be approximated requires 
a unique set of tab
 ese co
  cients are used to approximate a given function 
f(X) in the range X
u X  Xu  2m by evaluating the expression:
f(X)CCXCX
01121
2 e accuracy of each of the function estimates ranges from 22 t
 cand 
bits. Example function statistics are shown in Figure C.6.1.
 e IEEE 754 standard sp
 es exact-rounding requirements for division 
and square root; however, for many GPU applications, exact compliance is not 

required. Rather, for those applications, higher computational throughput is more 

important than last-bit accuracy. For the SFU special functions, the CUDA math 

library provides both a full accuracy function and a fast function with the SFU 

instruction accuracy.
Another specialized arithmetic operation in a GPU is attribute interpolation. 
Key 
attributes
 are usually sp
 ed for vertices of primitives that make up a scene 
to be rendered. Example attributes are color, depth, and texture coordinat
 ese 
attributes must be interpolated in the (x,y) screen space as needed to determine the 
special function unit 
(SFU)
 A hardware unit 
that computes special 
functions and interpolates 

planar attributes.
FunctionInput intervalAccuracy(good bits)ULP*error% exactly roundedMonotonic
1/x[1, 2)24.020.9887Yes
1/sqrt(x)[1, 4)23.401.5278Yes
2x[0, 1)22.511.4174Yes
log2x[1, 2)22.57N/A
**N/AYes
sin/cos[0, /2)22.47N/AN/ANo
*ULP: unit in the last place. **N/A: not applicable.FIGURE C.6.1 Special function approximation statistics. For the NVIDIA GeForce 8800 special 
function unit (SFU).

C-44 Appendix C Graphics and Computing GPUs
values of the attributes at each pixel locatio
 e value of a given attribute 
U in an 
(x, y) plane can be expressed using plane equations of the form:
U(xy)AxByC
uuu
,where 
A, B, and 
C are interpolation parameters associated with each attribute 
U.  e interpolation parameters 
A, B, and 
C are all represented as single precision 
 oating-point numbers.
Given the need for both a function evaluator and an attribute interpolator in a 
pixel shader processor, a single SFU that performs both functions fo
  ciency can 
be designed. Both functions use a sum of products operation to interpolate results, 
and the number of terms to be summed in both functions is very similar.
Texture Operations
Texture mapping an
 ltering is another key set of sp
 oating-point 
arithmetic operations in a GPU
 e operations used for texture mapping include:
1. Receive texture address (s, t) for the current screen pixel (x, y), where s and 
t are single precisio
 oating-point numbers.
2. Compute the level of detail to identify the correct texture 
MIP-map
 level.
3. Compute the trilinear interpolation fraction.
4. Scale texture address (s, t) for the selected MIP-map level.

5. Access memory and retrieve desired texels (texture elements).

6. Perform 
 ltering operation on texels.
Texture mapping requir
 cant amount of
 oating-point computation 
for full-speed operation, much of which is done at 16-bit half precision. As an 
example, the GeForce 8800 Ultra delivers about 500 GFLOPS of proprietary format 

 oating-point computation for texture mapping instructions, in addition to its 
conventional IEEE single precisio
 oating-point instructions. For more details on 
texture mapping an
 ltering, see Foley and van Dam [1995].
Performance
 e 
 oating-point addition and multiplication arithmetic hardware is fully 
pipelined, and latency is optimized to bal
ance delay and area. While pipelined, 
the throughput of the special functions is less than th
 oating-point addition 
and multiplication operations. Quarter-speed throughput for the special functions 

is typical performance in modern GPUs, with one SFU shared by four SP cores. 

In contrast, CPUs typically hav
 cantly lower throughput for similar 
functions, such as division and square root, albeit with more accurate resul
 e attribute interpolation hardware is typic
ally fully pipelined to
 enable full-speed 
pixel shaders.
MIP-map
 A Latin 
phrase 
multum in parvo
, or much in a small space. 
A MIP-map contains 

precalculated images of 

 erent resolutions, used 
to increase rendering 

speed and reduce 

artifacts.

 C.6 Floating-point Arithmetic C-
45Double precisionNewer GPUs such as the Tesla T10P also support IEEE 754 64-bit double precision 
operations in hardware. Standar
 oating-point arithmetic operations in double 
precision include addition, multiplication, and conversions betw
 erent 
 oating-point and integer forma
 
 oating-point standard 
includes sp
 cation for the 
fused-multiply-add
 (FMA) operation, as discussed 
in Chapt
 e FMA operation perform
 oating-point multiplication 
followed by an addition, with a single roundin
 e fused multiplication and 
addition operations retain full accuracy in intermediate calculation
 is behavior 
enables more accurate
 oating-point computations involving the accumulation 
of products, including dot products, matrix multiplication, and polynomial 

evaluatio
 e FMA instruction also enab
  cient so
 ware implementations 
of exactly rounded division and square root, removing the need for a hardware 

division or square root unit.
A double precision hardware FMA unit implements 64-bit addition, 
multiplication, conversions, and the FMA operation itse
 e architecture of a 
Multiplier Array53 x 53ExpDiff BCACarry Propagate Adder64Alignmentshifter Inversion3-2 CSA  161 bitsComplementerNormalizerRounder53SumCarry
ShiftedC16164645353SumCarryFIGURE C.6.2 Double precision fused-multiply-add (FMA) unit. Hardware to implemen
 oating-
point A 
 B  C for double precision.

C-46 Appendix C Graphics and Computing GPUs
double precision FMA unit enables full-speed denormalized number support on 
both inputs and outputs. Figure C.6.2 shows a block diagram of an FMA unit.
 cands of A and B are multiplied to form a 106-
  in carry-save form. In parallel, the 53-bit addend C is 
 e sum and carry results 
of the 106-bit product are summed with the aligned addend through a 161-bit-
wide carry-save adder e carry-save output is then summed together in 
a carry-propagate adder to produce an unrounded result in nonredundant, two’s 
 e result is conditionally recomplemented, so as to return a 
 e complemented result is normalized, and then 
 t within the target format.
 C.7 Real Stuff: The NVIDIA GeForce 8800
 e NVIDIA GeForce 8800 GPU, introduced in Novemb
 ed vertex 
and pixel processor design that also supports parallel computing applications written 

in C using the CUDA parallel programming model. It is th
 rst implementation 
of the T
 ed graphics and computing architecture described in Section C.4 
and in Lindholm et al. [2008]. A family of Tesla architecture GPUs addresses the 

 erent needs of laptops, desktops, workstations, and servers.
Streaming Processor Array (SPA)
 e GeForce 8800 GPU shown in Figure C.7.1 contains 128 
streaming processor 
(SP) cores organized as 16 
streaming multiprocessors 
(SMs). Two SMs share a texture 
unit in each 
texture/processor cluster 
(TPC). An array of eight TPCs makes up the 
streaming processor array 
(SPA), which executes all graphics shader programs and 
computing programs.
 e host interface unit communicates with the host CPU via the PCI-Express 
bus, checks command consistency, and performs context switchin
 e input 
assembler collects geometric primitives (points, lines, triang
 e work 
distribution blocks dispatch vertices, pixels, and compute thread arrays to the 

TPCs in the SP
 e TPCs execute vertex and geometry shader programs and 
computing programs. Output geometric data is sent to the viewport/clip/setup/

raster/zcull block to be rasterized into pixel fragments that are then redistributed 

back into the SPA to execute pixel shader programs. Shaded pixels are sent across 

the interconnection network for processing by the ROP uni
 e network also 
routes texture memory read requests from the SPA to DRAM and reads data from 

DRAM through a level-2 cache back to the SPA.

 C.7 Real Stuff: The NVIDIA GeForce 8800 C-
47Texture/Processor Cluster (TPC)
Each TPC contains a geometry controller, an SMC, two SMs, and a texture unit as 
shown in Figure C.7.2.
 e geometry controller maps the logical graphics vertex pipeline into recir-
culation on the physical SMs by directing all primitive and vertex attribute and 

topolog
 ow in the TPC.
 e SMC controls multiple SMs, arbitrating the shared texture unit, load/store 
path, and I/O pat
 e SMC serves three graphics workloads simultaneously: 
vertex, geometry, and pixel.
 e texture unit processes a texture instruction for one vertex, geometry, or pixel 
quad, or four compute threads per cycle. Texture instruction sources are texture 

coordinates, and the outputs are weighted samples, typically a four-component 

(RGB
 oating-point color.
 e texture unit is deeply pipelined. Although it 
contains a streaming cache to captur
 ltering locality, it streams hits mixed with 
misses without stalling.
GPUHost CPUSystem Memory
DRAMDRAMDRAMDRAMDRAMDRAM
ROPL2ROPL2ROPL2ROPL2ROPL2ROPL2
TPCSPA
TPCTPCTPCTPCTPCTPCTPC
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
Texture Unit
Tex L1
SMSMSMSMSMSMSMSMSMSMSMSMSMSMSMSM
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
SPSPSPSP
Vertex Work
Distribution 
Input Assembler
Host Interface
Bridge
Pixel Work
Distribution 
Viewport/Clip/
Setup/Raster/ ZCullCompute Work
Distribution
Interconnection Network
Interface
Display
High-DefinitionVideo ProcessorsSharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
SharedMemory
FIGURE C.7.1 NVIDIA Tesla uniﬁ
 ed graphics and computing GPU architecture.
 is GeForce 8800 has 128 streaming processor 
(SP) cores in 16 streaming multiprocessors (SMs), arranged in eight texture/processor cluster
 e processors connect with six 64-bit-
wide DRAM partitions via an interconnection network. Other GPUs implementing the Tesla architecture vary the number of SP cores
, SMs, 
DRAM partitions, and other units.

C-48 Appendix C Graphics and Computing GPUs
Streaming Multiprocessor (SM) e S
 ed graphics and computing multiprocessor that executes vertex, 
geometry, and pixel-fragment shader programs and parallel computing programs. 
 e SM consists of eight SP thread processor cores, two SFUs, a multithreaded 
instruction fetch and issue unit (MT issue), an instruction cache, a read-

only constant cache, and a 16 KB read/write shared memory. It executes scalar 

instructions for individual threads.
 e GeForce 8800 Ultra clocks the SP cores and SFUs at 1.5 GHz, for a peak of 
36 GFLOPS per SM. To optimize power and are
  ciency, some SM nondatapath 
units operate at half the SP clock rate.
SMCGeometry ControllerTPCTexture UnitTex L1SPSharedMemorySPSPSPSPSPSPSPI-CacheMT IssueC-CacheSFUSFUSPSharedMemorySPSPSPSPSPSPSPI-CacheMT IssueC-CacheSFUSFUSharedMemoryI-CacheMT IssueC-CacheSFUSFUSMSMSM
SPSPSPSPSP
SPSPSPFIGURE C.7.2 Texture/processor cluster (TPC) and a streaming multiprocessor (SM).
 Each SM has eight streaming processor 
(SP) cores, two SFUs, and a shared memory.

 C.7 Real Stuff: The NVIDIA GeForce 8800 C-
49To e
  ciently execute hundreds of parallel threads while running sev
 erent 
programs, the SM is hardware multithreaded. It manages and executes up to 768 
concurrent threads in hardware with zero scheduling overhead. Each thread has its 

own thread execution state and can execute an independent code path.
A warp consists of up to 32 threads of the same type—vertex, geometry, pixel, 
or compute.
 e SIMT design, previously described in Section C.4, shares the SM 
instruction fetch and issue uni
  ciently across 32 threads but requires a full warp 
of active threads for full performa
  ciency.
 e SM schedules and executes multiple warp types concurrently. Each issue 
cycle, the scheduler selects one of the 24 warps to execute a SIMT warp instruction. 

An issued warp instruction executes as four sets of 8 threads over four processor 

cyc
 e SP and SFU units execute instructions independently, and by issuing 
instructions between them on alternate cycles, the scheduler can keep both fully 

occupied. A scoreboard qu
 es each warp for issue each cycle.
 e instruction 
scheduler prioritizes all ready warps and selects the one with highest priority for 

issue. Prioritization considers warp type, instruction type, and “fairness” to all 

warps executing in the SM.
 e SM executes 
cooperative thread arrays
 (CTAs) as multiple concurrent warps 
which access a shared memory region allocated dynamically for the CTA.
Instruction Set
 reads execute scalar instructions, un
like previous GPU vector instruction 
architectures. Scalar instructions are simpler and compiler friendly. Texture 

instructions remain vector based, taking a source coordinate vector and returning 

 ltered color vector.
 e register-based instruction set includes all th
 oating-point and integer 
arithmetic, transcendental, logical
 ow control, memory load/store, and texture 
instructions listed in the PTX instruction table of Figure C.4.3. Memory load/store 

instructions use integer byte addressing with register-plus-o
 set address arithmetic. 
For computing, the load/store instructions access three read-write memory spaces: 

local memory for per-thread, private, temporary data; shared memory for low-

latency per-CTA data shared by the threads of the CTA; and global memory for data 

shared by all threads. Computing programs use the fast barrier synchronization 
bar.sync instruction to synchronize threads within a CTA that communicate 
with each other via shared and global memory
 e latest Tesla architecture GPUs 
implement PTX atomic memory operations, which facilitate parallel reductions 

and parallel data structure management.
Streaming Processor (SP) e multithreaded SP core is the primary thread processor, as introduced in 
Section C.4. Its regist
 le provides 1024 scalar 32-bit registers for up to 96 threads 
(more threads than in the example SP of Section C.4). I
 oating-point add and 

C-50 Appendix C Graphics and Computing GPUs
multiply operations are compatible with the IEEE 754 standard for single precision 
FP numbers, including not-a-number (NaN) an
 nity. 
 e add and multiply 
operations use IEEE round-to-nearest-even as the default rounding mode
 e SP 
core also implements all of the 32-bit and 64-bit integer arithmetic, comparison, 

conversion, and logical PTX instructions in Figur
 e processor is fully 
pipelined, and latency is optimized to balance delay and area.
Special Function Unit (SFU) e SFU supports computation of both transcendental functions and planar 
attribute interpolation. As described in Section C.6, it uses quadratic interpolation 

based on enhanced minimax approximations to approximate the reciprocal, 

reciprocal square root, log
2x, 2x, and sin/cos functions at one result per cycle
 e SFU also supports pixel attribute interpolation such as color, depth, and texture 

coordinates at four samples per cycle.
RasterizationGeometry primitives from the SMs go in their original round-robin input order 

to the viewport/clip/setup/raster/zcull bloc
 e viewport and clip units clip 
the primitives to the view frustum and to any enabled user clip planes, and then 

transform the vertices into screen (pixel) space.
Surviving primitives then go to the setup unit, which generates edge equations 
for the rasterizer. A coarse-rasterization stage generates all pixel tiles that are at 

least partially inside the primitive
 e zcull unit maintains a hierarchical 
z surface, 
rejecting pixel tiles if they are conservatively known to be occluded by previously 

drawn pixe
 e rejection rate is up to 256 pixels per clock. Pixels that survive zcull 
then go t
 ne-rasterization stage that generates detailed coverage information 
and depth values.
 e depth test and update can be performed ahead of the fragment shader, or 
 er, depending on current state.
 e SMC assembles surviving pixels into warps 
to be processed by an SM running the current pixel shader
 e SMC then sends 
surviving pixel and associated data to the ROP.
Raster Operations Processor (ROP) and Memory System
Each ROP is paired with a sp
 c memory partition. For each pixel fragment 
emitted by a pixel shader program, ROPs perform depth and stencil testing and 

updates, and in parallel, color blending and updates. Lossless color compression 

(up to 8:1) and depth compression (up to 8:1) are used to reduce DRAM bandwidth. 

Each ROP has a peak rate of four pixels per clock and supports 16-bi
 oating-
point and 32-bi
 oating-point HDR formats. ROPs support double-rate-depth 
processing when color writes are disabled.

 C.7 Real Stuff: The NVIDIA GeForce 8800 C-
51Antialiasing support includes up to 16
 multisampling and supersamplin
 e coverage-sampling antialiasing (CSAA) algorithm computes and stores Boolean 
coverage at up to 16 samples and compresses redundant color, depth, and stencil 

information into the memory footprint and a bandwidth of four or eight samples 

for improved performance.
 e DRAM memory data bus width is 384 pins, arranged in six independent 
partitions of 64 pins each. Each partition supports double-data-rate DDR2 and 

graphics-oriented GDDR3 protocols at up to 1.0 GHz, yielding a bandwidth of 

about 16 GB/s per partition, or 96 GB/s.
 e memory controllers support a wide range of DRAM clock rates, protocols, 
device densities, and data bus widths. Texture and load/store requests can occur 

between any TPC and any memory partition, so an interconnection network routes 

requests and responses.
Scalability e T
 ed architecture is designed for scalability. Varying the number of 
SMs, TPCs, ROPs, caches, and memory partitions provides the right balance for 

 erent performance and cost targets in GPU market segments. Scalable link 
interconnect (SLI) connects multiple GPUs, providing further scalability.
Performance
 e GeForce 8800 Ultra clocks the SP thread processor cores and SFUs at 1.5 GHz, 
for a theoretical operation peak of 576 GFLO
 e GeForce 8800 GTX has a 1.35 
GHz processor clock and a corresponding peak of 518 GFLOPS.
 e following three sections compare the performance of a GeForce 8800 GPU 
with a multicore CPU on thre
 erent applications—dense linear algebra, fast 
Fourier transforms, and sortin
 e GPU programs and libraries are compiled 
CUDA C code
 e CPU code uses the single precision multithreaded Intel MKL 
10.0 library to leverage SSE instructions and multiple cores.
Dense Linear Algebra Performance
Dense linear algebra computations are fundamental in many applications. Volkov 

and Demmel [2008] present GPU and CPU performance results for single precision 

dense matrix-matrix multiplication (the SGEMM routine) and LU, QR, and 

Cholesky matrix factorizations. Figure C.7.3 compares GFLOPS rates on SGEMM 

dense matrix-matrix multiplication for a GeForce 8800 GTX GPU with a quad-

core CPU. Figure C.7.4 compares GFLOPS rates on matrix factorization for a GPU 

with a quad-core CPU.
Because SGEMM matrix-matrix multiply and similar BLAS3 routines are the 
bulk of the work in matrix factorization, their performance sets an upper bound on 

factorization rate. As the matrix order increases beyond 200 to 400, the factorization 

C-52 Appendix C Graphics and Computing GPUs
FIGURE C.7.4 Dense matrix factorization performance rates.
 e graph shows GFLOPS rates 
achieved in matrix factorizations using the GPU and using the CPU alone. Adapted from Figure 7 of Volkov 
and D
 e black lines are for a 1.35 GHz NVIDIA GeForce 8800 GTX, CUDA 1.1, Windows 
XP attached to a 2.67 GHz Intel Core2 Duo E6700 Windows XP, including all CPU–GPU data transfer times. 

 e blue lines are for a quad-core 2.4 GHz Intel Core2 Quad Q6600, 64-bit Linux, Intel MKL 10.0.
FIGURE C.7.3 SGEMM dense matrix-matrix multiplication performance rates.
 e graph 
shows single precision GFLOPS rates achieved in multiplying square N
N matrices (solid lines) and thin 
N64 and 64
N matrices (dashed lines). Adapted from Figure 6 of Volkov and D
 e black 
lines are a 1.35 GHz GeForce 8800 GTX using Volkov’s SGEMM code (now in NVIDIA CUBLAS 2.0) on 

matrices in GPU memory
 e blue lines are a quad-core 2.4 GHz Intel Core2 Quad Q6600, 64-bit Linux, 
Intel MKL 10.0 on matrices in CPU memory.
2101801501209060300GFLOPSN641282565121024204840968192
A:NN, B:NNA:N64, B:64NGeForce 8800 GTXCore2 QuadLUCholeskyQRCore2 Quad210180150
1209060300Order of Matrix
GFLOPSGeForce 8800 GTX + Core2 Duo
64128256512102420484096819216,384

 C.7 Real Stuff: The NVIDIA GeForce 8800 C-
53problem becomes large enough that SGEMM can leverage the GPU parallelism and 
overcome the CPU–GPU system and copy overhead. Volkov’s SGEMM matrix-

matrix multiply achieves 206 GFLOPS, about 60% of the GeForce 8800 GTX peak 

multiply-add rate, while the QR factorization reached 192 GFLOPS, about 4.3 

times the quad-core CPU.
FFT Performance
Fast Fourier Transforms are used in many applications. Large transforms and 

multidimensional transforms are partitioned into batches of smaller 1D transforms.
Figure C.7.5 compares the in-place 1D complex single precision FFT 
performance of a 1.35 GHz GeForce 8800 GTX (dating from late 2006) with a 2.8 

GHz quad-Core Intel Xeon E5462 series (code named “Harpertown,” dating from 

late 2007). CPU performance was measured using the Intel Math Kernel Library 

(MKL) 10.0 FFT with four threads. GPU performance was measured using the 

NVIDIA CUFFT 2.1 library and batched 1D radix-16 decimation-in-frequency 

FFTs. Both CPU and GPU throughput performance was measured using batched 

FFTs; batch size was 2
24/n, where 
n is the transform size
 us, the workload for 
every transform size was 128 MB. To determine GFLOPS rate, the number of 

operations per transform was taken as 5
n log2 n.FIGURE C.7.5 Fast Fourier Transform throughput performance.
 e graph compares the 
performance of batched one-dimensional in-place complex FFTs on a 1.35 GHz GeForce 8800 GTX with a 
quad-core 2.8 GHz Intel Xeon E5462 series (code named “Harpertown”), 6MB L2 Cache, 4GB Memory, 1600 

FSB, Red Hat Linux, Intel MKL 10.0.
80GeForce 8800GTX
Xeon 546270GFLOPS605040302010Number of Elements in One Transform
0128256512102420484096819216,38432,76865,536131,072262,144524,2881,048,5762,097,1524,194,304
C-54 Appendix C Graphics and Computing GPUs
Sorting Performance
In contrast to the applications just discussed, sort requires far more substantial 
coordination among parallel threads, and parallel scaling is correspondingly 

harder to obtain. Nevertheless, a variety of well-known sorting algorithms can 

be e
  ciently parallelized to run well on the GPU. Satish et al. [2008] detail the 
design of sorting algorithms in CUDA, and the results they report for radix sort 

are summarized below.
Figure C.7.6 compares the parallel sorting performance of a GeForce 8800 Ultra 
with an 8-core Intel Clovertown system, both of which date to earl
 e CPU cores are distributed between two physical sockets. Each socket contains a 

multichip module with twin Core2 chips, and each chip has a 4MB L2 cache. All 

sorting routines were designed to sort key-value pairs where both keys and values 

are 32-bit integer
 e primary algorithm being studied is radix sort, although 
the quicksort-based 
parallel_sort() procedure provided by Intel’
 reading 
Building Blocks is also included for comparison. Of the two CPU-based radix sort 

codes, one was implemented using only the scalar instruction set and the other 

utilizes carefully hand-tuned assembly langua
ge routines that take advantage of the 
SSE2 SIMD vector instructions.
 e graph itself shows the achieved sorting rat
 ned as the number of 
elements sorted divided by the time to sort—for a range of sequence sizes. It is 
FIGURE C.7.6 Parallel sorting performance.
 is graph compares sorting rates for parallel radix 
sort implementations on a 1.5 GHz GeForce 8800 Ultra and an 8-core 2.33 GHz Intel Core2 Xeon E5345 
system.
010203040506070
80100010,000100,0001,000,00010,000,000100,000,000
MillionsSequence SizeSorting Rate (pairs/sec)CPU quick sortCPU radix sort (scalar)GPU radix sortCPU radix sort (SIMD)
 C.8 Real Stuff: Mapping Applications to GPUs C-
55apparent from this graph that the GPU radix sort achieved the highest sorting 
rate for all sequences of 8K-elements and larger. In this range, it is on average 2.6 

times faster than the quicksort-based routine and roughly 2 times faster than the 

radix sort routines, all of which were using the eight available CPU cor
 e CPU 
radix sort performance varies widely, likely due to poor cache locality of its global 

permutations.
 C.8 Real Stuff: Mapping Applications to GPUs
 e advent of multicore CPUs and manycore GPUs means that mainstream 
processor chips are now parallel systems. Furthermore, their parallelism continues 

to scale with Moore’s law.
 e challenge is to develop mainstream visual computing 
and high-performance computing applications that transparently scale their 

parallelism to leverage the increasing number of processor cores, much as 3D 

graphics applications transparently scale their parallelism to GPUs with widely 

varying numbers of cores.
 is section presents examples of mapping scalable parallel computing 
applications to the GPU using CUDA.
Sparse Matrices
A wide variety of parallel algorithms can be written in CUDA in a fairly 

straightforward manner, even when the data structures involved are not simple 

regular grids. Sparse matrix-vector multiplication (SpMV) is a good example of an 

important numerical building block that can be parallelized quite directly using the 

abstractions provided by CUD
 e kernels we discuss below, when combined 
with the provided CUBLAS vector routines, make writing iterative solvers such as 

the conjugate gradient method straightforward.
A sparse 
n n matrix is one in which the number of nonzero entries 
m is only 
a small fraction of the total. Sparse matrix representations seek to store only the 

nonzero elements of a matrix. Since it is fairly typical that a sparse 
n n matrix 
will contain only 
m  O(n) nonzero elements, this represents a substantial saving 
in storage space and processing time.
One of the most common representations for general unstructured sparse 
matrices is the 
compressed sparse row
 (CSR) representatio
 e m nonzero 
elements of the matrix 
A are stored in row-major order in an array 
Av. A second 
array 
Aj records the corresponding column index for each entry of 
Av. Finally, an 
array 
Ap of 
n  1 elements records the extent of each row in the previous arrays; the 
entries for row 
i in Aj and 
Av extend from index 
Ap[i] up to, but not including, 
index Ap[i + 1] is implies that 
Ap[0] will always be 0 and 
Ap[n] will always 
be the number of nonzero elements in the matrix. Figure C.8.1 shows an example 

of the CSR representation of a simple matrix.

C-56 Appendix C Graphics and Computing GPUs
Given a matrix 
A in CSR form and a vector 
x, we can compute a single row of 
the product 
y  Ax using the 
multiply_row() procedure shown in Figure C.8.2. 
Computing the full product is then simply a matter of looping over all rows and 
computing the result for that row using 
multiply_row(), as in the serial C code 
shown in Figure C.8.3.
 is algorithm can be translated into a parallel CUDA kernel quite easily. We 
simply spread the loop in 
csrmul_serial() over many parallel threads. Each 
thread will compute exactly one row of the output vector 
y e code for this kernel 
is shown in Figure C.8.4. Note that it looks extremely similar to the serial loop 

used in the 
csrmul_serial() procedure.
 ere are really only two points of 
 erence. First, the row index for each thread is computed from the block and 
thread indices assigned to each thread, eliminating the for-loop. Second, we have a 

conditional that only evaluates a row product if the row index is within the bounds 

of the matrix (this is necessary since the number of rows 
n need not be a multiple 
of the block size used in launching the kernel).
float multiply_row(unsigned int rowsize,                   unsigned int *Aj, // column indices for row
                   float *Av,        // nonzero entries for row
                   float *x)         // the RHS vector
{
    float sum = 0;    for(unsigned int column=0; column<rowsize; ++column)        sum += Av[column] * x[Aj[column]];    return sum;}FIGURE C.8.2 Serial C code for a single row of sparse matrix-vector multiply.
3001Aa. Sample matrix 
A00
2004110010=Row 0
b. CSR representation of matrix
Row 2Row 3
Av[7]===Aj[7]{}}}{{Ap[5]0
30212303
124111
2257
FIGURE C.8.1 Compressed sparse row (CSR) matrix.

 C.8 Real Stuff: Mapping Applications to GPUs C-
57FIGURE C.8.3 Serial code for sparse matrix-vector multiply.
void csrmul_serial(unsigned int *Ap, unsigned int *Aj,                   float *Av, unsigned int num_rows,                   float *x, float *y){
    for(unsigned int row=0; row<num_rows; ++row)    {
        unsigned int row_begin = Ap[row];
        unsigned int row_end   = Ap[row+1];        y[row] = multiply_row(row_end-row_begin, Aj+row_begin,                              Av+row_begin, x);    }
}FIGURE C.8.4 CUDA version of sparse matrix-vector multiply.
__global__void csrmul_kernel(unsigned int *Ap, unsigned int *Aj,                   float *Av, unsigned int num_rows,                   float *x, float *y)
{
    unsigned int row = blockIdx.x*blockDim.x + threadIdx.x;    if( row<num_rows )    {        unsigned int row_begin = Ap[row];        unsigned int row_end   = Ap[row+1];        y[row] = multiply_row(row_end-row_begin, Aj+row_begin,                              Av+row_begin, x);    }}Assuming that the matrix data structures have already been copied to the GPU 
device memory, launching this kernel will look like:
unsigned int blocksize = 128; // or any size up to 512
unsigned int nblocks    =  (num_rows + blocksize - 1) / blocksize;csrmul_kernel<<<nblocks,blocksize>>>(Ap, Aj, Av, num_rows, x, y);
C-58 Appendix C Graphics and Computing GPUs
 e pattern that we see here is a very common one
 e original serial 
algorithm is a loop whose iterations are independent of each other. Such loops 
can be parallelized quite easily by simply assigning one or more iterations of the 

loop to each parallel thread
 e programming model provided by CUDA makes 
expressing this type of parallelism particularly straightforward.
 is general strategy of decomposing computations into blocks of independent 
work, and more sp
 cally breaking up independent loop iterations, is not unique 
to CUD
 is is a common approach used in one form or another by various 
parallel programming systems, including OpenMP and Intel’
 reading Building 
Blocks.
Caching in Shared memory
 e SpMV algorithms outlined above are fairly simp
 ere are a number of 
optimizations that can be made in both the CPU and GPU codes that can improve 

performance, including loop unrolling, matrix reordering, and register blocking. 

 e parallel kernels can also be reimplemented in terms of data parallel 
scan
 operations presented by Sengupta, et al. [2007].
One of the important architectural features exposed by CUDA is the presence of 
the per-block shared memory, a small on-chip memory with very low latency. Taking 

advantage of this memory can deliver substantial performance improvements. One 

common way of doing this is to use shared memory as a so
 ware-managed cache 
to hold frequently reused data. Modifcations using shared memory are shown in 

Figure C.8.5.
In the context of sparse matrix multiplication, we observe that several rows of 
A may use a particular array element 
x[i]. In many common cases, and particularly 
when the matrix has been reordered, the rows using 
x[i] will be rows near row 
i. We can therefore implement a simple caching scheme and expect to achieve some 

performance bene
 t. 
 e block of threads processing rows 
i through 
j will load x[i] through 
x[j] into its shared memory. We will unroll the 
multiply_row() loop and fetch elements of 
x from the cache whenever possible
 e resulting 
code is shown in Figure C.8.5. Shared memory can also be used to make other 

optimizations, such as fetching 
Ap[row+1] from an adjacent thread rather than 
refetching it from memory.
Because the Tesla architecture provides an explicitly managed on-chip shared 
memory, rather than an implicitly active hardware cache, it is fairly common to add 

this sort of optimization. Although this can impose some additional development 

burden on the programmer, it is relatively minor, and the potential performance 

bene
 ts can be substantial. In the example shown above, even this fairly simple 
use of shared memory returns a roughly 20% performance improvement on 

representative matrices derived from 3D sur
 e availability of an 
explicitly managed memory in lieu of an implicit cache also has the advantage 

that caching and prefetching policies can be sp
 cally tailored to the application 
needs.

 C.8 Real Stuff: Mapping Applications to GPUs C-
59__global__ void csrmul_cached(unsigned int *Ap, unsigned int *Aj,                   float *Av, unsigned int num_rows,                   const float *x, float *y)
{
    // Cache the rows of x[] corresponding to this block.
    __shared__ float cache[blocksize];    unsigned int block_begin = blockIdx.x * blockDim.x;    unsigned int block_end   = block_begin + blockDim.x;    unsigned int row         = block_begin + threadIdx.x;    // Fetch and cache our window of x[].    if( row<num_rows)  cache[threadIdx.x] = x[row];    __syncthreads();    if( row<num_rows )    {        unsigned int row_begin = Ap[row];        unsigned int row_end   = Ap[row+1];        float sum = 0, x_j;        for(unsigned int col=row_begin; col<row_end; ++col)        {            unsigned int j = Aj[col];                        // Fetch x_j from our cache when possible            if( j>=block_begin && j<block_end )
                x_j = cache[j-block_begin];            else                x_j = x[j];            sum += Av[col] * x_j;        }
        y[row] = sum;    }}FIGURE C.8.5 Shared memory version of sparse matrix-vector multiply.

C-60 Appendix C Graphics and Computing GPUs
 ese are fairly simple kernels whose purpose is to illustrate basic techniques 
in writing CUDA programs, rather than how to achieve maximal performance. 
Numerous possible avenues for optimization are available, several of which are 

explored by Williams, et al. [2007] on a handful o
 erent multicore architectures. 
Nevertheless, it is still instructive to examine the comparative performance of even 

these simplistic kernels. On a 2 GHz Intel Core2 Xeon E5335 processor, the 
csrmul_serial() kernel runs at roughly 202 million nonzeros processed per second, for 
a collection of Laplacian matrices derived from 3D triangulated surface meshes. 

Parallelizing this kernel with the 
parallel_for construct provided by Intel’s 
 reading Building Blocks produces parallel speed-ups of 2.0, 2.1, and 2.3 running 
on two, four, and eight cores of the machine, respectively. On a GeForce 8800 Ultra, 

the 
csrmul_kernel() and 
csrmul_cached() kernels achieve processing rates 
of roughly 772 and 920 million nonzeros per second, corresponding to parallel 

speed-ups of 3.8 and 4.6 times over the serial performance of a single CPU core.
Scan and ReductionParallel 
scan
, also known as parallel 
pre
 x sum
, is one of the most important 
building blocks for data-parallel algorithms [Blelloch, 1990]. Given a sequence 
a of 
n elements:
[,,,]
aaa
n011
–and a binary associative operator 
, the 
scan function computes the sequence:
scan(,)[,(),,()]
aaaaaaa
n
001011
––As an example, if we take 
 to be the usual addition operator, then applying scan 
to the input array
a[]31704163
will produce the sequence of partial sums:
scan()
a,[]
34111115162225
 is scan operator is an 
inclusive
 scan, in the sense that element 
i of the output 
sequence incorporates element 
ai of the input. Incorporating only previous elements 
would yield an 
exclusive
 scan operator, also known as a 
pre
 x-sum
 operation.
 e serial implementation of this operation is extremely simple. It is simply a 
loop that iterates once over the entire sequence, as shown in Figure C.8.6.
At
 rst glance, it might appear that this operation is inherently serial. However, 
it can actually be implemented in pa
  ciently. 
 e key observation is that 

 C.8 Real Stuff: Mapping Applications to GPUs C-
61FIGURE C.8.6 Template for serial plus-scan.
FIGURE C.8.7 CUDA template for parallel plus-scan.
template<class T>__host__ T plus_scan(T *x, unsigned int n){
    for(unsigned int i=1; i<n; ++i)
        x[i] = x[i-1] + x[i];
}template<class T>__device__ T plus_scan(T *x){
    unsigned int i = threadIdx.x;
    unsigned int n = blockDim.x;    for(unsigned int offset=1; offset<n; offset *= 2)
    {
        T t;        if(i>=offset)  t = x[i-offset];        __syncthreads();        if(i>=offset)  x[i] = t + x[i];        __syncthreads();    }    return x[i];}because addition is associative, we are free to change the order in which elements 
are added together. For instance, we can imagine adding pairs of consecutive 

elements in parallel, and then adding these partial sums, and so on.
One simple scheme for doing this is from Hillis and Steele [1989]. An 
implementation of their algorithm in CUDA is shown in Figure C.8.7. It assumes 

that the input array 
x[ ] contains exactly one element per thread of the thread 
block. It performs log
2 n iterations of a loop collecting partial sums together.
To understand the action of this loop, consider Figure C.8.8, which illustrates 
the simple case for 
n  8 threads and elements. Each level of the diagram represents 
one step of the loop
 e lines indicate the location from which the data is being 
fetched. For each element of the output (i.e., th
 nal row of the diagram) we are 
building a summation tree over the input elemen
 e edges highlighted in blue 
show the form of this summation tree for th
 nal elemen
 e leaves of this tree 
are all the initial elements. Tracing back from any output element shows that it 

incorporates all input values up to and including itself.

C-62 Appendix C Graphics and Computing GPUs
While simple, this algorithm is no
  cient as we would like. Examining 
the serial implementation, we see that it performs 
O(n) addition
 e parallel 
implementation, in contrast, performs 
O(n log n) additions. For this reason, it 
is not 
wo
  cient
, since it does more work than the serial implementation to 
compute the same result. Fortunately, there are other techniques for implementing 
scan that are work e
  cient. Details on mor
  cient implementation techniques 
and the extension of this per-block procedure to multiblock arrays are provided by 

Sengupta, et al. [2007].
In some instances, we may only be interested in computing the sum of all 
elements in an array, rather than the sequence of all pr
 x sums returned by 
scan.  is is the 
parallel reduction
 problem. We could simply use a scan algorithm to 
perform this computation, but reduction can generally be implemented more 

  ciently than scan.
Figure C.8.9 shows the code for computing a reduction using addition. In this 
example, each thread simply loads one element of the input sequence (i.e., it initially 

sums a subsequence of length 1). At the end of the reduction, we want thread 0 to 

hold the sum of all elements initially loaded by the threads of its bloc
 e loop in 
this kernel implicitly builds a summation tree over the input elements, much like 

the scan algorithm above.
At the end of this loop, thread 0 holds the sum of all the values loaded by this block. 
If we want th
 nal value of the location pointed to by 
total to contain the total of all 
elements in the array, we must combine the partial sums of all the blocks in the grid. 

One strategy to do this would be to have each block write its partial sum into a second 

array and then launch the reduction kernel again, repeating the process until we had 

reduced the sequence to a single value. A more attractive alternative supported by 

the Tesla GPU architecture is to use the 
atomicAdd() primitive, an
  cient atomic 
FIGURE C.8.8 Tree-based parallel scan data references.
x[0]x[0]x[0]x[0]x[1]x[1]x[1]x[1]x[2]x[2]x[2]
x[2]x[3]x[3]x[3]x[3]x[4]x[4]x[4]x[4]x[5]x[5]x[5]x[6]x[6]x[6]x[5]x[6]x[7]
x[7]x[i] +=x[iŒ1];
x[i] +=x[iŒ2];
x[i] +=x[iŒ4];
x[7]x[7]
 C.8 Real Stuff: Mapping Applications to GPUs C-
63read-modify-write primitive supported by the memory subsyst
 is eliminates 
the need for additional temporary arrays and repeated kernel launches.
Parallel reduction is an essential primitive for parallel programming and 
highlights the importance of per-block shared memory and low-cost barriers in 
making cooperation among thre
  cien
 is degree of data sh
  ing among 
threads would be prohibitively expensive if done in o
 -chip global memory.
Radix Sort
One important application of scan primitives is in the implementation of sorting 

rout
 e code in Figure C.8.10 implements a radix sort of integers across a 
single thread block. It accepts as input an array 
values containing one 32-bit 
integer for each thread of the block. Fo
  ciency, this array should be stored in 
per-block shared memory, but this is not required for the sort to behave correctly.
 is is a fairly simple implementation of radix sort. It assumes the availability of 
a procedure 
partition_by_bit() that will partition the given array such that 
__global__void plus_reduce(int *input, unsigned int N, int *total)
{
    unsigned int tid = threadIdx.x;
    unsigned int i   = blockIdx.x*blockDim.x + threadIdx.x;    // Each block loads its elements into shared memory, padding    // with 0 if N is not a multiple of blocksize    __shared__ int x[blocksize];    x[tid] = (i<N) ? input[i] : 0;    __syncthreads();    // Every thread now holds 1 input value in x[]    //    // Build summation tree over elements.    for(int s=blockDim.x/2; s>0; s=s/2)    {
        if(tid < s)  x[tid] += x[tid + s];        __syncthreads();    }    // Thread 0 now holds the sum of all input values    // to this block. Have it add that sum to the running total    if( tid == 0 )  atomicAdd(total, x[tid]);}FIGURE C.8.9 CUDA implementation of plus-reduction.

C-64 Appendix C Graphics and Computing GPUs
all values with a 0 in the designated bit will come before all values with a 1 in that 
bit. To produce the correct output, this partitioning must be stable.
Implementing the partitioning procedure is a simple application of sca
 read 
i holds the value 
xi and must calculate the correct output index at which to write 
this value. To do so, it needs to calculate (1) the number of threads 
j  i for which 
the designated bit is 1 and (2) the total number of bits for which the designated bit 

 e CUDA code for 
partition_by_bit() is shown in Figure C.8.11.
__device__ void radix_sort(unsigned int *values){
    for(int bit=0; bit<32; ++bit)
    {
        partition_by_bit(values, bit);
        __syncthreads();
    }
}FIGURE C.8.10 CUDA code for radix sort.
__device__ void partition_by_bit(unsigned int *values,
                                 unsigned int bit)
{
    unsigned int i    = threadIdx.x;    
    unsigned int size = blockDim.x;    unsigned int x_i  = values[i];
    unsigned int p_i  = (x_i >> bit) & 1;    values[i] = p_i;    __syncthreads();    // Compute number of T bits up to and including p_i.    // Record the total number of F bits as well.    unsigned int T_before = plus_scan(values);        unsigned int T_total  = values[size-1];    unsigned int F_total  = size - T_total;    __syncthreads();    // Write every x_i to its proper place    if( p_i )        values[T_before-1 + F_total] = x_i;    else        values[i - T_before] = x_i;}FIGURE C.8.11 CUDA code to partition data on a bit-by-bit basis, as part of radix sort.

 C.8 Real Stuff: Mapping Applications to GPUs C-
65A similar strategy can be applied for implementing a radix sort kernel that sorts 
an array of large length, rather than just a one-block array
 e fundamental step 
remains the scan procedure, although when the computation is partitioned across 
multiple kernels, we must double-b
 er the array of values rather than doing the 
partitioning in place. Details on performing radix sorts on large arra
  ciently 
are provided by Satish et al. [2008].
N-Body Applications on a GPU1Nyland, et al. [2007] describe a simple yet useful computational kernel with 

excellent GPU performance—the 
all-pairs N-body
 algorithm. It is a time-consuming 
component of many scien
 c applications. N-body simulations calculate the 
evolution of a system of bodies in which each body continuously interacts with 

every other body. One example is an astrophysical simulation in which each body 

represents an individual star, and the bodies gravitationally attract each other. 

Other examples are protein folding, where N-body simulation is used to calculate 

electrostatic and van der Waals forces; turbulen
 ow simulation; and global 
illumination in computer graphics.
 e all-pairs N-body algorithm calculates the total force on each body in the 
system by computing each pair-wise force in the system, summing for each body. 

Many scientists consider this method to be the most accurate, with the only loss of 

precision coming from th
 oating-point hardware operation
 e drawback is its 
O(n2) computational complexity, which is far too large for systems with more than 
10 bodies. To overcome this high cost, several simp
 cations have been proposed 
to yield O(
n log n) and O(
n) algorithms; examples are the Barnes-Hut algorithm, 
the Fast Multipole Method and Particle-Mesh-Ewald summation. All of the 
fast
 methods still rely on the all-pairs method as a kernel for accurate computation of 

short-range forces; thus it continues to be important.
N-Body MathematicsFor gravitational simulation, calculate the body-body force using elementary 

physics. Between two bodies indexed by 
i and 
j, the 3D force vector is:
frrijijijijijGmm||||
||||
2×r e force magnitude is calculated in th
  term, while the direction is computed 
in the right (unit vector pointing from one body to the other).
Given a list of interacting bodies (an entire system or a subset), the calculation is 
simple: for all pairs of interactions, compute the force and sum for each body. Once 

the total forces are calculated, they are used to update each body’s position and 

velocity, based on the previous position and velocity
 e calculation of the forces 
has complexity O(
n2), while the update is O(
n).1 Adapted from Nyland et al. [2007], “Fast N-Body Simulation with CUDA,” Chapter 31 of 
GPU Gems 3.
C-66 Appendix C Graphics and Computing GPUs
 e serial force-calculation code uses two nested for-loops iterating over pairs of 
bo
 e outer loop selects the body for which the total force is being calculated, 
and the inner loop iterates over all the bo
 e inner loop calls a function that 
computes the pair-wise force, then adds the force into a running sum.
To compute the forces in parallel, we assign one thread to each body, since the 
calculation of force on each body is independent of the calculation on all other 
bodies. Once all of the forces are computed, the positions and velocities of the 

bodies can be updated.
 e code for the serial and parallel versions is shown in Figure C.8.12 and Figure 
 e serial version has two nested for-loo
 e conversion to CUDA, 
like many other examples, converts the serial outer loop to a per-thread kernel 

where each thread computes the total force on a single body
 e CUDA kernel 
computes a global thread ID for each thread, replacing the iterator variable of the 

serial outer loop. Both ker
 nish by storing the total acceleration in a global 
array used to compute the new position and velocity values in a subsequent step. 
__global__ void accel_on_one_body(){ int i = threadIdx.x + blockDim.x * blockIdx.x;
 int j;
 float3 acc(0.0f, 0.0f, 0.0f);
 for (j = 0; j < N; j++) {
  acc = body_body_interaction(acc, body[i], body[j]);
 }
 accel[i] = acc;

}FIGURE C.8.13 CUDA thread code to compute the total force on a single body.
FIGURE C.8.12 Serial code to compute all pair-wise forces on N bodies.void accel_on_all_bodies(){
 int i, j;

 float3 acc(0.0f, 0.0f, 0.0f);
 for (i = 0; i < N; i++) {
 for (j = 0; j < N; j++) {
 acc = body_body_interaction(acc, body[i], body[j]);

 }
 accel[i] = acc;
 }
}
 C.8 Real Stuff: Mapping Applications to GPUs C-
67 e outer loop is replaced by a CUDA kernel grid that launches 
N threads, one 
for each body.
Optimization for GPU Execution e CUDA code shown is functionally correct, but is no
  cient, as it ignores 
key architectural features. Better performance can be achieved with three main 
optimizations. First, shared memory can be used to avoid identical memory reads 

between threads. Second, using multiple threads per body improves performance 

for small values of 
N ird, loop unrolling reduces loop overhead.
Using Shared memory
Shared memory can hold a subset of body positions, much like a cache, eliminating 

redundant global memory requests between threads. We optimize the code shown 

above to have each of 
p threads in a thread-block load 
one position into shared 
memory (for a total of 
p positions). Once all the threads have loaded a value into 
shared memory, ensured by 
__syncthreads(), each thread can then perform 
p interactions (using the data in shared memor
 is is repeated 
N/p times to 
complete the force calculation for each body, which reduces the number of requests 

to memory by a factor of 
p (typically in the range 32–128).
 e function called 
accel_on_one_body() requires a few changes to support 
this optimizatio
 e mo
 ed code is shown in Figure C.8.14.
__shared__ float4 shPosition[256];…__global__ void accel_on_one_body(){ int i = threadIdx.x + blockDim.x * blockIdx.x;
 int j, k;

 int p = blockDim.x;
 float3 acc(0.0f, 0.0f, 0.0f);
 float4 myBody = body[i];
 for (j = 0; j < N; j += p) {  // Outer loops jumps by p each time
  shPosition[threadIdx.x] = body[j+threadIdx.x];

  __syncthreads();

  for (k = 0; k < p; k++) { // Inner loop accesses p positions

   acc = body_body_interaction(acc, myBody, shPosition[k]);

  }

  __syncthreads();

 }
 accel[i] = acc;

}FIGURE C.8.14 CUDA code to compute the total force on each body, using shared memory to improve performance.

C-68 Appendix C Graphics and Computing GPUs
 e loop that formerly iterated over all bodies now jumps by the block dimension 
p. Each iteration of the outer loop loads 
p successive positions into shared memory 
(one position per thre
 e threads synchronize, and then 
p force calculations 
are computed by each thread. A second synchronization is required to ensure that 
new values are not loaded into shared memo
ry prior to all threads completing the 
force calculations with the current data.
Using shared memory reduces the memory bandwidth required to less than 
10% of the total bandwidth that the GPU can sustain (using less than 5 GB/s). 

 is optimization keeps the application busy performing computation rather than 
waiting on memory accesses, as it would have done without the use of shared 

memory
 e performance for varying values of N is shown in Figure C.8.15.
Using Multiple Threads per BodyFigure C.8.15 shows performance degradation for problems with small values of 
N (N  4096) on the GeForce 8800 GTX. Many researc
 orts that rely on N-body 
calculations focus on small 
N (for long simulation times), making it a target of 
our optimizatio
 orts. Our presumption to explain the lower performance was 
that there was simply not enough work to keep the GPU busy when 
N is small. 
 e solution is to allocate more threads per body. We change the thread-block 
dimensions from (
p, 1, 1) to (
p, q, 1), where 
q threads divide the work of a single body 
into equal parts. By allocating the additional threads within the same thread block, 

partial results can be stored in shared memory. When all the force calculations are 
FIGURE C.8.15 Performance measurements of the N-body application on a GeForce 8800 
GTX and a GeForce 9600.
 e 8800 has 128 stream processors at 1.35 GHz, while the 9600 has 64 at 0.80 
GHz (about 30% of th
 e peak performance is 242 GFLOPS. For a GPU with more processors, the 
problem needs to be bigger to achieve full performance (the 9600 peak is around 2048 bodies, while the 8800 
doesn’t reach its peak until 16,384 bodies). For small N, more than one thread per body ca
 cantly 
improve performance, but eventually incurs a performance penalty as N grows.
250N-Body Performance on GPUs
20015010050GFLOPSNumber of Bodies0512768102415362048307240966144819212,288
16,38424,57632,7681 thread, 88002 threads, 8800
4 threads, 8800
1 thread, 96002 threads, 9600
4 threads, 9600

 C.8 Real Stuff: Mapping Applications to GPUs C-
69done, the 
q partial results can be collected and summed to compute th
 nal result. 
Using two or four threads per body leads to large improvements for small 
N.As an example, the performance on the 8800 GTX jumps by 110% when 
N  1024 (one thread achieves 90 GFLOPS, where four achieve 190 GFLOPS). 
Performance degrades slightly on large 
N, so we only use this optimization for 
N smaller than
 e performance increases are shown in Figure C.8.15 for a 
GPU with 128 processors and a smaller GPU with 64 processors clocked at two-
thirds the speed.
Performance Comparison
 e performance of the N-body code is shown in Figure C.8.15 and Figure C.8.16. 
In Figure C.8.15, performance of high- and medium-performance GPUs is shown, 

along with the performance improvements achieved by using multiple threads per 

body.
 e performance on the faster GPU ranges from 90 to just under 250 GFLOPS.
Figure C.8.16 shows nearly identical code (C
 versus CUDA) running on 
Intel Core2 CPU
 e CPU performance is about 1% of the GPU, in the range of 
0.2 to 2 GFLOPS, remaining nearly constant over the wide range of problem sizes.
FIGURE C.8.16 Performance measurements on the N-body code on a CPU.
 e graph shows 
single precision N-body performance using Intel Core2 CPUs, denoted by their CPU model number. Note 
the dramatic reduction in GFLOPS performance (shown in GFLOPS on the 
y-axis), demonstrating how 
much faster the GPU is compared to the CPU
 e performance on the CPU is generally independent of 
problem size, except for an anomalously low performance when N
16,384 on the X9775 CPU
 e graph 
also shows the results of running the CUDA version of the code (using the CUDA-for-CPU compiler) 

on a single CPU core, where it outperforms the C
 code by 24%. As a programming language, CUDA 
exposes parallelism and locality that a compiler can exploi
 e Intel CPUs are a 3.2 GHz Extreme X9775 
(code named “Penryn”), a 2.66 GHz E8200 (code named “Wolfdale”), a desktop, pre-Penryn CPU, and a 

1.83 GHz T2400 (code named “Yonah”), a 2007 laptop CPU
 e Penryn version of the Core 2 architecture 
is particularly interesting for N-body calculations with its 4-bit divider, allowing division and square root 

operations to execute four times faster than previous Intel CPUs.
Number of BodiesN-Body Performance on Intel CPUs
21.81.61.41.210.8
0.60.40.20512GFLOPS768102415362048307240966144
819212,28816,38424,576
32,768T2400E8200X9775X9775-Cuda
C-70 Appendix C Graphics and Computing GPUs
 e graph also shows the results of compiling the CUDA version of the code 
for a CPU, where the performance improves by 24%. CUDA, as a programming 
language, exposes parallelism, allowing the compiler to make better use of the SSE 

vector unit on a single core
 e CUDA version of the N-body code naturally maps 
to multicore CPUs as well (with grids of blocks), where it achieves nearly perfect 

scaling on an eight-core system with N 
 4096 (ratios of 2.0, 3.97, and 7.94 on two, 
four, and eight cores, respectively).
ResultsWith a mo
 ort, we developed a computational kernel that improves GPU 
performance over multicore CPUs by a factor of up to 157. Execution time for 

the N-body code running on a recent CPU from Intel (Penryn X9775 at 3.2 GHz, 

single core) took more than 3 seconds per frame to run the same code that runs at a 

44 Hz frame rate on a GeForce 8800 GPU. On pre-Penryn CPUs, the code requires 

6–16 seconds, and on older Core2 processors and Pentium IV processor, the time 

is about 25 seconds. We must divide the apparent increase in performance in half, 

as the CPU requires only half as many calculations to compute the same result 

(using the optimization that the forces on a pair of bodies are equal in strength and 

opposite in direction).
How can the GPU speed up the code by such a large amoun
 e answer 
requires inspecting architectural detai
 e pair-wise force calculation requires 
 oating-point operations, comprised mostly of addition and multiplication 
instructions (some of which can be combined using a multiply-add instruction), 

but there are also division and square root instructions for vector normalization. 

Intel CPUs take many cycles for single precision division and square root 

instructions,
2 although this has improved in the latest Penryn CPU family with its 
faster 4-bit divider.
3 Additionally, the limitations in register capacity lead to many 
MOV instructions in the x86 code (presumably to/from L1 cache). In contrast, the 

GeForce 8800 executes a reciprocal square-root thread instruction in four clocks; 

see Section C.6 for special function accuracy. It has a larger regist
 le (per thread) 
and shared memory that can be accessed as an instruction operand. Finally, the 

CUDA compiler emits 15 instructions for one iteration of the loop, compared 

with more than 40 instructions from a variety of x86 CPU compilers. Greater 

parallelism, faster execution of complex instructions, more register space, and an 

  cient compiler all combine to explain the dramatic performance improvement 
of the N-body code between the CPU and the GPU.
2 e x86 SSE instructions reciprocal-square-root (RSQRT*) and reciprocal (RCP*) were 
not considered, as their accuracy is too low to be comparable.
3 Intel Corporation, 
Intel 64 and IA-32 Architectures Optimization Reference Manual
. November 2007. Order Number: 248966-016. Also available at www3.intel.com/design/
processor/manuals/248966.pdf.

 C.8 Real Stuff: Mapping Applications to GPUs C-
71On a GeForce 8800, the all-pairs N-body algorithm delivers more than 240 
GFLOPS of performance, compared to less than 2 GFLOPS on recent sequential 
processors. Compiling and executing the CUDA version of the code on a CPU 

demonstrates that the problem scales well to multicore CPUs, but is s
 cantly 
slower than a single GPU.
We coupled the GPU N-body simulation with a graphical display of the motion, 
and can interactively display 16K bodies interacting at 44 frames per second. 

 is allows astrophysical and biophysical events to be displayed and navigated at 
interactive rates. Additionally, we can parameterize many settings, such as noise 

reduction, damping, and integration techniques, immediately displaying their 

 ects on the dynamics of the syst
 is provides scientists with stunning visual 
imagery, boosting their insights on otherwise invisible systems (too large or small, 

too fast or too slow), allowing them to create better models of physical phenomena.
Figure C.8.17 shows a time-series display of an astrophysical simulation 
of 16K bodies, with each body acting as a galaxy
 e initial co
 guration is a 
FIGURE C.8.17 12 images captured during the evolution of an N-body system with 16,384 bodies.

C-72 Appendix C Graphics and Computing GPUs
spherical shell of bodies rotating about the 
z-axis. One phenomenon of interest to 
astrophysicists is the clustering that occurs, along with the merging of galaxies over 
time. For the interested reader, the CUDA code for this application is available in 

the CUDA SDK from www.nvidia.com/CUDA.
 C.9 Fallacies and Pitfalls
GPUs have evolved and changed so rapidly that many fallacies and pitfalls have 

arisen. We cover a few here.
Fallacy: GPUs are just SIMD vector multiprocessors.
 It is easy to draw the false 
conclusion that GPUs are simply SIMD vector multiprocessors. GPUs do have 
a SPMD-style programming model, in that a programmer can write a single 

program that is executed in multiple thread instances with multiple dat
 e execution of these threads is not purely SIMD or vector, however; it is 
single-
instruction multiple-thread
 (SIMT), described in Section C.4. Each GPU thread 
has its own scalar registers, thread private memory, thread execution state, thread 

ID, independent execution and branch path, an
 ective program counter, and 
can address memory independently. Although a group of threads (e.g., a warp of 32 

threads) executes mor
  ciently when the PCs for the threads are the same, this is 
not necessary. So, the multiprocessors are not purely SIMD
 e thread execution 
model is MIMD with barrier synchronization and SIMT optimizations. Execution 

is more
  cient if individual thread load/store memory accesses can be coalesced 
into block accesses, as well. However, this is not strictly necessary. In a purely 

SIMD vector architecture, memory/register accesses fo
 erent threads must be 
aligned in a regular vector pattern. A GPU has
 no such restriction for register or 
memory accesses; however, execution is more
  cient if warps of threads access 
local blocks of data.
In a further departure from a pure SIMD model, an SIMT GPU can execute 
more than one warp of threads concurrently. In graphics applications, there may 

be multiple groups of vertex programs, pixel programs, and geometry programs 

running in the multiprocessor array concurrently. Computing programs may also 

execut
 erent programs concurrentl
 erent warps.
Fallacy: GPU performance cannot grow faster than Moore’s law.
 Moore’s law 
is simply a rate. It is not a “speed of light” limit for any other rate. Moore’s law 

describes an expectation that, over time, as semiconductor technology advances 

and transistors become smaller, the manufacturing cost per transistor will decline 

 C.9 Fallacies and Pitfalls C-
73exponentially. Put another way, given a constant manufacturing cost, the number 
of transistors will increase exponentially. Gordon Moore [1965] predicted that this 

progression would provide roughly two times the number of transistors for the 

same manufacturing cost every year, and later revised it to doubling every two 

years. Although Moore made the initial prediction in 1965 when there were just 

50 components per integrated circuit, it has proved remarkably consisten
 e reduction of transistor size has historically had other b
 ts, such as lower power 
per transistor and faster clock speeds at constant power.
 is increasing bounty of transistors is used by chip architects to build processors, 
memory, and other components. For some time, CPU designers have used the 

extra transistors to increase processor performance at a rate similar to Moore’s law, 

so much so that many people think that processor performance growth of two 

times every 18–24 months is Moore’s law. In fact, it is not.
Microprocessor designers spend some of the new transistors on processor cores, 
improving the architecture and design, and pipelining for more clock speed
 e rest of the new transistors are used for providing more cache, to make memory 

access faster. In contrast, GPU designers use almost none of the new transistors to 

provide more cache; most of the transistors are used for improving the processor 

cores and adding more processor cores.
GPUs get faster by four mechanisms. First, GPU designers reap the Moore’s law 
bounty directly by applying exponentially more transistors to building more parallel, 

and thus faster, processors. Second, GPU designers can improve on the architecture 

over time, increasing th
  ciency of the processin
 ird, Moore’s law assumes 
constant cost, so the Moore’s law rate can clearly be exceeded by spending more for 

larger chips with more transistors. Fourth, GPU memory systems have increased their 

 ective bandwidth at a pace nearly comparable to the processing rate, by using faster 
memories, wider memories, data compression, and better cach
 e combination of 
these four approaches has historically allowed GPU performance to double regularly, 

roughly every 12 to 18 month
 is rate, exceeding the rate of Moore’s law, has been 
demonstrated on graphics applications for approximately ten years and shows no 

sign of
 cant slowdo
 e most challenging rate limiter appears to be the 
memory system, but competitive innovation is advancing that rapidly too.
Fallacy: GPUs only render 3D graphics; they can’t do general computation.
 GPUs 
are built to render 3D graphics as well as 2D graphics and video. To meet the demands 
of graphics so
 ware developers as expressed in the interfaces and performance/
feature requirements of the graphics APIs, GPUs have become massively parallel 

programmab
 oating-point processors. In the graphics domain, these processors 
are programmed through the graphics APIs and with arcane graphics programming 

languages (GLSL, Cg, and HLSL, in OpenGL and Direct3D). However, there is 

C-74 Appendix C Graphics and Computing GPUs
nothing preventing GPU architects from exposing the parallel processor cores to 
programmers without the graphics API or the arcane graphics languages.
In fact, the Tesla architecture family of GPUs exposes the processors through 
a so
 ware environment known as CUDA, which allows programmers to develop 
general application programs using the C language and soon C
. GPUs are 
Turing-complete processors, so they can run any program that a CPU can run, 

although perhaps less well. And perhaps faster.
Fallacy: GPUs cannot run double precisio
 oating-point programs fast.
 In the 
past, GPUs could not run double precisio
 oating-point programs at all, except 
through so
 ware emulation. And that’s not very fast at all. GPUs have made the 
progression from indexed arithmetic representation (lookup tables for colors) to 
8-bit integers per color component, t
 xed-point arithmetic, to single precision 
 oating-point, and recently added double precision. Modern GPUs perform 
virtually all calculations in single precisio
 oating-point arithmetic, and are 
beginning to use double precision in addition.
For a small additional cost, a GPU can support double precisio
 oating-point 
as well as single precisio
 oating-point. Today, double precision runs more slowly 
than the single precision speed, abou
 ve to ten times slower. For incremental 
additional cost, double precision performance can be increased relative to single 

precision in stages, as more applications demand it.
Fallacy: GPUs don’t d
 oating-point correctly.
 GPUs, at least in the Tesla 
architecture family of processors, perform single precisio
 oating-point 
processing at a level prescribed by th
 oating-point standard. So, in 
terms of accuracy, GPUs are the equal of any other IEEE 754-compliant processors.
Today, GPUs do not implement some of the sp
 c features described in the 
standard, such as handling denormalized numbers and providing precis
 oating-
point exceptions. However, the recently introduced Tesla T10P GPU provides full 
IEEE rounding, fused-multiply-add, and denormalized number support for double 

precision.
Pitfall: Just use more threads to cover longer memory latencies.
 CPU cores are 
typically designed to run a single thread at full speed. To run at full speed, every 
instruction and its data need to be available when it is time for that instruction to 

run. If the next instruction is not ready or the data required for that instruction is 

not available, the instruction cannot run and the processor stalls. External memory 

is distant from the processor, so it takes many cycles of wasted execution to fetch 

data from memory. Consequently, CPUs require large local caches to keep running 

 C.9 Fallacies and Pitfalls C-
75without stalling. Memory latency is long, so it is avoided by striving to run in the 
cache. At some point, program working set demands may be larger than any cache. 

Some CPUs have used multithreading to tolerate latency, but the number of threads 

per core has generally been limited to a small number.
 e GPU strateg
 erent. GPU cores are designed to run many threads 
concurrently, but only one instruction from any thread at a time. Another way to 

say this is that a GPU runs each thread slowly, but in aggregate runs the threads 

  ciently. Each thread can tolerate some amount of memory latency, because 
other threads can run.
 e downside of this is that multiple—many multiple threads—are required to 
cover the memory latency. In addition, if memory accesses are scattered or not 

correlated among threads, the memory system will get progressively slower in 

responding to each individual request. Eventually, even the multiple threads will 

not be able to cover the latency. So, the pitfall is that for the “just use more threads” 

strategy to work for covering latency, you have to have enough threads, and the 

threads have to be well-behaved in terms of locality of memory access.
Fallacy: O(
n) algorithms ar
  cult to speed up.
 No matter how fast the GPU is 
at processing data, the steps of transferring data to and from the device may limit 
the performance of algorithms with O(
n) complexity (with a small amount of work 
per dat
 e highest transfer rate over the PCIe bus is approximately 48 GB/
second when DMA transfers are used, and slightly less for nonDMA transfer
 e CPU, in contrast, has typical access speeds of 8–12 GB/second to system memory. 

Example problems, such as vector addition, will be limited by the transfer of the 

inputs to the GPU and the returning output from the computation.
 ere are three ways to overcome the cost of transferring data. First, try to leave 
the data on the GPU for as long as possible, instead of moving the data back and 

forth fo
 erent steps of a complicated algorithm. CUDA deliberately leaves data 
alone in the GPU between launches to support this.
Second, the GPU supports the concurrent operations of copy-in, copy-out and 
computation, so data can be streamed in and out of the device while it is computing. 

 is model is useful for any data stream that can be processed as it arrives. Examples 
are video processing, network routing, data compression/decompression, and even 

simpler computations such as large vector mathematics.
 e third suggestion is to use the CPU and GPU together, improving performance 
by assigning a subset of the work to each, treating the system as a heterogeneous 

computing platfor
 e CUDA programming model supports allocation of work 
to one or more GPUs along with continued use of the CPU without the use of 

threads (via asynchronous GPU functions), so it is relatively simple to keep all 

GPUs and a CPU working concurrently to solve problems even faster.

C-76 Appendix C Graphics and Computing GPUs
 C.10 Concluding Remarks
GPUs are massively parallel processors and have become widely used, not only 
for 3D graphics, but also for many other application
 is wide application was 
made possible by the evolution of graphics devices into programmable processors. 

 e graphics application programming model for GPUs is usually an API such 
as DirectX™ or OpenGL™. For more general-purpose computing, the CUDA 

programming model uses an SPMD (single-program multiple data) style, executing 

a program with many parallel threads.
GPU parallelism will continue to scale with Moore’s law, mainly by increasing 
the number of processors. Only the parallel programming models that can readily 

scale to hundreds of processor cores and thousands of threads will be successful 

in supporting manycore GPUs and CPUs. Also, only those applications that have 

many largely independent parallel tasks w
ill be accelerated by massively parallel 
manycore architectures.
Parallel programming models for GPUs are becoming mor
 exible, for both 
graphics and parallel computing. For example, CUDA is evolving rapidly in the 

direction of full C/C
 functionality. Graphics APIs and programming models 
will likely adapt parallel computing capabilities and models from CUDA. Its 

SPMD-style threading model is scalable, and is a convenient, succinct, and easily 

learned model for expressing large amounts of parallelism.
Driven by these changes in the programming models, GPU architecture is in 
turn becoming more
 exible and more programmable
 xed-function units 
are becoming accessible from general programs, along the lines of how CUDA 

programs already use texture intrinsic functions to perform texture lookups using 

the GPU texture instruction and texture unit.
GPU architecture will continue to adapt to the usage patterns of both graphics 
and other application programmers. GPUs will continue to expand to include 

more processing power through additional processor cores, as well as increasing 

the thread and memory bandwidth available for programs. In addition, the 

programming models must evolve to include programming heterogeneous 

manycore systems including both GPUs and CPUs.
Acknowledgments is appendix is the work of several authors at NVIDIA. We gratefully acknowledge 
th
 cant contributions of Michael Garland, John Montrym, Doug Voorhies, 
Lars Nyland, Erik Lindholm, Paulius Mici
kevicius, Massimiliano Fatica, Stuart 
Oberman, and Vasily Volkov.

 C.11 Historical Perspective and Further Reading C-
77 C.11 Historical Perspective and Further 
ReadingGraphics Pipeline Evolution3D graphics pipeline hardware evolved from the large expensive systems of the 
early 1980s to small workstations and then to PC accelerators in the mid- to late 

1990s. During this period, three major transitions occurred:
 Performance-leading graphics subsystems declined in price from $50,000 to 
$200. Performance increased from 50 million pixels per second to 1 billion pixels per 
second and from 100,000 vertices per second to 10 million vertices per second.
 Native hardware capabilities evolved from wireframe (polygon outlines) to 
 at shaded (constant colo
 lled polygons, to smooth shaded (interpolated 
colo
 lled polygons, to full-scene anti-aliasing with texture mapping and 
rudimentary multitexturing.
Fixed-Function Graphics Pipelines
 roughout this period, graphics hardware was co
 gurable, but not programmable 
by the application developer. With each generation, incremental improvements 

were
 ered. But developers were growing more sophisticated and asking for 
more new features than could be reasonably o
 ered as buil
 xed function
 e NVIDIA GeForce 3, described by Lindholm et al. [2001], took th
 rst step toward 
true general shader programmability. It exposed to the application developer what 

had been the private internal instruction set of th
 oating-point vertex engine. 
 is coincided with the release of Microso
 ’s DirectX 8 and OpenGL’s vertex shader 
extensions. Later GPUs, at the time of DirectX 9, extended general programmability 

an
 oating point capability to the pixel fragment stage, and made texture 
available at the vertex stage.
 e ATI Radeon 9700, introduced in 2002, featured 
a programmable 24-bi
 oating-point pixel fragment processor programmed 
with DirectX 9 and Op
 e GeForce FX added 32-bi
 oating-point pixel 
processor
 is was part of a general trend toward unifying the functionality of 
th
 erent stages, at least as far as the application programmer was concerned. 
NVIDIA’s GeForce 6800 and 7800 series were built with separate processor designs 

and separate hardware dedicated to the vertex and to the fragment processin
 e XBox 360 introduced an earl
 ed processor GPU in 2005, allowing vertex and 
pixel shaders to execute on the same processor.

C-78 Appendix C Graphics and Computing GPUs
Evolution of Programmable Real-Time Graphics
During the last 30 years, graphics architecture has evolved from a simple pipeline for 
drawing wireframe diagrams to a highly parallel design consisting of several deep 

parallel pipelines capable of rendering complex interactive imagery that appears 

three-dimensional. Concurrently, many of the calculations involved became far 

more sophisticated and user programmable.
In these graphics pipelines, certain stages do a great deal o
 oating-point 
arithmetic on completely independent data, such as transforming the position 

of triangle vertexes or generating pixel color
 is data independence is a key 
 erence between GPUs and CPUs. A single frame, rendered in 1/60th of a 
second, might have 1 million triangles and 6 million pixe
 e opportunity to use 
hardware parallelism to exploit th
is data independence is tremendous.
 e sp
 c functions executed at a few graphics pipeline stages vary with 
rendering algorithms and have evolved to be programmable. Vertex programs 

map the position of triangle vertices on to the screen, altering their position, color, 

or orientation. Typically a vertex shader thread inpu
 oating-point (x, y, z, w) 
vertex position and comput
 oating-point (x, y, z) screen position. Geometry 
programs operate on primitiv
 ned by multiple vertices, changing them or 
generating additional primitives. Pixel fragment shaders each “shade” one pixel, 

computin
 oating-point red, green, blue, alpha (RGBA) color contribution to 
the rendered image at its pixel sample (x, y) image position. For all three types of 

graphics shaders, program instances can be run in parallel, because each works on 

independent data, produces independent results, an
 ects.
Between these programmable graphics pipeline stages are dozens o
 xed-function 
stages which perform we
 ned tasks far more
  ciently than a programmable 
processor could and which would b
 t far less from programmability. For 
example, between the geometry processing stage and the pixel processing stage is 

a “rasterizer,” a complex state machine that determines exactly which pixels (and 

portions thereof) lie within each geometric primitive’s boundaries. Together, the 

mix of programmable an
 xed-function stages is engineered to balance extreme 
performance with user control over the rendering algorithms.
Common rendering algorithms perform a single pass over input primitives and 
access other memory resources in a highly coherent manner; these algorithms 

provide excellent bandwidth utilization and are largely insensitive to memory 

latency. Combined with a pixel shader workload that is usually compute-limited, 

these characteristics have guided GPUs alon
 erent evolutionary path than 
CPUs. Whereas CPU die area is dominated by cache memory, GPUs are dominated 

by
 oating-point datapath an
 xed-function logic. GPU memory interfaces 
emphasize bandwidth over latency (since latency can be readily hidden by a high 

thread count); indeed, bandwidth is typically many times higher than a CPU, 

exceeding 100 GB/second in some cas
 e far-higher number o
 ne-grained 
lightweight thre
 ectively exploits the rich parallelism available.

 C.11 Historical Perspective and Further Reading C-
79Beginning with NVIDIA’s GeForce 8800 GPU in 2006, the three programmable 
graphics stages are mapped to an array of
 ed processors; the logical graphics 
pipeline is physically a recirculating path 
that visits these processors three times, 
with much
 xed-function graphics logic between visits. S
 erent rendering 
algorithms present wildly
 erent loads among the three programmable stages, 
t
 cation provides processor load balancing.
Uniﬁ ed Graphics and Computing Processors
By the DirectX 10 generation, the functionality of vertex and pixel fragment 
shaders was to be made identical to the programmer, and in fact a new logical 

stage was introduced, the geometry shader, to process all the vertices of a primitive 

rather than vertices in isolatio
 e GeForce 8800 was designed with DirectX 10 
in mind. Developers were coming up with more sophisticated shading algorithms, 

and this motivated a sharp increase in the available shader operation rate, 

particularly
 oating-point operations. NVIDIA chose to pursue a processor design 
with higher operating frequency than standard-cell methodologies had allowed, 

to deliver the desired operation throughput as are
  ciently as possible. High-
clock-speed design requires substantially more engineerin
 ort, and this favored 
designing one processor, rather than two (or three, given the new geometry stage). 

It became worthwhile to take on the engineering challenges of
 ed processor 
(load balancing and recirculation of a logical pipeline onto threads of the processor 

array) to get the b
 ts of one processor design.
GPGPU: an Intermediate Step
As DirectX 9-capable GPUs became available, some researchers took notice of the 

raw performance growth path of GPUs and began to explore the use of GPUs to 

solve complex parallel problems. DirectX 9 GPUs had been designed only to match 

the features required by the graphics API. To access the computational resources, a 

programmer had to cast their problem into native graphics operations. For example, 

to run many simultaneous instances of a pixel shader, a triangle had to be issued to 

the GPU (with clipping to a rectangle shape if that’s what was desired). Shaders did 

not have the means to perform arbitrary scatter operations to memory
 e only 
way to write a result to memory was to emit it as a pixel color value, and co
 gure 
the frameb
 er operation stage to write (or blend, if desired) the result to a two-
dimensional frameb
 er. Furthermore, the only way to get a result from one pass 
of computation to the next was to write all parallel results to a pixel frameb
 er, 
then use that frameb
 er as a texture map as input to the pixel fragment shader of 
the next stage of the computation. Mapping general computations to a GPU in this 

era was quite awkward. Nevertheless, intrepid researchers demonstrated a handful 

of useful applications with painstakin
 or
 is 
 eld was called “GPGPU” for 
general purpose computing on GPUs.

C-80 Appendix C Graphics and Computing GPUs
GPU ComputingWhile developing the Tesla architecture for the GeForce 8800, NVIDIA realized its 
potential usefulness would be much grea
ter if programmers could think of the GPU 
as a processor. NVIDIA selected a programming approach in which programmers 

would explicitly declare the data-parallel aspects of their workload.
For the DirectX 10 generation, NVIDIA had already begun work on a high-
  ciency 
 oating-point and integer processor that could run a variety of 
simultaneous workloads to support the logical graphics pipeline
 is processor 
was designed to take advantage of the common case of groups of threads executing 

the same code path. NVIDIA added memory load and store instructions with 

integer byte addressing to support the requirements of compiled C programs. It 

introduced the thread block (cooperative thread array), grid of thread blocks, and 

barrier synchronization to dispatch and manage highly parallel computing work. 

Atomic memory operations were added. NVIDIA developed the CUDA C/C
 compiler, libraries, and runtime so
 ware to enable programmers to readily access 
the new data-parallel computation model and develop applications.
Scalable GPUsScalability has been an attractive feature of graphics systems from the beginning. 

Workstation graphics systems gave customers a choice in pixel horsepower by 

varying the number of pixel processor circuit boards installed. Prior to the mid-

1990s PC graphics scaling was almost nonexisten
 ere was one option—the 
VGA controller. As 3D-capable accelerators appeared, the market had room for a 

range of
 erings. 3dfx introduced multiboard scaling with the original SLI (Scan 
Line Interleave) on their Voodoo2, which held the performance crown for its time 

(1998). Also in 1998, NVIDIA introduced distinct products as variants on a single 

architecture with Riva TNT Ultra (high-performance) and Vanta (lo
 rst 
by speed binning and packaging, then with separate chip designs (GeForce 2 GTS & 

GeForce 2 MX). At present, for a given architecture generation, four or
 ve separate 
GPU chip designs are needed to cover the range of desktop PC performance and 

price points. In addition, there are separate segments in notebook and workstation 

system
 er acquiring 3dfx, NVIDIA continued the multi-GPU SLI concept in 
2004, starting with GeForce 6800—providing multi-GPU scalability transparently 

to the programmer and to the user. Functional behavior is identical across the 

scaling range; one application will run unchanged on any implementation of an 

architectural family.
CPUs are scaling to higher transistor counts by increasing the number of 
constant-performance cores on a die, rather than increasing the performance of 

a single core. At this writing the industry is transitioning from dual-core to quad-

core, with eight-core not far behind. Programmers are forced t
 nd fourfold to 
eightfold task parallelism to fully utilize these processors, and applications using 

task parallelism must be rewritten frequently to target each successive doubling of 

 C.11 Historical Perspective and Further Reading C-
81core count. In contrast, the highly multithreaded GPU encourages the use of many-
fold data parallelism and thread parallelism, which readily scales to thousands of 

parallel threads on many processor
 e GPU scalable parallel programming 
model for graphics and parallel computing is designed for transparent and 

portable scalability. A graphics program or CUDA program is written once and 

runs on a GPU with any number of processors. As shown in Section C.1, a CUDA 

programmer explicitly states bot
 ne-grained and coars
e-grained parallelism in 
a thread program by decomposing the problem into grids of thread blocks—the 

same program will r
  ciently on GPUs or CPUs of any size in current and 
future generations as well.
Recent Developments
Academic and industrial work on applications using CUDA has produced 

hundreds of examples of successful CUDA programs. Many of these programs run 

the application tens or hundreds of times faster than multicore CPUs are capable 

of running them. Examples include n-body simulation, molecular modeling, 

computatio
 nance, and oil and gas exploration data processing. Although 
many of these use single precisio
 oating-point arithmetic, some problems require 
double precisio
 e recent arrival of double precisio
 oating point in GPUs 
enables an even broader range of applications to b
 t from GPU acceleration.
For a comprehensive list and examples of current developments in applications 
that are accelerated by GPUs, visit CUDAZone: www.nvidia.com/CUDA.
Future Trends
Naturally, the number of processor cores will continue to increase in proportion to 

increases in available transistors as silicon processes improve. In addition, GPUs 

will continue to enjoy vigorous architectural evolution. Despite their demonstrated 

high performance on data-parallel applications, GPU core processors are still of 

relatively simple design. More aggressive techniques will be introduced with each 

successive architecture to increase the actual utilization of the calculating units. 

Because scalable parallel computing on GPUs is a ne
 eld, novel applications 
are rapidly being created. By studying them, GPU designers will discover and 

implement new machine optimizations.
Further Reading
Akeley, K. and T. Jermoluk [1988]. “High-Performance Polygon Rendering,” 
Proc. SIGGRAPH 1988
 (August), 
239–46.Akeley, K. [1993]. “RealityEngine Graphics.” 
Proc. SIGGRAPH 1993
 (August), 109–16.
Blelloch, G. B. [1990]. “Pr
 x Sums an
 eir Applications.” In John H. Reif (Ed.), 
Synthesis of Parallel 
Algorithms
, Morgan Kaufmann Publishers, San Francisco.
Blythe, D. [2006]. “
 e Direct3D 10 System,” 
ACM Trans. Graphics
, Vol. 25, no. 3 (July), 724–34.

C-82 Appendix C Graphics and Computing GPUs
Buck, I., T. Foley, D. Horn, J. Sugerman, K. Fatahlian, M. Houston, and P. Hanrahan [2004]. “Brook for 
GPUs: Stream Computing on Graphics Hardware.” 
Proc. SIGGRAPH 2004
, 777–86, August. http://doi.acm.
org/10.1145/1186562.1015800
Elder, G. [2002] “Radeon 9700.” Eurographics/SIGGRAPH Workshop on Graphics Hardware, Hot3D 
Session, www.graphicshardware.org/previous/www_2002/presentations/Hot3D-RADEON9700.ppt
Fernando, R. and M. J. Kilgard [2003]. 
 e Cg Tutoria
 e D
 nitive Guide to Programmable Real-Time 
Graphics
, Addison-Wesley, Reading, MA.
Fernando, R. ed. [2004]. 
GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics
, Addison-Wesley, Reading, MA. http://developer.nvidia.com/object/gpu_gems_home.html.

Foley, J., A. van Dam, S. Feiner, and J. Hughes [1995]. 
Computer Graphics: Principles and Practice, second 
edition in C
, Addison-Wesley, Reading, MA.
Hillis, W. D. and G. L. Steele [1986]. “Data parallel algorithms.” 
Commun. ACM
 29, 12 (Dec.), 1170–83. 
http:// doi.acm.org/10.1145/7902.7903.

IEEE Std 754-2008 [2008]. 
IEEE Standard for Floating-Point Arithmetic
. ISBN 978-0-7381-5752-8, STD95802, 
http://ieeexplore.ieee.org/servlet/opac?punumber
4610933 (Aug. 29).
Industrial Light and Magic [2003]. 
OpenEXR
, www.openexr.com.
Intel Corporation [2007]. 
Intel 64 and IA-32 Architectures Optimization Reference Manual.
 November. Order 
Number: 248966-016. Also: www3.intel.com/design/processor/manuals/248966.pdf.

Kessenich, J. [2006]. 
 e OpenGL Sha
ding Langua
ge, Language Version 1.20, Sept. 2006.
 www.opengl.org/
documentation/specs/.

Kirk, D. and D. Voorhies [1990]. “
 e Rendering Architecture of the DN10000VS.” 
Proc. SIGGRAPH 1990
 (August), 299–307.

Lindholm E., M. J. Kilgard, and H. Moreton [2001]. “A User- Programmable Vertex Engine.” 
Proc. SIGGRAPH 
2001 (August), 149–58.
Lindholm E., J. Nickolls, S. Oberman, and J. Montrym [2008]. “NVIDIA Tesla: A U
 ed Graphics and 
Computing Architecture.” 
IEEE Micro
, Vol. 28, no. 2 (March–April), 39–55.
Microso
  Corporation. Microso
  DirectX Sp
 cation, http://msdn.microso
 .com/directx/
Microso
  Corporation. [2003]. Microso
  DirectX 9 Programmable Graphics Pipeline, Microso
  Press, 
Redmond, WA.

Montrym, J., D. Baum, D. Dignam, and C. Migdal [1997]. “I
 niteReality: A Real-Time Graphics System.” 
Proc. SIGGRAPH 1997
 (August), 293–301 .
Montrym, J. and H. Moreton [2005]. “
 e GeForce 6800,” 
IEEE Micro
, Vol. 25, no. 2 (March–April), 41–51.
Moore, G. E. [1965]. “Cramming more components onto integrated circuits,” 
Electronics
, Vol. 38, no. 8 (April 
19).
Nguyen, H., ed. [2008]. 
GPU Gems 3, Addison-Wesley, Reading, MA.

 C.11 Historical Perspective and Further Reading C-
83Nickolls, J., I. Buck, M. Garland, and K. Skadron [2008]. “Scalable Parallel Programming with CUDA,” 
ACM 
Queue, Vol. 6, no. 2 (March–April) 40–53.
NVIDIA [2007]. CUDA Zone. www.nvidia.com/CUDA.
NVIDIA [2007]. 
CUDA Programming Guide 1.1
. http://developer.download.nvidia.com/compute/cuda/1_1/
NVIDIA_CUDA_Programming_Guide_1.1.pdf.

NVIDIA [2007]. 
PTX: Parallel
 read Execution ISA version 1.1
. www.nvidia.com/object/io_1195170102263. 
html.

Nyland, L., M. Harris, and J. Prins [2007]. “Fast N-Body Simulation with CUDA.” In 
GPU Gems 3, H. Nguyen 
(Ed.), Addison-Wesley, Reading, MA.

Oberman, S. F. and M. Y. Siu [2005]. “A High-Performance Are
  cient Multifunction Interpolator,” 
Proc. 
Seventeenth IEEE Symp. Computer Arithmetic
, 272–79.Patterson, D. A. and J. L. Hennessy [2004]. 
Computer Organization and Desig
 e Hardware/So
 ware Inter 
face, third edition, Morgan Kaufmann Publishers, San Francisco.
Pharr, M. ed. [2005]. 
GPU Gems 2: Programming Techniques for High-Performance Graphics and General-
Purpose Computation
, Addison-Wesley, Reading, MA.
Satish, N., M. Harris, and M. Garland [2008]. “Designin
  cient Sorting Algorithms for Manycore GPUs,” 
NVIDIA Technical Report NVR-2008-001.

Segal, M. and K. Akeley [2006]. 
 e OpenGL Graphics System: A Speci
 cation, Version 2.1, Dec. 1, 2006
. www.
opengl.org/documentation/specs/.

Sengupta, S., M. Harris, Y. Zhang, and J. D. Owens [2007]. “Scan Primitives for GPU Computing.” In 
Proc. of 
Graphics Hardware 2007
 (August), 97–106.
Volkov, V. and J. Demmel [2008]. “LU, QR and Cholesky Factorizations using Vector Capabilities of GPUs,” 
Technical Report No. UCB/EECS-2008-49, 1–11. www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-

49.html.
Williams, S., L. Oliker, R. Vuduc, J. Shalf, K. Yelick, and J. Demmel [2007]. “Optimization of sparse matrix-
vector multiplication on emerging multicore platforms,” In 
Proc. Supercomputing 2007
, November.

A custom format such 
as this is slave to the 

architecture of the 

hardware and the 

instruction set it serves. 

 e format must strike 
a proper compromise 

between ROM size, 

ROM-output decoding, 

circuitry size, and 

machine execution rate.
Jim McKevit, et al.
8086 design report, 1997
Mapping Control to Hardware
DD.1 Introduction 
D-3D.2 Implementing Combinational Control 
Units D-4D.3  Implementing Finite-State Machine 
Control D-8D.4 Implementing the Next-State Function with a 
Sequencer D-22APPENDIX
D.5 Translating a Microprogram to Hardware 
D-28D.6 Concluding Remarks 
D-32D.7 Exercises 
D-33 D.1
 IntroductionControl typically has two parts: a combinational part that lacks state and a sequential 
control unit that handles sequencing and the main control in a multicycle design. 

Combinational control units are o
 en used to handle part of the decode and 
control pro
 e ALU control in Chapter 4 is such an example. A single-cycle 
implementation like that in Chapter 4 can also use a combinational controller, 

since it does not require multiple states. Section D.2 examines the implementation 

of these two combinational units from the truth tables of Chapter 4.
Since sequential control units are larger and o
 en more complex, there are a wider 
variety of techniques for implementing a sequential control uni
 e usefulness of 
these techniques depends on the complexity of the control, characteristics such 

as the average number of next states for any given state, and the implementation 

technology.
 e most straightforward way to implement a sequential control function is with 
a block of logic that takes as inputs the current state and the opco
 eld of the 
Instruction register and produces as outputs the datapath control signals and the 

value of the next state.
 e initial representation may be eith
 nite-state diagram 
or a microprogram. In the latter case, each microinstruction represents a state.

D-4 Appendix D Mapping Control to Hardware
In an implementation usin
 nite-state controller, the next-state function will 
be computed with logic. Section D.3 constructs such an implementation both for 
a ROM and a PLA.
An alternative method of implementation computes the next-state function by 
using a counter that increments the current state to determine the next state. When 

the next state doesn’t follow sequentially, other logic is used to determine the state. 

Section D.4 explores this type of implementation and shows how it can be used to 

implemen
 nite-state control.
In Section D.5, we show how a microprogram representation of sequential 
control is translated to control logic.
 D.2
 Implementing Combinational Control UnitsIn this section, we show how the ALU control unit and main control unit for the 
single clock design are mapped down to the gate level. With modern 
computer-
aided design
 (CAD) systems, this process is completely mechanical
 e examples 
illustrate how a CAD system takes advantage of the structure of the control 

function, including the presence of don’t-care terms.
Mapping the ALU Control Function to GatesFigure D.2.1 shows the truth table for the ALU control function that was developed 

in Section 4.4. A logic block that implements this ALU control function will have 

four distinct outputs (called Operation3, Operation2, Operation1, and Operation0), 

each corresponding to one of the four bits of the ALU control in the last column 

of Figure D
 e logic function for each output is constructed by combining all 
the truth table entries that set that particular output. For example, the low-order 

bit of the ALU control (Operation0) is set by the last two entries of the truth table 

in Figure D
 us, the truth table for Operation0 will have these two entries.
Figure D.2.2 shows the truth tables for each of the four ALU control bits. 
We have taken advantage of the common structure in each truth table to 

incorporate additional don’t cares. For example, th
 ve lines in the truth table of 
Figure D.2.1 that set Operation1 are reduced to just two entries in Figure D.2.2. A 

logic minimization program will use the don’t-care terms to reduce the number of 

gates and the number of inputs to each gate in a logic gate realization of these truth 

tables.
A confusing aspect of Figure D.2.2 is that there is no logic function for Opera-
tio
 at is because this control line is only used for the NOR operation, which is 
not needed for the MIPS subset in Figure 4.12.
From the simp
 ed truth table in Figure D.2.2, we can generate the logic shown 
in Figure D.2.3, which we call the 
ALU control block.
 is process is straightforward 

 D.2 Implementing Combinational Control Units D-
5ALUOpFunct ﬁeld
OperationALUOp1ALUOp0F5F4F3F2F1F0
00XXXXXX0010
X1XXXXXX0110 
1XXX0000
0010 1XXX0010
0110 1XXX01000000
1XXX01010001 
1XXX1010
0111 FIGURE D.2.1
 The truth table for the 4 ALU control bits (called Operation) as a function of 
the ALUOp and function code ﬁ eld.  is table is the same as that shown in Figure 4.13.
ﬁ edoc noitcnuFpOULA elds
ALUOp1ALUOp0F5F4F3F2F1F0
01XXXXXX
1XXXXX1X
a. The truth table for Operation2 = 1 (this table corresponds to the second to left bit of the Operation 
 eld in Figure D.2.1)
ﬁ edoc noitcnuFpOULA elds
ALUOp1ALUOp0F5F4F3F2F1F0
0XXXXXXX
XXXXX0XX
b. The truth table for Operation1 = 1
ﬁ edoc noitcnuFpOULA elds
ALUOp1ALUOp0F5F4F3F2F1F0
1XXXXXX1
1XXX1XXX
c. The truth table for Operation0 = 1
FIGURE D.2.2
 The truth tables for three ALU control lines. 
Only the entries for which the output 
is 1 are sho
 e bits in eac
 eld are numbered from right t
  starting with 0; thus F5 is the most 
 cant bit of the function
 eld, and F0 is the le
 cant bit. Similarly, the names of the signals 
corresponding to the 4-bit operation code supplied to the ALU are Operation3, Operation2, Operation1, 
and Operation0 (with the last being the le
 cant bi
 us the truth table above shows the input 
combinations for which the ALU control should be 0010, 0001, 0110, or 0111 (the other combinations are 

not us
 e ALUOp bits are named ALUOp1 and AL
 e three output values depend on the 2-bit 
AL
 eld and, when that
 eld is equal to 10, the 6-bit function code in the instruction. Accordingly, when 
the AL
 eld is not equal to 10, we don’t care about the function code value (it is represented by an X). 
 ere is no truth table for when Operation3
1 because it is always set to 0 in Figure D.2.1. See Appendix B 
for more background on don’t cares.

D-6 Appendix D Mapping Control to Hardware
and can be done with a CAD program. An example of how the logic gates can be 
derived from the truth tables is given in the legend to Figure D.2.3.
 is ALU control logic is simple because there are only three outputs, and only a 
few of the possible input combinations need to be recognized. If a large number of 

possible ALU function codes had to be transformed into ALU control signals, this 

simple method would not b
  cient. Instead, you could use a decoder, a memory, 
or a structured array of logic gat
 ese techniques are described in Appendix B, 
and we will see examples when we examine the implementation of the multicycle 

controller in Section D.3.
Elaboration:  In general, a logic equation and truth table representation of a logic 
function are equivalent. (We discuss this in further detail in Appendix B. However, when a 
tr es the entries that result in nonzero outputs, it may not completely 

describe the logic function. A full truth table completely indicates all don’t-care entries. 

For example, the encoding 11 for ALUOp always generates a don’t care in the output. 

Thus a complete truth table would have XXX in the output portion for all entries with 11 

 eld. These don’ eld 10 and 
Operation2
Operation1
Operation0
Operation
ALUOp1F3F2
F1
F0F (5Œ0)ALUOp0ALUOpALU control block
Operation3
FIGURE D.2.3
  The ALU control block generates the four ALU control bits, based on the 
function code and ALUOp bits. is logic is generated directly from the truth table in Figure D.2.2. Only 
four of the six bits in the function code are actually needed as inputs, since the upper two bits are always don’t 
cares. Let’s examine how this logic relates to the truth table of Figure D.2.2. Consider the Operation2 output, 

which is generated by two lines in the truth table for Operatio
 e second line is the AND of two terms 
(F1  1 and ALUOp1 
 1); the top two-input AND gate corresponds to this ter
 e other term that causes 
Operation2 to be asserted is simply AL
 ese two terms are combined with an OR gate whose output 
is Operatio
 e outputs Operation0 and Operation1 are derived in similar fashion from the truth table. 
Since Operation3 is always 0, we connect a signal and its complement as inputs to an AND gate to generate 0.

 D.2 Implementing Combinational Control Units D-
701 with 1X and X1, respectively. Incorporating the don’t-care terms and minimizing the 
logic is both complex and error-prone and, thus, is better left to a program.
Mapping the Main Control Function to GatesImplementing the main control function with an unstructured collection of gates, 
as we did for the ALU control, is reasonable because the control function is neither 

complex nor large, as we can see from the truth table shown in Figure D.2.4. 

However, if most of the 64 possible opcodes were used and there were many more 

control lines, the number of gates would be much larger and each gate could have 

many more inputs.
Since any function can be computed in two levels of logic, another way to 
implement a logic function is with a structured two-level logic array. Figure D.2.5 

shows such an implementation. It uses an array of AND gates followed by an array 

of OR gat
 is structure is called a 
programmable logic array
 (PLA). A PLA is one 
of the most common ways to implement a control function. We will return to the 

topic of using structured logic elements to implement control when we implement 

th
 nite-state controller in the next section.
ControlSignal nameR-format
lwswbeq
InputsOp50110
Op40000

Op30010

Op20001
Op10110
Op00110
OutputsRegDst10XX
ALUSrc0110
MemtoReg01XX
RegWrite1100
MemRead0100

MemWrite0010
Branch0001
ALUOp11000

ALUOp00001
FIGURE D.2.4
 The control function for the simple one-clock implementation is completely speciﬁ ed by this truth table.
 is table is the same as that shown in Figure 4.22.

D-8 Appendix D Mapping Control to Hardware
 D.3
 Implementing Finite-State Machine 
ControlTo implement the contro
 nite-state machine, we mu
 rst assign a number to 
each of the 10 states; any state could use any number, but we will use the sequential 
numbering for simplicity. Figure D.3.1 shows th
 nite-state diagram. With 10 
states, we will need 4 bits to encode the state number, and we call these state bits S3, 

S2, S1, an
 e current-state number will be stored in a state register, as shown 
in Figure D.3.2. If the states are assigned sequentially, state 
i is encoded using the 
R-formatIwswbeq
Op0Op1Op2Op3Op4Op5InputsOutputsRegDstALUSrcMemtoRegRegWrite
MemReadMemWrite
Branch
ALUOp1ALUOp0FIGURE D.2.5
 The structured implementation of the control function as described by the 
truth table in Figure D.2.4.
 e structure, called a programmable logic array (PLA), uses an array of 
AND gates followed by an array of OR gat
 e inputs to the AND gates are the function inputs and their 
inverses (bubbles indicate inversion o
 e inputs to the OR gates are the outputs of the AND gates 
(or, as a degenerate case, the function inputs and invers
 e output of the OR gates is the function outputs.

 D.3 Implementing Finite-State Machine Control D-
9PCWrite
PCSource = 10ALUSrcA = 1ALUSrcB = 00ALUOp = 01PCWriteCond
PCSource = 01ALUSrcA = 1ALUSrcB = 00ALUOp = 10RegDst = 1RegWriteMemtoReg = 0MemWriteIorD = 1MemReadIorD = 1ALUSrcA = 1ALUSrcB = 10ALUOp = 00RegDst = 0
RegWriteMemtoReg = 1ALUSrcA = 0ALUSrcB = 11ALUOp = 00MemReadALUSrcA = 0IorD = 0IRWriteALUSrcB = 01ALUOp = 00PCWritePCSource = 00Instruction fetchInstruction decode/register fetchJumpcompletionBranchcompletionExecutionMemory addresscomputationMemory
accessMemory
accessR-type completionWrite-back step (Op = 'LW') or (Op = 'SW')(Op = R-type)(Op = 'BEQ')(Op = 'J') (Op = 'SW')(Op = 'LW')4019862753StartFIGURE D.3.1
 The ﬁ nite-state diagram for multicycle control.

D-10 Appendix D Mapping Control to Hardware
state bits as the binary number 
i. For example, state 6 is encoded as 0110
two
 or S3 
 0, S2  1, S1  1, S0  0, which can also be written as
S3 · S2 · S1 · S0 e control unit has outputs that specify the next state.
 ese are written into 
the state register on the clock edge and become the new state at the beginning of 
the next clock cycle following the active clock edge. We name these outputs NS3, 

NS2, NS1, and NS0. Once we have determined the number of inputs, states, and 

outputs, we know what the basic outline of the control unit will look like, as we 

show in Figure D.3.2.
PCWrite
PCWriteCond
IorDMemtoRegPCSource
ALUOp
ALUSrcBALUSrcARegWrite
RegDstNS3NS2NS1NS0Op5Op4
Op3
Op2
Op1
Op0S3S2
S1
S0State registerIRWrite
MemReadMemWrite
Instruction register
opcode fieldOutputsControl logic
InputsFIGURE D.3.2
 The control unit for MIPS will consist of some control logic and a register 
to hold the state. e state register is written at the active clock edge and is stable during the clock 
cycle

 D.3 Implementing Finite-State Machine Control D-
11 e block labeled “control logic” in Figure D.3.2 is combinational logic. We can 
think of it as a big table giving the value of the outputs in terms of the inpu
 e logic in this block implements the tw
 erent parts of th
 nite-state machine. 
One part is the logic that determines the setting of the datapath control outputs, 
which depend only on the state bi
 e other part of the control logic implements 
the next-state function; these equations 
determine the values of the next-state bits 
based on the current-state bits and the other inputs (the 6-bit opcode).
Figure D.3.3 shows the logic equations: the top portion shows the outputs, and 
the bottom portion shows the next-state functio
 e values in this table were 
pOsetats tnerruCtuptuOPCWritestate0 + state9
PCWriteCondstate8

IorDstate3 + state5

MemReadstate0 + state3

MemWritestate5

IRWritestate0

MemtoRegstate4

PCSource1state9

PCSource0state8

ALUOp1state6

ALUOp0state8

ALUSrcB1state1 +state2 

ALUSrcB0state0 + state1

ALUSrcAstate2 + state6 + state8

RegWritestate4 + state7

RegDststate7

NextState0state4 + state5 + state7 + state8 + state9

NextState1state0

NextState2state1
(Op = 'lw') + (Op = 'sw')   NextState3state2
(Op = 'lw') NextState4state3
NextState5state2
(Op = 'sw')  NextState6state1
(Op = 'R-type') NextState7state6
NextState8state1
(Op = 'beq') NextState9state1
(Op = 'jmp') FIGURE D.3.3
 The logic equations for the control unit shown in a shorthand form.
 Remember 
that “
” stands for OR in logic equation
 e state inputs and NextState outputs must be expanded by using 
the state encoding. Any blank entry is a don’t care.

D-12 Appendix D Mapping Control to Hardware
determined from the state diagram in Figure D.3.1. Whenever a control line is 
active in a state, that state is entered in the second column of the table. Likewise, the 

next-state entries are made whenever on
e state is a successor to another.
In Figure D.3.3, we use the abbreviation state
N to stand for current state 
N us, state
N is replaced by the term that encodes the state number 
N. We use NextState
N to stand for the setting of the next-state outputs to 
N is output is implemented 
using the next-state outputs (NS). When NextState
N is active, the bits NS[3–0] are 
set corresponding to the binary version of the value 
N. Of course, since a given 
next-state bit is activated in multiple next states, the equation for each state bit will 

be the OR of the terms that activate that signal. Likewise, when we use a term such 

as (Op  ‘lw’), this corresponds to an AND of the opcode inputs that sp
 es the 
encoding of the opcode 
lw in 6 bits, just as we did for the simple control unit in the 
previous section of this chapter. Translating the entries in Figure D.3.3 into logic 

equations for the outputs is straightforward.
Logic Equations for Next-State OutputsGive the logic equation for the low-order next-state bit, NS0.
 e next-state bit NS0 should be active whenever the next state has NS0 
 1 in the state encodin
 is is true for NextState1, NextState3, NextState5, 
NextState7, and NextStat
 e entries for these states in Figure D.3.3 supply 
the conditions when these next-state values should be active
 e equation for 
each of these next states is given below
 e 
 rst equation states that the next 
state is 1 if the current state is 0; the current state is 0 if each of the state input 
bits is 0, which is what the rightmost product term indicates.
NextState1 
 State0 
 S3 · S2 · S1 · S0 NextState3  
  State2 
· (Op[5-0]1w)  S3 
· S2 · S1 · S0 · Op5 · Op4 · Op3 · Op2 · Op1 · Op0EXAMPLEANSWER
 D.3 Implementing Finite-State Machine Control D-
13NextState5 
 State2 
· (Op[5-0]sw)  S3 
· S2 · S1 · S0 · Op5 · Op4 · Op3 · Op2 · Op1 · Op0NextState7 
 State6 
 S3 · S2 · S1 · S0NextState9 
 State1 
· (Op[5-0]jmp)
  S3 
· S2 · S1 · S0 · Op5 · Op4 · Op3 · Op2 · Op1 · Op0NS0 is the logical sum of all these terms.
As we have seen, the control function can be expressed as a logic equation for each 
outpu
 is set of logic equations can be implemented in two ways: corresponding 
to a complete truth table, or corresponding to a two-level logic structure that allows 

a sparse encoding of the truth table. Before we look at these implementations, let’s 

look at the truth table for the complete control function.
It is simplest if we break the control functio
 ned in Figure D.3.3 into two 
parts: the next-state outputs, which may depend on all the inputs, and the control 

signal outputs, which depend only on the current-state bits. Figure D.3.4 shows 

the truth tables for all the datapath control signals. Because these signals actually 

depend only on the state bits (and not the opcode), each of the entries in a table 

in Figure D.3.4 actually represents 64 (
 26) entries, with the 6 bits named Op 
having all possible values; that is, the Op bits are don’t-care bits in determining 

the data path control outputs. Figure D.3.5 shows the truth table for the next-state 

bits NS[3–0], which depend on the state input bits and the instruction bits, which 

supply the opcode.
Elaboration: There are many opportunities to simplify the control function by 
observing similarities among two or more control signals and by using the semantics of 
the implementation. For example, the signals PCWriteCond, PCSource0, and ALUOp0 are 

all asserted in exactly one state, state 8. These three control signals can be replaced 

by a single signal.

D-14 Appendix D Mapping Control to Hardware
s3s2s1s0s3s2s1s0s3s2s1s0
000010000011
10101001dnoCetirWCProfelbathturT.betirWCProfelbathturT.ac. Truth table for IorD
s3s2s1s0s3s2s1s0s3s2s1s0
000001010000
0011d. Truth table for MemReade. Truth table for MemWritef. Truth table for IRWrite
s3s2s1s0s3s2s1s0s3s2s1s0
010010011000
g. Truth table for MemtoRegh. Truth table for PCSource1i. Truth table for PCSource0
s3s2s1s0s3s2s1s0s3s2s1s0
011010000001
0010
1BcrSULArofelbathturT.l0pOULArofelbathturT.k1pOULArofelbathturT.js3s2s1s0s3s2s1s0s3s2s1s0
000000100100
000101100111
1000m. Truth table for ALUSrcB0n. Truth table for ALUSrcAo. Truth table for RegWrite
s3s2s1s0
0111p. Truth table for RegDstFIGURE D.3.4
 The truth tables are shown for the 16 datapath control signals that depend only on the current-state 
input bits, which are shown for each table.
 Each truth table row corresponds to 64 entries: one for each possible value of the six Op 
bits. Notice that some of the outputs are active under nearly the same circumstances. For example, in the case of PCWriteCond, 
PCSource0, 
and ALUOp0, these signals are active only in state 8 (see b, i, an
 ese three signals could be
 replaced by one signal.
 ere are other 
opportunities for reducing the logic needed to implement the control function by taking advantage of further similarities in th
e truth tables.

 D.3 Implementing Finite-State Machine Control D-
15A ROM Implementation
Probably the simplest way to implement the control function is to encode the truth 
tables in a read-only memory (RO
 e number of entries in the memory for the 
truth tables of Figures D.3.4 and D.3.5 is equal to all possible values of the inputs 

(the 6 opcode bits plus the 4 state bits), which is 2
# inputs
  210  e inputs 
Op5Op4Op3Op2Op1Op0S3S2S1S0
0000100001
0001000001
a. The truth table for the NS3 output, active when the next state is 8 or 9. This signal is activated when 
the current state is 1.
Op5Op4Op3Op2Op1Op0S3S2S1S0
0000000001
1010110010
XXXXXX0011
XXXXXX0110
b. The truth table for the NS2 output, which is active when the next state is 4, 5, 6, or 7. This situation 
occurs when the current state is one of 1, 2, 3, or 6.
Op5Op4Op3Op2Op1Op0S3S2S1S0
0000000001
1000110001

1010110001
1000110010
XXXXXX0110
c. The truth table for the NS1 output, which is active when the next state is 2, 3, 6, or 7. The next state 
is one of 2, 3, 6, or 7 only if the current state is one of 1, 2, or 6.
Op5Op4Op3Op2Op1Op0S3S2S1S0
XXXXXX0000
1000110010
1010110010
XXXXXX0110
0000100001
d. The truth table for the NS0 output, which is active when the next state is 1, 3, 5, 7, or 9. This happens 
only if the current state is one of 0, 1, 2, or 6.
FIGURE D.3.5
 The four truth tables for the four next-state output bits (NS[3–0]).
 e next-
state outputs depend on the value of Op[5-0], which is the opco
 eld, and the current state, given by S[3–
 e entries with X are don’t-care terms. Each entry with a don’t-care term corresponds to two entries, one 
with that input at 0 and one with that input at
 us an entry with 
n don’t-care terms actually corresponds 
to 
2n truth table entries.

D-16 Appendix D Mapping Control to Hardware
to the control unit become the address lines for the ROM, which implements 
the control logic block that was shown in Figure D
 e width of each entry 
(or word in the memory) is 20 bits, since there are 16 datapath control outputs and 

4 next-state bi
 is means the total size of the ROM is 2
10  20  20 Kbits.
 e setting of the bits in a word in the ROM depends on which outputs are active 
in that word. Before we look at the control words, we need to order the bits within 

the control input (the address) and output words (the contents), respectively. We 

will number the bits using the order in Figure D.3.2, with the next-state bits being 

the low-order bits of the control 
word
 and the current-state input bits being the 
low-order bits of the 
address.
 is means that the PCWrite output will be the high-
order bit (bit 19) of each memory word, and NS0 will be the low-order bi
 e high-order address bit will be given by Op5, which is the high-order bit of the 

instruction, and the low-order address bit will be given by S0.
We can construct the ROM contents by building the entire truth table in a form 
where each row corresponds to one of the 2
n unique input combinations, and a 
set of columns indicates which outputs are active for that input combination. We 

don’t have the space here to show all 1024 entries in the truth table. However, by 

separating the datapath control and next-state outputs, we do, since the datapath 

control outputs depend only on the current state.
 e truth table for the datapath 
control outputs is shown in Figure D.3.6. We include only the encodings of the state 

inputs that are in use (that is, values 0 through 9 corresponding to the 10 states of 

the state machine).
 e truth table in Figure D.3.6 directly gives the contents of the upper 16 bits of 
each word in the RO
 e 4-bit inpu
 eld gives the low-order 4 address bits of 
each word, and the column gives the contents of the word at that address.
If we did show a full truth table for the datapath control bits with both 
the state number and the opcode bits as inputs, the opcode inputs would all 

be don’t cares. When we construct the ROM, we cannot have any don’t cares, 

since the addresses into the ROM must be complete
 us, the same datapath 
control outputs will occur many times in the ROM, since this part of the ROM 

is the same whenever the state bits are identical, independent of the value of the 

opcode inputs.
Control ROM Entries
For what ROM addresses will the bit corresponding to PCWrite, the high bit 

of the control word, be 1?
EXAMPLE
 D.3 Implementing Finite-State Machine Control D-
17PCWrite is high in states 0 and 9; this corresponds to addresses with the 4 
low-order bits being either 0000 o
 e bit will be high in the memory 
word independent of the inputs Op[5–0], so the addresses with the bit high 

ar. , 1111110000, 

 e general form of this is XXXXX
X0000 or 
XXXXXX1001, where XXXXXX is any combination of bi
ts, and corresponds to the 6-bit 
opcode on which this output does not depend.
ANSWER)]0–3[S( seulav tupnIstuptuO0000000100100011010001010110011110001001
PCWrite1000000001
PCWriteCond0000000010
IorD0001010000
MemRead1001000000
MemWrite0000010000
IRWrite100000
0000MemtoReg0
000100000
PCSource10000000001
PCSource00000000010
ALUOp10
000001000
ALUOp00
000000010
ALUSrcB10110000000
ALUSrcB01100000000
ALUSrcA0010001010
RegWrite0
000100100
RegDst0000000100
FIGURE D.3.6
 The truth table for the 16 datapath control outputs, which depend only on 
the state inputs. e values are determined from Figure D.3.4. Although there are 16 possible values 
for the 4-bit stat
 eld, only ten of these are used and are shown here
 e ten possible values are shown at 
the top; each column shows the setting of the datapath control outputs for the state input value that appears 
at the top of the column. For example, when the state inputs are 0011 (state 3), the active datapath control 

outputs are IorD or MemRead.

D-18 Appendix D Mapping Control to Hardware
We will show the entire contents of the ROM in two parts to make it easier to 
show. Figure D.3.7 shows the upper 16 bits of the control word; this comes directly 
from Figure D
 ese datapath control outputs depend only on the state inputs, 
and this set of words would be duplicated 64 times in the full ROM, as we discussed 

above.
 e entries corresponding to input values 1010 through 1111 are not used, 
so we do not care what they contain.
Figure D.3.8 shows the lower four bits of the control word corresponding to the 
next-state outpu
 e last column of the table in Figure D.3.8 corresponds to all the 
possible values of the opcode that do not match the sp
 ed opcodes. In state 0, the 
next state is always state 1, since the instruction was still being fetched. A
 er state 1, 
the opco
 eld must be valid
 e table indicates this by the entries marked illegal; 
we discuss how to deal with these exceptions and interrupts opcodes in Section 4.9.
Not only is this representation as two separate tables a more compact way to 
show the ROM contents; it is also a more
  cient way to implement the ROM. 
 e majority of the outputs (16 of 20 bits) depends only on 4 of the 10 inpu
 e number of bits in total when the control is implemented as two separate ROMs 

is 24  16  210  4  256  4096  4.3 Kbits, which is about on
 h of the 
size of a single ROM, which requires 2
10  20  20 Kbi
 ere is some overhead 
associated with any structured-logic block, but in this case the additional overhead 

of an extra ROM would be much smaller than the savings from splitting the single 

ROM.
Lower 4 bits of the addressBits 19–4 of the word
00010000001010010000000110000000000010000010100000000000010000000000000011001100010000000100000000100000000000010100101000100010000000000110110000000000000011100010010100000010000100000000100000011001FIGURE D.3.7
 The contents of the upper 16 bits of the ROM depend only on the state 
inputs. ese values are the same as those in Figure D.3.6, simply rotate
 is set of control words 
would be duplicated 64 times for every possible value of the upper six bits of the address.

 D.3 Implementing Finite-State Machine Control D-
19Although this ROM encoding of the control function is simple, it is wasteful, 
even when divided into two pieces. For exa
mple, the values of the Instruction 
register inputs are o
 en not needed to determine the next state
 us, the next-
state ROM has many entries that are either duplicated or are don’t care. Consider 
the case when the machine is in state 0: there are 2
6 entries in the ROM (since the 
opco
 eld can have any value), and these entries will all have the same contents 
(namely, the control word
 e reason that so much of the ROM is wasted is 
that the ROM implements the complete truth table, providing the opportunity to 

have
 erent output for every combination of the inputs. But most combinations 
of the inputs either never happen or are redundant!
Op [5–0]Current state
S[3–0]000000(R-format)
000010(jmp)000100(beq)100011(lw)101011(sw)Any other
value0000000100010001000100010001
000101101001100000100010Illegal

0010XXXXXXXXXXXX00110101Illegal

001101000100010001000100Illegal

010000000000000000000000Illegal

010100000000000000000000Illegal

011001110111011101110111Illegal

011100000000000000000000Illegal

100000000000000000000000Illegal

100100000000000000000000Illegal
FIGURE D.3.8
 This table contains the lower 4 bits of the control word (the NS outputs), 
which depend on both the state inputs, S[3–0], and the opcode, Op[5–0], which correspond 
to the instruction opcode. 
 ese values can be determined from Figure D
 e opcode name is 
shown under the encoding in the headin
 e four bits of the control word whose address is given by the 
current-state bits and Op bits are shown in each entry. For example, when the state input bits are 0000, the 

output is always 0001, independent of the other inputs; when the state is 2, the next state is don’t care for 

three of the inputs, 3 for 
lw, and 5 for 
sw. Together with the entries in Figure D.3.7, this table sp
 es the 
contents of the control unit ROM. For example, the word at address 1000110001 is obtained b
 nding the 
upper 16 bits in the table in Figure D.3.7 using only the state input bits (0001) and concatenating the lower 

four bits found by using the entire address (0001 t
 nd the row and 100011 t
 nd the col
 e entry 
from Figure D.3.7 yields 0000000000011000, while the appropriate entry in the table immediately above is 

 us the control word at addr
 e column labeled “Any 
other value” applies only when the Op bits do not match one of the sp
 ed opcodes.

D-20 Appendix D Mapping Control to Hardware
A PLA ImplementationWe can reduce the amount of control storage required at the cost of using more 
complex address decoding for the control inputs, which will encode only the input 

combinations that are needed
 e logic structure most o
 en used to do this is 
a programmed logic array (PLA), which we
 mentioned earlier and illustrated in 
Figure D.2.5. In a PLA, each output is the logical OR of one or more minterms. 

A minterm,
 also called a 
product term,
 is simply a logical AND of one or more 
inpu
 e inputs can be thought of as the address for indexing the PLA, while 
the minterms select which of all possible address combinations are interesting. A 

minterm corresponds to a single entry in a truth table, such as those in Figure 

D.3.4, including possible don’t-care terms. Each output consists of an OR of these 

minterms, which exactly corresponds to a complete truth table. However, unlike 

a ROM, only those truth table entries that produce an active output are needed, 

and only one copy of each minterm is required, even if the minterm contains don’t 

cares. Figure D.3.9 shows the PLA that implements this control function.
As we can see from the PLA in Figure D.3.9, there are 17 unique minterms—10 
that depend only on the current state and 7 others that depend on a combination 

of th
 eld and the current-state bi
 e total size of the PLA is proportional 
to (#inputs 
 #product terms) 
 (#outputs 
 #product terms), as we can see 
symbolically from th
 gure. 
 is means the total size of the PLA in Figure D.3.9 is 
proportional to (10 
 17)  (20  17)  510. By comparison, the size of a single 
ROM is proportional to 20 Kb, and even the two-part ROM has a total of 4.3 Kb. 

Because the size of a PLA cell will be only slightly larger than the size of a bit in a 

ROM, a PLA will be a much mor
  cient implementation for this control unit.
Of course, just as we split the ROM in two, we could split the PLA into two 
PLAs: one with 4 inputs and 10 minterms that generates the 16 control outputs, 

and one with 10 inputs and 7 minterms that generates the 4 next-state outputs. 

 e 
 rst PLA would have a size proportional to (4 
 10)  (10  16)  200, and 
the second PLA would have a size proportional to (10 
 7)  (4  7)  is would yield a total size proportional to 298 PLA cells, about 55% of the size of a 

single PL
 ese two PLAs will be considerably smaller than an implementation 
using two ROMs. For more details on PLAs and their implementation, as well as 

the references for books on logic design, see Appendix B.

 D.3 Implementing Finite-State Machine Control D-
21Op5Op4Op3
Op2
Op1
Op0
S3
S2
S1
S0IorDIRWrite
MemReadMemWrite
PCWrite
PCWriteCond
MemtoRegPCSource1ALUOp1ALUSrcB0ALUSrcARegWrite
RegDstNS3
NS2
NS1
NS0ALUSrcB1ALUOp0PCSource0FIGURE D.3.9
 This PLA implements the control function logic for the multicycle implementation. e inputs to the control appear on th
  and the outputs on the righ
 e top half 
of th
 gure is the AND plane that computes all the minterm
 e minterms are carried to the OR plane 
on the vertical lines. Each colored dot corresponds to a signal that makes up the minterm carried on that 
line.
 e sum terms are computed from these minterms, with each gray dot representing the presence of the 
intersecting minterm in that sum term. Each output consists of a single sum term.

D-22 Appendix D Mapping Control to Hardware
 D.4
 Implementing the Next-State Function with a SequencerLet’s look carefully at the control unit we built in the last section. If you examine 
the ROMs that implement the control in Figures D.3.7 and D.3.8, you can see 

that much of the logic is used to specify the next-state function. In fact, for the 

implementation using two separate ROMs, 4096 out of the 4368 bits (94%) 

correspond to the next-state function! Furthermore, imagine what the control 

logic would look like if the inst
ruction set had many more
 erent instruction 
types, some of which required many clocks to implemen
 ere would be many 
more states in th
 nite-state machine. In some states, we might be branching to 
a large number of
 erent states depending on the instruction type (as we did in 
state 1 of th
 nite-state machine in Figure D.3.1). However, many of the states 
would proceed in a sequential fashion, just as states 3 and 4 do in Figure D.3.1.
For example, if we incl
 oating point, we would see a sequence of many 
states in a row that implement a multicyc
 oating-point instruction. Alternatively, 
consider how the control might look for a machine that can have multiple memory 

operands per instruction. It would require many more states to fetch multiple 

memory op
 e result of this would be that the control logic will be 
dominated by the encoding of the next-state function. Furthermore, much of the 

logic will be devoted to sequences of states with only one path through them that 

look like states 2 through 4 in Figure D.3.1. With more instructions, these sequences 

will consist of many more sequentially numbered states than for our simple subset.
To encode these more complex control function
  ciently, we can use a 
control unit that has a counter to supply the sequential next state
 is counter 
 en eliminates the need to encode the next-s
tate function explicitly in the control 
unit. As shown in Figure D.4.1, an adder is used to increment the state, essentially 

turning it into a counter
 e incremented state is always the state that follows 
in numerical order. However, th
 nite-state machine sometimes “branches.” For 
example, in state 1 of th
 nite-state machine (see Figure D.3.1), there are four 
possible next states, only one of which is the sequential next state.
 us, we need 
to be able to choose between the incremented state and a new state based on the 

inputs from the Instruction register and the current state. Each control word will 

include control lines that will determine how the next state is chosen.
It is easy to implement the control output signal portion of the control word, 
since, if we use the same state numbers, this portion of the control word will 

look exactly like the ROM contents shown in Figure D.3.7. However, the method 

 D.4 Implementing the Next-State Function with a Sequencer D-
23for selecting the next stat
 ers from the next-state function in th
 nite-state 
machine.
With an explicit counter providing the sequential next state, the control unit 
logic need only specify how to choose the state when it is not the sequentially 
following state.
 ere are two methods for doing t
 e 
 rst is a method we have 
already seen: namely, the control unit explicitly encodes the next-state function. 

 e 
 erence is that the control unit need only set the next-state lines when the 
designated next state is not the state that the counter indicates. If the number of 
AddrCtlOutputsPLA or ROM
StateAddress select logicOp[5Œ0]AdderInstruction register
opcode field1Control unit
InputPCWrite
PCWriteCond
IorDMemtoRegPCSourceALUOpALUSrcBALUSrcARegWrite
RegDstIRWrite
MemReadMemWrite
FIGURE D.4.1
 The control unit using an explicit counter to compute the next state. In this 
control unit, the next state is computed using a counter (at least in some states). By comparison, Figure D.3.2 
encodes the next state in the control logic for every st
ate. In this control unit, the signals labeled 
AddrCtl
 control how the next state is determined.

D-24 Appendix D Mapping Control to Hardware
states is large and the next-state function that we need to encode is mostly empty, 
this may not be a good choice, since the resulting control unit will have lots of 

empty or redundant space. An alternative approach is to use separate external logic 

to specify the next state when the counter does not specify the state. Many control 

units, especially those that implement large instruction sets, use this approach, and 

we will focus on specifying the control externally.
Although the nonsequential next state will come from an external table, the 
control unit needs to specify when this should occur and how t
 nd that next state. 
 ere are two kinds of “branching” that we must implement in the address select 
logic. First, we must be able to jump to one of a number of states based on the 

opcode portion of the Instruction register
 is operation, called a 
dispatch
, is usually implemented by using a set of special ROMs or PLAs included as part of the 

address selection logic. An additional set of control outputs, which we call AddrCtl, 

indicates when a dispatch should be done. Looking at th
 nite-state diagram 
(Figure D.3.1), we see that there are two states in which we do a branch based on a 

portion of the opcode
 us we will need two small dispatch tables. (Alternatively, 
we could also use a single dispatch table and use the control bits that select the table 

as address bits that choose from which portion of the dispatch table to select the 

address.)
 e second type of branching that we must implement consists of branching 
back to state 0, which initiates the execution of the next MIPS instruction. 

 us there are four possible ways to choose the next state (three types of branches, 
plus incrementing the current-state number), which can be encoded in 2 bits. Let’s 

assume that the encoding is as follows:
AddrCtl valueAction
0Set state to 0
1Dispatch with ROM 1

2Dispatch with ROM 2

3Use the incremented state
If we use this encoding, the address select logic for this control unit can be 
implemented as shown in Figure D.4.2.
To complete the control unit, we need only specify the contents of the dispatch 
ROMs and the values of the address-control lines for each state. We have already 

sp
 ed the datapath control portion of the control word using the ROM contents 
of Figure D.3.7 (or the corresponding portions of the PLA in Figure D
 e next-state counter and dispatch ROMs take the place of the portion of the control 

unit that was computing the next state, which was shown in Figure D.3.8. We are 

 D.4 Implementing the Next-State Function with a Sequencer D-
25only implementing a portion of the instruction set, so the dispatch ROMs will be 
largely empty. Figure D.4.3 shows the entries that must be assigned for this subset.
StateOpAdder1PLA or ROM
Mux3210Dispatch ROM 1
Dispatch ROM 2
0AddrCtlAddress select logicInstruction register
opcode fieldFIGURE D.4.2
 This is the address select logic for the control unit of Figure D.4.1.
2 MOR hctapsiD1 MOR hctapsiDOpOpcode nameValueOpOpcode nameValue
000000R-format0110100011
lw0011000010jmp1001101011
sw0101000100beq1000100011lw0010101011sw0010FIGURE D.4.3
 The dispatch ROMs each have 2
6  64 entries that are 4 bits wide, since 
that is the number of bits in the state encoding.
 is 
 gure only shows the entries in the ROM that 
are of interest for this subs
 e 
 rst column in each table indicates the value of Op, which is the address 
used to access the dispatch RO
 e second column shows the symbolic name of the opcode
 e third 
column indicates the value at that address in the ROM.
Now we can determine the setting of the address selection lines (AddrCtl) in 
each control word
 e table in Figure D.4.4 shows how the address control must 

D-26 Appendix D Mapping Control to Hardware
be set for every state.
 is information will be used to specify the setting of the 
AddrCt
 eld in the control word associated with that state.
 e contents of the entire control ROM are shown in Figure D
 e total 
storage required for the control is quite small
 ere are 10 control words, each 18 
bits wide, for a total of 180 bits. In addition, the two dispatch tables are 4 bits wide 
and each has 64 entries, for a total of 512 additional bi
 is total of 692 bits beats 
the implementation that uses two ROMs with the next-state function encoded in 

the ROMs (which requires 4.3 Kbits).
Of course, the dispatch tables are sparse and could be mor
  ciently implemented 
with two small PLA
 e control ROM could also be replaced with a PLA.
State numberAddress-control actionValue of AddrCtl
3etats detnemercni esU011 MOR hctapsid esU122 MOR hctapsid esU23etats detnemercni esU300 yb rebmun etats ecalpeR400 yb rebmun etats ecalpeR53etats detnemercni esU600 yb rebmun etats ecalpeR700 yb rebmun etats ecalpeR800 yb rebmun etats ecalpeR9FIGURE D.4.4
 The values of the address-control lines are set in the control word that corresponds to each state.
 State numberControl word bits 17–2Control word bits 1–0
1100010000001010010100001100000000000101001010000000000021100000000000011003000100000001000000400000000000001010051100100010000000006001100000000000000700001001010000001080000000000100000019FIGURE D.4.5
 The contents of the control memory for an implementation using an explicit 
counter.
 e 
 rst column shows the state, while the second shows the datapath control bits, and the last 
column shows the address-control bits in each control word. Bits 17–2 are identical to those in Figure D.3.7.

 D.4 Implementing the Next-State Function with a Sequencer D-
27Optimizing the Control ImplementationWe can further reduce the amount of logic in the control unit by tw
 erent 
techniq
 e 
 rst is 
logic minimization
, which uses the structure of the logic 
equations, including the don’t-care terms, to reduce the amount of hardware 
required
 e success of this process depends on how many entries exist in the 
truth table, and how those entries are related. For example, in this subset, only the 
lw and 
sw opcodes have an active value for the signal Op5, so we can replace the 
two truth table entries that test whether the input is 
lw or 
sw by a single test on 
this bit; similarly, we can eliminate several bits used to index the dispatch ROM 

because this single bit can be used t
 nd lw and 
sw in th
 rst dispatch ROM. Of 
course, if the opcode space were less sparse, opportunities for this optimization 

would be mor
  cult to locate. However, in choosing the opcodes, the architect 
can provide additional opportunities by choosing related opcodes for instructions 

that are likely to share states in the control.
 erent sort of optimization can be done by assigning the state numbers in a 
 nite-state or microcode implementation to minimize th
 is optimization, 
called state assignment
, tries to choose the state numbers such that the resulting 
logic equations contain more redundancy and can thus be simp
 ed. Let’s consider 
the case o
 nite-state machine with an
 encoded next-state contro
 rst, since it 
allows states to be assigned arbitrarily. For example, notice that in th
 nite-state 
machine, the signal RegWrite is active only in states 4 and 7. If we encoded those 

states as 8 and 9, rather than 4 and 7, we could rewrite the equation for RegWrite as 

simply a test on bit S3 (which is only on for states 8 an
 is renumbering allows 
us to combine the two truth table entries in part (o) of Figure D.3.4 and replace 

them with a single entry, eliminating one term in the control unit. Of course, we 

would have to renumber the existing states 8 and 9, perhaps as 4 and 7.
 e same optimization can be applied in an implementation that uses an explicit 
program counter, though we are more restricted. Because the next-state number is 

 en computed by incrementing the current-state number, we cannot arbitrarily 
assign the states. However, if we keep the states where the incremented state is used 

as the next state in the same order, we can reassign the consecutive states as a block. 

In an implementation with an explicit next-state counter, state assignment may 

allow us to simplify the contents of the dispatch ROMs.
If we look again at the control unit in Figure D.4.1, it looks remarkably like a 
computer in its own righ
 e ROM or PLA can be thought of as memory supplying 
instructions for the datapat
 e state can be thought of as an instruction address. 
Hence the origin of the name 
microcode
 or 
microprogrammed control.
 e control 
words are thought of as 
microinstructions
 that control the datapath, and the State 
register is called the 
microprogram counter.
 Figure D.4.6 shows a view of the control 
unit as 
microcode
 e next section describes how we map from a microprogram 
to microcode.

D-28 Appendix D Mapping Control to Hardware
 D.5
 Translating a Microprogram to Hardware
To translate a microprogram into actual hardware, we need to specify how each 
 eld translates into control signals. We can implement a microprogram with either 
 nite-state control or a microcode implementation with an explicit sequencer. If 
we choos
 nite-state machine, we need to co
nstruct the next-state function from 
PCWrite
PCWriteCond
IorDMemtoRegPCSourceALUOpALUSrcBALUSrcARegWrite
AddrCtlOutputsMicrocode memory
IRWrite
MemReadMemWrite
RegDstControl unit
InputMicroprogram counter
Address select logicOp[5Œ0]Adder1Instruction register
opcode fieldBWrite
DatapathFIGURE D.4.6
 The control unit as a microcode. e use of the word “micro” serves to distinguish between the program counter in the 
datapath and the microprogram counter, and between the microcode memory and the instruction memory.

 D.5 Translating a Microprogram to Hardware D-
29the microprogram. Once this function is known, we can map a set of truth table 
entries for the next-state outputs. In this section, we will show how to translate the 

microprogram, assuming that the next state is sp
 ed by a sequencer. From the 
truth tables we will construct, it would be straightforward to build the next-state 

function fo
 nite-state machine.
tnemmoCevitca slangiSeulaVeman dleiFALU controlAddALUOp = 00Cause the ALU to add.
SubtALUOp = 01Cause the ALU to subtract; this implements the compare for branches.

Func codeALUOp = 10Use the instruction’s function code to determine ALU control.
SRC1PCALUSrcA = 0 rst ALU input.

AALUSrcA = 1 rst ALU input.
SRC2BALUSrcB = 00Register B is the second ALU input.

4ALUSrcB = 01Use 4 as the second ALU input.

ExtendALUSrcB = 10Use output of the sign extension unit as the second ALU input.
ExtshftALUSrcB = 11Use the output of the shift-by-two unit as the second ALU input.
Register control srebmun retsiger eht sa RI eht fo sdle  tr dna sr eht gnisu sretsiger owt daeRdaeRand putting the data into registers A and B.
Write ALURegWrite,
RegDst = 1, 

MemtoReg = 0 eld of the IR as the register number and the 
contents of ALUOut as the data. Write MDRRegWrite, 
RegDst = 0, 

MemtoReg = 1Write a register using the r eld of the IR as the register number and the 

contents of the MDR as the data.Memory
Read PCMemRead, 

IorD = 0, IRWrite
Read memory using the PC as address; write result into IR (and the MDR).
Read ALUMemRead, 
IorD = 1Read memory using ALUOut as address; write result into MDR.
Write ALUMemWrite, 
IorD = 1Write memory using the ALUOut as address, contents of B as the data.
PC write controlALUPCSource = 00, 
PCWriteWrite the output of the ALU into the PC.ALUOut-condPCSource = 01, 
PCWriteCond If the Zero output of the ALU is active, write the PC with the contents of the 

register ALUOut.Jump addressPCSource = 10, 
PCWriteWrite the PC with the jump address from the instruction.
SequencingSeqAddrCtl = 11Choose the next microinstruction sequentially.
FetchAddrCtl = 00 rst microinstruction to begin a new instruction.
Dispatch 1AddrCtl = 01Dispatch using the ROM 1.

Dispatch 2AddrCtl = 10Dispatch using the ROM 2.
FIGURE D.5.1
 Each microcode ﬁ eld translates to a set of control signals to be set.
 es
 erent values of th
 elds specify 
all the required combinations of the 18 control lines. Control lines that are not set, which correspond to actions, are 0 by de
fault. Multiplexor 
control lines are set to 0 if the output matters. If a multiplexor control line is not explicitly set, its output is a don’t ca
re and is not used.

D-30 Appendix D Mapping Control to Hardware
Assuming an explicit sequencer, we need to
 do two additional tasks to translate 
the microprogram: assign addresses to the microinstructions an
 ll in the 
contents of the dispatch RO
 is process is essentially the same as the process 
of translating an assembly language program into machine instructions: th
 elds of the assembly language or microprogram instruction are translated, and labels on 
the instructions must be resolved to addresses.
Figure D.5.1 shows the various values for each microinstructio
 eld that 
controls the datapath and how thes
 elds are encoded as control signals. If the 
 eld corresponding to a signal that a
 ects a unit with state (i.e., Memory, Memory 
register, ALU destination, or PCWriteControl) is blank, then no control signal 

should be active. I
 eld corresponding to a multiplexor control signal or the ALU 
operation control (i.e., ALUOp, SRC1, or SRC2) is blank, the output is unused, so 

the associated signals may be set as don’t care.
 e sequencin
 eld can have four values: Fetch (meaning go to the Fetch 
state), Dispatch 1, Dispatch 2, and Seq.
 ese four values are encoded to set the 
2-bit address control just as they were in Figure D.4.4: Fetch 
 0, Dispatch 1 
 1, Dispatch 2 
 2, Seq 
 3. Finally, we need to specify the contents of the dispatch 
tables to relate the dispatch entries of the seq
 eld to the symbolic labels in 
the microprogram. We use the same dispatch tables as we did earlier in Figure 

D.4.3.
A microcode assembler would use the encoding of the sequencin
 eld, the 
contents of the symbolic dispatch tables in Figure D.5.2, the sp
 cation in Figure 
D.5.1, and the actual microprogram to generate the microinstructions.
Since the microprogram is an abstract representation of the control, there is a 
great deal of
 exibility in how the microprogram is translated. For example, the 
address assigned to many of the microinstructions can be chosen arbitrarily; the 

only restrictions are those imposed by the fact that certain microinstructions must 
2 elbat hctapsid edocorciM1 elbat hctapsidOpcode ﬁ eldOpcode nameValueOpcode ﬁ
 eldOpcode nameValue
000000R-formatRformat1100011
lwLW2
000010jmpJUMP1101011
swSW2000100beqBEQ1100011lwMem1101011swMem1FIGURE D.5.2
 The two microcode dispatch ROMs showing the contents in symbolic form 
and using the labels in the microprogram.
 D.5 Translating a Microprogram to Hardware D-
31occur in sequential order (so that incrementing the State register generates the 
address of the next instructio
 us the microcode assembler may reduce the 
complexity of the control by assigning the microinstructions cleverly.
Organizing the Control to Reduce the LogicFor a machine with complex control, there may be a great deal of logic in the 

control uni
 e control ROM or PLA may be very costly. Although our simple 
implementation had only an 18-bit microinstruction (assuming an explicit 

sequencer), there have been machines with microinstructions that are hundreds of 

bits wide. Clearly, a designer would like to reduce the number of microinstructions 

and the width.
 e ideal approach to reducing control store is t
 rst write the complete 
microprogram in a symbolic notation and then measure how control lines are set 

in each microinstruction. By taking measurements we are able to recognize control 

bits that can be encoded into
 eld. For example, if no more than one of 
eight lines is set simultaneously in the same microinstruction, then this subset of 

control lines can be encoded into a 3-bi
 eld (log2 8  is change saves
 ve 
bits in every microinstruction and does not hurt CPI, though it does mean the extra 

hardware cost of a 3-to-8 decoder needed to generate the eight control lines when 

they are required at the datapath. It may also have some small clock cycle impact, 

since the decoder is in the signal path. However, shavin
 ve bits
  control store 
width will usually overcome the cost of the decoder, and the cycle time impact will 

probably be small or nonexistent. For example, this technique can be applied to bits 

13–6 of the microinstructions in this machine, since only one of the seven bits of 

the control word is ever active (see Figure D.4.5).
 is technique of reducin
 eld width is called 
encoding
. To further save space, 
control lines may be encoded together if they are only occasionally set in the same 

microinstruction; two microinstructions instead of one are then required when 

both must be set. As long as this doesn’t happen in critical routines, the narrower 

microinstruction may justify a few extra words of control store.
Microinstructions can be made narrower still if they are broken into
 erent 
formats and given an opcode or 
format
 eld
 to distinguish th
 e forma
 eld gives all the unsp
 ed control lines their default values, so as not to change 
anything else in the machine, and is similar to the opcode of an instruction in a 

more powerful instruction set. For example, we could us
 erent format for 
microinstructions that did memory accesses from those that did register-register 

ALU operations, taking advantage of the fact that the memory access control lines 

are not needed in microinstructions controlling ALU operations.
Reducing hardware costs by using forma
 elds usually has an additional 
performance cost beyond the requirement for more decoders. A microprogram 

using a single microinstruction format can specify any combination of operations 

in a datapath and can take fewer clock cycles than a microprogram made up of 

restricted microinstructions that cannot perform any combination of operations in 

D-32 Appendix D Mapping Control to Hardware
a single microinstruction. However, if the full capability of the wider microprogram 
word is not heavily used, then much of the control store will be wasted, and the 

machine could be made smaller and faster by restricting the microinstruction 

capability.
 e narrow, but usually longer, approach is o
 en called 
vertical microcode,
 while 
the wide but short approach is called 
horizontal microcode.
 It should be noted that 
the terms “vertical microcode” and “horizontal microcode” have no universal 

 nition—the designers of the 8086 considered its 21-bit microinstruction to be 
more horizontal than in other single-chip computers of the time
 e related terms 
maximally encoded
 and 
minimally encoded
 are probably better than vertical and 
horizontal.
 D.6
 Concluding Remarks
We began this appendix by looking at how to translat
 nite-state diagram to an 
implementation usin
 nite-state machine. We then looked at explicit sequencers 
that us
 erent technique for realizing the next-state function. Although large 
microprograms are o
 en targeted at implementations using this explicit next-state 
approach, we can also implement a microprogram with
 nite-state machine. As 
we saw, both ROM and PLA implementations of the logic functions are possible. 

 e advantages of explicit versus encoded next state and ROM versus PLA 
implementation are summarized below.
Independent of whether the control is represente
 nite-state diagram 
or as a microprogram, translation to a hardware control implementation is 

similar. Each state or microinstruction asserts a set of control outputs and 

sp
 es how to choose the next state.
 e next-state function may be implemented by either encoding it in a 
 nite-state machine or using an explicit sequencer
 e explicit sequencer 
is more
  cient if the number of states is large and there are many 
sequences of consecutive states without branching.
 e control logic may be implemented with either ROMs or PLAs (or 
even a mix). PLAs are more
  cient unless the control function is very 
dense. ROMs may be appropriate if the control is stored in a separate 

memory, as opposed to within the same chip as the datapath.
The BIGPicture
 D.5 Exercises D-
33 D.7
 ExercisesD.1 
[10] §D.2
 Instead of using four state bits to implement th
 nite-state 
machine in Figure D.3.1, use nine state bits, each of which is a 1 only if th
 nite-
state machine is in that particular state (e.g., S1 is 1 in state 1, S2 is 1 in state 2, etc.). 
Redraw the PLA (Figure D.3.9).
D.2 
[5] §D.3
 We wish to add the instruction 
jal (jump and link). Make any 
necessary changes to the datapath or to the control signals if needed. You can 
photocopy
 gures to make it faster to show the additions. How many product terms 
are required in a PLA that implements the control for the single-cycle datapath for 
jal?D.3 
 [5] §D.3
 Now we wish to add the instruction 
addi (add immediate). 
Add any necessary changes to the datapath and to the control signals. How many 

product terms are required in a PLA that implements the control for the single-

cycle datapath for 
addiu?D.4 
 [10] §D.3
 Determine the number of product terms in a PLA that 
implements th
 nite-state machine for 
addi. e easiest way to do this is to 
construct the additions to the truth tables for 
addi.D.5 
 [20] §D.4
 Implement th
 nite-state machine of using an explicit counter 
to determine the next state. Fill in the new entries for the additions to Figure D.4.5. 

Also, add any entries needed to the dispatch ROMs of Figure D.5.2.
D.6  
[15] §§D.3–D.6
 Determine the size of the PLAs needed to implement the 
multicycle machine, assuming that the n
ext-state function is implemented with 
a counter. Implement the dispatch tables of Figure D.5.2 using two PLAs and the 
contents of the main control unit in Figure D.4.5 using another PLA. How does the 

total size of this solution compare to the single PLA solution with the next state 

encoded? What if the main PLAs for both approaches are split into two separate 

PLAs by factoring out the next-state or address select signals?

Computer Organization and Design. DOI: 
© 2013 Elsevier Inc. All rights reserved.
http://dx.doi.org/10.1016/B978-0-12-407726-3.00001-1
2013RISC: any computer 
announced a
 er 1985.
A Survey of RISC 
Architectures for 
Desktop, Server, and 

Embedded Computers
EAPPENDIXSteven Przybylskic
A Designer of the Stanford MIPS

E.1 Introduction E-3E.2 Addressing Modes and Instruction Formats 
E-5E.3 Instructions: The MIPS Core Subset 
E-9E.4 Instructions: Multimedia Extensions of the Desktop/Server 
RISCs E-16E.5  Instructions: Digital Signal-Processing Extensions of the
Embedded RISCs E-19E.6 Instructions: Common Extensions to MIPS Core 
E-20E.7 Instructions Unique to MIPS-64 
E-25E.8 Instructions Unique to Alpha 
E-27E.9 Instructions Unique to SPARC v9 
E-29E.10 Instructions Unique to PowerPC 
E-32E.11 Instructions Unique to PA-RISC 2.0 
E-34E.12 Instructions Unique to ARM 
E-36E.13 Instructions Unique to Thumb 
E-38E.14 Instructions Unique to SuperH 
E-39E.15 Instructions Unique to M32R 
E-40E.16 Instructions Unique to MIPS-16 
E-40E.17 Concluding Remarks 
E-43 E.1 IntroductionWe cover two groups of reduced instruction set computer (RISC) architectures in 
this appe
 e 
 rst group is the desktop and server RISCs:
 Digital Alpha
 Hewlett-Packard PA-RISC
 IBM and Motorola PowerPC
 MIPS INC MIPS-64
 Sun Microsystems SPARC

E-4 Appendix E A Survey of RISC Architectures
 e second group is the embedded RISCs:
 Advanced RISC Machines ARM
 Advanced RISC Mac
 umb
 Hitachi SuperH
 Mitsubishi M32R
 MIPS INC MIPS-16
Alpha MIPS I PA-RISC 1.1 PowerPC SPARCv8
Date announced            1992      1986    1986               1993           1987
Instruction size (bits) 32 32 32 32 32 

Address space (size, model)64 bits, ß
 at 32 bits, ß
 at 48 bits, 
segmented32 bits, ß
 at 32 bits, ß
 at  dengilA dengilanU dengilA dengilA dengilA tnemngila ataD
Data addressing modes 1 1 5 4 2 
 egaP egaP egaP egaP egaP noitcetorP
Minimum page size 8 KB 4 KB 4 KB 4 KB 8 KB 
 deppam yromeM deppam yromeM deppam yromeM deppam yromeM deppam yromeM O/I
Integer registers (number, model, size)
31 GPR × 64 bits 31 GPR 
× 32 bits31 GPR 
× 32 bits32 GPR 
× 32 bits 31 GPR 
× 32 bitsSeparate ß oating-point registers
31 × 32 or 31 × 64 bits 16 × 32 or 16 × 64 bits 56 × 32 or 28 × 64 bits 32 × 32 or 32 × 64 bits 32 × 32 or 32 × 64 bits Floating-point format IEEE 754 single, 
doubleIEEE 754 single, 
doubleIEEE 754 single, 

doubleIEEE 754 single, 

doubleIEEE 754 single, 

doubleFIGURE E.1.1  Summary of the ﬁ
 rst version of ﬁ
 ve architectures for desktops and servers. 
Except for the number of data 
address modes and some instruction set details, the integer instruction sets of these architectures are very similar. Contrast 
this with Figure 
E.17.1. Later versions of these architectures all suppor
 at, 64-bit address space.
ARM Thumb SuperH M32R MIPS-16 
 6991 7991  5991 2991 5891 decnuonna etaDInstruction size (bits) 32 16 16 16/32 16/32 

Address space (size, model)32 bits, ß
 at 32 bits, ß
 at 32 bits, ß
 at 32 bits, ß
 at 32/64 bits, ß
 at  dengilA dengilA dengilA dengilA dengilA tnemngila ataDData addressing modes 6 64 3 2 

Integer registers (number, model, size) 15 
GPR x 32 bits 8 GPR + SP
, LR x 32 bits16 GPR x 32 bits 16 GPR x 32 bits 8 GPR + SP
, RA x 32/64 bits  deppam yromeM deppam yromeM deppam yromeM deppam yromeM deppam yromeM O/IFIGURE E.1.2 Summary of ﬁ
 ve architectures for embedded applications. Except for number of data address modes and some 
instruction set details, the integer instruction sets of these architectures are similar. Con trast this with Figure E.17.1.

 E.2 Addressing Modes and Instruction Formats E-
5 ere has never been another class of computers so similar
 is similarity 
allows the presentation of 10 architectures in about 50 pages. Characteristics of the 
desktop and server RISCs are found in Figure E.1.1 and the embedded RISCs in 

Figure E.1.2.
Notice that the embedded RISCs tend to have 8 to 16 general-purpose registers 
while the desktop/server RISCs have 32, and that the length of instructions is 16 to 

32 bits in embedded RISCs but always 32 bits in desktop/server RISCs.
Although shown as separate embedded instruction set architectur
 umb 
and MIPS-16 are really optional modes of ARM and MIPS invoked by call 

instructions. When in this mode, they execute a subset of the native architecture 

using 16-bit-long instruction
 ese 16-bit instruction sets are not intended to be 
full architectures, but they are enough to encode most procedures. Both machines 

expect procedures to be homogeneous, with all instructions in either 16-bit mode 

or 32-bit mode. Programs will consist of procedures in 16-bit mode for density or 

in 32-bit mode for performance.
One complication of this description is that some of the older RISCs have been 
extended over the years. We have decided to describe the latest versions of the 

architectures: MIPS-64, Alpha version 3, PA-RISC 2.0, and SPARC version 9 for 

the desktop/server; ARM versio
 umb version 1, Hitachi SuperH SH-3, M32R 
version 1, and MIPS-16 version 1 for the embedded ones.
 e remaining sections proceed as follows: a
 er discussing the addressing 
modes and instruction formats of our RISC architectures, we present the survey of 

the instruction
 ve steps:
 Instructions found in the MIPS core, whic
 ned in Chapters 2 and 3 of 
the main text
 Multimedia extensions of the desktop/server RISCs
 Digital signal-processing extensions of the embedded RISCs
 Instructions not found in the MIPS core but found in two or more architectures
 e unique instructions and characteristics of each of the ten architectures
We give the evolution of the instruction sets in th
 nal section and conclude with 
speculation about future directions for RISCs.
 E.2 Addressing Modes and Instruction 
Formats
Figure E.2.1 shows the data addressing modes supported by the desktop 
architectures. Since all have one register that always has the value 0 when used in 

address modes, the absolute address mode with limited range can be synthesized 

using zero as the base in displacement addressin
 is register can be changed 

E-6 Appendix E A Survey of RISC Architectures
by ALU operations in PowerPC; it is always 0 in the other machines.) Similarly, 
register indirect addressing is synthesized 
by using displacement addressing with 
an
 set of 0. Simp
 ed addressing modes is one distinguishing feature of RISC 
architectures.
Figure E.2.2 shows the data addressing modes supported by the embedded 
architectures. Unlike the desktop RISCs, these embedded machines do not reserve 

a register to contain 0. Although most have two to three simple addressing modes, 

ARM and SuperH have several, including fairly complex calculations. ARM has 

an addressing mode that ca
  one register by any amount, add it to the other 
registers to form the address, and then update one register with this new address.
References to code are normally PC-relative, although jump register indirect 
is supported for returning from procedures, for 
case
 statements, and for pointer 
function calls. One variation is that PC-relative branch addresses ar
 ed 
  two bits before being added to the PC for the desktop RISCs, thereby increasing the 

branch distance
 is works because the length of all instructions for the desktop 
RISCs is 32 bits, and instructions must be
 aligned on 32-bit words in memory. 
Embedded architectures with 16-bit-long instructions usuall
  the PC-relative 
address by 1 for similar reasons.
Addressing mode Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9
Register + offset (displacement or based) X X X XX 
 X X )sdaoL( X )PF( X  )dexedni( retsiger + retsigeRRegister + scaled register (scaled)   X  

Register + offset and update register   X X  

Register + register and update register   X X  
FIGURE E.2.1  Summary of data addressing modes supported by the desktop architectures.
 PA RISC also has short address 
versions of the o
 set addressing modes. MIPS-64 has indexed addressing fo
 oating-point loads and stor
 ese addressing modes are 
described in Figure 2.18.)
Addressing mode ARMv4 Thumb SuperH M32R MIPS-16 
Register + offset (displacement or based) X X X X X
Register + register (indexed) X X X   
   X )delacs( retsiger delacs + retsigeRRegister + offset and update register X     

Register + register and update register X     
  X X   tceridni retsigeRAutoincrement, autodecrement X X X X 
)sdaol( X  X )sdaol( X X atad evitaler-CPFIGURE E.2.2 Summary of data addressing modes supported by the embedded architectures.
 SuperH and M32R have 
separate register indirect and register 
 set addressing modes rather than just putting 0 in the o
 set of the latter mode
 is increases the 
use of 16-bit instructions in the M32R, and it gives a wider set of address modes t
 erent data transfer instructions in SuperH. To get 
greater addressing range, ARM an
 umb 
  the o
 set 
  one or two bits if the data size is halfword or word
 ese addressing modes 
are described in Figure 2.18.)

 E.2 Addressing Modes and Instruction Formats E-
7Figure E.2.3 shows the format of the desktop RISC instructions, which include 
the size of the address. Each instruction set architecture uses these four primary 
instruction formats. Figure E.2.4 shows the six formats for the embedded RISC 

mac
 e desire to have smaller code size via 16-bit instructions leads to more 
instruction formats.
Register-registerAlphaMIPS
PowerPC
PA-RISC

SPARC
31292418131240
312520151040
Op6Opx11Opx6Opx11Opx8Opx11Op6Op6Op6Rs15Rs15Rs15Rd5Rd5Rd5Rd5Const5Op2Opx6Rs25Rs150Rs25Rs25Rs25Rs25Rs15Rd5Register-immediateAlphaMIPSPowerPC
PA-RISC
SPARC
3129241813120
051025213Op6Const16Const16Const16Const16Const13Op6Op6Op6Rd5Rs15Rs25Rd5Op2Opx6Rs15Rs151Rd5Rd5Rs15Rd5BranchAlphaMIPS
PowerPC
PA-RISC
SPARC
312918120
1051025213Op6Const21Const16Const14Opx2Const11OCConst19Op6Op6Op6Rs15Rs15Rs25Opx6Op2Opx11Opx3Opx5/Rs25Rs15Rs15Jump/callAlphaMIPSPowerPCPA-RISC
SPARC
31292015120
10025213Op6Const21Const26Const24Opx2Const21O1C1Const30Op6Op6Op6Rs15Op2OpcodeRegisterConstantFIGURE E.2.3  Instruction formats for desktop/server RISC architectures.
 ese four 
formats are fo
 ve architectur
 e superscr
  notation in this
 gure means the width of a 
 eld in bits.) Although the regist
 elds are located in similar pieces of the instruction, be aware that the 
destination and two sour
 elds are scrambled. Op 
 the main opcode, Opx 
 an opcode extension, Rd 
 the destination register, Rs1 
 source register 1, Rs2 
 source register 2, and Const 
 a constant (used as an 
immediate or as an address). Unlike the other RISCs, Al
pha has a format for immediates in arithmetic and 
logical operations that
 erent from the data transfer format shown here. It provides an 8-bit immediate 
in bits 20 to 13 of the RR format, with bits 12 to 5 remaining as an opcode extension.

E-8 Appendix E A Survey of RISC Architectures
OpcodeRegisterConstant
Register-registerARMThumb
SuperH
M32R
MIPS-1615107410
312719151130
Opx4Opx4Opx4Opx4Opx4Opx8Op6Op4Op4Rd4Rd4Rs24Op5Rs13Rs23Rs14Rd4Opx2Rd3Rs3Rs4Rd3Rs14Register-immediateARMThumbSuperHM32RMIPS-161510740
31271915110
Opx4Opx4Op3Const12Op5Op4Op4Rd4Rd4Op5Rs3Const5Rs14Rd4Rd3Const8Const8Rs4Rd3Const16Data transferARMThumbSuperHM32RMIPS-161510740
31271915110
Opx4Opx4Op3Const12Op5Op4Op4Rd4Rd4Rs4Op5Rs3Const5Rs14Rd4Const5Rs3Rd3Const4Rs4Rd3Const16BranchARMThumb
SuperH
M32RMIPS-16151070
0327213Opx4Opx4Opx4Op4Const24Op4Op8Op4Rd4Op5Const8Const8Const8Rs4Rd3Const16JumpARMThumbSuperHM32R
MIPS-1615100
0327213Opx4Opx4Op4Const24Op5Op4Op4Op5Const11Const11Const12Const8CallARM
ThumbSuperH
M32RMIPS-16052510327213Opx4Op8Op4Const24Op5Op4Op6Const26Const11Opx5Const11Const12Const24FIGURE E.2.4  Instruction formats for embedded RISC architectures.
 ese six formats are 
fo
 ve architectur
 e notation is the same as in Figure E.2.3. Note the similarities in branch, 
jump, and call formats, and the diversity in register-register, register-immediate, and data transfer formats. 
 e 
 erences result from whether the architecture has 8 or 16 registers, whether it is a 2- or 3-operand 
format, and whether the instruction length is 16 or 32 bits.

 E.3 Instructions: the MIPS Core Subset E-
9Figures E.2.5 and E.2.6 show the variations in extending constan
 elds to the 
full width of the registers. In this subtle point, the RISCs are similar but not 
identical.
 E.3 Instructions: the MIPS Core Subset
 e similarities of each architecture allow simultaneous descriptions, starting with 
the operations equivalent to the MIPS core.
MIPS Core Instructions
Almost every instruction found in the MIPS core is found in the other 

architectures, as Figures E.3.1 through E.3.5 show. (For reference
 nitions of the 
MIPS instructions are found in the MIPS Reference Data Card at the beginning 

of the book.) Instructions are listed under four categories: data transfer (Figure 

E.3.1); arithmetic/logical (Figure E.3.2); control (Figure E.3.3); an
 oating point 
(Figur
 h category (Figure E.3.5) shows conventions for register 
Format: instruction category Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9
ngiSngiSngiSngiSngiSlla:hcnarBngiSngiSngiSÑngiSlla:llac/pmuJRegister-immediate: data transferSign Sign Sign Sign Sign 
Register-immediate: arithmeticZero Sign Sign Sign Sign 
ngiSoreZÑoreZoreZlacigol:etaidemmi-retsigeRFIGURE E.2.5  Summary of constant extension for desktop RISCs.
 e constants in the jump and call instructions of MIPS 
are not sign-extended, since they only replace the lower 28 bits of PC, leaving the upper 4 bits unchanged. PA-RISC has no logi
cal immediate 
instructions.
Format: instruction category Armv4 Thumb SuperH M32R MIPS-16 
 ngiS ngiS ngiS ngiS ngiS lla :hcnarB Ñ ngiS ngiS oreZ/ngiS ngiS lla :llac/pmuJRegister-immediate: data transferZero Zero Zero Sign Zero 
Register-immediate: arithmeticZero Zero Sign Sign Zero/Sign 
 Ñ oreZ oreZ Ñ oreZlacigol :etaidemmi-retsigeRFIGURE E.2.6 Summary of constant extension for embedded RISCs.
 e 16-bit-length instructions have much shorter 
immediates than those of the desktop RISCs, typically only
 ve to eight bits. Most embedded RISCs, however, have a way to get a long address 
for procedure calls from two sequencial halfword
 e constants in the jump and call instructions of MIPS are not sign-extended, since they 
only replace the lower 28 bits of the PC, leaving the upper 4 bits unchanged
 e 8-bit immediates in ARM can be rotated right an even number 
of bits between 2 and 30, yielding a large range of immediate values. For example, all powers of two are immediates in ARM.

E-10 Appendix E A Survey of RISC Architectures
Data transfer (instruction formats) 
R-I R-I R-I, R-R R-I, R-R R-I, R-R 
Instruction name Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9 
Load byte signed 
LDBU; SEXTB LB LDB; EXTRW,S 31,8 LBZ; EXTSB LDSB Load byte unsigned 
LDBU LBU LDB, LDBX, LDBS LBZ LDUB Load halfword signed LDWU; SEXTW LH LDH; EXTRW,S 31,16 LHA LDSH Load halfword unsigned LDWU LHU LDH, LDHX, LDHS LHZ LDUH Load word LDLS LW LDW, LDWX, LDWS LW LD Load SP ß oat 
LDS*LWC1 FLDWX, FLDWS LFS LDF Load DP ß oat 
LDT LDC1 FLDDX, FLDDS LFD LDDF Store byte 
STB SB STB, STBX, STBS STB STB Store halfword STW SH STH, STHX, STHS STH STH Store word STL SW STW, STWX, STWS STW ST Store SP ß oat 
STS SWC1 FSTWX, FSTWS STFS STF Store DP ß oat 
STT SDC1 FSTDX, FSTDS STFD STDF Read, write special registers 
MF_, MT_ MF, MT_ MFCTL, MTCTL MFSPR, MF_, MTSPR, MT_ RD, WR, RDPR, WRPR, LDXFSR, STXFSR Move integer to FP register 
ITOFS MFC1/DMFC1 STW; FLDWX STW; LDFS ST; LDF Move FP to integer register 
FTTOIS MTC1/DMTC1 FSTWX; LDW STFS; LW STF; LD FIGURE E.3.1 Desktop RISC data transfer instructions equivalent to MIPS core.
 A sequence of instructions to synthesize a 
MIPS instruction is shown separated by semicolons. If there are several choices of instructions equivalent to MIPS core, they a
re separated 
by commas. For th
 gure, halfword is 16 bits and word is 32 bits. Note that in Alpha, LDS converts single precisio
 oating point to double 
precision and loads the entire 64-bit register.
usage and pseudoinstructions on each architecture. If a MIPS core instruction 
requires a short sequence of instructions in other architectures, these instructions 

are separated by semicolons in Figures E.3.1 through E.3.5. (To avoid confusion, 

the destination register will always be th
 most operand in this appendix, 
independent of the notation normally used with each architecture.) Figures E.3.6 

through E.3.9 show the equivalent listing for embedded RISCs. Note tha
 oating 
point is generally no
 ned for the embedded RISCs.
Every architecture must have a scheme for compare and conditional branch, but 
despite all the similarities, each of these architectures has fo
 erent way to 
perform the operation.
Compare and Conditional BranchSPARC uses the traditional four condition code bits stored in the program status 

word: 
negative, zero, carry,
 and 
over
 ow ey can be set on any arithmetic or logical 
instruction; unlike earlier architectures, this
 setting is optional on each instruction. 
An explicit option leads to fewer problems in pipelined implementation. Although 

condition codes can be s
 ect of an operation, explicit compares are 
synthesized with a subtract using r0 as the destination. SPARC conditional branches 

 E.3 Instructions: the MIPS Core Subset E-
11Arithmetic/logical  
(instruction formats) 
R-R, R-I R-R, R-I R-R, R-I R-R, R-I R-R, R-I 
Instruction name Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9 
Add ADDL ADDU, ADDUADDL,
 LD0, ADDI, UADDCMADD, ADDIADD
Add (trap if overß
 ow) ADDLVADD,
 ADDIADDO,
 ADDIOADDO; MCRXR; BCADDcc;
 TVSSub SUBLSUBUSUB,
 SUBISUBFSUB
Sub (trap if overß
 ow) SUBLVSUBSUBTO,
 SUBIOSUBF/oeSUBcc;
 TVSMultiply MULLMULT,
 MULTUSHiADD;...;
 (i=1,2,3)MULLW,
 MULLIMULX
Multiply (trap if overß
 ow) 
MULLVÑSHiADDO;...;ÑÑDivide ÑDIV, DIVUDS;...;
 DSDIVWDIVX
Divide (trap if overß
ÑÑÑÑÑ )wo And ANDAND,
 ,DNADNAIDNA ANDIAND
Or BISOR,
 ,ROROIRO ORIOR
Xor XORXOR,
 ,ROXROXIROX XORI XORLoad high part register 
LDAH LUI LDIL ADDIS SETHI (B fmt.)Shift left logical SLLSLLV,
 SLLDEPW,
 Z 31-i,32-i RLWINMSLL
Shift right logical SRLSRLV,
 SRLEXTRW,
 U 31, 32-iRLWINM
 32-iSRL
 Shift right arithmetic SRA SRAV, SRA EXTRW, S 31, 32-iSRAWSRA
Compare CMPEQ, CMPLT, CMPLESLT/U, ccBUSRLC)I(PMCBMOCU/ITLS r0,... FIGURE E.3.2  Desktop RISC arithmetic/logical instructions equivalent to MIPS core.
 Dashes mean the operation is not 
available in that architecture, or not synthesized in a few instructions. Such a sequence of instructions is shown separated by
 semicolons. If 
there are several choices of instructions equivalent to MIPS core, they are separated by commas. Note that in the “Arithmetic/l
ogical” category, 
all machines but SPARC use separate instruction mn
emonics to indicate an immediate operand; SPARC o
 ers immediate versions of these 
instructions but uses a single mnemonic. (Of course these are separate opcodes!)
Control (instruction formats) 
B, J/C B, J/C B, J/C B, J/C B, J/C
Instruction name Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9 
Branch on integer compareB_ (<, >, <=, >=, =, not=)BEQ, BNE, B_Z (<, >, <=, >=) COMB, COMIB BC BR_Z, BPcc (<, >, <=, >=, =, not=) Branch on ß oating-point 
compareFB_(<, >, <=, >=, =, not=)BC1T, BC1FFSTWX
 f0; LDW t; BB t BC FBPfcc (<, >, <=, >=, =,...) Jump, jump register 
BR, JMP J, JR BL r0, BLR r0 B, BCLR, BCCTR BA, JMPL r0,...Call, call register 
BSR JAL, JALR BL, BLE BL, BLA, BCLRL, BCCTRLCALL, JMPL Trap 
CALL_PAL GENTRAP BREAK BREAK TW, TWI Ticc, SIR Return from interrupt 
CALL_PAL REIJR;
 ERET RFI, RFIR RFI DONE, RETRY, RETURNFIGURE E.3.3 Desktop RISC control instructions equivalent to MIPS core.
 If there are several choices of instructions equivalent 
to MIPS core, they are separated by commas.

E-12 Appendix E A Survey of RISC Architectures
Floating point  (instruction formats) R-R R-R R-R R-R R-R 
Instruction name Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9
Add single, double 
ADDS, ADDT ADD.S, ADD.DFADD
 FADD/dbl FADDS, FADD FADDS, FADDD Subtract single, double 
SUBS, SUBT SUB.S, SUB.D FSUB FSUB/dbl FSUBS, FSUBFSUBS,
 FSUBD Multiply single, double 
MULS, MULT MUL.S, MUL.D FMPY FMPY/dblFMULS,
 FMULFMULS,
 FMULD Divide single, double 
DIVS, DIVT DIV.S, DIV.D FDIV, FDIV/dblFDIVS,
 FDIV FDIVS, FDIVD Compare CMPT_ (=, <, <=, UN)C_.S, C_.D (<, >, <=, >=, =,...) FCMP, FCMP/dbl (<, =, >) FCMP FCMPS, FCMPD Move R-R 
ADDT Fd, F31, FsMOV.S,
 MOV.D FCPY FMV FMOVS/D/Q Convert (single, double, 
integer) to (single, 

double, integer)
CVTST, CVTTS, CVTTQ, CVTQS, CVTQT CVT.S.D, CVT.D.S, CVT.S.W, CVT.D.W, CVT.W.S, CVT.W.DFCNVFF,s,d FCNVFF,d,s FCNVXF,s,s FCNVXF,d,d FCNVFX,s,s FCNVFX,d,sÑ, FRSP, Ñ, FCTIW,Ñ, Ñ FSTOD, FDTOS, FSTOI, FDTOI, FITOS, FITOD FIGURE E.3.4  Desktop RISC ﬂ oating-point instructions equivalent to MIPS core.
 Dashes mean the operation is not available 
in that architecture, or not synthesized in a few instructions. 
If there are several choices of instructions equivalent to MIPS
 core, they are 
separated by commas.
Conventions Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9
Register with value 0 r31 (source) r0 r0 r0 (addressing) r0 Return address register 
(any) r31 r2, r31 link (special) r31 No-op LDQ_U r31,... SLL r0, r0, r0OR
 r0, r0, r0 ORI r0, r0, #0 SETHI r0, 0 Move R-R integer 
BIS..., r31,... ADD..., r0,...OR...,
 r0,... OR rx, ry, ry OR..., r0,... Operand order OP Rs1, Rs2, RdOP
 Rd, Rs1, Rs2OP
 Rs1, Rs2, RdOP
 Rd, Rs1, Rs2OP
 Rs1, Rs2, Rd FIGURE E.3.5 Conventions of desktop RISC architectures equivalent to MIPS core.
test condition codes to determine all possible unsigned and signed relations. 
Floating point uses separate condition codes to encode the IEEE 754 conditions, 

requirin
 oating-point compare instruction. Version 9 expanded SPARC 
branches in four ways: a separate set of condition codes for 64-bit operations; a 

branch that tests the contents of a register and branches if the value is 
, not
, , , , or 
 0 (see MIPS below); three more sets o
 oating-point condition 
codes; and branch instructions that encode static branch prediction.
PowerPC also uses four condition codes—
less than, greater than, equal,
 and 
summary over
 ow—but it has eight copies of th
 is redundancy allows the 
PowerPC instructions to us
 erent condition codes without co
 ict, essentially 
giving PowerPC eight extra 4-bit registers. Any of these eight condition codes can 

be the target of a compare instruction, and any can be the source of a conditional 

branc
 e integer instructions have an option bit that behaves as if the integer op 

 E.3 Instructions: the MIPS Core Subset E-
13Instruction name ARMv4  Thumb SuperH M32R MIPS-16 
Data transfer  (instruction formats) DT DT DT DT DT 
Load byte signed 
LDRSB LDRSB MOV.B LDB LB Load byte unsigned 
LDRB LDRB MOV.B; EXTU.B LDUB LBU Load halfword signed LDRSH LDRSH MOV.W LDH LH Load halfword unsigned LDRH LDRH MOV.W; EXTU.W LDUH LHU Load word LDR LDR MOV.L LD LW Store byte 
STRB STRB MOV.B STB SB Store halfword STRH STRH MOV.W STH SH Store word STR STR MOV.L ST SW Read, write special registers 
MRS, MSR Ñ1 LDC, STC MVFC, MVTC MOVE FIGURE E.3.6 Embedded RISC data transfer instructions equivalent to MIPS core.
 A sequence of instructions to synthesize 
a MIPS instruction is shown separated by semicolons. Note tha
 oating point is generally no
 ned for the embe
 umb and 
MIPS-16 are just 16-bit instruction subsets of the ARM and MIPS architectures, so machines can switch modes and execute the ful
l instruction 
set. We use —
1 to show sequences that are available in 32-bit mode but not 16-bit mo
 umb or MIPS-16.
is followed by a compare to zero that sets th
 rst condition “register.” PowerPC 
also lets the second “register” be optionally set b
 oating-point instructions. 
PowerPC provides logical operations among these eight 4-bit condition code 
registers (
CRAND, CROR, CRXOR, CRNAND, CRNOR, CREQV), allowing more 
complex conditions to be tested by a single branch.
MIPS uses the contents of registers to evaluate conditional branches. Any two 
registers can be compared for equality (
BEQ) or inequality (
BNE), and then the 
branch is taken if the condition ho
 e set on less than instructions (
SLT, SLTI, SLTU, SLTIU) compare two operands and then set the destination register to 1 
if less and to 0 otherwise.
 ese instructions are enough to synthesize the full set 
of relations. Because of the popularity of comparisons to 0, MIPS includes special 

compare and branch instructions for all such comparisons: greater than or equal to 

zero (
BGEZ), greater than zero (
BGTZ), less than or equal to zero (
BLEZ), and less 
than zero (
BLTZ). Of course, equal and not equal to zero can be synthesized using 
r0 with 
BEQ and 
BNE. Like SPARC, MIPS I uses a condition code fo
 oating point 
with separate
 oating-point compare and branch instructions; MIPS IV expanded 
this to eight
 oating-point condition codes, with th
 oating point comparisons 
and branch instructions specifying the condition to set or test.
Alpha compares (
CMPEQ, CMPLT, CMPLE, CMPULT, CMPULE) test two registers 
and set a third to 1 if the condition is true and to 0 otherwise. Floating-point 

compares (
CMTEQ, CMTLT, CMTLE, CMTUN) set the result to 2.0 if the condition 
holds and to 0 otherwise.
 e branch instructions compare one register to 0 (
BEQ, BGE, BGT, BLE, BLT, BNE) or its le
 cant bit to 0 (
BLBC, BLBS) and 
then branch if the condition holds.

E-14 Appendix E A Survey of RISC Architectures
PA-RISC has many branch options, which we’ll see in Sectio
 e most 
straightforward is a compare and branch instruction (
COMB), which compares two 
registers, branches depending on the standard relations, and then tests the least 
 cant bit of the result of the comparison.
ARM is similar to SPARC, in that it provides four traditional condition codes 
that are optionally set. 
CMP subtracts one operand from the other and th
 erence 
sets the condition codes. Compare negative (
CMN) adds one operand to the other, 
and the sum sets the condition codes. 
TST performs logical AND on the two 
operands to set all condition codes but over
 ow, while 
TEQ uses exclusive OR to 
set th
 rst three condition codes. Like SPARC, the conditional version of the ARM 
branch instruction tests condition codes to determine all possible unsigned and 

signed relations.
Arithmetic/logical (instruction formats) 
R-R, R-I R-R, R-I R-R, R-I R-R, R-I R-R, R-I 
Instruction name ARMv4  Thumb SuperH M32R MIPS-16 
Add ADD ADD ADD ADD, ADDI, ADD3 ADDU, ADDIU Add (trap if overß
 ow) ADDS; SWIVS ADD; BVC .+4; SWIADDV
 ADDV, ADDV3 Ñ1 Subtract SUB SUB SUB SUB SUBU Subtract (trap if overß
 ow) SUBS; SWIVS SUB; BVC .+1; SWI SUBV SUBV Ñ1 Multiply MUL MUL MUL MUL MULT, MULTU Multiply (trap if overß
 ow) ÑDivide ÑÑ
DIV1, DIVoS, DIVoU DIV, DIVU DIV, DIVU Divide (trap if overß
ÑÑÑ )wo And AND AND AND AND, AND3 AND Or ORR ORR OR OR, OR3 OR Xor EOR EOR XOR XOR, XOR3 XOR Load high part register ÑÑ
SETH Ñ1 Shift left logical LSL3LSL2SHLL, SHLLn SLL, SLLI, SLL3 SLLV, SLL Shift right logical LSR3LSR2SHRL, SHRLn SRL, SRLI, SRL3 SRLV, SRL Shift right arithmetic ASR3ASR2SHRA, SHAD SRA, SRAI, SRA3 SRAV, SRA Compare CMP,CMN, TST,TEQ CMP, CMN, TSTCMP/cond,
 TST CMP/I, CMPU/I CMP/I2, SLT/I, SLT/IU FIGURE E.3.7  Embedded RISC arithmetic/logical instructions equivalent to MIPS core.
 Dashes mean the operation is not 
available in that architecture, or not synthesized in a few instructions. Such a sequence of instructions is shown separated by
 semicolons. If 
there are several choices of instructions equivalent to MIPS core, they are separated by co
 umb and MIPS-16 are just 16-bit instruction 
subsets of the ARM and MIPS architectures, so machines can switch modes and execute the full instruction set. We use —
1 to show sequences 
that are available in 32-bit mode but not 16-bit mo
 umb o
 e superscript 2 shows new instructions found only in 16-bit 
mode of
 umb or MIPS-16, such as CMP/I
2. ARM incl
 s as part of every data operation instruction, so th
 s with superscript 3 
are just a variation of a move instruction, such as LSR
3 .
 E.3 Instructions: the MIPS Core Subset E-
15As we shall see in Section E.12, one unusual feature of ARM is that every 
instruction has the option of executing conditionally depending on the condition 
co
 is bears similarities to the annulling option of PA-RISC, seen in 
Section E.11.)
Not surprisingly,
 umb follo
 e 
 erences are that setting condition 
codes are not optional, the 
TEQ instruction is dropped, and there is no conditional 
execution of instructions.
 e Hitachi SuperH uses a single T-bit condition that is set by compare 
instructions. Two branch instructions decide to branch if either the T bit is 1 

(BT) or the T bi
 e tw
 avors of branches allow fewer comparison 
instructions.
Mitsubishi M32R also o
 ers a single condition code bit (C) used for signed and 
unsigned comparisons (
CMP, CMPI, CMPU, CMPUI) to see if one register is less 
than the other or not, similar to the MIPS set on less than instructions. Two branch 

instructions test to see if the C bit is 1 or 0: BC and BN
 e M32R also includes 
instructions to branch on equality or inequality of registers (
BEQ and 
BNE) and all 
relations of a register to 0 (
BGEZ, BGTZ, BLEZ, BLTZ, BEQZ, BNEZ). Unlike 
BC and BNC, these last instructions are all 32 bits wide.
MIPS-16 keeps set on less than instructions (
SLT, SLTI, SLTU, SLTIU), but instead of putting the result in one of the eight registers, it is placed in a special 

register named T. MIPS-16 is always implemented in machines that also have the 

full 32-bit MIPS instructions and registers; hence, register T is really register 24 in 

the full MIPS architecture
 e MIPS-16 branch instructions test to see if a register 
is or is not equal to zero (
BEQZ and 
BNEZ ere are also instructions that branch 
Conventions ARMv4   Thumb SuperH M32R MIPS-16 
Return address reg. R14 R14 PR (special) R14 RA (special) 
No-op MOV r0, r0 MOV r0, r0 NOP NOP SLL r0, r0 

Operands, order OP Rd, Rs1, Rs2 OP Rd, Rs1 OP Rs1, Rd OP Rd, Rs1 OP Rd, Rs1, Rs2 
FIGURE E.3.9 Conventions of embedded RISC instructions equivalent to MIPS core.
Control (instruction formats) B, J, C B, J, C B, J, C B, J, C B, J, C 
Instruction name ARMv4   Thumb SuperH M32R MIPS-16 
Branch on integer compare B/cond B/cond BF, BT BEQ, BNE, BC, BNC, B__Z BEQZ
2, BNEZ
2, BTEQZ
2, BTNEZ2Jump, jump register MOV pc, ri MOV pc, ri BRA, JMP BRA, JMP B
2, JR 
Call, call register BL BL BSR, JSR BL, JL JAL, JALR, JALX
2 KAERB PART APART IWS IWS parT
Return from interrupt MOVS pc, r14
Ñ1Ñ ETR STR1FIGURE E.3.8  Embedded RISC control instructions equivalent to MIPS core.
 umb and MIPS-16 are just 16-bit instruction 
subsets of the ARM and MIPS architectures, so machines can switch modes and execute the full instruction set. We use —
1 to show sequences 
that are available in 32-bit mode but not 16-bit mo
 umb o
 e superscript 2 shows new instructions found only in 16-bit 
mode of
 umb or MIPS-16, such as BTEQZ
2.
E-16 Appendix E A Survey of RISC Architectures
if register T is or is not equal to zero (
BTEQZ and 
BTNEZ). To test if two registers are 
equal, MIPS added compare instructions (
CMP, CMPI) that compute the exclusive 
OR of two registers and place the result in register T. Compare was added since 
  out instructions to compare and branch if registers are equal or not 
(BEQ and 
BNE).Figures E.3.10 and E.3.11 summarize the schemes used for conditional branches.
 E.4 Instructions: Multimedia Extensions of 
the Desktop/Server RISCs
Since every desktop microprocessor by
 nition has its own graphical displays, 
as transistor budgets increased it was inevitable that support would be added for 
graphics operations. Many graphics systems use eight bits to represent each of the 

three primary colors plus eight bits for the location of a pixel.
Alpha MIPS-64 PA-RISC 2.0 PowerPC SPARCv9 
Number of condition code bits (integer and FP)0 8 FP 8 FP 
8 × 4 both 2 
× 4 integer, 4 
× 2 FP Basic compare instructions 

(integer and FP) 1 integer, 1 FP1 integer, 1 FP 4 integer, 2 FP 4 integer, 2 FP 1 FP 
Basic branch instructions 

(integer and FP) 1 2 integer, 1 FP 7 integer 1 both 3 integer, 1 FP 
Compare register with 
register/const and branch Ñ =, not= =, not=, <, <=, >, >=, 
even, odd
Ñ Ñ Compare register to zero and 
branch=, not=, <, <=, >, 

>=, even, odd 
=, not=, <, <=, 

>, >= 
=, not=, <, <=, >, >=, 

even, odd 
Ñ =, not=, <, <=, >, >= 
FIGURE E.3.10 Summary of ﬁ
 ve desktop RISC approaches to conditional branches.
 Floating-point branch on PA-RISC is 
accomplished by copying the FP status register into an integer register and then using the branch on bit instruction to test th
e FP comparison 
bit. Integer compare on SPARC is synthesized with an arithmetic instruction that sets the condition codes using r0 as the desti
nation.
ARMv4   Thumb SuperH M32R MIPS-16 
Number of condition code bits 4 4 1 1 1 
Basic compare instructions 4 3 2 2 2 

Basic branch instructions 1 1 2 3 2 

Compare register with register/const and branchÑ Ñ =, >, >= =, not= Ñ 
Compare register to zero and branch Ñ Ñ =, >, >= =, not=, <, <=, >, >= =, not= 
FIGURE E.3.11 Summary of ﬁ
 ve embedded RISC approaches to conditional branches

 E.4 Instructions: Multimedia Extensions of the Desktop/Server RISCs E-
17 e addition of speakers and microphones for teleconferencing and video 
games suggested support of sound as well. Audio samples need more than eight 
bits of precision, but 16 bits ar
  cient.
Every microprocessor has special support so that bytes and halfwords take 
up less space when stored in memory, but due to the infrequency of arithmetic 

operations on these data sizes in typical integer programs, there is little support 

beyond data transfer
 e architects of the Intel i860, which was ju
 ed as a 
graphical accelerator within the company, recognized that many graphics and 

audio applications would perform the same operation on vectors of this data. 

Although a vector unit was beyond the transistor budget of the i860 in 1989, by 

partitioning the carry chains within a 64-bit ALU, it could perform simultaneous 

operations on short vectors of eight 8-bit operands, four 16-bit operands, or two 

32-bit opera
 e cost of such partitioned ALUs was small. Applications that 
lend themselves to such support include MPEG (video), games like DOOM (3-D 

graphics), Adobe Photoshop (digital photography), and teleconferencing (audio 

and image processing).
Like a virus, over time such multimedia support has spread to nearly every 
desktop microprocessor. HP was th
 rst successful desktop RISC to include such 
support. As we shall see, this virus spread unevenly
 e PowerPC is the only 
holdout, and rumors are that it is “running a fever.”
 ese extensions have been called subword parallelism, vector, or SIMD (single-
instruction, multiple data) (see Chapter 6). Since Intel marketing uses SIMD to 

describe the MMX extension of the 8086, that has become the popular name. 

Figure E.4.1 summarizes the support by architecture.
From Figure E.4.1, you can see that in general, MIPS MDMX works on eight 
bytes or four halfwords per instruction, HP PA-RISC MAX2 works on four half-

words, SPARC VIS works on four halfwords or two words, and Alpha doesn’t do 

muc
 e Alpha MAX operations are just byte versions of compare, min, max, and 
absolut
 erence, leaving it up to so
 ware to isolat
 elds and perform parallel 
adds, subtracts, and multiplies on bytes and halfwords. MIPS also added operations 

to work on two 32-bi
 oating-point operands per cycle, but they are considered 
part of MIPS V and not simply multimedia extensions (see Section E.7).
One feature not generally found in general-purpose microprocessors is 
saturating operations. Saturation means that when a calculation over
 ows, the 
result is set to the largest positive number or most negative number, rather than a 

modulo calculation as in two’s complement arithmetic. Commonly found in digital 

signal processors (see the next section), these saturating operations are helpful in 

routines fo
 ltering.
 ese machines largely used existing register sets to hold operands: integer 
registers for Alpha and HP PA-RISC an
 oating-point registers for MIPS and Sun. 
Hence data transfers are accomplished with standard load and store instructions. 

MIPS also added a 192-bit (3*64) wide register to act as an accumulator for some 

operations. By having three times the native data width, it can be partitioned to 

accumulate either eight bytes with 24 bits p
 eld or four halfwords with 48 bits 

E-18 Appendix E A Survey of RISC Architectures
p
 is wide accumulator can be used for add, subtract, and multiply/ add 
instructions. MIPS claims performance advantages of two to four times for the 
accumulator.
Perhaps the surprising conclusion of this table is the lack of consistency.
 e only operations found on all four are the logical operations (AND, OR, XOR), 

which do not need a partitioned ALU. If we leave out the frugal Alpha, then the 

only other common operations are parallel adds and subtracts on four halfwords.
Each manufacturer states that these are instructions intended to be used in 
hand-optimized subroutine libraries, an intention likely to be followed, as a 

compiler that works well with multimedia extensions of all desktop RISCs would 

be challenging.
Instruction category Alpha MAX MIPS MDMX PA-RISC MAX2 PowerPC SPARC VIS 
 W2 ,H4 H4 H4 ,B8 tcartbus/ddA H4 H4 ,B8 bus/dda gnitarutaS H/B4 H4 ,B8ylpitluM )=< ,> ,=ton ,=( W2 ,H4 )=<,<,=( H4 ,B8 )=>( B8 erapmoC H4 H4 ,B8 tfel/thgir tfihS H4 H4 citemhtira thgir tfihS H4 ,B8 dda dna ylpitluMShift and add (saturating) 4H  W2 ,H4 ,B8 W2 ,H4 ,B8 W2 ,H4 ,B8 W2 ,H4 ,B8 rox/ro/dnAAbsolute difference 8B 
8B Max/min 8B, 4W 8B, 4H 
Pack (2
n bits --> n ,B2>-W2 ,H2>-W2B8>-H4*2 B8>-H4*2 ,H4>-W2*2 B4>-H4 ,B2>-W2 )stib 4H->4B B8>-B4*2 ,H4>-B4H4>-H2*2 ,B8>-B4*2 H4>-B4 ,W2>-B2 egrem/kcapnU H4 H4 ,B8e ßfuhs/etumreP .tP .lF regetnI .ccA b291 + .tP .lF regetnI stes retsigeRFIGURE E.4.1 Summary of multimedia support for desktop RISCs.
 B stands for byte (8 bits), H for half word (16 bits), and W 
for word (32 bi
 us 8B means an operation on eight bytes in a single instruction. Pack and unpack use the notation 2*2W to mean two 
operands each with two words. Note that MDMX has vector/scalar operations, where the scalar is sp
 ed as an element of one of the vector 
register
 is table is a simp
 cation of the full multimedia architectures, leaving out many details. For example, MIPS MDMX includes 
instructions to multiplex between two operands, HP MAX2 includes an
 instruction to calculate averages, and SPARC VIS includes i
nstructions 
to set registers to constants. Also, this table does not include the memory alignment operation of MDMX, MAX, and VIS.

 E.5 Instructions: Digital Signal-Processing Extensions of the Embedded RISCs E-
19 E.5 Instructions: Digital Signal-Processing 
Extensions of the Embedded RISCsOne feature found in every digital signal processor (DSP) architecture is support 
for integer multiply-accumulate.
 e multiplies tend to be on shorter words than 
regular integers, such as 16 bits, and the accumulator tends to be on longer words, 

such as 64 bi
 e reason for multiply-accumulate is t
  ciently implement 
digit
 lters, common in DSP applications. S
 umb and MIPS-16 are subset 
architectures, they do not provide such support. Instead, programmers should use 

the DSP or multimedia extensions found in the 32-bit mode instructions of ARM 

and MIPS-64.
Figure E.5.1 shows the size of the multiply, the size of the accumulator, and 
the operations and instruction names for the embedded RISCs. Machines with 

accumulator sizes greater than 32 and less than 64 bits will force the upper bits 

to remain as the sign bits, thereby “saturating” the add to set to maximum and 

minim
 xed-point values if the operations over
 ow.
ARMv4Thumb SuperH M32R MIPS-16 
Size of multiply 32B × 32B Ñ 32B × 32B, 16B × 16B 32B × 16B, 16B × 16B Ñ 
 Ñ B65 B46/B84 ,B24/B23 Ñ B46/B23 rotalumucca fo eziS Ñ CCA LCAM ,HCAM Ñ sRPG fo sriap ro RPG ynA eman rotalumuccAOperations 32B/64B product + 64B 
accumulate signed/unsignedÑ 32B product + 42B/32B 
accumulate (operands in memory); 64B product 
+ 64B/48B accumulate 
(operands in memory); clear 

MAC 
32B/48B product + 
64B accumulate, 

round, move
Ñ Corresponding 
instruction names
MLA, SMLAL, UMLALÑ MAC, MACS, MAC.L, MAC.LS, 
CLRMAC
MACHI/MACLO, 

MACWHI/MACWLO, 
RAC, RACH, MVFACHI/

MVFACLO, MVTACHI/

MVTACLO 
Ñ FIGURE E.5.1 Summary of ﬁ
 ve embedded RISC approaches to multiply-accumulate.

E-20 Appendix E A Survey of RISC Architectures
 E.6 Instructions: Common Extensions to 
MIPS CoreFigures E.6.1 through E.6.7 list instructions not found in Figures E.3.5 through 
E.3.11 in the same four categories. Instructions are put in these lists if they appear 

in more than one of the standard architectur
 e instructions ar
 ned using 
the hardware description languag
 ned in Figure E.6.8.
Although most of the categories are self-explanatory, a few bear comment:
 e “atomic swap” row means a primitive that can exchange a register with 
memory without interruptio
 is is useful for operating system semaphores 
in a uniprocessor as well as for multiprocessor synchronization (see Section 

2.11 in Chapter 2).
 e 64-bit data transfer and operation rows show how MIPS, PowerPC, 
and SPAR
 ne 64-bit addressing and integer operations. SPARC simply 
 nes all register and addressing operations to be 64 bits, adding only 
Name DeÞ nitionAlphaMIPS-64PA-RISC 2.0PowerPCSPARCv9 
Atomic swap R/M 
(for locks and 
semaphores) Temp<---Rd;  Rd<ÐMem[x]; 
Mem[x]<---Temp
LDL/Q_L; STL/Q_C LL; SCÑ (see D.8)
LWARX; 
STWCX CASA, CASXLoad 64-bit integerRd<Ð
64 Mem[x] LDQ LD LDDLD LDX
Store 64-bit integerMem[x]<---
64 Rd STQ SD STD STD STX
Load 32-bit integer unsigned Rd32..63<Ð32 Mem[x]; Rd0..31<Ð32 0 LDL; EXTLL LWU LDWLWZ LDUW
Load 32-bit integer 
signed Rd32..63<Ð32 Mem[x]; 32 Rd0..31<Ð32 Mem[x]0 LDLLW LDW; EXTRD,S 
63, 8 LWA LDSWPrefetch Cache[x]<Ðhint FETCH, FETCH_M*PREF, PREFXLDD, r0 
LDW, r0 DCBT, DCBTST PRE-FETCH Load coprocessor Coprocessor<Ð Mem[x] Ñ  
LWCi CLDWX, CLDWS
Ñ  Ñ 
Store coprocessor Mem[x]<Ð Coprocessor Ñ  
SWCi CSTWX, CSTWS
Ñ Ñ Endian(Big/little endian?) 
EitherEitherEitherEither Either
Cache ß ush(Flush cache block at this 
address)ECBCP0opFDC, FICDCBFFLUSH
Shared memory 
synchronization(All prior data transfers 

complete before next data transfer may start)
WMBSYNCSYNCSYNCMEMBAR
FIGURE E.6.1 Data transfer instructions not found in MIPS core but found in two or more of the ﬁ
 ve desktop 
architectures. e load linked/store conditional pair of instructions gives Alpha and MIPS atomic operations for semaphores, allowing data 
to be read from memory, mo
 ed, and stored without fear of interrupts or other 
machines accessing the data in a multiprocessor (see Chapter 
2). Prefetching in the Alpha to external caches is accomplished with 
FETCH and 
FETCH_M; on-chip cache prefetches use 
LD_Q A, R31, and 
LD_Y A. F31 is used in the Alpha 21164 (see Bhandarkar [1995], p. 190).

 E.6 Instructions: Common Extensions to MIPS Core E-
21Name DeÞ nitionAlphaMIPS-64PA-RISC 2.0PowerPCSPARCv9 
64-bit integer arithmetic opsRd<Ð64Rs1 op64 Rs2ADD, 
SUB, MULDADD, DSUB 
DMULT, DDIVADD, SUB, 
SHLADD, DSADD, SUBF, 
MULLD, DIVDADD, SUB, 
MULX, 
S/UDIVX 64-bit integer 
logical opsRd<Ð64Rs1 op64 Rs2 AND, OR, 
XORAND, OR, 
XORAND, OR, XORAND, OR, XORAND, OR, 
XOR64-bit shifts Rd<Ð
64Rs1 op64 Rs2SLL, 
SRA, SRLDSLL/V, 
DSRA/V, DSRL/VDEPD,Z EXTRD,S 
EXTRD,USLD, SRAD, 
SRLDSLLX, SRAX, 
SRLX Conditional move if (cond) Rd<ÐRs 
CMOV_ MOVN/Z SUBc, n; ADD
Ñ MOVcc, MOVr 
Support for 

multiword integer addCarryOut, Rd <Ð Rs1 + 
Rs2 + OldCarryOut
Ñ ADU; SLTU; 
ADDU, DADU; 
SLTU; DADDUADDC ADDC, ADDE ADDcc 
Support for 
multiword integer sub CarryOut, Rd <Ð Rs1 

Rs2 + OldCarryOut
Ñ SUBU; SLTU; 
SUBU, 
DSUBU; 
SLTU; DSUBU SUBB SUBFC, SUBFE SUBcc  
And not Rd <Ð Rs1 & ~(Rs2) 
BICÑANDCM ANDC ANDN 
Or notRd <Ð Rs1 | ~(Rs2)
ORNOTÑÑORCORN
Add high immediate Rd
0..15<ÐRs10..15 + (Const<<16);ÑÑADDIL (R-I) ADDIS (R-I)
Ñ Coprocessor 
operations(DeÞ ned by coprocessor)Ñ
COPi COPR,iÑIMPDEPi FIGURE E.6.2 Arithmetic/logical instructions not found in MIPS core but found in two or more of the ﬁ
 ve desktop 
architectures.Name DeÞ nitionAlphaMIPS-64PA-RISC 2.0PowerPCSPARCv9 
Optimized delayed 

branches(Branch not always 

delayed) 
Ñ BEQL, BNEL, B_ZL (<, >, <=, >=)COMBT, n, COMBF, nÑ BPcc, A, 
FPBcc, A Conditional trap if (COND) {R31<---PC; PC 
<Ð0..0#i} Ñ T_,,T_I (=, 
not=, <, >, <=, >=)SUBc, n; BREAKTW, TD, TWI, 
TDI Tcc No. control registers 
Misc. regs (virtual 

memory, interrupts, . . .)
6equiv. 12323329
FIGURE E.6.3 Control instructions not found in MIPS core but found in two or more of the ﬁ
 ve desktop architectures.
special instructions for 64-bi
 s, data transfers, and branches. MIPS 
includes the same extensions, plus it adds separate 64-bit signed arithmetic 
instructions. PowerPC adds 64-bit righ
 , load, store, divide, and compare 
and has a separate mode determining whether instructions are interpreted as 

32- or 64-bit operations; 64-bit operations will not work in a machine that 

E-22 Appendix E A Survey of RISC Architectures
Name DeÞ nitionAlphaMIPS-64PA-RISC 2.0PowerPCSPARCv9 
Multiply and add Fd <Ð ( Fs1 × Fs2) + Fs3 Ñ MADD.S/D FMPYFADD sgl/dblFMADD/S
 Multiply and sub Fd <Ð ( Fs1 × Fs2) Ð Fs3 Ñ MSUB.S/D FMSUB/S Neg mult and add Fd <Ð -(( Fs1 × Fs2) + Fs3) Ñ NMADD.S/D FMPYFNEG sgl/dblFNMADD/S
 Neg mult and sub Fd <Ð -(( Fs1 × Fs2) Ð Fs3) Ñ S/BUSMNFD/S.BUSMN Square root Fd <Ð SQRT(Fs) SQRT_ 
SQRT.S/D FSQRT sgl/dbl FSQRT/SFSQRTS/D
Conditional move if (cond) Fd<ÐFs FCMOV_ 
MOVF/T, MOVF/T.S/D FTESTFCPY Ñ FMOVcc Negate Fd <Ð Fs ^ 
x80000000 CPYSN NEG.S/D FNEG sgl/dbl FNEG FNEGS/D/Q Absolute value Fd <Ð Fs & 
x7FFFFFFF Ñ ABS.S/D FABS/dbl FABS FABSS/D/Q FIGURE E.6.4  Floating-point instructions not found in MIPS core but found in two or more of the ﬁ
 ve desktop 
architectures.Name DeÞ nition ARMv4   Thumb SuperH M32R MIPS-16 
Atomic swap R/M (for 
semaphores)Temp<ÐRd; Rd<ÐMem[x]; 

Mem[x]<ÐTemp
SWP, SWPB Ñ1(see TAS) LOCK; UNLOCKÑ1 Memory management unitPaged address translation Via coprocessor 
instructions
Ñ1LDTLB Ñ1Endian (Big/little endian?) Either Either Either Big Either 
FIGURE E.6.5 Data transfer instructions not found in MIPS core but found in two or more of the ﬁ
 ve embedded 
architectures. We use —
1 to show sequences that are available in 32-bit mode but not 16-bit mo
 umb or MIPS-16.
only supports 32-bit mode. PA-RISC is expanded to 64-bit addressing and 
operations in version 2.0.
 e “prefetch” instruction supplies an address and hint to the implementation 
about the data. Hints include whether the data is likely to be read or written 

soon, likely to be read or written only once, or likely to be read or written 

many times. Prefetch does not cause exceptions. MIPS has a version that 

adds two registers to get the address fo
 oating-point programs, unlike 
no
 oating-point MIPS programs.
 In the “Endian” row, “Big/little” means there is a bit in the program 
status register that allows the processor to act either as big endian or little 

endian (see App
 is can be accomplished by simply complementing 
some of the le
 cant bits of the address in data transfer instructions.

 E.6 Instructions: Common Extensions to MIPS Core E-
23Name DeÞ nition ARMv4   Thumb SuperH M32R MIPS-16 
Load immediate Rd<---Imm
MOV MOV MOV, MOVA LDI, LD24 LI Support for multiword integer addCarryOut, Rd <--- Rd + Rs1 + 
OldCarryOut
ADCS ADC ADDC ADDX Ñ1 Support for multiword integer subCarryOut, Rd <--- Rd Ð Rs1 + 
OldCarryOut
SBCS SBC SUBC SUBX Ñ1  1sR Ð 0 ---< dR etageNNEG2NEG NEG NEG  )1sR(~ ---< dRtoNMVN MVN NOT NOT NOT  1sR ---< dR evoMMOV MOV MOV MV MOVE 
Rotate right Rd <--- Rs i, 
>> Rd0. . . iÐ1 <--- Rs31Ði. . . 31ROR ROR ROTC )2sR(~ & 1sR ---< dR ton dnABIC BIC FIGURE E.6.6 Arithmetic/logical instructions not found in MIPS core but found in two or more of the ﬁ
 ve embedded architectures. We use —
1 to show sequences that are available in 32-bit mode but not in 16-bit mo
 umb o
 e superscript 
2 shows new instructions found only in 16-bit mode of
 umb or MIPS-16, such as NEG
2.Name DeÞ nition ARMv4   Thumb SuperH M32R MIPS-16 
No. control registers Misc. registers 2129 9536 
FIGURE E.6.7 Control information in the ﬁ
 ve embedded architectures.
 e “shared memory synchronization” helps with cache-coherent multi-
processors: all loads and stores executed before the instruction must complete 
before loads and stores a
 er it can start. (See Chapter 2.)
 e “coprocessor operations” row lists several categories that allow for the 
processor to be extended with special-purpose hardware.
 erence that needs a longer explanation is the optimized branches. Figure 
E.6.9 shows the option
 e Alpha and PowerPC o
 er branches that tak
 ect immediately, like branches on earlier architectures. To accelerate branches, these 

machines use branch prediction (see Chapter 4). All the rest of the desktop RISCs 

 er delayed branches (see App
 e embedded RISCs generally do not 
support delayed branch, with the exception of SuperH, which has it as an option.
 e other three desktop RISCs provide a version of delayed branch that makes it 
easier t
 ll the delay slo
 e SPARC “annulling” branch executes the instruction 
in the delay slot only if the branch is taken; otherwise the instruction is annulled. 

 is means the instruction at the target of the branch can safely be copied into the 
delay slot, since it will only be executed if the branch is tak
 e restrictions are 
that the target is not another branch and that the target is known at compile time. 

(SPARC also o
 ers a nondelayed jump because an unconditional branch with the 
annul bit set does not execute the following instruction.) Later versions of the MIPS 

E-24 Appendix E A Survey of RISC Architectures
(Plain) branch Delayed branch Annulling delayed branch 
Found in architecturesAlpha, PowerPC, ARM, Thumb, 
SuperH, M32R, MIPS-16
MIPS-64, PA-RISC, 
SPARC, SuperH
MIPS-64, SPARCPA-RISC 
Execute following instructionOnly if branch 
not taken Always Only if branch 
taken
If forward branch 
not taken or backward 

branch taken 
FIGURE E.6.9 When the instruction following the branch is executed for three types of branches.
 gninaeM elpmaxE gninaeM noitatoN<--Data transfer. Length of transfer is given by 
the destinationÕs length; the length is speciÞ
 ed when not clear. 
Regs[R1]<--Regs[R2]; Transfer contents of R2 to R1. 
Registers have a Þ xed length, so 

transfers shorter than the register 

size must indicate which bits are 
used. M Array of memory accessed in bytes. The 
starting address for a transfer is indicated as 

the index to the memory array. 
Regs[R1]<--M[x]; Place contents of memory location x 

into R1. If a transfer starts at 
M[i] and requires 4 bytes, the transferred 

bytes are 
M[i], M[i+1], M[i+2], and M[i+3]. <--n Transfer an 
n-bit  Þ eld, used whenever length 
of transfer is not clear.
M[y]<--16M[x]; Transfer 16 bits starting at memory 

location x to memory location y. The 
length of the two sides should match. Xn Subscript selects a bit. Regs[R1]0<--0; Change sign bit of 
R1 to 0. (Bits are numbered from MSB starting at 0.) 
Xm..n Subscript selects a Þ eld. 
Regs[R3]24..31<--M[x]; Moves contents of memory location x 
into low-order byte of R3. 
Xn Superscript replicates a bit Þ eld. 
Regs[R3]0..23<--024; Sets high-order three bytes of 
R3 to 0. ## Concatenates two Þ elds. 
Regs[R3]<--240## M[x]; F2##F3<--64M[x]; Moves contents of location x into low 

byte of 
R3; clears upper three bytes. 
Moves 64 bits from memory starting 
at location x; 1st 32 bits go into F2, 2nd 32 into F3. *, & Dereference a pointer; get the address of a 
variable.p*<--&x; Assign to object pointed to by p the 
address of the variable x. <<, >> C logical shifts (left, right). 
Regs[R1] << 5 Shift R1 left 5 bits. ==, !=, >, <, 

>=, <= 
C relational operators; equal, not equal, 

greater, less, greater or equal, less or equal. 
(Regs[R1]== Regs[R2]) & (Regs[R3]!=Regs[R4])True if contents of 
R1 equal the contents of R2 and contents of R3 do not equal the contents of R4. &, |, ^, ! C bitwise logical operations: AND, OR, 
exclusive OR, and complement. 
(Regs[R1] & (Regs[R2]| Regs[R3])) Bitwise AND of R1 and bitwise OR of R2 and R3. FIGURE E.6.8 Hardware description notation (and some standard C operators).

 E.7 Instructions Unique to MIPS-64 E-
25architecture have added a branch likely instruction that also annuls the following 
instruction if the branch is not taken. PA-RISC allows almost any instruction to 

annul the next instruction, including branches. Its “nullifying” branch option will 

execute the next instruction depending on the direction of the branch and whether 

it is taken (i.e., if a forward branch is not taken or a backward branch is taken). 

Presumably this choice was made to optimize loops, allowing the instructions 

following the exit branch and the looping branch to exe cute in the common case. 
Now that we have covered the similarities, we will focus on the unique features 
of each architecture. We
 rst cover the desktop/server RISCs, ordering them by 
length of description of the unique features from shortest to longest, and then the 

embedded RISCs.
 E.7 Instructions Unique to MIPS-64
MIPS has gone throug
 ve generations of instruction sets, and this evolution has 
generally added features found in other architectures. Here are the salient unique 

features of MIPS, th
 rst several of which were found in the original instruction set.
Nonaligned Data Transfers
MIPS has special instructions to handle mi
saligned words in memory. A rare event 
in most programs, it is included for supporting 16-bit minicomputer applications 

and for doing 
memcpy and 
strcpy faster. Although most RISCs trap if you try to 
load a word or store a word to a misalig
ned address, on all architectures misaligned 
words can be accessed without traps by using four load byte instructions and then 

assembling the result usin
 s and logical O
 e MIPS load and store word 
  and right instructions (
LWL, LWR, SWL, SWR) allow this to be done in just 
two instructions: LWL loads th
  portion of the register and LWR loads the right 
portion of the register. SWL and SWR do the corresponding stores. Figure E.7.1 

shows how they wo
 ere are also 64-bit versions of these instructions.
Remaining Instructions
Below is a list of the remaining unique details of the MIPS-64 architecture:
 NOR
 is logical instruction calculates 
(Rs1 | Rs2). Constant sh
  amount
—Nonvariab
 s use the 5-bit constan
 eld shown 
in the register-register format in Figure E.2.3.
 SYSCALL
 is special trap instruction is used to invoke the operating 
system.

E-26 Appendix E A Survey of RISC Architectures
 Move to/from control registers
—CTCi and 
CFCi move between the integer 
registers and control registers.
 Jump/call not PC-relative
 e 26-bit address of jumps and calls is not added 
to the PC. It
 ed 
  two bits and replaces the lower 28 bits of the PC. 
 is would only mak
 erence if the program were located near a 256 MB 
boundary.
 TLB instructions
—Translation-lookaside b
 er (TLB) misses were handled 
in so
 ware in MIPS I, so the instruction set also had instructions for 
manipulating the registers of the TLB (see Chapter 5 for more on TLBs). 
 ese registers are considered part of the “system coprocessor.” Since MIPS I 
Case 1        BeforeM[100]100101102103
DAV
M[104]R2R2AfterAfter104105106107
EEOJHN
NLWL R2, 101:DAV
R2LWR R2, 104:DAV
Case 2        BeforeM[200]200201202203
DM[204]R4R4AfterAfter204205206207
EVAEOJHN
NLWL R4, 203:DOH
R4LWR R4, 206:DAV
FIGURE E.7.1 MIPS instructions for unaligned word reads.
 is 
 gure assumes operation in 
big-endian mode. Case
 rst loads the three bytes 101, 102, and 103 into th
  of R2, leaving the least 
 cant byte undisturbed
 e following LWR simply loads byte 104 into the le
 cant byte of 
R2, leaving the other bytes of the register unchanged using LWL. Cas
 rst loads byte 203 into the most 
 cant byte of R4, and the following LWR loads the other three bytes of R4 from memory bytes 204, 
205, and 206. LWL reads the word with th
 rst byte from memory
 s to th
  to discard the unneeded 
byte(s), and changes only those bytes in Rd
 e byte(s) transferred are from th
 rst byte to the lowest-order 
byte of the word
 e following LWR addresses the last byte, righ
 s to discard the unneeded byte(s), and 
 nally changes only those bytes of Rd
 e byte(s) transferred are from the last byte up to the highest-order 
byte of the word. Store wor
  (SWL) is simply the inverse of LWL, and store word right (SWR) is the 
inverse of LWR. Changing to little-endian mo
 ips which bytes are selected and discarded. (If big-little, 
 -right, load-store seem confusing, don’t worry; they work!)

 E.8 Instructions Unique to Alpha E-
27the instruction
 er among versions of the architecture; they are more part 
of the implementations than part of the instruction set architecture.
 Reciprocal and reciprocal square root
 ese instructions, which do not 
follow IEEE 754 guidelines of proper rounding, are included apparently for 
applications that value speed of divide and square root more than they value 

accuracy.
Conditional procedure call instructions
—BGEZAL saves the return address and 
branches if the content of Rs1 is greater than or equal to zero, and 
BLTZAL does the same for less than zero
 e purpose of these instructions is to get a 
PC-relative call
 ere are “likely” versions of these instructions as well.)
 Parallel single precisio
 oating-point operations
—As well as extending 
the architecture with parallel integer operations in MDMX, MIPS-64 also 

supports two parallel 32-bit
 oating-point operations on 64-bit registers 
in a single instruction. “Paired single” operations include add (
ADD.PS), subtract (
SUB.PS), compare (
C.__.PS), convert (
CVT.PS.S, CVT.S.PL, CVT.S.PU), negate (
NEG.PS), absolute value (
ABS.PS), move (
MOV.PS, MOVF.PS, MOVT.PS), multiply (
MUL.PS), multiply-add (
MADD.PS), and 
multiply-subtract (
MSUB.PS). ere is no sp
 c provision in the MIPS architecture fo
 oating-point execution 
to proceed in parallel with integer execution, but the MIPS implementations of 

 oating point allow this to happen by checking to see if arithmetic interrupts are 
possible early in the cycle. Normally, exception detection would force serialization 

of execution of integer an
 oating-point operations.
 E.8 Instructions Unique to Alpha
 e Alpha was intended to be an architecture that made it easy to build high-
performance implementations. Toward that goal, the architects originally made 

two controversial decisions: imprecis
 oating-point exceptions and no byte or 
halfword data transfers.
To simplify pipelined execution, Alpha does not require that an exception 
should act as if no instructions past a certain point are executed and that all before 

that point have been executed. It supplies the 
TRAPB instruction, which stalls until 
all prior arithmetic instructions are guaranteed to complete without incurring 

arithmetic exceptions. In the most conservative mode, placing one TRAPB per 

exception-causing instruction slows execution by roughl
 ve times but provides 
precise exceptions (see Darcy and Gay [1996]).

E-28 Appendix E A Survey of RISC Architectures
Code that does not include 
TRAPB does not obey th
 oating-point 
standard
 e reason is that parts of the standard (NaN
 nities, and denormals) 
are implemented in so
 ware on Alpha, as they are on many other microprocessors. 
To implement these operations in so
 ware, however, programs mu
 nd the 
 ending instruction and operand values, which cannot be done with imprecise 
interrupts!
When the architecture was developed, it was believed by the architects that byte 
loads and stores would slow down data transfers. Byte loads require a
 er in the data transfer path, and byte stores require that the memory system perform 
a read-modify-write for memory systems with error correction codes, since the 

new ECC value must be recalculated
 is omission meant that byte stores required 
the sequence load word, replaced the desired byte, and then stored the word. 

(Inconsistently,
 oating-point loads go through considerable byte swapping to 
convert the obtuse V
 oating-point formats into a canonical form.)
To reduce the number of instructions to get the desired data, Alpha includes 
an elaborate set of byte manipulation instructions: extrac
 eld and zero rest of a 
register (
EXTxx), inser
 eld (INSxx), mask rest of a register (
MSKxx), zer
 elds of a register (
ZAP), and compare multiple bytes (
CMPGE).Apparently, the implementors were not as bothered by load and store byte as 
were the original architects. Beginning with the shrink of the second version of the 

Alpha chip (21164A), the architecture does include loads and stores for bytes and 

halfwords.
Remaining Instructions
Below is a list of the remaining unique instructions of the Alpha architecture:
 PAL code
—To provide the operations that the VAX performed in microcode, 
Alpha provides a mode that runs with all privileges enabled, interrupts 

disabled, and virtual memory mapping turned o
  for instructions. PAL 
(privileged architecture library) code is used for TLB management, atomic 

memory operations, and some operating system primitives. PAL code is 

called via the 
CALL_PAL instruction.
 No divide
—Integer divide is not supported in hardware.
  “
Unaligned
” load-store
—LDQ_U and 
STQ_U load and store 64-bit data using 
addresses that ignore the le
 cant three bits. Extract instructions 
then select the desired unaligned word using the lower address bi
 ese 
instructions are similar to 
LWL/R, SWL/R in MIPS. Floating-point single precision represented as double precision
—Single precision 
data is kept as conventional 32-bit formats in memory but is converted to 64-

bit double precision format in registers.
 Floating-point regist
 xed at zero
—To simplify comparisons to zero.

 E.9 Instructions Unique to SPARC v9 E-
29 VA
 oating-point formats
—To maintain compatibility with the VAX 
architecture, in addition to the IEEE 754 single and double precision formats 
called S and T, Alpha supports the VAX single and double precision formats 

called F and G, but not VAX format D. (D had too narrow an exponen
 eld to be useful for double precision and was replaced by G in VAX code.)
 Bit count instructions
—Version 3 of the architecture added instructions to 
count the number of leading zeros (
CTLZ), count the number of trailing zeros 
(CTTZ), and count the number of ones in a word (
CTPOP). Originally found 
on Cray computers, these instructions help with decryption.
 E.9 Instructions Unique to SPARC v9
Several features are unique to SPARC.
Register Windows e primary unique feature of SPARC is register windows, an optimization for 
reducing register tra
  c on procedure calls. Several banks of registers are used, with 
a new one allocated on each procedure call. Although this could limit the depth of 

procedure calls, the limitation is avoided by operating the banks as a circular bu
 er, 
providing unlimited dept
 e knee of the cost/performance curve seems to be six 
to eight banks.
SPARC can have between 2 and 32 windows, typically using 8 registers each 
for the globals, locals, incoming parameters, and outgoing parameters. (Given that 

each window has 16 unique registers, an implementation of SPARC can have as 

few as 40 physical registers and as many as 520, although most have 128 to 136, so 

far.) Rather than tie window changes with call and return instructions, SPARC has 

the separate instructions 
SAVE and 
RESTORE. SAVE is used to “save” the caller’s 
window by pointing to the next window of registers in addition to performing an 

add instructio
 e trick is that the source registers are from the caller’s window 
of the addition operation, while the destination register is in the callee’s window. 

SPARC compilers typically use this instruction for changing the stack pointer to 

allocate local variables in a new stack frame. 
RESTORE is the inverse of SAVE, 
bringing back the caller’s window while acting as an add instruction, with the 

source registers from the callee’s window and the destination register in the caller’s 

window
 is automatically deallocates the stack frame. Compilers can also make 
use of it for generating the callee’s
 nal return value.
 e danger of register windows is that the larger number of registers could slow 
down the clock rate
 is was not the case for early implementation
 e SPARC 
architecture (with register windows) and the MIPS R2000 architecture (without) 

E-30 Appendix E A Survey of RISC Architectures
have been built in several technologies since 1987. For several generations, the 
SPARC clock rate has not been slower than the MIPS clock rate for implementations 

in similar technologies, probably because cache access times dominate register 

access times in these implementation
 e current-generation machines took 
 erent implementation strategies—in order versus out of order—and it’s unlikely 
that the number of registers by themselves determined the clock rate in either 

machine. Recently, other architectures have included register windows: Tensilica 

and IA-64.
Another data transfer feature is alternate space option for loads and stores. 
 is simply allows the memory system to identify memory accesses to input/ 
output devices, or to control registers for devices such as the cache and memory 

management unit.
Fast Traps
Version 9 SPARC includes support to make traps fast. It expands the single level 

of traps to at least four levels, allowing the window over
 ow and under
 ow trap 
handlers to be interrupted.
 e extra levels mean the handler does not need to 
check for page faults or misaligned stack pointers explicitly in the code, thereby 

making the handler faster. Two new instruct
ions were added to return from this 
multilevel handler: 
RETRY (which retries the interrupted instruction) and DONE 
(which does not). To support user-level traps, the instruction RETURN will return 

from the trap in nonprivileged mode.
Support for LISP and Smalltalk
 e primary remaining arithmetic feature is tagged addition and subtraction. 
 e designers of SPARC spent some time thinking about languages like LISP and 
Smalltalk, and th
 uenced some of the features of SPARC already discussed: 
register windows, conditional trap instructions, calls with 32-bit instruction 

addresses, and multiword arithmetic (see Taylor, et al. [1986] and Ungar, et al. 

[1984]). A small amount of support is o
 ered for tagged data types with operations 
for addition, subtraction, and, hence, compariso
 e two le
 cant bits 
indicate whether the operand is an integer (coded as 00), so 
TADDcc and 
TSUBcc set the over
 ow bit if either operand is not tagged as an integer or if the result is 
too large. A subsequent conditional branch or trap instruction can decide what to 

do. (If the operands are not integers, so
 ware recovers the operands, checks the 
types of the operands, and invokes the correct operation based on those types.) It 

turns out that the misaligned memory access trap can also be put to use for tagged 

data, since loading from a pointer with the wrong tag can be an invalid access. 

Figure E.9.1 shows both types of tag support.

 E.9 Instructions Unique to SPARC v9 E-
31Overlapped Integer and Floating-Point Operations
SPARC allo
 oating-point instructions to overlap execution with integer 
instructions. To recover from an interrupt during such a situation, 
SPARC has a queue of pendin
 oating-point instructions and their addresses. 
RDPR allows the 
processor to empty the queue
 e seco
 oating-point feature is the inclusion of 
 oating-point square root instructions 
FSQRTS, FSQRTD, and 
FSQRTQ.Remaining Instructions
 e remaining unique features of SPARC are as follows:
 JMPL uses 
Rd to specify the return address register, so specifying 
r31 makes 
it similar to 
JALR in MIPS and specifying 
r0 makes it like 
JR. LDSTUB loads the value of the byte into 
Rd and then stores 
FF16 into 
the addressed byte
 is version 8 instruction can be used to implement 
synchronization (see Chapter 2).
 CASA (
CASXA
) atomically compares a value in a processor register to a 
32-bit (64-bit) value in memory; if and only if they are equal, it swaps the 
value in memory with the value in a second processor register
 is version 9 
a.  Add, sub, orcompare integers
(coded as 00)TADDcc r7, r5, r6000000(R5)(R7)(R6)b.  Loading viavalid pointer
(coded as 11)LD rD, r4, Œ3Œ11003(R4)(Wordaddress)FIGURE E.9.1 SPARC uses the two least signiﬁ
 cant bits to encode different data types for 
the tagged arithmetic instructions.
 a. Integer arithmetic takes a single cycle as long as the operands 
and the result are integers. b
 e misaligned trap can be used to catch invalid memory accesses, such as 
trying to use an integer as a pointer. For languages with paired data like LISP, an o
 set of –3 can be used to 
access the even word of a pair (CAR) and 
1 can be used for the odd word of a pair (CDR).

E-32 Appendix E A Survey of RISC Architectures
instruction can be used to construct wait-free synchronization algorithms 
that do not require the use of locks.
 XNOR calculates the exclusive OR with the complement of the second operand.
 BPcc, BPr, and 
FBPcc include a branch prediction bit so that the compiler 
can give hints to the machine about whether a branch is likely to be taken or not.
 ILLTRAP causes an illegal instruction trap. Muchnick [1988] explains how 
this is used for proper execution of aggregate returning procedures in C.
 POPC counts the number of bits set to one in an operand, also found in the 
third version of the Alpha architecture.
 Nonfaulting loads
 allow compilers to move load instructions ahead of 
conditional control structures that control their use. Hence, nonfaulting 

loads will be executed speculatively.
 Quadruple precision
 oating-point arithmetic and data transfer
 allow the 
 oating-point registers to act as eight 128-bit registers fo
 oating-point 
operations and data transfers.
 Multiple precision
 oating-point results for multiply
 mean that two single 
precision operands can result in a double precision product and two double 

precision operands can result in a quadruple precision produc
 ese 
instructions can be useful in complex arithmetic and some models o
 oating-
point calculations.
 E.10 Instructions Unique to PowerPC
PowerPC is the result of several generations of IBM commercial RISC machines— 

IBM RT/PC, IBM Power1, and IBM Power2—plus the Motorola 8800.
Branch Registers: Link and Counter
Rather than dedicate one of the 32 general-purpose registers to save the return 

address on procedure call, PowerPC puts the address into a special register called 

the 
link register
. Since many procedures will return without calling another 
procedure, the link doesn’t always have to be saved. Making the return address 

a special register makes the return jump faster, since the hardware need not go 

through the register read pipeline stage for return jumps.
In a similar vein, PowerPC has a 
count register
 to be used in 
for loops where the 
program iterat
 xed number of times. By using a special register, the branch 

 E.10 Instructions Unique to PowerPC E-
33hardware can determine quickly whether a branch based on the count register is 
likely to branch, since the value of the register is known early in the execution cycle. 

Tests of the value of the count register in a branch instruction will automatically 

decrement the count register.
Given that the count register and link register are already located with the 
hardware that controls branches, and that one of the problems in branch prediction 

is getting the target address early in the pipeline (see Appendix A), the PowerPC 

architects decided to make a second use of these registers. Either register can hold 

a target address of a conditional branc
 us, PowerPC supplements its basic 
conditional branch with two instructions that get the target address from these 

registers (
BCLR, BCCTR).Remaining Instructions
Unlike most other RISC machines, register 0 is not hardwired to the value 0. It 

cannot be used as a base register—that i
s, it generates a 0 in this case—but in base 
 index addressing it can be used as th
 e other unique features of the 
PowerPC are as follows:
 Load multiple and store multiple
 save or restore up to 32 registers in a single 
instruction.
 LSW and 
STSW permit fetching and storing of
 xed- and variable-length 
strings that have arbitrary alignment.
Rotate with mask
 instructions support bit
 eld extraction and insertion. One 
version rotates the data and then per forms logical 
AND with a mask of ones, 
thereby extractin
 e other version rotates the data but only places 
the bits into the destination register where there is a corresponding 1 bit in 

the mask, thereby insertin
 eld.Algebraic right shi
  sets the carry bit (
CA) if the operand is negative and any 
1 bits ar
 ed ou
 us, a signed divide by any constant power of two that 
rounds toward 0 can be accomplished with an 
SRAWI followed by 
ADDZE, which adds 
CA to the register.
CBTLZ will count leading zeros.
SUBFIC computes (immediate - RA), which can be used to develop a one’s or 
two’s complement.
Logical shi
 ed immediate
 instruction
  the 16-bit immediate to th
  16 
bits before performing AND, OR, or XOR.

E-34 Appendix E A Survey of RISC Architectures
 E.11 Instructions Unique to PA-RISC 2.0
PA-RISC was expanded slightly in 1990 with version 1.1 and change
 cantly 
in 2.0 with 64-bit extensions in 1996. PA-RISC perhaps has the most unusual 
features of any desktop RISC machine. For example, it has the most addressing 

modes and instruction formats, and, as we shall see, several instructions that are 

really the combination of two simpler instructions.
Nulliﬁ cationAs shown in Figure E.6.9, several RISC machines can choose not to execute the 

instruction following a delayed branch to improve utilization of the branch slot. 

 is is called 
nulli
 cation
 in PA-RISC, and it has been generalized to apply to any 
arithmetic/logical instruction as well as to all branch
 us, an 
add instruction 
can add two operands, store the sum, and cause the following instruction to be 

skipped if the sum is zero. Like co
nditional move instructions, n
 cation allows 
PA-RISC to avoid branches in cases where there is just one instruction in the 
then
 part of an 
if statement.
A Cornucopia of Conditional Branches
Given n
 cation, PA-RISC did not need to have separate conditional branch 
instruction
 e inventors could have recommended that nullifying instructions 
precede unconditional branches, thereby simplifying the instruction set. Instead, 

PA-RISC has the largest number of conditional branches of any RISC machine. 

Figure E.11.1 shows the conditional branches of PA-RISC. As you can see, several 

are really combinations of two instructions.
Synthesized Multiply and DividePA-RISC provides several primitives so that multiply and divide can be synthesized 

in so
 ware. Instructions tha
  one operand 1, 2, or 3 bits and then add, trapping 
or not on over
 ow, are useful in multiplies. (Alpha also includes instructions that 
multiply the second operand of adds and subtracts by 4 or by 8: 
S4ADD, S8ADD, S4SUB, and 
S8SUB e divide step performs the critical step of nonrestoring 
divide, adding or subtracting depending on the sign of the prior result. Magen-

heimer, et al. [1988] measured the size of operands in multiplies and divides to 

show how well the multiply step would work. Using this data for C programs, 

Muchnick [1988] found that by making special cases, the average multiply by a 

constant takes 6 clock cycles and the multiply of variables takes 24 clock cycles. 

PA- RISC has ten instructions for these operations.

 E.11 Instructions Unique to PA-RISC 2.0 E-
35 e original SPARC architecture used similar optimizations, but with increasing 
numbers of transistors the instruction set was expanded to include full multiply 
and divide operations. PA-RISC gives some support along these lines by putting 

a full 32-bit integer multiply in th
 oating-point unit; however, the integer data 
must
 rst be moved to
 oating-point registers.
Decimal OperationsCOBOL programs will compute on decimal values, stored as four bits per digit, 

rather than converting back and forth between binary and decimal. PA-RISC has 

instructions that will convert the sum from a normal 32-bit add into proper decimal 

digits. It also provides logical and arithmetic operations that set the condition codes 

to test for carries of digits, bytes, or halfword
 ese operations also test whether 
bytes or halfwords are zero
 ese operations would be useful in arithmetic on 8-bit 
ASCII characters. Five PA-RISC instructions provide decimal support.
Remaining Instructions
Here are some remaining PA-RISC instructions:
 Branch vectored
 s an index regist
  three bits, adds it to a base register, 
and then branches to the calculated address. It is used for 
case
 statements.
 Extract
 and 
deposit
 instructions allow arbitrary bit
 elds to be selected from 
or inserted into registers. Variations include whether the extracte
 eld is 
sign-extended, whether the bi
 eld is sp
 ed directly in the instruction or 
indirectly in another register, and whether the rest of the register is set to zero 

o
  unchanged. PA-RISC has 12 such instructions.
noitatoNnoitcurtsnIemaNCOMB Compare and branch }21tesffo+CP--<CP{))2sR,1sR(dnoc(fiCOMIB Compare immediate and branch }21tesffo+CP--<CP{))2sR,5mmi(dnoc(fiMOVB Move and branch 
Rs2 <-- Rs1, if (cond(Rs1,0)) {PC <-- PC + offset12} 
MOVIB Move immediate 

and branch Rs2 <-- imm5, if (cond(imm5,0)) {PC <-- PC + offset12} 
ADDB Add and branch Rs2 <-- Rs1 + Rs2, if (cond(Rs1 + Rs2,0)) {PC <-- PC + offset12} 
ADDIB Add immediate 
and branch Rs2 <-- imm5 + Rs2, if (cond(imm5 + Rs2,0)) {PC <-- PC + offset12} 
BB Branch on bit }21tesffo+CP--<CP{))0,psR(dnoc(fiBVB Branch on variable bit }21tesffo+CP--<CP{))0,rassR(dnoc(fiFIGURE E.11.1 The PA-RISC conditional branch instructions.
 e 12-bit o
 set is called 
offset12 in this table, and the 5-bit 
immediate is called 
imm5 e 16 conditions are 
, ,  , odd, signed over
 ow, unsigned no over
 ow, zero or no over
 ow unsigned, 
never, and their respective complemen
 e BB instruction selects one of the 32 bits of the register and branches depending on whether its 
value is 0 o
 e BVB selects the bit to branch using th
  amount register, a special-purpose register
 e subscript notation sp
 es a 
bit
 eld.
E-36 Appendix E A Survey of RISC Architectures
 To simplify use of 32-bit address constants, PA-RISC includes 
ADDIL, which 
 -adjusted 21-bit constant to a register and places the result in 
regist
 e following data transfer instruction uses o
 set addressing to 
add the lower 11 bits of the address to regist
 is pair of instructions 
allows PA-RISC to add a 32-bit constant to a base register, at the cost of 
changing register 1.
 PA-RISC has nine debug instructions that can set breakpoints on instruction 
or data addresses and return the trapped addresses.
 Load
 and 
clear
 instructions provide a semaphore or lock that reads a value 
from memory and then writes zero.
 Store bytes short
 optimizes unaligned data moves, moving either th
 most or the rightmost bytes in a word to th
 ective address, depending on the 
instruction options and condition code bits.
 Loads and stores work well with caches by having options that give hints 
about whether to load data into the cache if it’s not already in the cache. For 

example, a load with a destination of regist
 ned to be a so
 ware-
controlled cache prefetch.
 PA-RISC 2.0 extended cache hints to stores to indicate block copies, 
recommending that the processor not load data into the cache if it’s not 

already in the cache. It also can suggest that on loads and stores, there is 

spatial locality to prepare the cache for subsequent sequential accesses.
 PA-RISC 2.0 also provides an optional branch target stack to predict indirect 
jumps used on subroutine returns. So
 ware can suggest which addresses get 
placed on and removed from the branch target stack, but hardware controls 

whether or not these are valid.
 Multiply/add
 and 
multiply/subtract
 ar
 oating-point operations that can 
launch two independen
 oating-point operations in a single instruction in 
addition to the fused multiply/add and fused multiply/negate/add introduced 

in version 2.0 of PA-RISC.
 E.12 Instructions Unique to ARM
It’s hard to pick the most unusual feature of ARM, but perhaps it is the conditional 

execution of instructions. Every instruction starts with a 4-bi
 eld that determines 
whether it will act as a nop or as a real instruction, depending on the condition 

codes. Hence, conditional branches are properly considered as conditionally 

executing the unconditional branch instruction. Conditional execution allows 

 E.12 Instructions Unique to ARM E-
37avoiding a branch to jump over a single instruction. It takes less code space and 
time to simply conditionally execute one instruction.
 e 12-bit immediate
 eld has a novel interpretatio
 e eight le
 cant 
bits are zero-extended to a 32-bit value, then rotated right the number of bits sp
 ed in th
 rst four bits of th
 eld multiplied by two. Whether this split actually catches 
more immediates than a simple 12-bi
 eld would be an interesting study. One 
advantage is that this scheme can represent all powers of two in a 32-bit word.
Opera
 ing is not limited to immediat
 e second register of all 
arithmetic and logical processing operations has the option of bein
 ed before 
being operated o
 e 
  options ar
  
  logical
  right logical
  right arithmetic, and rotate right. Once again, it would be interesting to see how 

 en operations like rotate-and-add
 -right-and-test, and so on occur in ARM 
programs.
Remaining Instructions
Below is a list of the remaining unique instructions of the ARM architecture:
 Block loads and stores
—Under control of a 16-bit mask within the 
instructions, any of the 16 registers can be loaded or stored into memory 

in a single instructio
 ese instructions can save and restore registers on 
procedure entry and retur
 ese instructions can also be used for block 
memory copy—o
 ering up to four times the bandwidth of a single register 
load-store—and today, block copies are the most important use.
 Reverse subtract
—RSB allows th
 rst register to be subtracted from the 
immediate o
 ed register. RSC does the same thing, but includes the 
carry when calculating th
 erence.
 Long multiplies
—Similarly to MIPS, Hi and Lo registers get the 64-bit signed 
product (SMULL) or the 64-bit unsigned prod uct (UMULL).
 No divide
—Like the Alpha, integer divide is not supported in hardware.
 Conditional trap
—A common extension to the MIPS core found in desktop 
RISCs (Figures E.6.1 through E.6.4), it comes for free in the conditional 

execution of all ARM instructions, including SWI.
 Coprocessor interface
—Like many of the deskto
 nes a 
full set of coprocessor instructions: data transfer, moves between general- 

purpose and coprocessor registers, and coprocessor operations.
 Floating-point architecture
—Using the coprocessor interface
 oating-point 
architecture has be
 ned for ARM. It was implemented as the FPA10 
coprocessor.
 Branch and exchange instruction sets
 e BX instruction is the transition 
between ARM an
 umb, using the lower 31 bits of the register to set the PC 
and th
 cant bit to determine if the mode is ARM (1) o
 umb (0).

E-38 Appendix E A Survey of RISC Architectures
 E.13 Instructions Unique to Thumb
In the ARM version 4 model, frequently executed procedures will use ARM 
instructions to get maximum performance, with the less frequently executed ones 

usin
 umb to reduce the overall code size of the program. Since typically only a 
few procedures dominate execution time, the hope is that this hybrid gets the best 

of both worlds.
Althou
 umb instructions are translated by the hardware into conventional 
ARM instructions for execution, there are several restrictions. First, conditional 

execution is dropped from almost all instructions. Second, only th
 rst eight 
registers are easily available in all instructions, with the stack pointer, link register, 

and program counter used implicitly in some instruction
 ird, 
 umb uses a two-
operand format to save space. Fourth, the uniq
 ed immediates an
 ed second operands have disappeared and are replaced by separat
  instructions. 
 h, the addressing modes are simp
 ed. Finally, putting all instructions into 16 
bits forces many more instruction formats.
In many ways, the simp
 ed 
 umb architecture is more conventional than 
ARM. Here are additional changes made from ARM in going t
 umb:
Drop of immediate logical instructions
—Logical immediates are gone.
Condition codes implicit
—Rather than have condition codes set optionally, 
they ar
 ned by the opcode. All ALU instructions and none of the data 
transfers set the condition codes.
 Hi/Lo register access
 e 16 ARM registers are halved into Lo registers 
and Hi registers, with the eight Hi registers including the stack pointer (SP), 

link register, an
 e Lo registers are available in all ALU operations. 
Variations of ADD, BX, CMP, and MOV also work with all combinations 

of Lo and Hi registers. SP and PC registers are also available in variations of 

data transfers and add immediates. Any other operations on the Hi registers 

require one MOV to put the value into a Lo register, perform the operation 

there, and then transfer the data back to the Hi register.
Branch/call distance
—Since instructions are 16 bits wide, the 8-bit conditional 
branch addr
 ed by 1 instead of by 2. Branch with link is sp
 ed in two instructions, concatenating 11 bits from each instruction an
 ing 
th
  to form a 23-bit address to load into PC.
Distance for data transfer
 sets
 e o
 set is no
 ve bits for the general-
purpose registers and eight bits for SP and PC.

 E.14 Instructions Unique to SuperH E-
39 E.14 Instructions Unique to SuperH
Register 0 plays a special role in SuperH address modes. It can be added to 
another register to form an address in 
indirect indexed addressing and PC-relative 
addressing. R0 is used to load constants to give a larger addressing range than can 

easily be
 t into the 16-bit instructions of the SuperH. R0 is also the only register 
that can be an operand for immediate versions of AND, CMP, OR, and XOR. Below 

is a list of the remaining unique details of the SuperH architecture:
 Decrement and test
—DT decrements a register and sets the T bit to 1 if the 
result is 0.
 Optional delayed branch
—Although the other embedded RISC machines 
generally do not use delayed branches (see Appendix B), SuperH o
 ers 
optional delayed branch execution for 
BT and 
BF. Many multiplies
—Depending on whether the operation is signed or unsigned, 
if the operands are 16 bits or 32 bits, or if the product is 32 bits or 64 bits, the 

proper multiply instruction is 
MULS, MULU, DMULS, DMULU, or 
MUL e product is found in the MACL and MACH registers.
 Zero and sign extension
—Byte or halfwords are either zero-extended (
EXTU) or sign-extended (
EXTS) within a 32-bit register.
 One-bit shi
  amounts
—Perhaps in an attempt to make th
 t within the 
16-bit instruction
  instructions onl
  a single bit at a time.
 Dynamic shi
  amount
 ese variab
 s test the sign of the amount in a 
register to determine whether they
  
  (positive) o
  right (negative). 
Both logical (
SHLD) and arithmetic (
SHAD) instructions are supported.
 ese 
instructions help o
 set the 1-bit constan
  amounts of standard
 s. Rotate
—SuperH o
 ers rotations by 1 bi
  (
ROTL) and right (
ROTR), which 
set the T bit with the value rotated, and also have variations that include the 

T bit in the rotations (
ROTCL and 
ROTCR). SWAP
 is instruction swaps either the high and low bytes of a 32-bit word 
or the two bytes of the rightmost 16 bits.
 Extract word
 (XTRCT e middle 32 bits from a pair of 32-bit registers are 
placed in another register.
 Negate with carry
—Like 
SUBC (Figure E.6.6), except th
 rst operand is 0.
 Cache prefetch
—Like many of the desktop RISCs (Figures E.6.1 through 
E.6.4), SuperH has an instruction (
PREF) to prefetch data into the cache.

E-40 Appendix E A Survey of RISC Architectures
 Test-and-set
—SuperH uses the older test-and-set (
TAS) instruction to 
perform atomic locks or semaphores (see Chapter 2). 
TAS rst loads a byte 
from memory. It then sets the T bit to 1 if the byte is 0 or to 0 if the byte is not 
0. Finally, it sets th
 cant bit of the byte to 1 and writes the result 
back to memory.
 E.15 Instructions Unique to M32R
 e most unusual feature of the M32R is a slight VLIW approach to the pairs of 
16-bit instructions. A bit is reserved in th
 rst instruction of the pair to say whether 
this instruction can be executed in parallel with the next instruction— that is, the 

two instructions are independent—or if these two must be executed sequentially. 

(An earlier machine that o
 ered a similar option was the Inte
 is feature is 
included for future implementations of the architecture.
One surprise is that all branch displacements ar
 ed 
  2 bits before being 
added to the PC, and the lower 2 bits of the PC are set to 0. Since some instructions 

are only 16 bits long, t
  means that a branch cannot go to any instruction 
in the program: it can only branch to instructions on word boundaries. A similar 

restriction is placed on the return address for the branch-and-link and jump-and-

link instructions: they can only return to a word boundary
 us, for a slightly 
larger branch distance, so
 ware must ensure that all branch addresses and all 
return addresses are aligned to a word boundary
 e M32R code space is probably 
slightly larger, and it probably executes more 
nop instructions than it would if the 
branch address was only
 ed 
  1 bit.
However, the VLIW feature above means that a 
nop can execute in parallel with 
another 16-bit instruction so that the padding doesn’t take more clock cyc
 e code size expansion depends on the ability of the compiler to schedule code and to 

pair successive 16-bit instructions; Mitsubishi claims that code size overall is only 

7% larger than that for the Motorola 6800 architecture.
 e last remaining novel feature is that the result of the divide operation is the 
remainder instead of the quotient.
 E.16 Instructions Unique to MIPS-16
MIPS-16 is not really a separate instruction set but a 16-bit extension of the full 

32-bit MIPS architecture. It is compatible with any of the 32-bit address MIPS 

architectures (MIPS I, MIPS II) or 64-bit architectures (MIPS III, IV
 e ISA 
mode bit determines the width of instructions: 0 means 32-bit-wide instructions 

 E.16 Instructions Unique to MIPS-16 E-
41and 1 means 16-bit-wide instruction
 e new 
JALX instruction toggles the ISA 
mode bit to switch to the other ISA. 
JR and 
JALR have been re
 ned to set the ISA 
mode bit from th
 cant bit of the register containing the branch address, 
and this bit is not considered part of the address. All jump-and-link instructions 
save the current mode bit as th
 cant bit of the return address.
Hence, MIPS supports whole procedures containing either 16-bit or 32-bit 
instructions, but it does not support mixing the two lengths together in a single 

procedure.
 e one exception is the 
JAL and 
JALX: these two instructions need 
32 bits even in the 16-bit mode, presumably to get a large enough address to branch 

to far procedures.
In picking this subset, MIPS decided to include opcodes for some three-operand 
instructions and to keep 16 opcodes for 64-bit operation
 e combination of this 
many opcodes and operands in 16 bits led the architects to provide only eight easy-

to-use registers—just like
 umb—whereas the other embedded RISCs o
 er about 
16 registers. Since the hardware must include the full 32 registers of the 32-bit 

ISA mode, MIPS-16 includes move instructions to copy values between the eight 

MIPS-16 registers and the remaining 24 registers of the full MIPS architecture. 

To reduce pressure on the eight visible registers, the stack pointer is considered 

a separate register. MIPS-16 includes a variety of separate opcodes to do data 

transfers using SP as a base register and to increment SP: 
LWSP, LDSP, SWSP, SDSP, ADJSP, DADJSP, ADDIUSPD, and 
DADDIUSP.To
 t within the 16-bit limit, immediate
 elds have generally been shortened to 
 ve to eight bits. MIPS-16 provides a way to extend its shorter immediates into the 
full width of immediates in the 32-bit mode. Borrowing a trick from the Intel 8086, 

the 
EXTEND instruction is really a 16-bit pr
 x that can be prepended to any MIPS-
16 instruction with an address or immediat
 eld. 
 e pr
 x supplies enough bits 
to turn the 5-bi
 eld of data transfers and 5- to 8-bi
 elds of arithmetic immediates 
into 16-bit constants. Alas, there are two exceptions. 
ADDIU and 
DADDIU start with 
4-bit immediate
 elds, but since 
EXTEND can only supply 11 more bits, the wider 
immediate is limited to 15 bits. 
EXTEND also extends the 3-bi
  
 elds into 5-bit 
 elds fo
 s. (In case you were wondering, the 
EXTEND pre
 x does 
not need to 
start on a 32-bit boundary.)
To further address the supply of constants, MIPS-16 added a new addressing 
mode! PC-relative addressing for load word (
LWPC) and load double (
LDPC s an 8-bit immediate
 eld by two or three bits, respectively, adding it to the PC with 
the lower two or three bits cleared
 e constant word or doubleword is then loaded 
into a register
 us 32-bit or 64-bit constants can be included with MIPS-16 code, 
despite the loss of 
LIU to set the upper register bits. Given the new addressing 
mode, there is also an instruction (
ADDIUPC) to calculate a PC-relative address and 
place it in a register.
 ers from the other embedded RISCs in that it can subset a 64-bit 
address architecture. As a result it has 16-bit instruction-length versions of 64-bit 

E-42 Appendix E A Survey of RISC Architectures
data operations: data transfer (
LD, SD, LWU), arithmetic operations (
DADDU/IU, DSUBU, DMULT/U, DDIV/U), an
 s (
DSLL/V, DSRA/V, DSRL/V).Since MIPS plays such a prominent role in this book, we show all the additional 
changes made from the MIPS core instructions in going to MIPS-16:
 Drop of signed arithmetic instructions
—Arithmetic instructions that can trap were 
dropped to save opcode space: 
ADD, ADDI, SUB, DADD, DADDI, DSUB. Drop of immediate logical instructions
—Logical immediates are gone too: 
ANDI, ORI, XORI. Branch instructions pared down
—Comparing two registers and then branching 
did no
 t, nor did all the other comparisons of a register to zero. Hence these 
instructions didn’t make it either: 
BEQ, BNE, BGEZ, BGTZ, BLEZ, and 
BLTZ. As mentioned in Section E.3, to help compensate MIPS-16 includes 
compare instructions to test if two registers are equal. Since compare and set 
on less than set the new T register, branches were added to test the T register.
 Branch distance
—Since instructions are 16 bits wide, the branch address is 
 ed by one instead of by two.
 Delayed branches disappear
 e branches tak
 ect before the next 
instruction. Jumps still have a one-slot delay.
 Extension and distance for data transfer o
 sets
 e 5-bit and 8-bi
 elds are zero-extended instead of sign-extended in 32-bit mode. To get greater 

range, the immediate
 elds ar
 ed 
  one, two, or three bits depending 
on whether the data is halfword, word, or doubleword. If the 
EXTEND pre
 x is prepended to these instructions, they use the conventional signed 16-bit 

immediate of the 32-bit mode.
 Extension of arithmetic immediates
 e 5-bit and 8-bi
 elds are zero-
extended for set on less than and compare instructions, for forming a PC-

relative address, and for adding to SP and placing the result in a register 

(ADDIUSP, DADDIUSP). Once again, if the 
EXTEND pre
 x is prepended to 
these instructions, they use the conventional signed 16-bit immediate of the 

32-bit mode
 ey are still sign-extended for general adds and for adding to 
SP and placing the result back in SP (
ADJSP, DADJSP). Alas, code density 
and orthogonality are strange bedfellows in MIPS-16!
  ning 
  amount of 0
 nes the value 0 in the 3-bi
   eld to mean
  of 8 bits.
 New instructions added due to loss of register 0 as zero
—Load immediate, 
negate, and not were added, since these operations could no longer be 

synthesized from other instructions using r0 as a source.

 E.17 Concluding Remarks E-
43 E.17 Concluding Remarks
 is appendix covers the addressing modes, instruction formats, and all instructions 
found in ten RISC architectures. Although the later sections of the appendix 
concentrate on th
 erences, it would not be possible to cover ten architectures in 
these few pages if there were not so many similarities. In fact, we would guess that 

more than 90% of the instructions executed for any of these architectures would 

be found in Figures E.3.5 through E.3.11. To contrast this homogeneity, Figure 

E.17.1 gives a summary for four architectures from the 1970s in a format similar 

to that shown in Figure E.1.1. (Imagine trying to write a single chapter in this style 

for those architectures!) In the history of computing, there has never been such 

widespread agreement on computer architecture.
IBM 360/370 Intel 8086 Motorola 68000 DEC VAX 
Date announced 1964/1970 1978 1980 1977 
Instruction size(s) (bits) 16, 32, 48 8, 16, 24, 32, 40, 48 16, 32, 48, 64, 80 8, 16, 24, 32,
..., 
432 Addressing (size, model) 24 bits, ß
 at/31 bits, 
ß at 
4 + 16 bits, 
segmented24 bits, ß
 at 32 bits, ß
 at Data aligned? Yes 360/No 370 No 16-bit aligned No 
41=953/2sedomgnisserddaataDegaPlanoitpOenoNegaPnoitcetorPBK5.0BK23ot52.0ÑBK4&BK2ezisegaPdeppamyromeMdeppamyromeMedocpOedocpOO/IInteger registers (size, 
model, number)
16 GPR × 32 bits 8 dedicated data 
×16 bits 8 data and 8 address ×32 bits 15 GPR × 32 bits Separate ß oating-point registers 
4 × 64 bits Optional: 8 
× 80 bits Optional: 8 
× 80 bits 0 Floating-point format IBM (ß oating 
hexadecimal) IEEE 754 single, 

double, extended 
IEEE 754 single, 

double, extended 
DEC FIGURE E.17.1 Summary of four 1970s architectures.
 Unlike the architectures in Figure E.1.1, there is little agreement between 
these architectures in any category.
 is style of architecture cannot remain static, however. Like people, instruction 
sets tend to get bigger as they get older. Figure E.17.2 shows the genealogy of these 
instruction sets, and Figure E.17.3 shows which features were added to or deleted 

from generations of desktop RISCs over time.
As you can see, all the desktop RISC machines have evolved to 64-bit address 
architectures, and they have done so fairly painlessly.

E-44 Appendix E A Survey of RISC Architectures
1960CDC 66001963Cray-11976M32R1997Thumb1995ARMv41995ARM31990ARM21987ARM11985SPARCv8
1987SPARCv9
1994MIPS-161996MIPS I1986MIPS II1989MIPS III1992Alpha1992PA-RISC
1986PA-RISC 1.1
1990PA-RISC 2.0
1996RT/PC1986Power11990PowerPC1993Power21993Alphav31996MIPS IV1994MIPS V1996MIPS-642002MIPS-322002Berkeley RISC-11981Stanford MIPS1982Digital PRISM1988IBM ASC 1968IBM 8011975America1985SuperH199219651970
1975
198019851990199520002002FIGURE E.17.2 The lineage of RISC instruction sets.
 Commercial machines are shown in plain text and research machines in 
bold
 e CDC 6600 and Cray-1 were load-store machines with regist
 xed at 0, and with separate integer an
 oating-point registers. 
Instructions could not cross word boundaries. An early IBM research machine led to the 801 and America research projects, with 
the 801 
leading to the unsuccessful RT/PC and America 
leading to the successful Power architecture. Some people who worked on the 801 l
ater 
joined Hewlett-Packard to work on the PA
 e two university projects were the basis of MIPS and SPARC machines. According to 
Furber [1996], the Berkeley RISC project was the inspiration of the ARM architecture. While ARM1, ARM2, and ARM3 were names of 
both 
architectures and chips, ARM version 4 is the name of the architecture used in ARM7, ARM8, and StrongARM chi
 ere are no ARMv4 and 
ARM5 chips, but ARM6 and early ARM7 chips use the ARM3 architecture.) DEC built a RISC microprocessor in 1988 but did not intro
duce it. 
Instead, DEC shipped workstations using MIPS microprocessors for three years before they brought out their own RISC instruction
 set, Alpha 
21064, which is very similar to MIPS III and PRIS
 e Alpha architecture has had small extensions, but they have not been formalized with 
version numbers; we used version 3 because that is the version of the reference manual
 e Alpha 21164A chip added byte and halfword loads 
and stores, and the Alpha 21264 includes the MAX multimedia and bit count instructions. Internally, Digital names chips a
 er the fabrication 
technology: EV4 (21064), EV45 (21064A), EV5 (21164), EV56 (21164A), and EV6 (21264). “EV” stands for “extended VAX.”

 Further Reading E-
45We would like to thank the following people for comments on dra
 s of this 
appendix: Professor Steven B. Furber, University of Manchester; Dr. Dileep 
Bhandarkar, Intel Corporation; Dr. Earl Killian, Silicon Graphics/MIPS; and Dr. 

Hiokazu Takata, Mitsubishi Electric Corporation.
Further Reading
Bhandarkar, D. P. [1995]. 
Alpha Architecture and Implementations,
 Newton, MA: Digital Press.
Darcy, J. D., and D. Gay [1996]. “FLECKmarks: Measurin
 oating point performance using a full IEEE 
compliant arithmetic benchmark,” CS 252 class project, U.C. Berkeley (see 
HTTP.CS.Berkeley.EDU/
darcy/ 
Projects/cs252/
).Digital Semiconductor [1996]. 
Alpha Architecture Handbook,
 Version 3, Maynard, MA: Digital Press, Order 
number EC-QD2KB-TE (October).
rewoPSPIMCRAPSCSIR-APFeature 1.0 1.1 2.0 v8   v9 I II III IV V 1 2 PC 
Interlocked loads X Ó Ó X Ó + Ó Ó X Ó Ó 
Load-store FP double X Ó Ó X Ó + Ó Ó X Ó Ó 

Semaphore X Ó Ó X Ó + Ó Ó X Ó Ó 

Square root X Ó Ó X Ó + Ó Ó + Ó 
+ÓÓÓXÓXÓÓXspoPFnoisicerpelgniS
Memory synchronize X Ó Ó X Ó + Ó Ó X Ó Ó 

Coprocessor X Ó Ó X Ñ X Ó Ó Ó 
ÓÓX+ÓXÓÓXgnisserddaxedni+esaBÓÓXÓ++ÓÓsretsigerPFtib-4623.viuqEAnnulling delayed branch X Ó Ó X Ó + Ó Ó 
Branch register contents X Ó Ó + X Ó Ó Ó 
+ÓÓÓX+Ó+naidneelttil/giBÓÓXÓÓ++tibnoitciderphcnarBÑÓX++evomlanoitidnoCÓÓX+++ehcacotniatadhcteferP+Ó+++spo.tni/gnisserddatib-4632-bit multiply, divide + Ó + X Ó Ó Ó X Ó Ó 
Ñ++dauqPFerots-daoLÓÓX++dda/lumPFdesuFÑÓXÓÓXsnoitcurtsnignirtSXXÓXtroppusaidemitluMFIGURE E.17.3 Features added to desktop RISC machines.
 X means in the original machine, 
 means added later, ” means 
continued from prior machine, and — means removed from architec
ture. Alpha is not included, but it added byte and word loads an
d stores, 
and bit count and multimedia extensions, in version 3. MIPS V added the MDMX instructions and paired sing
 oating-point operations.

E-46 Appendix E A Survey of RISC Architectures
Furber, S. B. [1996]. 
ARM System Architecture,
 Harlow, England: Addison-Wesley. (See 
www.cs.man.ac.uk/ 
amulet/publications/books/ARMsysArch.)
Hewlett-Packard [1994]. 
PA-RISC 2.0 Architecture Reference Manual,
 3rd ed.
Hitachi [1997]. 
SuperH RISC Engine SH7700 Series Programming Manual. (
See 
www.halsp.hitachi.com/tech_ 
prod/
 and search for title.
)IBM [1994].  e PowerPC Architecture,
 San Francisco: Morgan Kaufmann.
Kane, G. [1996]. 
PA-RISC 2.0 Architecture,
 Upper Saddle River, NJ: Prentice Hall PTR.
Kane, G., and J. Heinrich [1992]. 
MIPS RISC Architecture,
 Englewood
 s, NJ: Prentice Hall.
Kissell, K. D. [1997]. 
MIPS16: High-Density for the Embedded Market.
 (See 
www.sgi.com/MIPS/arch/MIPS16/ 
MIPS16.whitepaper.pdf.)

Magenheimer, D. J., L. Peters, K. W. Pettis, and D. Zuras [1988]. “Integer multiplication and division on the 
HP precision architecture,” 
IEEE Trans. on Computers
 37:8, 980–90.MIPS [1997]. MIPS16 Application Speci
 c Extension Product Description.
 (See 
www.sgi.com/MIPS/arch/ 
MIPS16/mips16.pdf.)
Mitsubishi [1996]. 
Mitsubishi 32-Bit 
Single Chip Microcomputer 
M32R Family So
  ware Manual
 (September).
Muchnick, S. S. [1988]. “Optimizing compilers for SPARC,” 
Sun Technology
 1:3 (Summer), 64–77.
Seal, D. 
Arm Architecture Reference Manual
, 2nd ed, Morgan Kaufmann, 2000.
Silicon Graphics [1996]. 
MIPS V Instruction Set.
 (See 
www.sgi.com/MIPS/arch /ISA5/#MIPSV_indx.
)Sites, R. L., and R. Witek (eds.) [1995]. 
Alpha Architecture Reference Manual,
 2nd ed. Newton, MA: Digital 
Press.

Sloss, A. N., D. Symes, and C. Wright, 
ARM System Developer’s Guide,
 San Francisco: Elsevier Morgan 
Kaufmann, 2004.

Sun Microsystems [1989]. 
 e SPARC Architectural Manual,
 Version 8, Part No. 800-1399-09, August 25.
Sweetman, D. 
See MIPS Run
, 2nd ed, Morgan Kaufmann, 2006.
Taylor, G., P. Hi
 nger, J. Larus, D. Patterson, and B. Zorn [1986]. “Evaluation of the SPUR LISP architecture,” 
Proc. 13th Symposium on Computer Architecture
 (June), Tokyo.
Ungar, D., R. Blau, P. Foley, D. Samples, and D. Patterson [1984]. “Architecture of SOAR: Smalltalk on a 
RISC,” 
Proc. 11th Symposium on Computer Architecture
 (June), Ann Arbor, MI, 188–97.
Weaver, D. L., and T. Germond [1994]. 
 e SPARC Architectural Manual,
 Version 9, Englewoo
 s, NJ: Prentice Hall.
Weiss, S., and J. E. Smith [1994]. 
Power and PowerPC,
 San Francisco: Morgan Kaufmann.



Chapter 1 This book presents many 
examples of computer systems. It presents them in enough detail so that meaningful engineering 
study and analysis are possible. Most of these examples are presented by using the original descriptions 
of them in the technical literature. 
Others have been 
redescribed by us, especially where the original descriptions existed only in technical 
manuals. In both cases there are considerable discussion and analysis of the computer struc- tures: what problems they were intended 
to solve, what solutions were adopted, 
and how these solutions 
have fared. Yet the em- phasis has remained 
on detailed 
descriptions precise enough 
so that the systems themselves are available for independent study. Why should one want to produce such 
a book? 
Collections of reprintings from the technical literature 
are common in many science and engineering fields, e.g., 
ﬁProgramming Systems and Languagesﬂ [Rosen, 19671. We have departed from this tradi- 
tional exercise in two ways, both of which seem important to us. First, we have presented substantial amounts 
of detail: in effect, 
block diagrams 
of computer structures and the 
equivalents of programming manuals. 
These constitute neither 
good reading nor 
a way of communicating the ﬁessential ideasﬂ 
in the field. Second, ? we have introduced 
a system 
of notation and have used 
it not only in the parts we ourselves have written but also to provide addi- 
tional (sometimes 
redundant) descriptions of computer systems in the reprinted articles. Why should there be a book like this? The reasons are several and require some background 
discussion. opment of this science 
and technology of computers (one of us also likes to build computers). To understand why this 
particular book seems 
to us to be the right way 
to push this development at this particular time 
requires characterizing the current state of computer-systems technology. 
A computer system is complex in several ways. Figure 1 shows the most important. There are 
at least four 
levels of system descrip- tion, possibly five, that can be used for a computer. These are not alternative descriptions in the sense that anything said one way can be said another. On the contrary, each level arises from 
ab- straction of the levels below it. Each does a job 
that the lower levels could not perform became of the unnecessary detail they 
would be forced to carry around. 
A system (at any 
level) is characterized by a set of components, of which certain properties are 
posited, and a set of ways of com- bining components 
to produce systems. When formalized appro- priately, the behavior of the systems is determined by the behavior of its components and the 
specific modes of combination used. I- -, i I ‚L ‚1 I, .f.LY - ,A, i ™ . Computer systems /II r, Computer systems are one example of man™s more complex 
arti- ficial systems.l They have existed as successful engineering prod- 
ucts long enough 
to undergo 
radical evolution 
and to give rise to a number of basic, unique technologies. They are 
sufficiently complex that they have 
given rise 
to a science, that is, to a con- tinuing, institutionalized endeavor 
to understand what 
sort of beast has been brought 
forth.2 Our fundamental interest 
is in the devel- & IWe need not argue that they are his most complex system. That view is myopic. Setting aside quasi-natural systems, such 
as cities and 
economies, it is still the 
case that a modern aircraft carrier is more complex than a modern computer by any reasonable measure. 2Here uniqueness can be claimed, perhaps, 
since few other artifactual systems (again, excluding the quasi-natural ones) provide new phenomena that require sustained 
scientific investigation to understand them. There Structures. Network/#, computer/C B Components. Processors/P. memories/M, 
switches/S. controls/K, transducers / T; data operators /D, links /L , Circuits: Arithmetic unit 
, Components: Registers, 
transfers, controls, data operators (+, -, etc.) 1: ,T.,: Circuits: Counters, controls, sequential transducer, function generator, 
. register arrays 
I Components: Flip- flops 
-, reset-set / US, JKs delay/ D, toggle/ 7; Iotch, deloy, one 
shot . - /, Circuits: Encoders, decoders, transfer arrays, dot0 ops, selectors, distribu ors, iterative networks Compoynts 8ND. OR. NOT, NAND, 
NOR tf State system level lh Componen states. in outputs Circuits: Amplifiers, 
delays, ottenuators, multivibrators, clocks, gates, differentiator transistors Passive components: Resistor/ 
U, capacitor/ C, inducter/L, diode, deloy lines 
1 Active components: Relays, vacuum tubes, 
1 B ,, certainly is no science of aircraft carriers. 
But there is a computer science. Fig. 1. Hierarchy of levels: computer structure. 
4 Part 1 1 The structure of computers Elementary circuit theory is an almost prototypic example. The components are R™s, L™s, C™s, and voltage sources. 
The mode of combination is to run wires between the terminals of components, which corresponds to an identification of current and voltage at these terminals. The algebraic and differential equations of circuit theory provide the means whereby the behavior of a circuit can be computed from the properties of its components and the way the circuit is constructed. There is a recursive feature to most system 
descriptions. A system, composed 
of components structured in a given way, 
may be considered a component 
in the construction of yet other sys- tems. There are, 
of course, some 
primitive components 
whose properties are not explicable as 
the resultant of a system of the same type. For example, a resistor is not to be 
explained by a subcircuit but is taken as a primitive. Sometimes there are 
no absolute primitives, it 
being a 
matter of convention what basis is taken. For example, one can build 
logical design systems from 
many different primitive sets of logical operations (AND and 
NOT, NAND, OR and NOT, 
etc.). A system level, as we have 
used the term 
in Fig. 1, is charac- terized by a 
distinct language for representing the system (that is, the components, modes of combination, and laws of behavior). These distinct languages reflect 
special properties 
of the types of components and 
of the way they combine. Otherwise, 
there would be no point in adopting a special representation. 
Nevertheless, these levels exist in the system analyst™s 
way of describing the same Structure Behavior - 3.0VOltS ﬂ a c -3 e w 270 uuf t™= 0 ic + /; - i. = 0 ic = a ie where 1 >> 1 At f™ = 0: e, = 0 and O= +15- i R-c: 6‚; di™ ec=eo=Oat t™=O AI t™= ot e, = o for 3-volt step, input where e, h- 3.0 volts) .u 5 in 0 1-a (e,(t™) = -15(1-e-ﬂRCs) - 9 Fig. 2. Electronic-circuit level: inverter circuit. 
physically existing system. 
The fact that the 
languages are highly distinct makes it possible to be confident about the existence of different system levels. 
Where we are fuzzy, as in the existence of an additional intermediate level, it 
is because new representa- 
tions have not yet 
congealed into distinct formal languages. 
As we noted, within each 
level there exists a whole hierarchy of systems and subsystems. However, as long as these are all described in the same language, e.g., a subroutine hierarchy, all given 
in machine-assembly language, they 
do not constitute separate sys- tem levels. With this general view, let us work through the levels of com- puter systems, starting at the bottom. Each 
level in Fig. 
1 actually has two languages or 
representations associated with it: an alge- 
braic one and a graphical 
one. These are isomorphic to each other, the same entities, properties, and relations being given in both. 
The lowest level 
in Fig. 1 is the circuit level. Here the com- ponents are R™s, L™s, C™s, voltage sources, 
and nonlinear devices. The behavior of the system is measured in terms of voltage, current, and magnetic flux. These are continuously varying 
quantities asso- ciated with various components, and so there is continuous be- 
havior through time. The components have 
a discrete 
number of terminals, whereby they 
can be connected to other components. 
Figure 2 shows both an algebraic and graphical description 
of an inverter circuit, 
as well as 
an algebraic and graphical descrip- tion of its behavior. We note that its structure is specified first as a circuit (a directed graph), with 
symbols for 
the arcs and nodes. The particular circuit still 
is an abstraction because the transistor Q1, the resistor R, and the stray capacitors C, are given only token 
values. The structure can be described symbolically by first writing the relationship describing each of the components (i.e., Ohm™s law, Faraday™s law, etc.) and then the 
equation which 
describes the interconnection of the components (i.e., Kirchhoffs laws). We observe the behavior of the circuit (probably using an oscilloscope) 
by applying an input ei(t) and observing an output 
e,(t). Alterna- tively, if we solve the equations which specify 
the structure, we obtain expressions which 
describe the behavior explicitly. The circuit level is not in fact the 
lowest level 
that might be used in describing a 
computer system. The devices themselves 
require a 
different language, either that of electromagnetic theory 
or of quantum mechanics (for the solid-state devices). It is usually an exercise in a course 
on Maxwell™s equations to show that circuit theory can be derived 
as a specialization under appropriately restricted boundary conditions. Actually, 
even at its level of ab- straction, circuit theory is not quite adequate 
to describe computer technology since there are 
a number 
of mechanical devices which 
must be represented. Magnetic 
tapes and 
drums are most likely 

Chapter 1 5 to come to mind first, but card readers, card punches, and Teletype terminals are other examples. These devices obey laws of motion and are 
analyzed in units of mass, length, and time. The next level is the logic level. It is unique to digital technol- 
ogy, whereas the circuit level (and below) is what digital technol- 
ogy shares with the 
rest of electrical engineering. The behavior of a system 
is now described by 
discrete variables which take on 
only two values, called 0 and 1 (or + and - , true and 
false, high and low). The components perform logical 
functions: AND, 
OR, NOT, NAND, etc. Systems are constructed in the same way 
as at the circuit level, by connecting the terminals of components, which thereby 
identify their behavioral values. The laws of bool- ean algebra are 
used to compute the 
behavior of a system from 
the behavior and properties of its components. 
‚The previous 
paragraph described combinatorial circuits 
whose outputs are directly related to 
the inputs at any instant of time. If the circuit has the ability to hold 
values over 
time (store infor- mation), we get sequential circuits. The problem that the com- binatorial-level analysis solves is the production of a set 
of outputs at time t as a function of a number of inputs at the same time t. As described in textbooks, the analysis abstracts from any trans- 
port delays between input and output; 
however, in engineering practice the analysis of delays is usually considered to be still part of the combinatorial level. 
In Fig. 3 we show a 
combinatorial network formed from combinatorial elements which realize 
three boolean output expressions, O,, O,, and O,, as a function of the input boolean variables 
A and B. Note that in the symbolic representa- tion of the structure we can write 
an expression that reflects the structure of the combinatorial network, 
but, on reduction, the boolean equations 
no longer reflect the actual structure of the combinatorial circuit but become a model to predict 
its behavior. 
The representation of a sequential switching circuit is basically .\the same as that of a combinatorial switching circuit, although 
-. one needs to add memory components, such 
as a delay element (which produces as output at time t the input at time t - T). Thus the equations that specify structure must be difference equations 
involving time. 
Again, there is a distinction (even 
in representa- tion) between 
synchronous circuits and asynchronous circuits, namely, whether behavior can be 
represented by a sequence of values at integral time 
points (t = 1, 2, 3, . . 
.) or must deal in continuous time. But this is a minor variation. 
Figure 4 gives a sequential logic circuit in both an 
algebraic and a graphical form and shows also the representation of the behavior of the system. Now it is clear that logic circuits are simply a subspecies of general circuits. Indeed, to design the logic components one 
con- structs circuit-level descriptions 
of them. For instance, Fig. 
5 ., I , c. shows a 
circuit for a 
NAND (or 
NOR) gate plus a table of its behavior. It is evident that its behavior corresponds 
to that of the NAND gate only if certain restrictions hold; namely, 
that one does not look at the voltage (which is identified as the behavior variable 
in the logic circuit) during certain 
periods when it is transient (ﬁsettling down,ﬂ to 
use the common phrase). 
Thus the logic level is an instance 
of the circuit level only 
in the same sense that the 
circuit level is an instance of Maxwell™s equations-as 
a limiting case in which certain features are deliberately ignored. 
One buys a 
great deal 
from the specialization to logic circuits, since one can 
compute the behavior of circuits at the logic level that are 
extremely complex 
at the circuit level. The techniques for doing so use an entirely different mathematical apparatus. 
In general, we cross into another 
level when the representation at the previous level provides information that is no longer relevant. 
A lower level 
is concerned with 
explaining the behavior of a certain structure, 
whereas the next highest level takes 
the lower level as given (a primitive). The higher level 
is concerned not about 
internal behavior but only how primitives are combined. A glance at Fig. 1 shows that we 
have described only 
the lower part of the logic level. 
There is another part, 
called the register- transfer level 
(or RT level). 
This is still an uncertain level, a matter -. I! Time, t or. alternatively, Fig. 3. Combinatorial-switching-circuit sublevel of the logic level: realiza- 
tion of three logic expressions. 
6 Part 1 I The structure 
of computers Structure Behavior ﬂ r .- a Y mi Sum n 0 0 0 I Ill II 0 Time, f Sinput xr Rinput = 7 XI A 7 X :7(XrVX) pmj 0 l,o 0,o l,o 0,o 0,1 Sum (output) table Fig. 4. Sequential-switching-circuit sublevel of 
the logic level: computa- tion of x + 1 from serial input string 
x. we will discuss 
after we have finished describing it. The com- ponents of an RT system are registers and functional transfers 
between registers. A register is a device that holds a set 
of bits.™ The behavior of the system is given by the time course of values of these registers, i.e., 
their bit sets. The system undergoes discrete operations, 
whereby the values of various registers are combined according to some rule 
and then 
are stored in another 
register (thus ﬁtransferred™)). 
The law of combination may 
be almost anything, from the simple unmodified transfer (A t B) to logical combination (A t B A C) to arithmetic (A t B + C). Thus a specification 
of the behavior, equivalent 
to the boolean equations 
of sequential circuits or 
the differential equations of the circuit level, is a set of expressions (often called productions) which 
give the conditions under which such 
transfers will be made. In Fig. 6 we give a 
picture of an RT system to compute the sum of integers. The figure includes the specification ‚This assumes that the elementary 
state variable of the system holds a bit (i.e., one of two values, such 
as 0 or 1). This need not 
be; sometimes the elementary variable holds a decimal digit 
(one of 10 values) or a character (one of, say, 48 values). For 
present purposes we can talk 
in terms of bits, without losing anything thereby. 
of its behavior 
and a table that shows the resulting behavior 
over time. Here the graphical structure of the system includes registers (N, I, S), transfers (S c S + l), data operators (S + 1, I > N, etc.). The flowchart shows the behavior of the control with time. 
The register-transfer level 
is still uncertain because there is substantial agreement neither 
on the exact language 
to be used for the level nor 
on the techniques of analysis and synthesis that go with it. As we will note below, for both the circuit level and the logic-circuit level 
there exist well-defined representations, guaranteed, so to speak, by standard textbooks and college courses 
that teach these levels. Standard texts on digital computers 
make only informal vse 
of the RT $vel. We have indeed 
a systems 
level in emergence here. 
If one restricts the transfer operations 
to boolean operations 
and thinks of a register as simply a set of 1-bit memories, 
one can write 
a set of logic equations for any 
register-transfer system. Furthermore, if one considers the role of logic design 
in digital computers, this has encompassed 
both sequential 
circuits and the register-transfer e e Table 4 of NAND Table of NOR Inputs behavior Inputs behavior 111 000 NOR logic element 1 1 0 0 NAND logic element 
O O 
™ 0 (Structure) 0 (Structure) 1 0 01 1 1 O 100 1 101 1 110 1 111 0 000 Circuit level -15voltS output -15 volts Inputs +10JOltS A Node Multiple input inverter 
Circuit (Structure) Table of circuit behavior 0-3 0 0 -3 -3 -3 -3 -3 0 -3 -3 -3 0 -3 -3 -3 0 (Behavior) Fig. 5. Change of representation at 
the circuit level combinatorial- 
switching sublevel boundary. 
Chapter 1 7 I LJ Present state N C level. The practicing logic designer (by now 
an institutionalized position, on a 
par with that 
of circuit designer) has 
sequential and combinatorial circuits 
as his basic 
analytic tools, and he attempts to design systems on 
the register-transfer level 
(e.g., central proc- essors) with these as tools. The register-transfer level 
has emerged from the informal attempts to create a notation closer to the job to be done. Recently there have been 
a number of efforts to construct formalized register-transfers 
systems. Most of them are built around the construction of a programming system or language 
that permits computer simulation of systems on the RT level. Although 
there is agreement on the basic components and types of opera- tions, there is much less agreement on the representation of the laws of the system (corresponding to tKe production system in Fig. XrJ wtS 00 01 ]I* N NIC N CIC u Y 0 t A start A -run-(S-O; I-0, start-0; run-I), t A run-((I5N)-(I-I t 1; S-S + 1); t, (I>N)-(run- 0)); m 1 s IS abbreviation for start 2 r 1s abbreviation for run 3 combinational network 4 clock event time, t 5A:4 X (N+ I) Fig. 6. Register-transfer sublevel 
of the logic level: 
computation of the sum of integers. I \" ,I Structure Behavior Y - 3 Output table 
(sum1 ' conventions (condition) + (output) Fig. 7. State-system representation 
of the logic level: 
computation of x + 1 from serial input string 
x. 6) or on the way to represent the dynamic behavior (correspond- 
ipg to the 
behavior table in 
the figure). There is another representation 
used at the logic level, 
the state-system representation, but it has been put 
at one side in Fig. 1. The state system is the most general representation of a discrete system avai1able.l A system is represented as capable of being in one of N abstract states 
at any instant of time. (For digital 
systems, N is finite or enumerable.) Its behavior 
is specified by a transition function that takes as arguments the current state and 
the current input and 
determines the next state (and the 
concomitant output). A digital computer is, in principle, representable as a 
state system, but the 
number of states is far too large 
to make it useful to do so. Instead, the state system becomes a useful 
representation in 
dealing with various subparts of the total machine, such 
as the sequential circuit 
that controls a magnetic tape. Here 
the number of states is small enough 
to be tractable. Thus, we have placed 
state systems at one side as an auxiliary to the 
logic level. In Fig. 7 we give the common representations 
of the state 
system. Co- lThere have been energetic 
attempts to apply the 
state-system approach to control 
systems of a more general nature [Zadeh and Desoer, 
19831, although they do not concern 
us here. 
8 Part 1 1 The structure 
of computers incidently, we use the representations of Fig. 7 for the sequential switching circuit 
of Fig. 4. That is, Fig. 7 may be viewed as an abstraction of the physical system 
in Fig. 
4. To the logic designer the state system is a useful abstraction of a logic design. A design 
usually passes 
through the following problem representations: 
1 2 The problem exists in a natural language. The problem is converted to a state diagram (output as a function of state, and input). 
The state 
diagram is represented as a 
state table 
and output table. States are assigned (physical memory elements are used). The excitation table and output tables are formed. 
The excitation and output 
logic equations are written (constrained by the actual logic elements). The sequential circuit 
is drawn. 3 4 5 6: 7 Let us go 
to the next higher level, 
the program leoel. This not only is a unique level of description for digital technology (as was the logic level) but is uniquely associated with computers, 
namely, with those digital devices that have a central component that interprets a programming language. 
There are 
many uses of digital technology, especially 
in instrumentation and digital con- trols, which 
do not require 
such an interpretation device and hence have a logic 
level but no program 
level. The components of the program level are a set of memories and a set of operations. The memories hold data structures which 
represent things both inside and outside the 
memory, e.g., num- bers, payrolls, molecules, 
other data structures, etc. 
The operations take various data structures as inputs and produce new 
data struc- tures, which 
again reside 
in memories. Thus the behavior of the system is the time pattern of data structures held 
in its memories. The unique feature of the program level is the representation it 
provides for combining components, 
that is, for 
specifying what operations are 
to be 
executed on what data structures. This 
is the program, which 
consists of a sequence of instructions. Each in- struction specifies that a given 
operation (or operations) 
be exe- cuted on specified data structures. Superimposed on this is a control structure that specifies which instruction is to be interpreted next. Normally this is done in the order in which 
the instructions are 
given, with jumps out 
of sequence specified by branch instructions. Again, Fig. 8 shows a simple program, 
the data 
structures, and the behavior. Two things separate the logic level from the program level. First, computer systems at the 
logic level 
are parallel devices, with all components active 
simultaneously. At the program level, com- 
puters are represented essentially as serial devices. Second, 
the program level, but not the logic level, is essentially linguistic 
in nature. At the program level 
things can be named, abbreviations 
can be used, decisions 
can be made, instructions 
are interpreted 
- all concepts 
that are strikingly absent from physical systems. Of course, they are not 
ﬁreallyﬂ absent since one can 
give a full 
description of the operation of a program at the 
logic level. But one does so by carrying in mind the set of physical behaviors 
discovered for computers that make them show the appropriate linguistic behavior at the program level. Thus, one 
does not ﬁgo 
to ALPHA if accumulator is negative™; one has a logic circuit that transfers the contents of the address field of the instruction register to the program counter, 
ANDing that transfer with the 
sign of the accumulator, so that it 
does not take place 
if the accumulator is not negative. 
Such a 
translation reveals how distinct 
is the system boundary between the 
register-transfer level 
and the pro- gram level. 
The size of the gap is also revealed in 
the ability of people to become expert programmers 
without knowing anything about any representations below 
the programming level. The program level constitutes an entire technology in its own right, and one that carries within it 
most of the emergent charac- teristics of computer systems that make them worthy 
of a science. Among the programming languages alone, there are 
levels of lan- guage which 
are so distinct from each other as to constitute system levels fully as 
important as the ones exhibited in 
Fig. 1. Never- theless, from the viewpoint of someone basically 
concerned with hardware systems, these can 
all be accounted 
a single level, at least for 
the present. The one aspect 
of programming systems that should be of most concern, that of operating systems, is still in such a fragmented state that it does not even 
begin to be a distinct system level. 
One peculiarity of the program level is that there exists no 
universal representation for it, as there does for the circuit or logic-circuit level 
(and, it is to be hoped, soon for 
the register- transfer level). 
Each machine has its own machine language 
(and its own assemblers 
and command languages built on those ma- 
chine languages). Each of these languages forms a complete sys- tem at the 
program level, applicable 
only to the machine in 
question. There is no universal machine language, although 
there is much in common at a conceptual level between all existing 
machine languages. There has existed a long-standing 
attempt within the programming field to develop an UNCOL 
(for Uni- versal Computer Oriented 
Language) [Steel, 19611 that would play this role, 
but it has never been successful. The reasons are not far to seek. The role of the machine language is to be 
inter- 
Chapter 1 9 E, 0 0 preted by 
the machine in order to produce behavior. It is not free to have arbitrarily 
desirable properties 
from our human viewpoint, since its details 
affect the efficient operation of the computer too 
much - how much 
space is devoted to the program, how much 
time is saved by a special order oriented 
to matrix multiply, 
etc. UNCOL was also attempting to fill the same role 
as machine languages, being one 
from which to compile a machine code 
for an arbitrary machine. Another 
reason why there has been no universal programming 
representation is that each particular machine language is a language, and so a universal description 
would seem to be a description of a class 
of languages. This is by no means impossible, as 
the wide use of notations such 
as Backus Normal Form (BNF) sh0w.l Nevertheless, 
it has contrib- 
uted to the lack of any universal notation. We now 
move to the fourth and last level. 
In Fig. 1 it is called I-I+l, the P~~-&xway-%~ She4 for-4kert. The name is not recognized, nor 
is any other, 
since the level exists only informally. Nevertheless, its existence 
is hardly in doubt. 
It is the view one takes of a computer system when one considers only its most aggregate behavior. 
It then consists of central proc- essors, core memories, tapes, 
disks, input/output processors, com- munication lines, printers, tape controllers, busses, Teletypes, scopes, etc. The system is viewed as processing a 
medium, infor- mation, which 
can be measured in bits (or digits, 
characters, words, etc.). Thus the components have capacities 
and flow rates as their operating characteristics. All details of the program are sup- pressed, although many gross distinctions of encoding and infor- 'We will propose a notation 
later. See also 
the work by F. Haney in his Generalized Instruction System (GIS) [Haney, 19681. Structure Start POP-8 symbolic machine language 
program LOC Oper. Start clo dca S dca I Loop tad S tad I - dca S n tad N + cia 72 tad I D 0 0 smo cla Stop hlt n IS2 I 5 imp loop m s- I- NN Action Comments S-0; 1-0; clear AC deposlt AC in M,clear AC twos complement add I-N; negate AClin twos complement) hait I-Itl; index(byI),skip if0 lump I=N? rs+I; skipit-AC,clear AC sum =O,OtI, .... 0+1+ ...+ N integers 0,1, ..... N volue of N,,where: OCS'2 ALGOL program Start S- 0; Stop ! f~rI-O;~pl N @S-S+I; Behavior <- I Time, f Time11 5 ps 0 1 3 5 7 9 11 13 14 16 17 19 ZG 15X(N+1 It I 15x(N+l)tZ 15X(Nt1)+3 Program AC I S counter stort 0 0 0 
start+l 0 Q Q start t2 0 0 0 loop G 0 0 
loop+l 0 0 0 loop +2 0 0 
0 loop+3 0 0 
0 loop+4 N 0 0 loop+6 -N 0 0 lOOP+8 0 0 0 lOOP+9 0 1 0 loop 0 1 0 bp+5 -N 0 0 . .. . .. . .. loop +6 -NtN N 1+2+ ...+ N stop 0 N 1+2+ ... +N stop +1 0 N 1+2+ ...+ N (halted) Fig. 8. Programming level: computation of the sum of integers. 
10 Part 1 I The structure of computers mation type remain, depending on 
the analysis. Thus one 
may distinguish program from data, or file space from resident monitor. 
One may remain 
concerned with 
the fact that input data are 
in alphameric and must be converted into binary, or 
are bit-serial and must be converted to bit-parallel. We might 
characterize this level 
as the ﬁchemical engineering 
view of a digital computer,ﬂ which likens it more to a continuous- process petroleum-distilling plant than to a place where 
complex FORTRAN programs 
are applied to matrices 
of data. Indeed, this system level is more nearly an abstraction 
from the logic level than from the program level, since 
it returns 
to a simultaneously operating flow system. One might question 
whether there is a distinct systems level here. In the 
early days of computers almost all computer systems could be represented as in the diagram in M.I.T.™s Whirlwind computer programming manual in Fig. 
9: with classic boxes of 
memory (storage), control, 
arithmetic, and input/output. 
Actually, this view of the computer in 1953 was considerably advanced; 
few texts on 
the logic design 
of computers in 
the 1960s have such a detailed model. This model has secondary memory 
(magnetic tape and 
drums in the Whirlwind™s case). 
The most interesting aspect of the model, which text writers omit, 
is any kind 
of switch- ing (the 
bus of Fig. 9). The bus provides a 
communication path to link the other components. Certainly the pushbuttons (actually 
the console) is novel for 
such a model. Compare this with 
the diagram of a modern computer system in Fig. 10, which shows a two-processor 
UNIVAC 1108, the level of abstraction being 
the same as in 
Fig. 9. The arithmetic element 
of Fig. 9 has disap- 
Difference LA u u Fig. 9. Automatic digital computation. (From the 
Whirlwind Computer 
Manual, M.I.T. 
By permission of the publishers.) peared and is replaced by a processor 
(a combined control 
and arithmetic element) 
in Fig. 10. The central control of Fig. 9 is now distributed throughout 
the remaining components. 
The control in Fig. 10 is a combined unit for transforming a serial character- information stream into words. It also manages the transmission of a word vector between the 
primary memory 
and a terminal or a secondary memory. 
The Resource Allocation Diagram is in- troduced in 
Fig. 10 to describe the allocation (use), 
hence be- havior, of the PMS components as a 
function of time. Chapter 2 describes these figures more fully. 
Another indication 
of the emergence of the PMS level lies in 
the models used in most operations-research types 
of studies on computer systems. Again, 
in the early 1960s these were practi- 
cally nonexistent. Now, 
with the advent of multiprogramming, multiprocessing, and time sharing, and the imminent arrival of computer networks, there are substantial numbers of such studies. 
The level of abstraction is always one that considers only 
flows and stocks of information, measured 
in bits (or an equivalent), perhaps divided into several subtypes. 
The concerns are bottle- 
necks, capacities, total flow rates, queuing problems, buffer 
sizes, and the like. All this indicates 
a system 
level above both 
the logic level and the 
program level. 
There is no uniform language for representation at this level and even, as we noted, no standard name. 
We have used the term PMS in analogy to the 
use of RT for the register-transfer level. 
Processors, memories, and switches are the main kinds of com- ponents out of which systems at this level are built. If one names a number of components at the PMS level, as we did 
previously, one finds few switches in the list. ﬁBussesﬂ in our list would be one, although 
many would think 
first of their data transfer charac- 
teristics. But, as 
this book amply shows, what makes the PMS level both interesting and complex is the existence of switches which 
govern the pattern of information flow through the system. One reason why they seem buried is their association with other com- ponents as addressing systems. There are other components besides processors, memories, 
and switches, namely, 
links, transducers, and controls. But the first three, P, M, and S, seem appropriate to 
characterize the level. It is not known whether there 
will be yet other systems levels, 
say one above the PMS level, as networks come into existence. The simplicity of the top level argues against 
it, but that may only show our narrow 
vision. It is important to realize that these levels are not sacrosanct. They depend strongly on physical 
technology. Thus, as we move toward integrated 
circuitry, there may emerge representations other than register-transfer diagrams, 
and the 
lat- ter may never develop into a clear systems level. One could even 
-. 
Chapter 1 11 imagine something 
happening to the 
circuit level, as continuous distributions became more important (although the use of equiva- lent circuits is well embedded in the engineering culture). We are not concerned with predicting any 
particular changes. We wish only to emphasize that the system-levels diagram 
of Fig. 1 is a reflection both of current technology and of our ways of analyzing given physical systems. 
As such, these levels have 
a certain im- permanency about them. What is the problem? The systems levels 
we have 
just described correspond 
to the 
tech- nologies that are available 
for the analysis and synthesis of com- puter systems. Each of these levels exists, 
in fact, 
precisely to the extent that a technology has 
become well developed. Thus 
both the circuit level and the lower half of the logic level 
(combinato- rial and sequential 
circuits) are highly polished technologies. 
They are what one learns today, 
if one wants to become a 
computer en- gineer. Textbooks exist, courses 
are taught, and there 
is a flourish- ing, cumulative technical 
literature. As we progress up the systems levels, matters become progressively worse. 
The register-transfer level is not yet 
well established, although there is considerable current activity in 
the area, and the next few years 
may see its universal establishment. Although programming is certainly well defined, each machine 
is a king in his own court, with no 
common technology of the program level 
that is relevant to the 
design of computer systems. The latter phrase must be added since we are 
taking a 
very specialized viewpoint 
here. We 
do not consider the world of programming research at all, it being entirely divorced from computer-systems 
design.l Finally, 
at the top, there is practi- cally no consensus on 
the nature of the systems level. 
There is nothing very surprising about this state of affairs. It reflects accurately the fundamental fact that only in the past few 
years have computer systems become complex enough for 
the higher levels to emerge as distinct systems levels. 
When most computers could be described in the diagram of Fig. 9-and such a diagram 
was reprinted innumerable times in the first decade- there was no need to haire a technology 
at the 
PMS level. When registers were so expensive that one could count 
the registers of a processor on the fingers of one hand 
(no thumbs allowed), one did not need a register-transfer language 
in order to describe the 'This is not entirely true. 
Each level must provide coupling with 
adjacent levels. A major issue in computer-design 
is the trade-off between hardware and software. M$- S - Y 2 + 0 + m where: Graphic -Pic -T.console - -Pc-T.console - i I Kio (#I : 16) -Kio(#l :16) -Kio(#l :16) SK - T. cards- 0 > 0 r m m SK-S-T(Telephone) Mprprimary memory; Ms/secondary memory; PC/central Processor; 
T/terminal: and L/link Uswitch: K/control: Kio/control for io equipment; IMp(#0:7; core; 32760 word) Resource allocation diagram 
Time, Fig. 10. PMS level: UNIVAC 1108. 
12 Part 1 1 The structure 
of computers flows. In both 
cases, an informal block 
diagram conveyed all the information adequately. 
The question of the programming level is somewhat different, since this level has existed as 
a formal language from the very start. Here the key aspect, it seems to us, is that, since well-defined languages existed, 
there was little pressure to find a better one. The fact that such languages 
were completely idiosyncratic to 
the machine, since they emerged 
as a product 
of the design itself, 
simply did not worry anyone overly much. Each language provided 
a design framework 
one could 
work into, and this seemed to suffice. It led, it is true, to the 
game of ﬁWe have another bit left in 
the mode field of the instruction-got another mode you™d like?ﬂ 
But this has only 
made computer designers feel that creating an order code 
was something of an art. Thus we 
feel that the increased complexity of computer systems is making 
these higher 
system levels 
of increasing importance. 
Since this 
is only the second decade of the serious development of computer systems, these upper levels are not in very good shape. For instance, textbooks devote very little attention 
to the 
area. Textbooks (especially good ones) 
tend to be 
technique-oriented, giving most 
attention to what is known. (When we were students 
we always used 
to wonder why there were no mathematics texts which told 
you about the problems that were not 
solvable in closed form.) Thus the present need 
for some 
material at these higher 
levels constitutes a major motivation for this book. 
There is a second feature of the current scene that enters into 
our motivation for this book. Around 1,000 different computer systems have been built. This represents a 
substantial amount of pragmatic experimentation. This is especially true at the program- ming level and PMS level, and also to some extent at the register- transfer level. Many things 
have been tried, 
many found worth- 
while, and many found wanting. A good deal of reinvention goes on. Thus we 
are concerned that this history 
of experimentation not be lost. It is true that, if the underlying technology 
changes enough, the experience may become largely irrelevant, 
but this does not 
appear to us to be an imminent development. 
We will admit also to a third concern, which 
does not stem 
from our role as computer engineers concerned 
with design, but from our role as 
computer scientists, fascinated with the phenom- ena of computers. The variety of about 1,000 computers represents 
the beginning of a proliferation of a species. It is not under biologi- cal control but rather under economic and 
intellectual control. Nevertheless, it is in every sense of the word an 
evolutionary population. We find ourselves feeling a little like naturalists must have felt when 
confronted with the proliferation of the organic world. We were at one time tempted 
to call this book 
ﬁComputer Botanyﬂ and at another ﬁComputer 
Taxonomy.ﬂ We feel that the 
attempt to 
gather, document, and 
classify these existing computers is a worthy endeavor 
in its own right. 
One might think that all this material is easily available. But the record fades rapidly, especially when much 
of it exists only as 
manufacturers™ manuals 
and papers in assorted proceedings. 
The main reasons for 
producing this book and for its particular character are 
by now evident. There 
is a need 
for material on the upper levels of computer systems, both for teaching new students 
of computer science and engineering and for making 
the past record available for professional designers. Since 
the technologies are not well developed 
for the upper levels, it is not possible to write a textbook, making use only 
of well-accepted techniques, 
nJtations, and 
results. Instead, one settles for making available 
a collection of examples of systems, so that they can be studied and analyzed directly. Notations It remains to say a word about two 
notations we have introduced, 
both about 
our motivations 
for doing so and about their character. 
Some, but not all, of this is already implicit in 
the foregoing ac- count. We started simply to produce a 
set of readings in computer systems, motivated by the lack of detailed examples we could use in a course one 
of us (GB) was giving 
on computer design. As noted, we felt the need to expose the students to real examples of complex computer structures. As we gathered material we became im- pressed (depressed is actually a better term) with the diversity of ways of describing these higher 
levels. Even more, the amount of clumsy description-downright verbosity-even 
in purely technical manuals acted as a further depressant. The thought of putting such a congeries of descriptions between hard covers for 
one person to peruse and study was almost 
too much to contem- plate. Gradually, we began 
to rewrite and condense many of the descriptions. As we did so, a set of common notations 
developed. Becoming aware of what was happening, we devoted a 
substantial amount of attention and 
effort to creating notational systems that have some consistency 
and, we hope, some chance of doing the job required. These are the 
PMS descriptive system for 
the PMS level (sic) and the ISP (Instruction-set processor) descriptive sys- tem for the program level. 
Each of these requires some comment on its nature and the role we think 
it should play. 
The PMS descriptive‚system is meant to provide a notation 
for the top level of computer systems. Figure 10 is given in this 
notation. On the surface it 
is largely self-explanatory, given 
the 
Chapter 1 13 mnemonics of P for processor, 
M for memory, 
S for switch, T for transducer (hence also terminal), and K for control (since C is for 
computer). There 
is also L for link, but in most computer struc- tures it is unnecessary to distinguish a separate link component, except to show connectivity. (It does become appropriate if com- munication delays exist.) 
There is an issue about whether this small 
set of components is an appropriate set 
of primitives, but the 
issue is not of major proportions. The real issues in the development of the notation come from 
the stress of two opposite forces. On the one hand, one wants extremely compact notations 
for expressing computer sys- tems. The systems are large in 
any event, and if there is much extra notational freight in the 
way of fixed formats, forced 
writing of what is already known and assumed, etc., then the notation will be neither useful nor used. 
On the other hand, there 
is a tremen- dous variety and quantity 
of information that potentially must be capable of being written into a description: word size, 
capacity, flow, operation rate, data-types, variations of operation rate for 
different classes of instructions, parity 
checking, technology, 
and on and on. Thus one needs a notation 
that responds to both these demands-and without being 
hopelessly complex 
and difficult to learn. Our attempt at 
a solution involves 
a basically simple lan- guage with comprehensive 
(and we think 
natural) ways of sys- tematic abbreviation and abstraction. The ISP descriptive system is meant to provide a 
uniform way of describing instruction sets, that is, of giving the information contained in a programming manual. 
It must provide the instruc- tion format, the registers referenced by 
the instructions, the rules of interpretation of the instruction, and the semantics of each instruction in 
the processor's repertoire. It must be able to do this for any existing computer, plus the expected extensions into the 
future. Its 
homeliest virtue is to make it possible to read the descriptions of the forty-odd computer systems described in this book, without having 
to fight a new notation 
for each system, and still to know in 
detail what the instructions really do. 
Our attempt at 
a solution turns out not 
to be 
a generalized 
sort of instruction. Rather, it is very similar 
in flavor to a register- transfer scheme. The differences lie in 
being able 
to suppress all timing information and all detail that is not essential 
to under- standing the instructions. ISP is not a 
variety of UNCOL, in 
which one can program; rather it is a language in which one can describe 
what any particular instruction set 
does. We thus avoid many of the pitfalls of the UNCOL-like efforts. 
There is a price to 
be paid for introducing new notations, 
for they must be learned. We feel that the two systems we have introduced here are natural 
enough to require almost no 
learning for superficial 
use (e.g., looking 
at Fig. 10) and only modest 
amounts for full 
exploitation. They seem to us vastly preferable to the array of ad hoc notations 
that we were faced with 
initially (and with which we almost faced the reader). Still we are aware of the price. A word should 
be said about antecedents. The 
PMS descriptive system is close to the 
way computer scientists talk 
informally about the top 
level of computer systems; no one effort in the environment stands out 
as a predecessor. Some notations, such as CPU (for central processing units), have become widespread. 
We clearly have assimilated them. Our 
modifications, such as Pc instead of CPU, are dictated 
entirely by the attempt to 
build a 
consistent notation over the whole range of computer systems. With respect to ISP, we have been heavily influenced by 
the work on register- transfer languages.' The one 
that we used most 
as a kernel from which to grow ISP 
was the work of Darringer and Parnas [Dar- ringer, 19691. In particular, their 
decision to work within the framework of ALGOL suited our own sensibilities, even 
though the final version of ISP departs from a sequential algorithmic language in a number 
of respects. Finally, a word should be said about innocence 
and aspirations. We are putting 
PMS and ISP forward as two notations. They are that. But they also imply a 
particular view of digital processing. Thus they are 
not entirely innocent. It would be appropriate to explore fully this view 
and to justify the particular decompositions and definitions used. This 
is not to say that these views are pecu- 
liarly ours. They are 
implicit in the informal use of similar descrip- tive systems. However, the attempt to 
formalize a notation makes them more accessible. 
We accept the obligation to perform such 
an exploration. But this volume is not the place to do so, for that would turn it into something between a treatise and 
a textbook. For this book, 
it is appropriate to take these notations at face value. We have a companion 
volume in preparation that attempts the other job. This 
is an aspiration. We have other aspirations as well. Notations 
in the computer 
world should 
turn into working tools. 
There are 
many tasks, such 
as the communicative one of this book, 
where the notation by itself is useful. Others are easy to imagine: writing specifications for 
new machines; being sure what the computer 
salesmen are selling; standardization of programming manuals, so that learning about 
a new machine is easier; etc. But there are other 
tasks where the 'We have not been influenced in a direct way by the work of Iverson [Falkoff, Iverson, 
and Sussenguth, 19641 in the sense of patterning our notation after his. Nevertheless, his creation of a full description of the IBM System/30 in APL stands as an important milestone in moving toward formal descriptions of machines. 
14 Part 1 I The structure of computers notations must become formal programming languages, so that analysis and synthesis procedures can be carried on automatically in their terms. As we have 
noted, the development of ISP and PMS germinated from purely notational issues. We 
have not 
let our aspirations to 
turn them into simulation languages delay 
our use of them for purely descriptive purposes. Thus we 
accept the obli- gation also to develop them 
as operational tools. That is also an aspiration and cannot 
be dealt with anywhere within 
this book. 
Plan of the book We now have enough 
background to explain the structure of the book. Two other chapters complete the 
introductory part. Chapter 2 provides an exposition 
of the PMS and ISP descriptive systems. As we have 
just noted, this does 
not attempt to explore seriously 
the view of digital processing implicit in these notations, although it does provide 
a small amount of motivation. A summary of the language conventions and 
parameter values is given at the end of the book in the appendix. Chapter 3 provides a description 
of the space of computer systems. One can 
view all 
computer systems as occupying a space 
whose dimensions 
are the various important systems features. Many features 
of the actual systems are relatively locked together. For example, word size 
and number 
of instructions in 
the reper- toire covary; no 12-bit 
machine has 
200 instructions but several with over 32 bits do. Thus the number of significant dimensions 
of variation is much less than the total 
number of features of computer systems. Such 
a space 
provides a basic frame in which to choose representative computer systems for inclusion 
in the book. We hope Chap. 
3 will also justify 
our feeling that there is a diversity and proliferation of computer systems that is worthy of serious study. The remainder of the book is divided into five parts (2 to 
6, with the 
introduction constituting Part l), and each 
part into sections. Each chapter gives a description of a computer system that is an instance of the part and section. Usually a chapter describes only 
one computer or computer system, although there are a few exceptions 
in Part 6 on computer families. A word needs 
to be 
said about the 
ﬁVirtualﬂ Table of Contents. Many of the example computers are relevant to more than one part and section. Physically, they have to be 
located at one place. But we have permited multiple entries 
in the Contents, so that, for instance, Chap. 33 on the IBM 1800 appears in Sec. 1 of Part 2 
as an example 
of a one-address ISP, 
in Sec. 1 of Part 4 as a terminal 
control, and finally in 
Sec. 2 of Part 5 as an example 
of a PMS with one 
central processor and multiple 
input/output processors (1 Pc, multi-Pio); 
physically it is located in the latter 
section. By using different 
type faces we hope the reader will not 
become confused between virtual and actual. There is little point in outlining the content 
of the various parts and sections here. This is better done at the 
end of Chap. 3 after the computer space 
has been laid out. 
References Brackets are used to enclose author(s) and year of publication, e.g., [Dar- ringer, 19691 or [Falkoff, Iverson, 
and Sussenguth, 
19641. A list of all the references in a chapter is given in code at the 
end of the chapter. The code refers to the bibliography at the end of the book. This 7- or R-char- acter code is as follows: Characters 1:4 Character 5 Characters 6:7 Character 8 First four characters of the last name of author (or first author) First initial of author (or first author) Year of publication- 1900 (Optional) a, h, c, . . 
. , used to denote multiple 
refer- enced publications of author in a year. References DarrJ69; FaIkA64; HaneF68; RoseS67; SteeT61; ZadeL63. 

The PMS and ISP descriptive systems 
The task of this chapter is to provide an introduction to the PMS descriptive system for the top computer-system level 
and to the 
ISP descriptive 
system for 
the program level. We take the view that informal notations exist and are in use. PMS and ISP are an attempt to tidy up these notations-to make them consistent and more powerful. Thus we depend on 
the reader already to under- 
stand implicitly much of the notation and how it is to be used. In consequence, there is no attempt in this chapter to provide a formal treatment of the whole system. The appendix 1, at the end of the book contains a complete summary of the notation rules, including the component attributes and values, and their abbreviations (i.e., the main technical vocabulary). We 
will pro- 
vide a brief discussion 
of the conceptual view underlying 
the two systems, since it is an appropriate way to make the notation understandable. But this is informal and heuristic. The two descriptive systems are not independent. 
There is a common set 
of notational conventions for abbreviating, for giving 
parameter values, and so on. (The Appendix separates them.) 
Likewise, there exists, in effect, an ISP description for every PMS component, or, conversely, ISP 
statements imply particular PMS component structures. A natural way is to present PMS first, which will also serve to introduce 
the main notational devices. Then we 
will give 
ISP. Finally, we will add more comments 
on the rela- tionship between PMS and ISP. PMS level of description Digital systems can be characterized most generally as systems 
that at any time exist in one 
of a discrete set of states and that 
undergo discrete changes 
of state with 
time. This is a highly ab- stract view. Nothing is said about what 
physical state corresponds to a system 
state; nothing is said about what laws of physics trans- form the system from one state 
to another. The states are given abstract labels: S,, S,, . . . . 
The transitions are provided by 
a state-transition table with 
many entries 
of the form: If the system is in state Si and the input is Ij, then the 
system is transformed to state S, and evokes output 0,. (Alternatively, a state diagram has the same information.) 
The virtue of this "state-system" 
view is that it truly 
seems to capture what we mean by 
a dis- 
crete (or digital) system. Its disadvantage 
lies in this same 
com- prehensiveness, which makes it impossible to deal with large 
systems because of their immense number of states (of the order of 10'O 'O states for a 
big computer).' 
Existing digital computers can be viewed as discrete state systems that are 
specialized in three ways. These three speciali- zations make 
possible a 
much more compact and useful description of these systems, the one that we call 
the PMS description. First, the state is realized by 
a medium, called information, 
which is stored in memories. Thus, a core store 
of N words each of 32 bits is a digital device 
that can exist in one of 232N states. Sim- ilarly, all the states of a processor 
are made explicit in a set 
of registers: an accumulator, an address register, an instruction register, status register, etc. Each holds a specified 
number of bits. No permanent information is kept in digital devices except 
as encoded in 
bits in a 
memory. There are 
two qualifications to this blanket statement. 
First, the basic unit of information need not 
be the bit; it could be any base: One can 
have ternary 
machines, decimal machines, etc. Second, the sequential logic circuits that carry out operations in the system have intermediate 
states. But this is a strictly temporary affair while the operation is occurring, for example, the intermediate, inaccessible, partial results during a multiply operation. At the end-when the smoke has cleared, so to speak-all information carried over to the next operation has been encoded 
into bits in 
memories somewhere. At the PMS level we care only about the end 
result of such operations. 
The second specialization 
of the general state-system view 
is that current digital computer systems consist 
of a small 
number of discrete subsystems linked together by flows of information. There is a distinct component called the memory, another called the central processor, another called the ,card reader, etc. This is analogous to the lumped-parameter specialization at the circuit level. Thus the natural representation 
of a digital computer system is as a 
graph which has 
component systems at the nodes and information flows as 
branches. Now, in fact, 
the discrete character of digital encoding 
in bits prevents there being any truly continu- 
ous digital devices (in analogy 
to the continuously distributed parameter circuits). But one can have distributed 
networks with very small components. Such 
iterated arrays are a topic of much 'As we noted in Fig. 1 of Chap. 1, we actually describe some parts of the control mechanisms of computers by state-system diagrams; however, 
these are 
exceedingly small pieces. 
An example may be seen in Fig. 7 on page 7. 15 
16 Part 1 I The structure 
of computers current investigation, as the possibility of manufacturing them by integrated-circuit techniques has emerged. These 
distributed net- 
works look 
very different from the computer systems of today, although they are still digital systems. Thus, the representation as a 
flow network with functionally specialized nodes 
is a real specialization. The third specialization of the general state-system viewpoint is that associated with each component in a digital system is a small number of discrete operations 
for changing its own 
state or the state of neighboring components. 
All transitions must occur 
through the application of these few operations, which 
are evoked as a 
function of the current state of the component. The total behavior of the system is built up from the repeated execution of the operations as the conditions for their execution become realized by 
the results of prior operations. 
The general state-system view is more general. The state-transition table for a system 
may exhibit an arbitrary pattern of immediate state transitions, without regard to how such transition would 
be physically realized. To summarize, within this 
specialized view 
one wants a way of describing a system 
of an interconnected 
set of components, which are individual devices 
that have associated with them a set of operations that work on a medium of infomation, measured in bits 
(or some 
other base). The major complication in this picture is the amount of detail involved in describing actual computers. It takes a whole manual, 
for instance, to describe the operations of a major 
computer, such as the IBM 7090. Thus the descriptive system must 
permit very compressed descriptions. 
It must also permit description of only those aspects 
of the components that are of interest, ignoring 
the rest. And what is of interest at the PMS level? Besides a description of the gross structure of a computer system, it is primarily the analysis of the amounts of information held in 
various components, the flows of information between components, and the 
distribution of the control that accomplishes these 
flows. Thus a PMS-level 
description is analogous to the chemical engineer™s diagram of a refinery in which he is interested in 
various kinds of liquid and gas flow. 
He has to account for matter and 
energy loss with the system at various stages involving the trans- duction of materials from one form to another. 
A specific 
chemical plant™s external performance 
is measured in terms of its production flow rate for a given 
cost. With computers, external performance 
is concerned with 
the economical accomplishment 
of discrete tasks, but at the PMS level this translates 
into operation rates and cost of operations. For the PMS level we ignore all 
the fine structure of informa- tion processing and consider a system consisting 
of components that work on a 
homogeneous medium called information. 
Infor- mation comes in packets, called 
i-units (for information units), 
and is measured in bits (or equivalent units, such 
as characters). I-units 
have the sort of hierarchical structure indicated 
by the phrase: A record consists of 300 words; a 
word consists of 4 bytes; a byte consists of 8 bits. A record, then, contains 300 X 4 X 8 = 9,600 bits. Each of these numbers-300, 4, 8-is called a length, since one often thinks of an i-unit as a 
spatial sequence of the next lower i-units of which it is composed. For example, one speaks of ﬁword lengthﬂ and of a record being ﬁ300 words long.ﬂ Other than 
being decomposable 
into a hierarchy of factors, i-units have no other structure 
at the PMS level. They do have a referent, that is, a meaning. Thus it is possible to say of an i-unit that it refers to an employer™s payroll, 
to the pressure of a boiler, or to a prime number satisfying certain conditions. To do so, of course, the i-units encode the information necessary to make the reference. At the PMS level we are 
not concerned with 
what is referred to, 
but only with the fact that certain components transform i-units 
but do not modify their meaning. In 
fact, these meaning-preserving operations 
are the most basic 
information- processing operations of all, and they provide the basic classi- fication of computer components. PMS primitives In PMS there are 
seven basic component types, each distinguished by the kinds of operations it performs: Memory, M. A component that holds or stores information 
(i.e., i-units) 
over time. 
Its operations are reading i-units out of the memory and writing i-units 
into the memory. Each memory that holds more than a single i-unit 
has associated 
with it an addressing system by means of which particular 
i-units can be designated or selected. A memory can also be consid- ered as a 
switch to a number of submemories. The i-units are not changed in any 
way by being stored 
in a memory. Link, L. A component that transfers information (i.e., i-units) 
from one place to another 
in a 
computer system. It has fixed ports. The operation is that of transmitting an 
i-unit (or a sequence of them) from the component at one port 
to the 
component at the other. Again, except for the change in spatial 
position, there is no change of any sort 
in the i-units. Control, K. A component that evokes the operations of other components in the system. All other components 
are taken to consist of a set of discrete operations, 
each of which, when evoked, accomplishes some 
discrete transformation 
of state. 
Chapter 2 I The PMS and ISP descriptive systems 
17 With the 
exception of a processor, 
P, all other components are essentially passive and require some other active agent 
(a K) to set them into 
small episodes 
of activity. Switch, S. A component that constructs a link between other 
components. Each switch has associated 
with it 
a set of possible links, and its operations 
consist of setting some of these links and breaking others. 
Transducer, T. A component that changes the i-unit used to encode a given 
meaning (i.e., a 
given referent). The change may involve the medium used to encode the basic bits (e.g., voltage 
levels to magnetic flux, or voltage levels 
to holes in a paper card), or it may involve 
the structure of the i-unit (e.g., bit-serial to bit-parallel). Note 
that T™s are meaning-preserving but not necessarily information-preserving 
(in number 
of bits), since the encoding of the (invariant) meaning 
need not be equally opti- 
mal. Data-operation, D. A component that produces i-units with 
new meanings. It is this component that accomplishes all 
the data-operations, e.g., arithmetic, logic, shifting, 
etc. Processor, P. 
A component that is capable of interpreting a program in order 
to execute a sequence of operations. It consists of a 
set of operations of the types already 
mentioned-M, L, K, S, T, and D-plus the control necessary to obtain instruc- 
tions from a memory and interpret them as operations to be 
carried out. 
Throughout PMS (and ISP, too) an operation is taken to mean a transformation of bits from one specific memory to another. For 
instance, it 
is an operation to transmit a word of information from memory M to memory M™; it is a different operation 
to transmit a word from memory M™ to Mﬂ. Similarly, it is an operation to add the 
contents of memory M 
to that of M™ and a different 
operation to add the contents of M™ to Mﬂ. The reason for 
emphasizing this point is that one often talks as if addition were an operation, ignoring the specific locus 
of the operands. In a discussion of computer systems, an operation must include specification of the locus of its operands. 
The reason is that the physical devices 
that realize operations 
are always local- 
ized in space. 
If, for instance, we wish to have a physical 
device that corresponds to addition on operands anywhere in 
some mem- 
ory, we must 
couple the physical device that adds with other devices that either 
transmit information to and from the memory to the adder or (more exotic) that modify the adder to 
have differ- ent cells of memory as its terminals. Thus 
the symbol + is to be 
taken as an incomplete 
specification of an operation. 
Computer model (in PMS) Components of the seven types can 
be connected to 
make stored- program digital 
computers, abbreviated by C. For 
instance, the classical configuration 
for a computer is C : = Mp-Pc-T-X Here Pc 
indicates a central processor and Mp a primary memory, namely, one which 
is directly accessible from a P and holds the program for it. T is a 
transducer connected to the external environ- ment, represented by X. (The colon-equals (: =) indicates that C is the name of what follows to the right.) Thus 
a computer is a central processor connected to 
its primary 
memory on the one hand and to 
a transducer on the other, which is what an input/ 
output device is. Actually the classic diagram had four components, since 
it decomposed the Pc into 
a control (K) and an 
arithmetic unit or data-operation (D): b~p- K-T~MS~-X or M~-D--T/MS-X D I ‚.\I; where the 
solid information-carrying lines are for instructions and their data, and the dotted 
lines signify control. Often logic operations were 
lumped with control, instead 
of with data 
operations, but this no longer seems 
to be the 
appro- priate way to decompose the system functionally. If we associate local 
control of each component with 
the ap- propriate component, we get L J where the 
solid lines carry the information in which we are 
inter- ested, and the 
dotted lines carry information about when to evoke operations on the respective components. 
The solid information- ‚The ‚‚Iﬂ expresses mutually exclusive alternatives. Here, a T 
or Ms exists at the periphery. 
18 Part 1 I The structure of computers carrying lines 
between K and Mp are instructions. Now, suppress- 
ing the K™s, then lumping 
the processor state memory, the data operators, and the 
control of the data-operations, and processor state memory to form a central processor, we again get Mp-Pc-T-X Computer systems can be described in 
PMS at varying levels 
of detail. For 
instance, in the diagrams above 
we did not 
write in the links (L™s) as separate components. These 
would be of inter- est only if the delays in transmission 
were significant to the dis- cussion at hand or if the i-units transmitted by the L were different from those available at its terminals. 
Since this 
is not usually the case in current computers, one indicates simply that two com- ponents (e.g., an Mp and 
a Pc) are connected together. Similarly, often the encoding of information into i-units is unimportant; then 
there is no reason to show the T™s. The same statement holds for K™s. Sometimes one 
wants to show the locus of control, say when there is one control 
for many components, as in a tape controller, but often this is not of interest. Then there 
is no reason to show K™s in a PMS diagram. As a somewhat different case, 
D™s never occur in 
PMS diagrams of computers, since in the present design technology D™s occur only as 
subcomponents of P™s. If we were 
to make PMS-type 
diagrams of analog computers, 
D™s would show extensively as multipliers, summers, integrators, etc. There would be few mem- ories and variable switches. The rather large patchboard would be represented as a 
very elaborate manually fixed switch. Components are often decomposable into arrangements of other components. Thus, most memories are composed of a switch-the addressing switch-and a number of submemories. Thus a memory 
is recursively defined. 
The decomposition stops 
with the unit 
memory, which is one that stores only a single 
i-unit and hence requires no addressing. Likewise, a 
switch is often composed of a cascade 
of one-way to n-way switches. 
For example, the switch that addresses a 
word on 
a multiple-headed disk might look like - S (random)-S (random)-S (I i near)-S (cyclic)-M (word) \ \ \ \ The first S(random) selects a 
specific Ms.disk,drive,unit; the sec- ond S (random) is a switch with 
random addressing 
that selects the head (hence the platter 
and side); S(1inear) is a switch with linear 
accessing that selects the track; and S(cyc1ic) is a switch with 
cyclic addressing that finally selects the M(word) along 
the circular track. Note 
that the 
switches are realized by differing technologies. 
The first two S(random)™s are generally electronic (AND-OR gates) with selection times 
of 10 - 100 microseconds or 
perhaps electro- 
mechanical (relay). 
The S(1inear) is the electromechanical action 
of a stepping motor 
or a pneumatic-driven, servomechanism- controlled arm which 
holds the read-write heads; the selection time for a new track 
is 50 - 500 milliseconds. Finally, the S(cyclic) is determined by 
the rotation time of the disk and requires from 16 - 60 milliseconds, depending on the speed (3,600 - 1,000 We can write such decompositions 
of a component into sub- components either when we actually know the structure 
of the component or even when 
we know only 
the behavior. For 
example, we could write a memory 
as random access (M.random) 
even if it was, in fact, 
cyclic, as long 
as its behavior as far as the larger system was concerned took no account 
of its cyclic character, accepting the average access time as the random-access time. When people speak of the control element of a computer, they often refer mainly to the processors-not to the control of a disk or magnetic tape, which, however, 
can often be more complex. 
When we suppress 
detail, the control often disappears 
from a PMS diagram. Similarly, when we 
agglomerate primitive components 
(as we did 
above when combining 
Mp and K(Mp) to be 
just Mp) into the 
physically distinct subparts 
of a computer system, a sepa- rate control, K, often occurs. 
The functionally and physically separate controll has evolved 
in the past decade. These 
controls, often as big as a Pc, can 
be computers with stored control pro- 
grams. When we decompose a 
compound control, 
we find data- operations (D) for calculating addresses or 
for error detection and 
error correction 
data; transducers (T) for changing logic signal 
levels and information flow widths; memory (M) as it is used in D, T, K, and for buffering; and finally a large 
control (K) which coordinates the activities of all the other 
primitives. It should be clear from the above discussion that components are named according 
to the function they 
perform and that they can be composed of many different 
types of components. Thus, 
a control (K) may have memory 
(M) as a 
subcomponent, and a memory M may have a 
transducer (T) as well as a 
switch (S) as subcomponents. All these sibcomponents exist to accomplish the total function of the component and do not 
make the component also some 
other type. For instance, the M that does a 
transduction (T) from voltages on its 
input wires to magnetism in its 
cores and a second 
transduction from magnetism to voltages on its output wires does 
not thereby become 
a transducer as far as the total 
‚A variety of names for Ks are used: controller, adapter, channel, buffer, interface, etc. r™pm). 
Chapter 2 I The PMS and ISP descriptive systems 
19 system functioning is concerned. To the rest of the system all the M can do 
is to remember i-units, accepting and delivering them in the same form (voltages). In the 
Appendix at the end 
of this book we define for 
each type both 
a simple component and a compound component, reflecting in part 
this fact that complex subsystems can be put together 
to perform a single 
function from the viewpoint of the total system. For example, a typewriter may have 4-6 simple information transduction channels. 
PMS notation In the 
above discussions we used various 
notations to designate additional specifications for a 
component, for example, Mp for a functional classification, and S(cyclic) for a 
type of access function. There are 
many other additional specifications one wants to 
give- so many that it makes no sense to enumerate them all in advance. A fixed position notation, such as standard function notation, F(x,y,z), where the first, second, and third argument 
places have fixed interpretation, is not suitable. Instead we agree 
on a single 
general way of providing additional 
specifications. If X is a com- ponent, we can write 
X(a,:v,;a,:v,; . . .) to indicate that X is further specified by attribute 
a, having value 
vl, attribute a2 having value 
v2, etc. Each parameter (as we call the pair a:v) is well defined 
independently of whatever other 
parameters are given; hence there 
is no significance to the order in which they are written 
or the number which have to 
be written. According to this notation we should have written M(function: primary) or S(access-function:random) rather than 
Mp or S(ran- dom). This shows immediately the price paid for the general convention: It requires an excessive amount of writing (which would be even more 
apparent if a large number of parameters were given), and the extra information 
seems to be redundant in 
some cases. We 
compensate for these disadvantages by several 
conventions for abbreviating and abstracting parameters. 
All these conventions are listed in the Appendix. Let us illustrate them by showing some alternative ways of writing Mp: M(functi0n:primary) Complete specification. M(primary) M.primary Drop the attribute ﬁfunction,ﬂ since 
it can 
be inferred from the value. Use the value outside 
the parentheses, concatenated with a dot. Use an explicitly given 
abbreviation, namely, primary/p (only if it is not ambiguous). Drop the concatenation marker (the dot), if it is not needed to 
recover the two parts (all components are 
given by a single 
capital letter-here M). Each of these rules corresponds 
to a natural tendency to abbreviate 
when redundant information is given; each has as 
its condition 
that recovery must 
be possible. In the full description 
in the appendix each component is defined and given a 
large number of parameters, Le., attributes with their domain of values. Throughout, we use the slash (/) to introduce abbreviations or aliases as 
we go.™ Thus p is introduced as an abbreviation for ﬁprimaryﬂ by writing 
primary/p when ﬁprimaryﬂ is given as one of the values of the attribute 
ﬁfunctionﬂ of a memory with respect 
to processors (see page 607). The list of parameters in the Appendix does 
not exhaust those 
aspects of a component that one might want to talk about. For instance, there are many distinct dimensions for 
any component in addition to the information dimension: packaging, 
physical size, physical 
lo- cation, energy use, cost, 
weight, style 
and color, reliability, main- 
tainability, etc. Furthermore, each 
of these dimensions includes an entire set of parameters, just as the information dimension 
breaks out into 
the set of parameters we have given in the Appen- dix. Thus the descriptive system is 
an open 
one, and new param- eters are definable at any occasion. The very large 
number of parameters provides one of the major challenges to creating 
a viable scheme to 
describe computer sys- tems. We 
have responded to this in part by providing automatic ways in which one can compress the descriptions by 
appropriate abbreviation while still avoiding 
a highly cryptic encoding of each separate aspect. Abstraction 
is another major area in which some conventions can help to handle the large numbers 
of parameters. It often happens that one has only imperfect information about an attribute, or 
one wishes to give its value only approximately 
or partially. 
For instance, one attribute of a processor 
is the time taken by its operations. This 
attribute can be defined with a com- 
plex value: Pc(operation-times: add:4 
ps, store:4 p, load:4 ps, multiply:16 ps, . . 
.) That is, the value is a list 
of times for 
each separate 
operation. However, one might wish to give only the range of these numbers; 
‚There is no difficulty in distinguishing this 
use from the use of the slash as a division sign; the latter takes priority, since it 
is the more specific 
use of the slash. 
20 Part 1 1 The structure 
of computers this is done without introducing 
a new attribute (i.e., operation- time-range) simply by indicating that the value is a range: Pc(operation-time: 4 -16 ps) Similarly, one could have 
given typical times or average times (under some assumed frequency mix of instructions): Pc(operation-time: 4 ps) Pc(operation-time: average: 
8.1 ps) The primary advantage 
of this notational convention, which per- 
mits descriptions 
of values to be used in place 
of actual values whenever desired, is that it keeps the number of attributes that have to be 
defined much smaller than otherwise. A PMS example using the DEC PDP-8 Let us now describe the PMS structure of an actual, though 
small, general-purpose 
computer, the DEC LINC-8, which 
is a PDP-8 with 
a LINC processor. Figure 1 gives the detailed PMS diagram. In explaining it, we will concentrate on making the notation clear 
rather than 
on discussing substantive features 
of the system (which 
are described 
in Chap. 5). A simplified PMS 
diagram of the system shows its essential structure: P.disp1ay-T- PC (‚L I NC) MS- L This shows the basic Mp-Pc-T-X structure of a C 
with the addition of a secondary memory (Ms) and two processors, one of which, Pc(™LINC), has its own Ms. Two switches 
are used: the 1/0 Bus which permits 
access to all the devices, and the 
Data Break to Mp via Pc for high-data-rate devices. There are many 
other switches in the actual 
system, as one can see from 
Fig. 1; for example, Mp 
is really one to eight separate modules connected by a switch 
S to Pc. Also there are 
many T™s connected to the input/output switch, Sio, which we collapsed as a single 
T, and similarly for S(™ Data Break). Consider the Mp 
module. The specifications assert 
that it 
is made with core 
technology, that its word size 
is 13 bits (12 data bits plus one other with 
a different 
function); that its size is 4,096 words; and that its operation time 
is 1.5 ps. We could have 
written the same information 
as M(functi0n:primary; techno1ogy:core; operation-time: 1.5 ps; size: 4096 w; 
word: (12 
+ 1) b) In Fig. 1 we wrote only the values, suppressing the attributes, 
since moderate familiarity with memories permits an immediate 
infer- ence about what attributes are 
involved. For example, it is com- mon knowledge that computer memories store information in words; therefore 4096 w must 
be the number of words in the memory. As another example, we did not 
specify the function of the additional bit in the word when 
we wrote (12 + 1) b. An informed reader will assume this 
to be a parity bit, since 
this is the common reason for 
having an extra bit in a word. 
If the extra bit had 
some unusual function, we would have 
needed to define it. That is, in 
the absence of additional information, the most common interpretation is to be assumed. In fact, we could have 
been even more cryptic and 
still com- 
municated with 
most readers: M.core(1.S ps/w; 4 
kw; 12 b) This corresponds to the phrase ﬁA 12-bit, 1.5-ps, 4k core store,ﬂ 
which is intelligible to any computer engineer. The 4 kw stands for 4 x 1,024 = 4,096, which again is known to computer engineers; however, if someone less informed took it to be 
4 X 1,000 = 4,000, no real harm 
would be done. Consider the magnetic tapes for Pc. Since 
there are 
eight possible tapes that make use of the same controller, 
K, through a switch S, we label them #0 through #7. Actually, # is an abbreviation for index, which is an attribute like any other, 
whose values are integers. 
Since the attribute 
is a unique character, we do not have to write 
#:3 (although we could). The additional parameters give information about the physical attributes of the encoding. These are alternative values, and any tape has only one of them. We use a vertical bar ( I ) to indicate 
this (as in BNF notation for grammars). Thus, 
75 1 112 in/s says that one can have a tape with a speed of 75 inches per second or one with 
112 inches 
per second, but not a tape which can be 
switched dynamically to run at either speed. For many of the components no further 
information is given. Thus, knowing 
that M.magnetic,tape is connected to a control and from there to the Pc tells generally 
what that K does. It is a ﬁtape controllerﬂ which evokes all the actions of the tape, 
such as 
read, write, rewind; therefore these actions do 
not have 
to be 
done by Pc. The fact that there is only one K for many Ms™s implies that only one tape can be accessed at a time. Other 
infor- 
Chapter 2 I The PMS and ISP descriptive systems 
21 Multiplexor; radial: from: 7 P,K; I T. consol e 
- Mp @0;7) !.-S2-Sdc?.-S4- - - K5- TCTeletype; IO char/s; 8 b/char; 64 char)- paper tape; (reader; 300 char/s)I (punch: - 100 char/s): 8 b/char 3 1 K--,[ "164 char/col 3 "I 30 us/point; .01 1.005 in/point 3 K-T incremental point 
plot; 300 point/s; .01 4 c i n/poi nt K-T(card; reader: 2001800 card/min) + K-T(card; punch; 100 card/min) + line; printer; 
300 line/min; 120 col/line: 
- CRT: display: 
area: IO x IO in215 x 5 in2; + K- T(liqht; pen)8 K- T(Dataphone; 1.2 
-4.8 kb/s)- K(#l : IO)-L(analog; output; 0 - -10 volts)+ K-S-L(#0:63; analog: input; 0 - -10 volts)- -K- S- K(#0:63; Teletype; 110, 180 b/s)- f12,l parity) b/w 2 P(disp1ay; '338) T(#0:3; CRT: display: 
area: IO x IO in )-, T(#0:3; light: pen)> T(#0:3: push buttons: 
console)+ T.console T(#0:15; knobs, analog; input)+ T(CRT: display: 5 x 5 in2)+ T(digita1; input. output)- 
T('Data Terminal Panel: digital; input, 
output)- 'Mp(core; 1.5 p/w; 4096 w: (12 
+ I)b) "S('Memory Bus) 3Pc(l -2 w/instruction: data: 
w, i,bv; 12 b/w: M.proc~ssor statei2; - 31) w: technolooy: transistors; 4S(tl/0 Bus; from: Pc; to; 64 K) 'K(I .- 4 instructions; M.buffer(l char-2 w)) antecedents: PDP-5; descpndants; PDP-85, PDP-81, PDF-L) Fig. 1. DEC LINC-8-PDP-8 PMS diagram. 
22 Part 1 I The structure 
of computers mation could 
be given, although that just provided is all that is usual in specifying a 
controller in 
an overall description of a sys- tem. (The 
next level of detail goes to the structure of the actual 
operations and instructions and belongs to the ISP level, not 
the PMS level.) We have used 
several different 
ways of saying the same thing in Fig. 1 in order to show the range of descriptive notations. Thus the 64 Teletypes are shown by describing a 
single connection through a switch 
and putting 
the number of links in the switch above the connecting line. 
Consider, finally, the Pc in 
Fig. 1. We have given a few 
param- eters: the data-types, the processor state, the 
descendants, etc. These few parameters hardly 
define a processor. Several 
other important parameters are easily inferred from the Mp. The basic operation time in a processor is a small 
multiple of the read time 
of its Mp. Thus it 
is predictable that Pc 
stores and reads informa- 
tion in 
2 x 1.5 ps (one for instruction fetch, one for data fetch). Again, where this is not the case (as in the CDC 6600) it is neces- sary to say so. Similarly, the word size in the Pc is the same as the word size 
of the Mp: 12 data bits. More generally, 
the Pc must have instructions 
that take care of evoking all 
the components of the PMS structure. These instructions 
do not see 
the switches and controls as distinct entities; 
rather, they speak directly to the oper- ation of the M™s and T™s connected via these switches 
and controls. Other summary parameters could have 
been given for the Pc. None of them would come close to specifying its 
behavior uniquely, although 
to those knowledgeable 
in computers 
still more 
can be inferred from the parameters given. For instance, knowing both the 
data-types available 
in a Pc and the number of instruc- tions, one can 
come very close 
to predicting exactly what the instructions are. 
Nevertheless, the way to describe a 
Pc in full detail is not to add larger and larger numbers 
of summary param- eters. It is more direct and more revealing to develop a description at the 
level of instructions, which is the ISP description. Let us end this introduction to the PMS descriptive system by returning to a critical item in 
its design philosophy. A descriptive scheme for systems as 
complex and detailed as digital computers 
must have the ability to range from extremely 
complete to highly simplified descriptions. It must permit highly compressed 
descrip- tions as well as extensive ones 
and must permit the selective suppression or amplification 
of whatever aspects 
of the computer 
system are of interest to the user. PMS attempts to 
fulfill these criteria by 
providing simple conventions 
for detailed description 
with additional 
conventions that permit abbreviation and abstrac- tions, almost 
without limit. 
The result is a notation that may seem 
somewhat fluid, especially on first 
contact in such a 
brief intro- duction as this. 
But once assimilated, PMS seems to allow some of the flexibility of natural language within enough notational controls to enhance 
communication considerably. ISP level of description The behavior of a processor is completely determined 
by the nature and sequence of its operations. 
This sequence 
is completely determined by a 
set of bits in Mp, called 
the program, and a set of interpretation rules that specify how particular 
bit configura- tions evoke the operations. Thus, if we specify the nature 
of the operations and the rules of interpretation, the actual 
behavior of the processor depends solely on 
the particular program in 
Mp (and also on the initial state of data). This is the level at which the programmer wants 
the processor described-and which 
the pro- gramming manual 
provides-since he himself wishes 
to determine 
the program. Thus the 
ISP (Instruction-set processor) description must provide a scheme for specifying any set 
of operations and any rules of interpretation. Actually, the ISP descriptive scheme 
need only be general enough to cover some 
broad range 
of possibilities adequate for past and current 
generations of machines along with their 
likely descendants. As we saw 
earlier when 
discussing the PMS level, there are certain 
restrictions that can be placed on the nature 
of a computer system, specializing it 
from the more general concept of a discrete state system. It processes a medium, called 
informa- tion; it is a system 
of discrete components 
linked together by information transfers; and each component 
is characterized by a 
small set of operations. These 
assumptions are built into the 
PMS descriptive scheme in 
an integral way. Similarly, 
for the ISP level we can 
add two more such restrictions, 
which will in 
turn provide the shape of its descriptive 
scheme. The first specialization is that a program can be conceived as a distinct set 
of instructions. Operationally, this means 
that some set of bits is read from the program in Mp to a memory 
within P, called the instruction register, M.instrnction/M.i. This set 
of bits then determines the immediately following sequence of oper- ations. Only a single 
operation may be determined, 
as in setting a bit in the internal state of the P; or a 
substantial number 
of operations may be determined, as in 
a ﬁrepeatﬂ instruction that evokes a search through Mp. 
In a typical one- 
or two-address 
machine the number of operations per instruction ranges from two to five. In any event, after this sequence of operations has occurred, the next instruction to be 
fetched from Mp is determined and obtained. Then the entire cycle repeats itself. 
Chapter 2 I The PMS and ISP descriptive systems 
23 The cycle of activity we have just described is called the inter- pretation cycle, and the part of the P that performs it is called the interpreter. The effect of each instruction can be expressed entirely in terms of the information held in memories at the end of the cycle (plus 
any changes made to the 
outside world). 
During execution, operations may have 
internal states of their own as sequential circuits which 
are not represented 
as bits in memories. 
But by the end of the interpretation cycle, whatever effect is to be carried on to a later time has been staticized in 
bits in some mem0ry.l The second additional specialization 
is on the data-operations. A processor™s total set 
of operations can 
be divided into two parts. One part 
contains those 
necessary to operate other components given in 
the PMS diagram: links, switches, memories, transducers, etc. The 
operations associated 
with these components 
and the 
extent to which they can be 
indirectly controlled 
from P 
are highly restrained by 
the basic nature of the components and their con- trols. The second part contains those operators associated with a processor™s D component. So far we have said nothing at all about them, except to exclude them completely from all PMS com- ponents except 
P. These are the operations that produce bit pat- terns with new meaning-that do all the ﬁrealﬂ processing 
or changing of informatiom2 If it were not 
for data-operations, the system would merely 
transmit information. As we noted in our original definitions (page 17) a 1™ (including a D) is the only com- 
ponent capable 
of directly changing information. 
A P can create, modify, and destroy information 
in a 
single operation. As we noted earlier, D™s are like the primitive components 
in an analog com- 
puter. Later, 
when we express instruction sets 
as simple arithmetic expressions, the D™s are the primitive operators, 
for example, ‚This description holds true for a P with a single active control (the inter- preter). Some Ps (e.p., the 
CDC %OO) have several 
active controls and 
get involved in ﬁoverlappingﬂ several instructions and 
in reordering opera- 
tions according to the data and devices available. 
With these, a more complex statement is required to express the same general restriction we have been 
stating for simple P™s: that the 
program can 
be decomposed into a sequence of bit sets (the instructions), each of which has local 
control over the behavior of the P for a limited period of time, with all 
interinstruc- tion effects being staticized as bits in M™s. 21n principle, this view that only 11 components do ﬁrealﬂ processing is false. It can be shown that a universal Turing machine 
can be built from M, S, L, and K components. The key operation is the write operation into 
M, which suffices to construct arbitrary bit patterns under suitably con- trolled switches. Hence 
arbitrary data 
Operations can be 
built up, The stated view is correct in practice in that the 
data-operations provided in a P are highly efficient for their 
bit transformations. Only the foolish add integers 
in a modern computer by table look-up. +, -, X, /, x 2ﬂ, A, V, @, concatenation, etc., 
which are evoked by the instruction-set-interpreter part of a processor. 
The specialization is that all the data-operations can be char- acterized as working on various datu-types. For example, there is a data-type called the signed integer, and there are data-opera- tions that add 
two signed integers, 
subtract them, 
multiply them, take their 
absolute value, test 
for which of the two is greater, etc. A data-type is a compound of two things: the referent of the bit pattern (e.g., that this set 
of bits refers to an 
integer in a 
certain range) and the 
representation in the bit pattern 
(e.g., that bit 
31 is the sign, and bits 30 to 0 are the coefficients of successive powers of 2 in the binary representation 
of the integer). Thus 
a processor 
may have several data-types 
for representing numbers: 
unsigned integers, 
signed integers, single precision floating 
point, double precision floating 
point, etc. Each of these is a distinct data-type, because it requires distinct operations 
to process it. On 
occasion, operations for several 
data-types may all 
be encoded into a single 
instruction with a data-type subfield that selects whether the data are fixed or floating point. The operations are still sepa- 
rate, no 
matter how packaged, 
and so their data-types 
remain distinct. With these two additional specializations-instructions and data-types-we can define an ISP description of a processor. 
A processor is completely described 
at the ISP level 
by giving its 
instruction set and its interpreter in terms 
of its operations, data- 
types, and memories. Let us concentrate first on the instruction set, leaving 
the interpreter until later. 
The effect of each instruction is described by an instruction-expression, which has the form condition + action-sequence The condition describes when the instruction will he evoked, and the action-sequence describes what transformations of data take place between what 
memories. The right arrow (+) is the control action (of a K) of evoking an operation. Recall that all operations 
in a 
computer system result 
in modi- fications of hits in memories. 
Thus each action in a 
sequence ultimately has 
the form memory-expression t data-expression The left arrow 
(t) is the transmit operation of a link and corre- sponds to the ALGOL assign operation. The left side must 
describe the memory location that is affected; the right side must 
describe the information pattern that 
is to be placed in that memory loca- 
tion. The details of data expressions and memory expressions are patterned on standard mathematical notation 
and are 
communi- 
24 Part 1 I The structure of computers cated most easily 
by examples. The same is true of the condition, which is a standard expression involving 
boolean values and rela- tions among memory contents. 
Before we get to 
the examples, let us note two 
features of the action sequence. The first is 
that each action in 
the sequence may itself be conditional, Le., of the form, ﬁcondition + action-se- quence.ﬂ The second is that some actions are sequentially de- 
pendent on each other, 
because the result of one is used as an input to the other; on other 
occasions a 
set of actions are inde- pendent and can 
occur in 
parallel. The normal situation 
is the parallel one. Thus, 
in the action sequence 
Y, tx,; Y, t x,; Y, t x,; Y, tx, all the transfers of information may be considered simultaneous. 
In particular, all 
the X™s have their 
values defined 
by the situation before the transfer. For example, if A and B are two 
registers, then (AtB; BtA) exchanges the contents of A and B. When sequence is required, the term ﬁnextﬂ is used: thus (A t B; next B 
t A) transfers the contents of B to A and then 
transfers it back to B, leaving both A and B holding the original contents of B (and so this contrived example is essentially just A 
t B). An ZSP example using the DEC PDP-8 The memories, operations, instructions, 
and data-types all need 
to be declared for a processor. 
Again these are most easily 
ex- plained by example, although full 
definitions are given in the Appendix at the end 
of the book. Consequently, let us examine the ISP description 
of the Pc of the PDP-8, given in 
Fig. 2 (the PDP-8 is explained fully in Chap. 5). Throughout the book the ISP descriptions 
of computers follow a 
more highly structured format than the ISP notation requires, in order to help the reader see the similarities among the computers. Processor state. We first need to specify the memories of the Pc 
in detail, providing names 
for the various bits. Thus, 
AC(0:ll) the accumulator is a memory called 
AC, with 12 bits, labeled at 0 and 11 from the left. Comments are given in italics™-in this case 
that AC is ‚There are a few features of the notation, such as the use of italics, which are not easily carried over into current computer character sets. Thus, the ISP of Fig. 2 is a publication language. 
called the accumulator (by the designers of the PDP-8). AC corre- sponds to an actual 
register in the Pc. However, 
the ISP does not imply any particular implementation, 
and names may 
be assigned to various sets of bits purely for descriptive convenience. 
The colon is used to denote a range or list of values. Alternatively, we could have listed each bit, separating 
the bit names by 
commas, as AC(0,1,2,3,4,5,6,7,8,9,10,11) Having defined a 
second memory, 
L (which has 
only a single 
bit), one could define a 
combined register, LAC, 
in terms of L and AC as LAC(L,0:11): = LOAC The colon-equal (:=) is used for definition, and the middle square box (0) denotes concatenation. Note 
that the bit 
named L of register LAC merely happens to 
correspond to the 1-bit L register. 
Primary memory state. In dealing with addressed memory, 
either Mp or various forms 
of working memory 
within the processor, we need to indicate multidimensional arrays. 
Thus Mp[0:7777,] (0: 11) gives primary memory 
as consisting of 10000, (Le., base 8) words of 12 bits each, 
being addressed as indicated. Such an address does not necessarily reflect 
the switching structure through which 
the address occurs, though it often will. (Needless 
to say, it reflects only addressing space, 
and not how much 
actual M is available in a PMS structure.) In general, only memory within the processor will occur as operands of the processor™s operators. The one ex- ception is primary memory 
(Mp), which was defined 
as a memory external to a P 
but directly accessible from 
it. In writing memories it is natural to use base 10 for 
all numbers 
and to consider the basic i-unit of the memory to be a bit. This is always assumed unless 
otherwise indicated. Since we used base 
8 numbers above 
for specifying 
the addressing range, we 
indicated the change of number base by a subscript, in standard 
fashion. If a unit of information other than the bit were to be used, we would subscript 
the angle brackets. Thus Mp[0:7777,](0: 1)64 reflects the same memory. 
The choice carries 
with it, 
of course, some presumption of organization in terms of base 64 characters, but this would 
show up in the specification of the operators (and 
is not true, in fact, 
of the PDP-8). We can also have multi- dimensional memories 
(Le., arrays), though 
no examples occur in 

Chapter 2 I The PMS and ISP descriptive systems 
25 OP II Fig. 2. These add the extra dimensions with an extra pair 
of brack- ets, for example, 
i p pagedddress 111ll1 M[a:b][c:d]. . . [g:h](x:y) The PDP-8 memory might better be 
described as: Mp[0:7][0:31][0: 127]( 
0: 11) representing 8 
memory fields with 32 pages 
per field, 128 words 
per page, and 
12 bits per word. Instruction fomat. It is possible to have several names for 
the same set of bits; e.g., having defined instruction(0:ll) we define the format of the instruction as follows: op(0:2) : = instruction(0:2) indirect,bit/ib : = instruction(3) page,O,bit/p: = instruction(4) page,address(0:6) : = instruction(5:ll) The colon-equal (: =) is used to allow us to assign names 
to various parts of the instruction. In effect, we are making a definition which is equivalent to the conventional diagram 
for the instruction: Notice that in page-address the names of all the bits have been shifted, e.g., page-address(4) 
: = instruction(9). The Appendix gives the permissible alphabet of symbols for 
ISP. In 
general, a ﬁnameﬂ 
can be any combination of uppercase and lowercase letters and 
numerals, not including names which 
would be considered numbers 
(integers, mixed numbers, fractions, etc.). A compound name can be sequences 
of names separated by spaces ( ). In order to make certain compound names more reada- ble, a space 
symbol (-) may optionally 
be used to signify the non-printing character. Periods (.) and hyphens (-) are also used. 
The instruction set. With all the registers defined, 
we can 
give the instructions. These are shown on the second page of Fig. 2 (there are 
some unexplained parts left on the bottom of the first page, to which we will return). The 
second page is actually a single expression, named Instruction-execution, 
which consists 
of a list of instructions. They are listed vertically down 
the page for ease 
of reading. Each instruction consists of a condition and 
an action sequence, separated by the condition arrow 
(+). In this case 
the condition is an expression 
of the form (op = octal digit). Recall that op is instruction(0:2), and so this expresses 
the condition that the operation code 
of the machine have a particular value. Each condition has been given a name 
in passing; e.g., ﬁandﬂ is the name of (op = 0). This provides 
the correspondence between the opera- tion code and the mnemonic name of the operation code. If this correspondence had been 
established elsewhere, or if we did not care what numerical operation 
code the ﬁandﬂ 
instruction is, we could have written and + (AC t AC A M[z]) We would not have 
known what condition the name ﬁandﬂ stood for but could have surmised 
(with little 
difficulty) that it 
was simply an 
equality test 
on the operation code. 
We will do this on a number 
of the ISP descriptions later in the book. Most gener- ally the form of an instruction is written as two™s complement add/tad(: = op = 1) + (LOAC tLOAC + M[z]) Here, we simultaneously define 
the action of the tad instruction, its name, an abbreviation for the name, and the conditions for tad™s 
execution. The parentheses are, in effect, a remark 
to allow an inline definition. For example, the above single ISP 
statement is equivalent to two™s complement add/tad+ (LOAC +- LOAC + M[z]) followed by 
tad := (op = 1) All the instructions in the list constitute the total instruction repertoire of the Pc. Since 
all the conditions are disjoint, one and only one condition 
will be satisfied when a 
given instruction is interpreted; hence 
one and 
only one action sequence 
will occur. Actually, all 
operation codes might not be 
present, and so there would be some illegal 
op codes that would evoke no 
action se- quence. The act of selection is usually called operation decoding. Again, ISP implies no 
particular mechanism by which this 
is car- ried out. 
Normally a logic circuit works directly on the op part 
of the instruction register, 
and the way op codes are assigned is significant for 
the complexity of this decoding circuit. Thus, some- 
times one 
exhibits the instructions in 
a two-dimensional decoding diagram that makes it evident what 
these bit patterns are 
(see Fig. 
2 in Chap. 5), rather than 
in a linear list. It might be wondered why we do not in general introduce 
some 
26 Part 1 I The structure 
of computers Pc State AC4: I I> L PC4: 1 I> Run I nte rrupt ,s ta te IO$ulse,l; I04ulseJ; I0,pulseA Mp State &tended memory is not included. M[O:77778]4:l I> Page,O[O:177 I4:l I> := M[O:177 ]&:I I> Auto,index[O:7]4:ll> := Pageg[lO :I7 ]4:ll> 88 8 8 Accumulator Link bit/AC extension for overf low and carry Program Counter 1 when Pc is interpreting instructions 
or "running" 1 when Pc can be interrupted; under programmed control IO pulses to IO devices special array of directly addressed memory registers special array when addressed indirectly,is incremented by 1 1'c Console State Keys for start, 
stop, continue, examine (load from memory), 
and deposit (store in memory) are not included. Oata switchesa:ll> Instruction Format instruction/idl:ll> opco : 2> PageJQ i t/p indirect,bit/ib page,address4 : 6> this,page<0:4> PC'<O: I I> I 0,se 1 ec t<O : 5> i o,pl ,b i t i o,p2,b i t iod4,bit s ma sza snl := i4:2> := i<3> := i<4> := i<5:11> := PC'<O:4> := (PC<O:II> -1) := i<3:8> := i<lI> := i<IO> := id> := i<5> := i<6> := i<D data entered via console op code 0, direct; 1 indirect memory refereme 0 seZects page 0; 1 selects this page selects a T or Ms device these 3 bits control the 
selective generation of -3 volts, 0.4 ks pulses to 1/0 devices p bit for skip on minus AC, operate 2 gy.oup u. bit for skip on zero AC + bit for skip on non zero Link Effective Address Calculation Process r<0:11> := ( -,i b + 2"; ib A (IO < z" < 17 ) --f (M[z"] +M[z"] + 1; next); ib 3 M[z"]) 8- 8 z'<O:11> := (7 ib +z"; ib +M[z"]) z"<O: I I> := (page,O,bi t 4 this,pageopage,address; -,page,O,b i t + Oopage,address) LI microcoded instruction or instruction bitls) within an instruction effective auto indexing 
direct address Fig. 2. DEC PDP-8 ISP description. 
Chapter 2 1 The PMS and ISP descriptive systems 27 Instruction Interpretation 
Process Run A (Interrupt,request h Interrupt-state) --f ( instruction cM[PCI; PC cPC + I; next instruction-execution) ; Run A Interrupt-request A Interrupt-state + ( M[O] t PC; Interrupt-state t 0; PC t 1) Instruction Set and Instruction Execution Process Instruction-execution := ( and (:= op = 0) --f (AC tAC A M[zl); tad (:= op = I) + (LOAC c LOAC + M[zl); isz (:= op = 2) + (M[z'l +M[zl + 1; next (M[z'l = 01 + @C ePC t 111; dca (:= op = 3) --f (ME21 t AC; AC t 0); jms (:= op = 4) --f (M[zl c.PC; next PC cz + 1); jmp (:= op = 5) + (PC + z); iot (:= op = 6) + ( io-pl-bit --f IO-pulse-1 c I; next io,p2,bit + I0,pulse-2 c I; next io,ph,bit i IO,pulse,lt c I); opr (:= op = 7) +Operate,execution ) no interrupt interpreter 
fetch execute interrupt interpreter 
logical and two's complement add index and skip if zero deposit and clear AC jwnp to subroutine jWI0 p in out transfer, microurogrammed to generate up to 3 pulses to an io device addressed by I0,select the operate instruction is defined below end Instruction execution Operate Instruction Set The microprogramed operate instructions: operate group 1, operate group 2, and extended arithmetic are defined as a separate instruction set. Operate-execution := ( cla (:= i<4> = 1) + (AC c 0); opr-l (:= io> = 0) + ( operate group 3 c11 (:= i<5 = 1) + (L 0); next p clear link 
cma (:= id> = 1) + (AC C- AC); u. complement AC cml (:= i<;r> = I) + (L +7 L); next IL complement L iac (:= i<lI> = 1) --f (Lmc CL~C + 1); next u. increment AC ral (:= id:IO> = 2) + (LWC +LmC x 2 {rotate)); p rotate left rt~ (:= i<8:10> = 3) + (LOAC ~LOAC x 2' (rotate3); rar (:= i<8:10> = 4) + (LOAC CLOAC / 2 (rotate)); rtr (:= i<8:10> = 5) + (LOAC cLOAC / Z2 (rotatel)); clear AC. Connnon to all 
o.oerate instructions. u, rotate twice left u rotate right p rotate twice right opr3 (:= i<3,1I> = 10) i ( operate group 2 skip condition 62 (id> = 1) --f (PC +PC + I); next skip condition := ((ma A (AC < 0)) v (sza A (AC = 0)) v (snl A L)) u AC,L skip test nsr (:= i-'9 = 1) + (AC <- AC v Data switches); hlt (:= i<103= 1) - (Run to)); w "or" switches )I halt or stop optional FA1 description EAE (:= i4,11> = 11) -tEAF,instruction~xecution) 
28 Part 1 I The structure of computers additional conventions into the language, e.g., list 
the instructions in a table with their mnemonic names in a special column, 
rather than write the whole affair as 
an expression. (In fact, 
if you ex- amine the first page of Fig. 2, you will 
note that the entire 
descrip- tion of the PDP-8 Pc is a single expression.) 
The reason is that although many processors fit such a 
format very well, 
not all do so, e.g., microprogrammed machines. By making the ISP descrip- tion a general 
expression for evoking 
action-sequences, we obtain 
the generality we 
need to cover all the variations. We will have two examples with the PDP-8 itself: the microprogrammed feature and the fact that the 
interpretive cycle simply becomes 
part of the total expression for 
the behavior of the processor. Let us now consider the action-sequence. We use 
standard mathematical infix notation. Thus we write AC t AC A M[z] This indicates that the 
word in Mp at address z 
is ANDed with the accumulator and 
the result left 
in the accumulator. It is as- sumed that the 
operation designated 
hy A is well understood. (The c, of course, is the transmit operation.) Each processor will 
have a basic set of operations that work on data-types of the machine. Here the data-type is simply the 12-hit word viewed as 
an array of hits. Operators need not 
involve memories 
actually within the Pc (the processor state). Thus, expresses a change 
in a word in Mp directly. That this must be mechanized in the PDP-8 by means 
of some register 
in Pc is irrelevant to the ISP description. We also use functional notation; 
for example, 
AC t abs(AC) replaces the contents of the AC with its absolute value. When an action has an unspecified 
function or operation we generally 
write A+f(A,B, ...) or AtuB or AtBbC for function, unary operation, and binary operation, respectively. Efective-address calculation process. 
In the examples just given 
we used z as the address in Mp. This 
is the effective address and 
is defined as 
a conditional 
expression (in the manner of ALGOL or LISP): 
z(0:ll) := ( -, ib + zﬂ; ib A (10, Q zﬂ < 17,) + (M[zﬂ] t M[zﬂ] + 1); next ib + M[zﬂ]) The right arrow (+) is analogous to the 
conditional sign used 
in the main instruction, equivalent to the 
ﬁif. . . 
then . . .ﬂ 
of ALGOL. The parentheses are used to indicate grouping in the usual fashion. 
However, we arrange 
expressions on the page to make reading easier. As the expression for 
z shows, we permit conditionals within conditionals and also the nesting of definitions (z is defined in terms of zﬂ). Again, we should emphasize that the 
structure of such definitions may reflect the underlying hardware 
organization, hut it need not. When describing existing processors, as 
in this book, 
the ISP description often reflects 
the hardware. But if one were 
designing a processor, the ISP expressions would 
he stated 
as design objectives for 
the RT structure, and 
the latter might differ 
considerably. Special note should he taken of the opr instruction (op = 7) in Fig. 2, 
since it provides a microprogramming 
feature. There are two separate 
options depending 
on instruction(3) being 0 or 1. But common to both is the operation of clearing the AC (or not), associated with instruction(4). Then, within one option 
(instruction(3) = 0) there are a series of independently executable 
actions (following the clearing of L); within the other (instruc- tion(3) = l), there are three 
independently settable control ac- tions. The nested conditionals and the use of ﬁnextﬂ to force se- quential behavior make it easy to see 
exactly what is going on 
(in fact a good deal easier than describing it in natural language, as we have been 
doing). The instruction interpreter. 
We now have all the instructions defined for 
the PDP-8, including the effective-address computation (z). It remains to define the interpreter. From a hardware point 
of view, an 
interpreter consists of the mechanisms for 
fetching a 
new instruction, for decoding that instruction and executing the operations so designated, and 
for determining the next instruction. A substantial amount of this total job has already been taken 
care of in the part of the ISP that we have just explained. 
Each instruc- tion carries with it a condition that amounts to one fragment 
of the decoding operation. Likewise, any further decoding of the instruction that might he done in common 
by the interpreter 
Chapter 2 I The PMS and ISP descriptive systems 29 (rather than 
by the individual operation circuits) is implied in the expressions for 
each instruction, and by the expression for 
the effective address. 
The only thing that is left is to fetch the next instruction and to execute it. In a standard machine, there is a basic principle that defines operationally what 
is meant by 
the ﬁnext instruction.ﬂ Normally the current instruction address is incremented by 1, but other 
principles are used (e.g., 
on a processor with a 
cyclic Mp). In addition, several specific 
operations exist in the repertoire that can affect what program is in control. 
The basic principle acts like a default condition: 
If nothing specific happens to determine program control, the normal ﬁnextﬂ 
instruction is taken. Thus, in the PDP-8 we get an interpretation process that is essentially the classic fetch-execute cycle (ignoring interrupts): Run + (instruction t M[PC]; PC c PC + 1; next fetch Instruction-execution) execute The sequence is evoked so long as Run is true (i.e., its bit 
value is 1). The processor will simply 
cycle through 
the sequence, fetch- ing and then 
executing the instruction. In the PDP-8 there exists a halt operation that sets Run 
to be 
0, and the console keys can, of course, stop the 
computer. It should be noted that the 
ISP descriptions in this 
book do not, generally, include console behavior. A state diagram (Fig. 3) is useful to represent the behavior of the instruction-interpretation process. As an instruction is inter- preted, the 
system moves from state to state. Any of the states can be null, in 
which case 
a simple 
transition is to be 
made to the successor of the null state. The K(instruction interpreter) con- fetch (read) Determines the instruction q operotion calculation decoding 
iﬁ \ Request operond from Mp / ?rand Multiple operands -L PCZ PC2 operotion specified calculotion (0v.r) operand store (write) ii Restore results operond address calculation (ov. w: Return for string Instruction complete or vector data fetch next instructioh ™Mp controlled state ‚Pc controlled state Note: Any state may be null State name soq/oq saq/oq so. o/a.o sav.r/ov.r sav.r/av r so/o sov.w/ov.w sav.w/ov w Time in a stote toq taq ta.0 tov. r tav. r to tav. w tav. w Meaning Operation to determine the instruction q Access (to Mp) for the instruction q Operation to decode the operotion of q Operation to determine the variable address v Access (to Mp) read the variable v Operation specified in q 
Operation to determine the variable address v Access (to Mp) to write voriable v Fig. 3. ISP interpretation state diagram. 

30 Part 1 I The structure 
of computers trols these movements 
according to the information in the instruc- tion. Which states are 
null and which of multiple alternative 
transitions occur depend on the instruction being interpreted. 
Within each state, 
various operations are carried out, 
under the control of subordinate K™s. Note that the upper states in Fig. 3 are controlled by the 
Mp whereas the lower ones 
are controlled by the Pc. We have tried to 
use a simple mnemonic scheme 
to label these states: 
o for operation, q for instruction, 
a for access, 
r for 
read, and w for write. Similarly, we prefix the state with t to indicate the time duration of the state, and we may prefix the state by s. Figure 3 is somewhat more 
detailed than is usual. We will use 
it in 
Chap. 3 to describe a number of different processors. However, the figure simplifies 
the familiar fetch-execute 
cycle: Fetch: {oq, aq} t.fetch = toq + taq Execute: (00, ov.r, av.r, 0, ov.w, av.w} t.execute = too + t0v.r + tav.r + . . 
. + t0v.r + tav.r + . . . 
+ to + t0v.w + tav.w Consider, by 
way of example, the tad instruction of the PDP-8, using the general state diagram of Fig. 3. From the ISP, the net effect is Run + (instruction t M[PC]; PC t PC + 1; next tad (: = op = 1) + (LU AC +LO AC + M[z])) where z(0: 11) : = (specijies the effective-address calculation process) 
The state diagram has more detail to 
explain the computer™s behavior with respect to timing and its temporary registers. (Note a complete state diagram for the physical PDP-8 is given in 
Fig. 11 of Chap. 5.) The actual state table appears on page 31. Notice again that the 
ISP description does 
not determine 
the way the processor is to be organized to achieve this sequencing or to take advantage of the fact that many instructions 
lead to 
similar sequences. 
All it does is specify unambiguously 
what oper- ations must be carried out for a program in Mp. The 1SP descrip- tion does specify 
the actual 
format of the instruction and how it enters into 
the total operation, although sometimes indirectly. 
For example, in the case of the and instruction (op = 0), the definition of AC shows that the 
AC does not depend 
on the instruction, and the definition of z shows 
that z depends on other fields of the instruction (indirect-bit, 
page,O,bit, page-address). Likewise, 
the form of the ISP expression 
shows that AC and PC both enter into the instruction implicitly. 
That is, in the 
ISP description all 
de- pendence on memory 
is exp1icit.l Data-types and data-operations This completes the description of the ISP for the PDP-8. For more complex machines 
the number of data-types and the 
operations on them are much more extensive. Then the data-types may be declared independently 
of the instruction set, 
in the same manner as we declared memory. In fact, the one major piece of organization in the structure of processors at the ISP le,vel 
that has not appeared in our example 
involves the data-types. Each data-type has a set of operations that are proper 
to it. Add, subtract, multiply, and divide are all proper to any numerical data-type, as well as 
absolute value 
and negation. Not all of these need exist in a computer just because it has the data-type, since there are 
several alternative bases, as well as some levels 
of completeness. For instance, notice 
that the PDP-8 first of all does 
not have 
multiply and divide (unless one has its special 
option), thus having 
a relatively minimal level 
of arithmetic operations, and second, it does not have 
a subtract operation, using 
a two™s complement add, which permits 
negation (- AC) to be accomplished by 
complementation (TAC) followed by add 1. Still, the options are rather 
few, provided 
one has de- cided to include a given 
data-type in 
the repertoire. In the Ap- pendix at the end 
of the book are given with each of the data-types (or classes thereof) the sets of operations that are 
proper to 
that data-type. The PDP-8, for example, does 
not have several data representa- tions for what is, externally considered, 
the same entity. An oper- ator that does a floating 
add and 
one that does an integer 
add are not the same. However, we will denote both 
by the same symbol (in this case, 
+ ), indicating the difference parenthetically after the expression. Alternatively, the specification of the data 
type can be attached 
to the data. Thus, in the IBM 7094 we have 
the instructions ‚This is not correct, actually. In physically realizing an ISP description, additional memories may be utilized 
(they may even be necessary). It can be said 
that in the ISP description these memories 
are implicit. However, a consistent and complete description 
of an ISP can be made without use of these additional memories whereas 
with, say, a single-address machine 
it does not seem possible to describe 
each instruction without 
some refer- ence to the implicit memories-as we see in the effective-address calcula- tion procedures where definitions 
look much like registers. 
Chapter 2 I The PMS and ISP descriptive systems 
31 Stutes Time 
soq [ toq ISP effect Operational description 
MA t PC; Calculate the address 
of the instruction, q, and calculate the address 
of the next instruction, q + 1. The address is stored in the address register, 
MA, used to control 
the access. 
PC c PC + 1 Sfetch 1 1 taq 1 ME tM[MA] saq s0v.r I Sexecute sav.r so Y Fetch the data from memory 
location, M[MA] (i.e., essentially M[PC]), and place the result in a buffer (temporary) register. 
t0v.r MA tf(MB,IR) Calculate the address of the data. 
~~ tav.r MB t M[MA] Fetch the data 
from Mp. to L 0 ACtL 0 AC + ME Do the operation specified 
by the instruction. 
so0 7 ] too ] IR tMB(O:2) 1 Calculate and decode 
the instruction 
Add -+ (AC t AC + M[e]); Add and carry logical word/ACL + ( AC t AC + M[e] {unsignedinteger}); Floating add/FAD -+ (AC c AC + M[e] {sf}); 
Unnormalized floating 
add/UFA -+ (AC c AC + M[e] {suf}); Double-precision floating 
add/DFAD + ( ACMQ t ACMQ + M[e]OM[e + 11 {df}); Double-precision unnormalized floating add/DUFA + ( ACMQ t ACMQ + M[e] 0 M[e + 11 {duf}) The first one, without a 
special indicator 
of data-type, is taken to be 
integer addition; 
the next, unsigned 
integer; the next, single 
precision floating 
point; the next, unnormalized single precision 
floating point; the next, double precision floating 
point; and the last, unnormalized double 
precision floating 
point. Although there are often clues 
that could be used to infer 
which form 
of addition is being defined (e.g., 
double precision takes two words) we label all but the 
integer operation. 
We use 
braces { } to differentiate 
which operation is being performed in the above examples. 
Thus, above, the data-type is enclosed in braces and 
refers to all the memory elements (oper- ands) of the expression. Alternatively, we use braces as a modifier on any memory to signify the information meaning. 
For example, a fixed point to floating point data-conversion operation 
would be given as AC{floating} t AC{fixed} We also use 
braces as a modifier for 
the operation-type. For 
exam- ple, shifting (left 
or right) can be a multiplication or division by 
a base, but it is not always an 
arithmetic operation. In the PDP-8, for instance, we have 
L 0 AC t L 0 AC x 2 {rotate} where the end bits L and AC(l1) are connected 
when a shift occurs (the operator is also referred to as a circular shift). 
In general, the nature of the operations used in processors are sufficiently familiar 
to the 
computer professional that no definitions 
are required, and they 
can all be taken as primitive. It is necessary only to have agreed upon 
conventions for the different data repre- sentations used. The Appendix provides 
the basic abbreviations. In essence, 
a data-type 
is made up recursively of a concatenation 
of subparts, which themselves 
are data-types. This concatenation may be an iteration of a data-type to form an array. Fig. 4 shows the structure of various data-types and 
how each is built from more 
primitive data-types. If required, an operation can be defined in terms of other (presumably more primitive) operations. 
It is necessary first to define the data format explicitly 
(including perhaps some addi- 
tional memory). Variables for 
the operands are permitted 
in the natural way. For example, binary single-precision floating-point 
multiplication on a 36-bit machine could 
be defined in terms of the data fields as follows: 
32 Part 1 I The structure of computers t' Stacks Linked Vector n elements(lineor list. Matrix-n xm elements (Zdimen. 
d, xdex xdn elements Simple multiple type structures 
un-normal flaoting/uf \ / I/ Double flooting Complex Double complex '?ore normally considered non -decomposable primitives Fig. 4. Common data-types recognized by processor 
hardware. sf mantissa/mantissa : = (0:27) sf exponent/exponent : = (28:35) sf exponent-sign : = (28) sf sign/sign : = (0) xl:= x2 x x3{sf}:= ( xl mantissa : = x2 mantissa x x3 mantissa; xl exponent : = x2 exponent + x3 exponent; next xl : = normalize (xl) {sf}) where normalize is xl:= normalize(x2) {sf} := ( (xl mantissa = 0) -+ (xl exponent : = 0); ((xi? mantissa # 0) A (x2(0) = x2( 1))) + ( xl mantissa := xi? mantissa x 2; xl exponent : = x2 exponent - 1; next xl : = normalize(x2) {sf})) Three additional aspects 
need to be noted with 
respect to data- types: two substantive and one notational. First, 
not everything 
one does with an item of data makes use 
of all the properties of its data-type. For 
example, numbers have 
to be moved from 
place to place. This 
operation is not a numerical operation and does not depend on the item being 
a number. In fact, for the purpose of data transmission, the item is only a word (assuming it fits into a single 
word) and can be treated as such. Second, 
one can often embed one kind of operation in another, so as to coalesce data- types. We 
saw this to a small 
extent in 
the example above 
of the PDPS arithmetic operations. A more pervasive example 
is encod- ing the Mp 
addresses into the 
same integer data-type 
as is used for regular arithmetic. Then there need 
be no separate data-type 
for addresses.' The upshot of both these aspects 
can be seen below where we 
present an outline structure of data-types that shows how one data-type can 
be embedded in another 
for various purposes. Data-types embedded in other data-types for 
common operations word integer fraction mixed unsigned integer address integer boolean (single 
bit) integer sign (divide or multiply by two operations) field single precision 
unnormalized floating boolean vector single precision floating 
double word double precision 
integer fraction mixed double precision unnormalized floating 
point double precision floating 
point character string digit string 'However logical such 
a course may seem, it is not always done this way. For example, the IBM 7090 (and other 
members of that family) have 
a 15-bit address 
data-type and a 36-bit integer 
data-type, with separate operations for each. 
Chapter 2 1 The PMS and ISP descriptive systems 
33 The notational aspect is our use in 
ISP of a mnemonic abbre- viation scheme 
for data-types. We 
have already 
used sf for single 
precision floating 
point. More generally, 
as Table 1 shows, an abbreviation is made up of a letter giving the precision, a letter giving the name, and a letter giving the length. A full treatment can be found in the Appendix. The simple naming convention does 
not take into account 
all that is known about a data-type. The information carrier for the data is only partially included 
in the length characteristic. Thus the carrier should also include the data base and the 
sign conven- tion for representing negative numbers. 
The common sign con- ventions are sign magnitude, true complement (i.e., two™s comple- ment for base 2), and radix-1 complement (i.e., one™s complement for base 2). For each 
of the data-types the processor must 
have the implied operators. In fact, being 
able to represent a particular entity 
is useful only 
if particular transformations can be carried out 
on the entity. The most primitive operation is data movement (i.e., trans- mission). Data movement can be 
thought of as a complex operation 
consisting of accessing (locating), reading, 
and writing. Data-types which represent numbers require 
the ability to perform the arith- metic operations +, -, X, /, abs ( ), sqrt, max, min, etc. The address integer is a special case 
of an arithmetic quantity, 
and often only additive arithmetic 
operations (+ and -) are available for it. Boolean scalars 
(or vectors) 
require some subset 
of the 16 logical operations (sufficient subsets are l, A or l, V). When character strings are represented, 
the concatenation, deletion, and transmission operations are required. Alternatively, we can 
look to string processing languages like 
SNOBOL or COMIT to see the operations they require. 
If the strings also represent numeric quan- 
tities, then the arithmetic operations are necessary. Almost all arithmetic and symbolic data require relational operations be- 
tween two quantities, 
yielding a boolean result 
(true or false). These relational operators are = and #, but for arithmetic quanti- 
ties includes 
>, >, <, <. The more complex structured data- 
types (e.g., vectors and arrays) also have a range of certain primi- tive operations such 
as scalar accessing and transmission. Typical operations of vectors are search and element-by-element compare operations. Relationship between 
PMS and ZSP In the 
introduction to this chapter we 
discussed briefly the rela- tionship between PMS and ISP. With the two described, we can now be more precise. There are 
really two questions here. First, 
where do these two descriptive systems fit in with 
respect to the general hierarchical view of computer structures 
discussed in Table 1 Abbreviations used to name data-types Precision Data-type-name Length-type 
fractional/f boolean/b 
* sca I a r quarter/q half/h ﬁsingle/s double/d triple/t sign vector/v decimal digit/digit/d matrix octal digit/octal/o array 
character/char/ch/c string/st 
byte/by quad r u ple/q syllable word/w multiple/m +integer (eq. 10) signed integer/i unsigned integer/ui fraction/fr fixed / m ixed / mx floating/real/f unnormalized-floating/uf complex real/complex/cx Examples: w word bv boolean vector 
i integer sfr single precision fraction mx mixed di double integer 
10d 10 decimal digit 
(scalar) 3.ch 3 character (scalar) 
chst character string 
sf single precision floating suf single precision unnormalized 
floating df double precision floating duf double precision unnormalized 
floating *May be optionally omitted from name Chap. 1. Second, what is the relationship between a PMS diagram of a processor 
and the ISP of that same processor. The questions are related, but each is best answered separately. 
With respect to the 
first question, the PMS system describes the topmost system level (recall Fig. 1 of Chap. l), above the programming, logic, and circuit levels. It lacks a 
characteristic that all these other levels share, namely, 
that of providing a complete description of the computer™s performance. 
The programming manual (with 
timing) tells everything 
that is significant about the performance of the computer (if it runs error-free). 
The same is true of the full description at the register-transfer level, the logic- circuit level, and on down to the electrical circuit 
level. But 
the PMS level is only an approximate description, from which only 
certain aspects of the system™s performance can be 
calculated. 
34 Part 1 I The structure of computers The ISP does not constitute a distinct system level. 
Rather, it describes the interface between two 
levels, the register-transfer level and the 
programming level. 
It is used to define the compo- nents of the programming level-instructions, operations, and seqnences of instructions-in terms of the next lower level. 
In principle, and usually in fact, the language of the lower level is used to describe the components and 
modes of connections, one 
level up. In 
many ways ISP is a register-transfer language (in symbolic rather than graphical form-but as we noted in Chap. 1, there appear 
always to be two such 
isomorphic notations at each system level). However, ISP has 
been extended 
by allowing 
the instruction-expression to be a general 
linguistic expression for 
a computation, 
just as 
if ISP were FORTRAN or ALGOL. This is what permits 
us to talk of ISP as not necessarily determining the exact set of physical registers 
and transfer paths. 
The instruc- tion-expressions describe the functions to be performed without 
entirely committing to the RT structure. If the ISP is the interface language between the 
RT and pro- gramming levels, what is its relationship to 
PMS, which is one level above? 
Every PMS component has associated 
with it a set of operations and a control structure for getting those operations executed in connection with 
the arrival of various external signals. As we noted 
earlier in 
the chapter, there 
is an ISP description for each operation 
in its context of control. That is, ISP is the interface language for describing all PMS components in terms of the register-transfer level, not just P. It happens that only one of these PMS components, the processor, carries with it an entire 
new systems level-the programming level. All the other compo- nents have no analog 
of the programming level and interface directly to 
the register-transfer level 
(or even in simple cases to the logic-circuit level). Precisely because of the simplicity, we 
have not bothered 
to develop ISP descriptions of other components 
of components other than processors. The second question, namely, the relation between the 
ISP and PMS descriptions of the same processor, arises from 
the ability to represent PMS components recursively as PMS structures made up from more 
elementary PMS components. Thus, Mp(32 
kw, 16 b) can be considered as compounded of 32k memories, M(l w, 16 
b), with an addressing switch, %random. Indeed, if one carries this to the limit, where the M™s are single bit memories (flip-flops), 
the S™s are one bit gates, a couple 
of specific K™s are defined for 
AND and OR, etc., then it is possible to draw a 
PMS diagram isomorphic to any logic circuit. Thus, a 
processor (P) can be rep- resented as a PMS involving M™s, K™s, D™s, s™s, etc., and at varying levels of detail. Since we also have 
a description of this same 
P in ISP, it is appropriate to consider the correspondence. First of all, every memory in 
the ISP description corresponds to a memory in the PMS description. The data operations in ISP imply corresponding 
D™s in PMS and every occurrence of transmit (c) implies a corresponding link between the 
M™s and D™s on the right hand side and the M on the left, being written into. That the instructions of the ISP are evoked only 
under certain condi- tions implies 
that a control 
(Koperation-decode) exist in 
the PMS structure. Similarly, the simple, two-state stored-program 
model (instruction-fetch, instruction-execute) 
for the interpreter implies an interpreter 
control (Kinterpreter). The action-sequence of each instruction, if it contains any 
semi-colons or next™s, 
requires addi- tional K and possibly additional M (if the structure involves em- bedded operations such as (A + B) x (C + D)). Thus for every 
ISP component there is an implied component 
in the PMS struc- ture of the processor. The PMS diagram model for a computer shown initially on page 17 has 
the ﬁnatural 
unitsﬂ implied by the ISP description (with 
the exception of the instruction format part) as suggested on page 
24. The data-operations D are therefore implied each time 
an operation is written. Each process implies 
a control which we lump into the single K of the figure. The model also shows 
both the arrival of instructions and the flow of data between 
the proc- essor (P) and 
memory (Mp). There are 
several memories 
within Pc which are not explicitly shown on page 17. These 
include temporary 
memory within D and the K for carrying out complex arithmetic operations. The interpreter control has temporary memory, of course. Finally, other kinds of memories have been 
omitted to simplify the model. In multiprogrammed computers 
a mapping control and memory would be used, and in pipeline or highly 
parallel processors there would be temporary memory for various buffering (e.g., 
instruc- tions and data). The 
Appendix lists 
the various memories 
of the processor. K(P), the control for the processor above, 
controls data move- ment among the Mp and M.processor,state and evokes the data- operations of D. Functionally, K(P) can be broken into several parts, each of which is responsible for 
a part of the overall instruc- tion interpretation and execution process, 
and each 
corresponds to a part of the ISP description. This decomposition 
is allowed in PMS, and if we did so, each component 
would contain an independent control for its 
own domain, e.g., a K(D), K(Mp), 
K(1nstruction-set interpreter). More elaborate processor structures imply having 
controls for functions like multiprogram mapping. 
The K(1nstruction-set interpreter) is the supervisory component which causes other processor K™s to be 
utilized in a complex processor. In an 
ISP description of a C, the interpreter 
usually 
Chapter 2 I The PMS and ISP descriptive systems 35 selects only the next instruction and then 
after decoding (or 
exam- ining it) proceeds to have 
the instruction executed by K(instruction execution). Resource Allocution. At the PMS level the concept of resources, their uses and allocation, becomes 
a major focus 
of analysis. This is obvious by now in multiprogramming and multiprocessing sys- tems where many programs share the same Mp and hence must be allocated space. 
But this holds equally well 
at all levels of detail. By giving a resource 
allocation diagram along 
with the state diagram (Fig. 
5) we show the relationship of resources, their func- tion, and time for the instruction-interpretation process. In Fig. 5 the add instruction for a 
simple 1 accumulator computer 
con- sisting of 1Pc-2Mp is given. The interpretation for Fig. 5 in ISP is as follows: 
Calculates the address of instruction q in 
state soq. t, - to = toq. PC + PC + 1; next aduunce the program counter The instruction is fetched (accessed) from Mp in state saq. t, - t, = taq. M.instruction t Mp[PC]; next The operation o to be performed and the address part, v, for the data 
in M.instruction to be added to A are obtained in state so0 + s0v.r. t, 
- t, = too + t0v.r M.address t Minstruction (v); next The data 
Mp[v] are fetched in state sav.r. t, - t, = tav.r M.temporary t Mp[M.address]; next 
The operation part o of the instruction is carried out on A; that is, the actual addition 
is performed on the data previously accessed 
in the state so. t, - t, = to. A t M.temporary + A; next In the state diagram, each state represents the time spent 
for a given 
activity. The two states at the top 
of the state 
diagram (Fig. 5) are waiting for primary memory accesses, 
and the three lower states represent 
processor activity waits. If we were to specialize the state 
diagram for the conventional 1 address/ instruction computer, we would 
need one 
additional state, repre- senting operand 
storage, sav.w, and this would occur 
after state, 
so. Note that we 
have ignored the operation decoding state, s0.0. Of course, conditional 
state transformation paths 
have to be added to describe all instructions 
(e.g., a 
complement-the-accumulator instruction has 
only states soq, saq, and so). Similarly, we could 
Instruction Data operand fetch from fetch from 
Mptl 1 Mp # 0 \ ~ I Instruction Data 
Instruction address address execution 
calculation calculation (operation 
on 1 J™ t- time spent in a state processor state) I t.cycle data fetch 
The instruction being interpreted IS Mp # 1 tad -(A-AtM[zl); to tl f2 Instruction execution 30 Instruction Data address address colculotion colculotion 
Fig. 5. State and resource allocation 
diagram for a 1Pc-2Mp add instruc- 
tion-interpretation process. make a more general 
state diagram to handle the different proc- essors (e.g., multiple addresses/instruction, stack, 
and general reg- isters), as shown in 
Fig. 4. At the PMS level, a derivative of the state diagram, the resource allocation diagram 
is more useful be- cause it relates to the physical structure. A resource allocation diagram 
expresses the above instruction activity in terms of the time each unit 
is occupied with 
a particular activity. In this diagram 
a slightly 
more complex 
computer struc- ture with two primary memories has 
been assumed. In the case of the add instruction, the long memory-cycle 
time suggests that two memories can be used so that an operand be fetched while the instruction memory restoration 
occurs. These diagrams show 
the time various resources 
are utilized; thus performance 
and utilization can be measured. Resource allocation diagrams 
can express other time 
scales. Interest in 
operating-system software analysis is often in the ac- tivities on a 
longer time scale of the resources utilization as a 

36 Part 1 I The structure of computers function of various programs 
and subprograms. They may show Mp memory occupancy in a 
multiprogrammed environment. 
Some other time scales of particular interest are the 
instruction(s), short 
instruction sequences or subprograms, and the 
program times. 
The first two time 
scales are influenced predominantly 
by the hardware, and the latter time 
scale is influenced by software and the ex- ternal environment. The resource allocation diagrams also 
can describe the utiliza- tion of the C™s resources over 
time (e.g., throughout the instruc- tion-interpretation process) and provide 
a basis for more detailed analysis and design. The design problem 
at the PMS-ISP interface is mainly one of resources scheduling. 1 A fixed set of operations have 
to be performed 
on the jobs (here, a job is 
an instruction). Each instruction may 
create a few other small but definitive subjobs. There can be a 
fixed set of operators which handle various parts of the operations. Jobs (or instructions) 
enter P sequentially. 2 3 4 We may ask: 
1 2 How many 
operators of each type do we have? What is the scheduling policy for assigning 
instructions to the operators? How many instructions 
can be in P at one time, and 
in what order must the processing be performed? How are the jobs interlocked? 3 We do not attempt to 
answer the above questions but intend only to show the relationship of the various parts which define 
the problem. ISP implies a certain structure 
(conversely, PMS behavior is specified in terms of the ISP language). A particular ISP structure and a 
program denote a certain path 
through a 
state space as specified by 
a state diagram. Finally, the physical re- 
sources (in 
PMS) are constrained to operate according to the 
state diagram as expressed by using 
a resources allocation diagram. The resource allocation diagram can then 
be used to evaluate the structure™s performance (in PMS) at a higher 
level (e.g., 
the number of instructions/second it executes). I State diaaram \ ,,,(behayj ISP (description and progrom) - - - - - - - 
- - - - - - 
- - - - - - - - - - 
- - - 
- - - - - - - - - - - - - 
. RT( description \ behavior ) RT level Summary The ISP descriptions of computers are usually given 
as an appendix to a chapter. We organize the description into the 
following units: I™ State P Console State Memory Declaration Instruction Format Data-type Formats and Special Data Effective-address Calculation Process Operation Definitions Process Interpreter and 
Instruction Interpretation Process Formats and Operators the Instruction- Instruction-set and Instruction Execution set Execution [ The above description format 
conveys a rather narrow-minded view of the ISP structure of computer systems. However, almost 
all present computers 
fit easily into such a format. 
We do not 
presume to say whether it will suffice for future ISPs. With the introduction given here and 
with the definitions and example in the Appendix at the end of the book, it should be possible to understand all the PMS diagrams and ISP descriptions 
used throughout the book. 
Chapter 3 The computer space Introduction The preceding two chapters 
have provided a 
view of a computer system as 
an organized hierarchy 
of many levels: physical devices, 
electronic circuits, 
logic circuits, register-transfer 
systems, pro- grams, and PMS systems. We must remember that these are levels of description for what, after 
all, remains the same physical system. 
Each higher level describes 
more of the total 
system, but with a loss of detail. As this is an engineered system, great care is taken that each level represent adequately all the behavior necessary to determine the performance of the system. In natural 
systems too there are 
often many levels of description (e.g., in 
biological systems, from the molecule to the 
organelle to the cell to the tissue to 
the organ to the organism). However, in natural systems we usually depend on statistics to eliminate the details of lower levels and permit aggregation, 
and they 
always do 
so imperfectly. In computer 
systems, on the other hand, 
the aggregation is intended to be perfect. It fails, of course, and so both error detection and 
error correction exist as fundamental activities in 
computer systems. But these 
imperfec- tions are ascribed to the system itself 
and not 
to our description 
of it, which 
is just the opposite from how we 
treat natural systems. Only the PMS level of description is natural, in the sense of not being the intended result of the design. This 
is because perform- ance is defined ultimately at the programming level. The aggrega- tions and simplifications that go into a PMS description (e.g., measuring power by bits per second) 
are approximations, just 
as they are for any natural system (e.g., measuring 
the productivity of the economy by 
gross national product). We have provided 
descriptive systems for the top levels of the hierarchy: the PMS level and the ISP level, the latter defining the basic components of the programming level in terms of the RT level just 
below. These 
are the two descriptions that are 
of most concern in the overall design 
of a computer system. We did not 
define the lower levels, 
because they 
go beyond the focus of this book. Neither did we define the program level, partly because there exists no 
uniform description (no common programming language) and partly 
because the computer 
designer works mostly at the interface, defining the instruction set. 
This latter is what the ISP pr0vides.l 
'An increasingly popular 
view is that the 
program and 
RT levels (with ISP in between) 
are one, thus erasing the difference between hardware 
PMS and ISP permit the description of an indefinite 
number of computer systems-indeed, all that come within the scope of the current design art. (They might even 
be taken as a definition of what that current art is.) Some lo4 - lo5 individual computer systems have in fact 
come into existence, 
each of which can be described in PMS and ISP. They are not all radically 
individual. There are about 
lo3 types of computer systems represented, if we define two systems with the same Pc to be 
of the same type. (By exercising various options, 
a single computer type could take 
on lo5 different forms.) 
Of these thousand-odd types, 
we present 
in this book just 40.2 What sort of total population do we have here? What does our miniscule sample look like when compared 
with the 
whole? More fundamentally, what 
are the significant aspects of the computer systems that should be used in a comparison or classification? 
These are the 
questions we will try to deal with 
in this chapter. We can be neither 
comprehensive nor 
elegant. There has simply not yet 
been done 
the necessary study on which to base an adequate 
taxonomy of computer systems. Hut we can present a 
rough picture based on 
the common lore of the field, filled in with our own predilections. For any system, either an entire computer, C, or a component, 
such as P, M, or S, it is convenient to distinguish its function, its 
performance, and its structure. The system is designed to operate in some task 
environment; to accomplish such 
tasks is its function. How well it does these tasks is its performance. Evaluation of performance is normally restricted to 
these tasks. Although it is always noteworthy when a system can perform adequately outside 
its specified domain (e.g., when a 
business computer is also 
a good control computer), it is rarely worth noting 
when a 
system cannot perform those tasks 
it was not built to perform. Thus, 
function denotes scope, and performance denotes 
an evaluation within 
that scope. Structure denotes those aspects of the system that allow it to perform. This includes descriptions of its subcomponents and how they are organized. Performance 
of subcomponents often may be considered structure as far as 
the whole system 
is concerned, especially if the performance can be taken as given. For example, early digital 
transmission-oriented telephone lines came in two capacities, -200 bits/sec and -2,000 bits/sec. From the view- point of the telephone system, these are performance measures; and software. The boundary appears 
to us not quite so invisible. We take the important task to be drawing 
the boundary in 
the right place 
for any specific design. 2Counting each of the families in 
Part 6 as one computer. The 
IBM Sys- tem/360 is actually a series. 37 
38 Part 1 I The structure of computers from the viewpoint of a computer system with remote 
terminals, these are structural 
parameters. Typically, design 
proceeds in a context in which the function of the to-be-developed system 
is taken as given 
and certain struc- tures are available; the problem is to construct a structure that achieves adequate performance. These terms 
apply to any designed system. 
For example, con- 
sider automotive vehicles. Function is a classification by use: cars 
to carry people, 
trucks to carry goods, racers to win competitions, antiques to satisfy nostalgia 
and collectors™ pride. Performance is those aspects of behavior relevant 
to function: maximum 
speed, power-to-weight ratio, cargo capacity, run versus not run 
for an antique, and 
so on. Structure is such things as 
number of wheels, shape of the vehicle, stroke volume, 
and gear ratios. 
Structure determines performance, although 
from the standpoint of design, of course, causality runs 
the other way: from 
function to perform- ance to structure. There are, 
then, three main ways to classify or describe a 
computer system: according to its function, its performance, or its structure. Each consists in turn of a number 
of dimensions. It is useful to think of all these dimensions as making 
up a large space in which any computer system can be located as a point. In such 
a space all the thousand computer types built to 
date constitute a sparse scatter, clustering (it is to be hoped) 
in various regions 
that make sense 
functionally and economically. The 40 computer types in this book sample this larger scatter in some way, to give a picture both of the entire space and of the part 
already explored. How many 
dimensions are there in this 
computer space? In- definitely many, 
if one wants to locate 
a computer 
with ultimate 
precision. In 
fact, if one wants to go all the way, one might as 
well give 
the PMS and ISP descriptions (and down through the RT, logic, 
circuit, and device 
levels). The virtue of thinking of such a space 
is to abstract to a small number of dimensions, and to select those that are 
most relevant. Of the functions, one wants 
those that most influence 
the design; of the performance, one 
wants those that make the largest difference; 
of structure those that not only affect 
performance but represent possible design 
choices by 
the computer engineer. In addition, one wants 
dimen- sions along which there is significant variation. Those aspects of computer systems which 
are common to all, 
such as 
the use of binary devices, though 
of supreme interest are not part of the computer space. What are the dimensions of the computer space? As we re- marked earlier, there is no sufficiently comprehensive 
theory of computer systems to tell 
us. Considerable lore has grown 
LIP from experience to date in designing machines. But 
at some point one must simply propose 
a set of dimensions and let them justify themselves after the fact. Table 1 gives our set for function and structure. Table 3 (page 52) gives our set for performance. 
Table 1 gives only 
a single dimension 
for computer system func- tion and 19 for computer structure; 
Table 3 gives 8 for per- formance. However, the dimensions are not all 
independent. Many of the structure dimensions are highly (though not perfectly) correlated. Thus, in Table 1 we have put the structure dimen- sions in seven horizontal groups, with the one at the left-hand side being the most relevant. (In the first structure group, we have also added two temporal dimensions, since 
a strong 
correla- tion with time exists.) For performance, the dimensions form 
a tree structure, 
where the higher dimensions are essentially aggre- 
gate summaries of the lower ones. 
Finally, there is a general 
correlation between overall performance and 
the various structure dimensions, in Table 1, with increasing performance 
as one moves down the dimensions. We have left off two important dimensions because we 
do not have 
values; these 
are reliability (mean time between failures per operation) and physical size 
density (e.g., bits/ft3), both of which increase with generation. With each dimension we have indicated the range of possible values. For some (Pcspeed, for example) this 
is a numerical quan- 
tity. However, for most, 
the range is a discrete set 
of design choices, which 
may or may not have 
a simple ordering. Clearly, 
these discrete values are selections from 
a meaningful subspace of design choices, 
but mostly we do not know how 
to construct that subspace. The values given 
are those that have arisen 
in practice, and they 
serve to classify the computers in the book. Obtaining a 
more rational subspace is a task for 
future research. The body of the chapter 
will be taken up with a discussion of each of these dimensions, 
where we will discuss 
further their 
definition, the basis for 
their selection, and the reasons behind the arrangements of Tables 1 and 3. We give the entire 
set of dimensions here at the beginning, both 
for later reference and to emphasize the view of a single computer space in which com- 
puter systems can be located. We will refer 
to Tables 1 and 3 from now on simply as 
the computer space or, more narrowly, 
as the computer structure space, the computer 
performance space, etc. History Like all systems 
subject to variation 
and selection, computers have evolved through time. So striking and rapid has been this evolution that the 
concept of ﬁgenerationﬂ has become firmly embedded in the computer engineering culture (to say nothing of the marketing culture and the view of the lay public). It is at best an ambiguous term, having none 
of the sharpness of its root 
term in biological evolution, where it is possible to draw a 
strict genealogical tree. 
Chapter 3 I The computer space 39 Nevertheless, the term is useful in stressing 
that the history of computer systems is not just a story of particular men discovering or building 
particular things, but of a somewhat more impersonal 
and widespread series of advances that have changed computer 
systems radically. The generations are best defined solely in terms of logic tech- nology: The first generation is that of vacuum tubes (1945 - 1958), the second generation 
is that of transistors (1958 - 1966), and the third generation is that of integrated circuits (1966~). In fact, current usage describes hybrid 
logic technology machines, 
such as the IBM System/360, as 
third generation, and so this extension must he included. What will be called fourth generation is yet to emerge; most likely 
it will he medium and large scale 
integrated circuits with possibly integrated circuit 
primary memory. 
It is a measure of American industry™s generally ahistorical view 
of things that the title of ﬁfirstﬂ generation has been allowed to be attached to a collection of machines which 
were some genera- tions removed 
from the beginnings by any reasonable accounting. 
Mechanical and electromechanical computers existed prior to electronic ones. Furthermore, they were 
the functional equivalents 
of electronic computers 
and were realized to be such. They were also separated by a wide gap in performance and structure, both 
from each other 
and from vacuum tube machines. Thus, by rea- 
sonable reckoning, we 
are currently in 
the fifth generation of com- puters, not the third. But usage 
is now too well established to 
change. Actually, it was not always viewed thus. 
Figure 1 reproduces a genealogical tree of the early computers prepared 
by the Na- Present ™ generation First generation > Predecessors 5 Roots . . . 
.™ I Fig. 1. The ﬁfamily treeﬂ of 
computer design. 
The remarkable growth of electronic computing systems in the Western world began primarily through government support of research and development 
in the universities. The 
need for data-processing facilities of 
increased capacity inspired 
further support for their development in both educational institutions and private industry. 
The current generation of computers is predominantly 
the result of 
development by private industry. The tree lists many of the machines developed in these ways. At the roots are the contributions of 
many existing technologies to the rapid growth from 
electromechanical to electronic systems. Some 
of the milestones are 
ENIAC (Electronic Numerical 
Integrator and Computer), the first 
electronic computer; 
EDVAC (Electronic Discrete 
Variable Automatic Computer), 
the first internally stored- program computer and first acoustic delay-line storage; 
MADM (Manchester Automatic 
Digital Machine), the first index registers 
(6 lines) and 
first cathode-raytube electrostatic storage; MTC 
(Memory Test Computer), the first core-storage computer. (Courtesy 
of National Science Foundation.) 
40 Part 1 I The structure of computers Table 1 The computer-space dimensions Computer function Scientific Business Control Communications File control (switchinglstore and forward) Terminal Time sharing 
Logic Historical 
Cost/operation technologq Generation date Pc.speed (sec) ($/hit 1s) Mechanical Electromechanical 1930 10-1 1000 (Fluidics) (1970) 10-2 Vacuum tube first 1945 10-3 10 Transistor second 1958 10-5 -1 Hybrid 1964 10-6 Integrated/lC third 1966 10-7 0.1 Medium to large- fourth? 197? 10-8 0.01 scale integrated/ MSI - LSI Word size Base Data-types 8b binary word decimal integer1 address (integer) 
bitlbit vector instruction floating point 
I 12 b 24 b 32 b 48 b character 64b character string 16 b 31 ?' character (6b) word vector character (8b) vector matrix array lists, stacks Addresses/instruction M.processor state (excluding program counter) 0 address (stack) stack 
1 address 1 Accumulator 1 + x (index) address 1 + g (general register) address 
2 address 3 address no explicit state n + 1 address Language determined Compound Microprogrammed accumulator and index registers 
general registers 
array 
Chapter 3 1 The computer 
space 41 PMS structure Switching Processor function 1 Pc 1Pc-nPio n:m 
(time-multiple x) Pc (no io) 1 Pc-nPio-P(display) 
Pi0 
2C (duplex) 2:n (dual-duplex) P.display 
nPc( mu1 ti processing) nPc-P(array1 special algorithm) P.array nPc(paralle1 processing) P.vector move C (network) P.algorithm Network n/2:n/2 (non-hierarchy) P.language Accessing algorithm Mp.size 
Ms.size Mp.speed (b/s) Ms.speed (b/s) l:n (duplex) P.microprogram 
lPc(interrupt) Pc n:m (cross-point) Linear (stack) 
Linear (queue) Bilinear Cyclic-random 
Cyclic Random Content Associative tape (large) disk (medium) 
magnetic card (large)l drum (large) drum (small) photostore (large) 
> 106 core (medium) core 
(smaller) > 107 film (small) > 108 integrated circuit 
> 109 r > 105 Mp concurrency lnterprocess communication 1 program subroutines and traps 1 program with interrupts interrupts 
1 program with 
multiple concurrent subprograms (for 
example, 1Pc-nPio) Monitor or fixed program(M) + 1 program m + n swapped programs m + n programs (multiprogramming) interprocessor interrupts extracodes (programmed operators 
for monitor calls) No relocation 1 segment 2 segments (pure, impure) >2 segments Pages Fixed length, paged segments Multiple-length paged segments 
m + n segments with shared programs intersegment communication 
Variable-length segments Named segments 
Processor concurrency <- Serial by bit Parallel by word Multiple instruction 
streams, 1Pc Multiple data streams (arrays) 1 instruction buffer 
n instruction buffer Look-aside memories Pipeline processing 

42 Part 1 I The structure of computers tional Science Foundation in 1959. 
Notice that the 
Harvard Mark machines, which were constructed 
from relays (hence electro- mechanical) are accorded the place of honor as first 
generation (but Babbage is nowhere to be seen). It is not appropriate to provide here an adequate history of computer technology. The early story has often 
been told, starting with Babbage and early mechanical calculators, through Hollerith 
punched cards, on to the relay calculators at Bell Laboratories and Harvard, up to 
the birth 
of electronic machines with ENJAC, and finally to the stored-program concept with the von Neumann machine at the Institute 
for Advanced Studies 
(IAS), EDSAC at Cambridge University, and EDVAC at the University of Pennsyl- vania (with the contemporary developments 
by ZUSE in Germany 
often left 
out). And there have been a few scattered attempts 
to tell some of the story of the last three generations. But to date no really satisfactory historical 
account has been given. This is due in part to 
recency and in part to the difficulties of evaluating and sorting out the 
significant developments of a very complex technology undergoing rapid growth. 
What is appropriate here is to view the evolution of computer systems as 
measured by 
the dimensions of computer space and 
to localize the examples of this book in relation 
to calendar time 
and other 
computers. The concept of generation has led others 
to attempt the 
same thing by constructing a family 
tree, Fig. 1 being but one example. But the relationships between computers is not nearly 
as simple as such a 
tree implies. We 
prefer to plot a straightforward time chart,™ as 
shown in Fig. 2, in which we group the machines by 
manufacturer and within each group, by ac- knowledged family 
relationship (for example, 701-704-709-etc.). There is clearly relatively closer kinship 
within a company than ‚Whereas we have checked 
the Time Chart numerous times for accuracy, 
we make 
no claim about the nuniber of errors it still 
has. We have relied 
on the following source 
data: (1) Original papers. These 
are mostly shown on the chart 
as ﬁpﬂ. Normally the reader can infer 
that the work pre- 
sented in 
a paper 
occurs prior 
to the actual publication. 
There are notable exceptions (e.g., the core memory, and 
Atlas papers) which 
were first pub- lished to lay claims 
to certain 
ideas. (2) Historical reviews. Primary his- 
torical papers include: 
Rosen [1969] 
and Serrell 
[1962]. Secondary his- torical review papers include: Bowden 
[1953], Campbell [1952], Chase 
[1952], Nisenoff [l966], and Samuel [1957]. 
(3) Encyclopedia. (4) Computer surveys. Two sources have been used: 
The Adams Associates 
Computer Characteristics Quarterly, published since 
1960 [Adams, 1960; Adams 
Assoc., 1966, 1967, and 1968); and Martin H. Weik™s four Surtieys of Domestic Electronic Digital Computer 
Systems [Weik, 1955; Weik, 1961 
(third); and 
Weik, 1964 (fourth)]. The Adams™ Charts give the date 
of first delivery, and the Weik Survey 
gives the date the 
computer was first operating. (5) Manufacturer, organization or person supplied dates. 
In a 
few cases we 
have asked directly 
for sDecific oeerational and delivery 
between companies. One advantage of such a 
time chart 
is its depiction of the life history 
of a single system, showing 
how long it takes for computer systems to go from paper through prototype 
to production. Not all 
computer types are shown on the chart, there 
being about 250 out of the estimated 1,000 types. Lack of space (and 
of perseverance) accounts 
for the omissions. The major United States manufacturers, 
as well as some minor 
ones, and all ma- chines of substantial historical interest 
are represented. All the machines discussed in 
this book are gathered together on a sep- 
arate line (though they 
also occur elsewhere, if appropriate). Foreign machines are omitted, 
unless they are described in 
this book. In addition, the machines of many early minor manufac- 
turers are missing (ALWAC, ELECOM, etc.). The second part of the time chart arranges many computers by word size, 
to give the reader our classification. Unfortunately, only a few samples 
are given, owing 
to space limitations. Thus, the density on 
the graph does not indicate 
the true density of existing machines. Many 
small computers, which are dedicated to 
a particular task, are beginning to be built and a comparatively small number of very large computers 
have been built. On the bottom fine line we place the machines in this book. 
The third part of the time chart 
deals with technology by 
listing events 
along various dimensions 
that have been significant in the evolution of computers. Besides the dimensions in the computer space we have also added some dimensions describing 
software systems. Although we have not been able to deal with 
the programming level 
in this book (except for the ISP interface), its development 
is clearly as important as that of the hardware, and there exists strong mutual interaction between 
the two. The fourth (and final) part of the time chart 
gives selected technological events leading 
up to the 
development of the com- puter. It includes 
the early work of Babbage, desk calculators, and the Bell Labs and Harvard calculators. 
Many stories 
can be read from the chart. For example, note that the early Bell Telephone Laboratories 
relay calculator was used remotely at Dartmouth in 1940, 
about 20 years prior 
to remote use of time-shared computers. 
Note also that successful manufacturers tend to have a small 
number of computer families, but add members as the technology dictates. (We 
omit the exodus of computer companies.) 
We hope the reader gets as much en- 
joyment from browsing the chart 
as we have (even after 
we put it together!). The computer space in 
Table 1 and the time chart 
in Fig. 2 
provide an 
overall framework. We 
are now ready to consider each of the dimensions individually, 
starting with those of system func- information. tion, then the 
performance, and finally structure. 
l9:2 1343 l9t4 19145 l9i6 IC7 19148 19149 iq50 1251 i9,52 1253 1954 l9,55 
19,56 I957 l9,58 I959 1960 1961 1962 1963 1964 1965 1966 
1967 1968 1969 1970 
SOS/Scientific Data Systems DEC/Olqifal Eguipmevt Corporation CDClControl Data Corporation GE/General Electric Honeydel i Burroughs RCA/Radio Corporation of America POP-10 Ir 9100 k (36 b/w) POP-6 Iflme shared) - HIT LlNC based POP-8 LIN-8 Q~P-RIZ_PDP-~/~ PDP-B/L (12 b/wl k POP-5 - k PDP-4 ,PDP-7 - POP-9 - -p~P-15 [ (18 b/w) PDP-id -6400 6500 
7600 ku Large Scale Scientific (60 b/w) k 
6600 17?0(16 b/wr.- k 3200 Lioo33no -3500 k 3600 -3400 - 3800 - -- 160A 160G 
,8090,8092 (24 b/w) ~t (12 b/wl ku 160. = - 148 b/wl ku 1604. 1604A. - -- I36 b/wl 
k 635,645,62543pp 412 (24 b/wl. - - - - 215 235 
205 (24 b/w) 4040 4050.4~6OL4O20 b/w) -I - GE 210 16 d/w) 4050 I I 1 [ -- GE loo ERMA (7 d/w) e-. 'I: -2200,1200,120 ,4200-8200 200 series (6 b/char) IBM 1401 based 2oo - -224-124 (24 b/w) - - -316 l4OO-ldO - - - ku 800-400 Datamatic 1000 (12 d) .(48 b/w) - NOTE: not a family a+- -WRPw- Computer Controls Division , (48 b/w. stack. muitiproce5sor) 0-5000 0-825 
6000 85000 05500 88501 B6500? 88501 Bu5ine55 (8 b/char) ~B2500- B2501'B3500 (6b/char) k 8250 m- - , ,8300 0260 8263 8160 - 8270 E213 8170 ir (12 d/w-pluqboaid program) EIOl* E102* E103* 
- 8280 8283 0180 - 220- I10 d/w) k 204,205. oatatron Division ~~~ ~. .~ LCPC* 607" - 604* 609* 
608*.610* 6400 
- ,360/195 STRETCH 170301 164 b/w) - (Larqe Scale) (Accounting machines-ca:culators) Scienci f i c Business Hachines - ERA/Engineering Research ASSOL. 
(SciPntl I UNIVAC UNIVAC ;I (12 b/w) UNIVAC IlL(27 b/w, b b/char) I050 (30 b/w-bcd) (0~~111255) bM/Eckert-Mauchly (Business) I - - "fC* =%ivAc  ice university ,RICE MUSE +ATLAS ATLAS Manchester University MARK I (index registerslB-tvber) ATLAS-LATLAS-2 Feiranti Corporation NPL/National Physics Laboratory 
5-! and ACE Based Machines DYSEAC NBS/NafionalBureau of Standards SEAC (:tudV'(cirCYi~~~- NIT Lincoln Laboratory "TC/Memry ,e,', Computer -TX-o ceG24 - = ACE wlis Electric DEUCE ndix G-15 (one level extra codes)- - SERC _FX-I LINC/Laboratory lnstrumnt Computer PhW (cons t rUCt (tape,drum) (core memory) (50mhT10gic)-in Operation 
at Wolf R and D z -a Whirlwind I <(EDSAC bared) ~~~d ~~~~~~~~i~~ JQHNNIAC (tubes. selecfron mmor?) lmagnetlc core) (transistors for arithmetic element1 UniverSlty of 
Chicago MANIAC I - - I1 (Not IAS compatible) 111 - e -- P- University of Illinois ORDVAC (for BRL - ILLIAC I (also SILLIAC, CYCLONE, ILLIAC I1 ILLIAC 111 ILLIAC IV 
at lAs+(Burkr, Goldstine and vonNeumann) WElZAC and MISTIC from same design) (not 1;: based) (Solonan based) - - ,EDSAC I I *s "on Neumann DT IAS Based Cambridge University EOSAC (Willies) University of ~ennsy~vanIa 
(Moore School of Electrical Engineering) EDVAC -(Eckert, Hauchly and yon Neurnann) a Announcement for sale 51 Scheduled . Delivered first w WI thdrawn -P 0 Operational k Reasonably compatible series Bell TeleDhone LaboratoriTs * Non-stored Program Calculator Haryaard Universyty 'HARK I* MARK-lI* HARK 171. MARK IV* 5 Project Started ENlAC* '--- i (patent for e~ectdc circuits) - p Paper ku Upward compatible It* lit* IV*(BalIistici v* VI* Leprechan (transistor) - - - - 
- 15'42 14.43 194 1945 19.46 1947 If48 15149 1950 1951 is152 lm3 19% i5'55 lh6 1g57 l%8 15'59 1%~ 19'61 1g62 $63 184 1%5 i$66 19x7 1968 Is9 1970 Fig. 2a. Time chart: computers by originator. 
24-48 16-24 12-16 i-10 CHARACTER STRING BUSINESS 
DECIMAL WORD BUSINESS SHALL EARLY 
SCIENTIFIC E _- . _. , !940 !941 :942 1943 !944 1945 1946 
1947 194R 1949 1950 1951 
1952 1953 
1954 1955 1956 1957 
1958 1959 1960 l961 1962 1963 1964 
1965 1966 1967 
1968 (969 1970 :. 2b. Time chart: computers by word size. 
I i SOFTWARE LINES OPERATING SYSTEMS DISCRETE SIMULATION LANGUAGES .IST PROCESSING/STRING MANIPULATII ALGEBRAIC MANIPULATION LANGUAGES ALGORITHMIC LANGUAGES ASSEMBLERS. LOADERSiDERS 
HARDWARE LINES MAPPING Pc CONCURRENCY Pc FUNCTION PMS STRUCTURE SECONDARY MEMORY HEMORY TECHNOLOGY PRIMARY MEMORY (5 i ze;wi d t h ; t i me) SPEED (LOGIC TECHNOLOGY) 
46 Part 1 I The structure 
of computers Analytical i Charles Babbage Difference (1792-1871) ~~~i~~ . card controlled Hullers Difference Engine 50 digits/wordl (100~ wordr, 0- +-FIRST GENERFT I ON+SFCOND+TH I RD- Bell Telephone Labs I I/ Ill IV v VI ...... VACUUM TUBES -. 21 A: zi RLEGRAPH -- %j MFCHANICAL MEMORY j ELECTRO- ELECTROMAGNET. TELEPHONE. u; ................................................................................ I I I I I Fig. 2d. Time chart: pre-computer technology. 
I I I I I I 35 40 L5 50 55 60 6: 70 I 9 Operational p Paper Function The most striking fact 
about function is the existence of only a single dimension, and with only a 
few values. Perhaps we have taken a simplistic view 
of the functions that computers perform, but we think our 
computer space 
represents reality: 
To wit, tkere is remarkably little shaping 
of computer structure 
to fit the func- tion to he performed. At the root of this lies the general-purpose nature of computers, in which all 
the functional specialization occurs 
at the time of programming and not at the time of design. However, it might seem that specialized environments would not 
require all the gen- erality, so that functional adaptation would still be possible. But 
this appears not to 
be so for two reasons. First, the level of opera- tions of the Pc (as defined in 
the ISP) is too basic to reflect the kind of specialization offered by the environment (think of infor- mation-transfer or conditional-transfer operations). 
Second, all environments ultimately 
require a variety of tasks in addition to 
the main specialized 
task. These include at least language com- 
pilation or 
assembly, readable formatted 
output, debugging aids, and other utility routines. 
By the time these have been added, a substantial requirement for generality has 
been generated. However, this 
is not the whole story. 
A second part 
is the differ- ence between the computer type and the specific configuration 
assembled for a 
task. The latter is often carefully specialized 
to the function to be performed. But this is mostly the amount of Mp, the amount of types of Ms, and the number and types of T's. Within limit?, these 
are all items that can 
be attached to any type of computer (i.e., to any Pc) 
and are 
handled in 
an environment- independent way. Thus there 
is little specialization of computer types, but great specialization of particular configurations. That this should be the 
case indicates something 
about the nature of the functional specialization-that 
it can 
be expressed adequately in gross PMS 
terms, as more bits of storage and more data rate. 
There is still more to the story. Some 
functional specialization 
exists, as indicated in 
the dimension. This depends 
primarily on two kinds of things beyond 
the reach of the configurational adapta- tion described 
above. The first consists of demands for reliability, ruggedness, small size, 
etc. These have strong 
effects on design, 
but below the ISP and PMS levels. The second consists of demands for large amounts 
of processing power. One response to this again 
affects design 
at the lower levels of logic, devices, 
and circuitry and has little impact on design at the ISP and PMS level. But response is also possible in terms of the data-types that are 
built into the ISP. Large machines have data-types 
that are appropriate 
to their tasks (with operations to match), 
and these affect the 
Chapter 3 I The computer 
space 47 design. In fact, 
this effect is the substance of the functional spe- cialization shown in the computer-space dimension. Finally, there is one last part of the story, and it is the most interesting of all. Various groups of computer engineers have felt 
strongly from 
time to time that functional specialization should exist, and they have 
set out to create such machines. These efforts 
have often produced machines 
that were different from 
the exist- ing main line of computers, i.e., were appropriately specialized. But the net effect of almost all such 
attempts has been 
that the 
new idea 
was seen to be 
good in general for all 
computers and was taken back 
into the 
main line of computers. Thus, 
what started 
out to be 
a functional separation 
turned out to be 
simply a way to produce rapid development 
of a more 
universally applicable computer. A classic example 
is the expansion of input/output facilities in 
creating a functionally specialized 
business machine, which simply led to better 1/0 facilities for all 
computers. We will have more to say about such examples 
as we discuss the values along the dimension. Computer-system function Scientific. The first machines were clearly designed for scientific 
calculations. In fact, 
Aberdeen Proving Grounds 
funded the early work on 
the ENIAC for the computation of ballistic firing tables. 
And the image used 
frequently by the early computer designers was the computer as a statistical clerk, 
the arithmetic unit 
being the desk calculator, the memory the work sheet, and the program the instructions that the 
mathematician gave to the clerk. From a design standpoint, scientific computation has posed two 
striking requirements. The first is the great accuracy of the num- bers, which has led to word lengths of 36 to 60 bits (11 to 18 decimal digits of significance) and arises from 
the propagation of roundoff error during repeated arithmetic 
operations. The second is the emphasis on fast 
arithmetic operations, i.e., for arithmetic power. In the 
early machines the standard rule for estimating computation times was 
to count the number of multiplications in 
a program; 
all else 
could be neglected. The arithmetic unit 
has developed to where the floating point multiply 
is hardly more 
expensive than floating point add. This requirement on fast arith- metic, however, 
has really 
been directed 
at the logical design 
level, not at the ISP or PMS level. Thus, the main effect 
at the ISP is the adoption of long word lengths, floating point data-types (in 
addition to integers), and an 
extensive repertoire of arithmetic operations in the ISP. The main PMS effect is the emphasis on the classic ﬁstatistical clerkﬂ PMS 
design. The press for increased arithmetic processing has led in 
recent times to the 
development of various forms 
of Pc concurrency, as in the look-ahead of Stretch (Chap. 34) and the n-instruction buffer of the CDC 6600 (Chap. 39). This might 
be considered a unique 
functional specialization 
for scientific 
computation. It is too early to tell, 
but it is our impression that, although the needs for sci- 
entific computation initiated the exploration of concurrency and parallelism, we will eventually see them in all 
computers above a certain power, whatever 
the task domain. Physical limits on 
component speed 
and signal propagation will make these 
tech- niques universally attractive. A better case for permanent specialization can be made in the special algorithm computers, which compute the fast Fourier transform or 
do vector operations. Here we finally have systems whose whole design 
is responsive to a narrow 
class of problems. This may extend to the 
very special kinds of Pc parallelism exhib- 
ited by the ILLIAC IV (Chap. 27), although there is substantial generality in 
such systems. 
Business. In the early days of electronic computing it was felt by many that there was a major functional separation 
between busi- ness computing and 
scientific c0mputing.l Scientific problems 
were ﬁlarge computing-small input/outputﬂ; business problems 
were ﬁsmall computing-large 
input/output.ﬂ Certainly most of the existing computers, designed for scientific computation, had poor input/output facilities. The IBM 701, for example, used 
the Pc 
to control everything dynamically, 
actually catching the 
bits from running tapes on the fly (by executing well-timed 
small loops). 
These design efforts for business 
computers resulted in 
the IBM 702 (and subsequently the IBM 705,708, and 7080). This 
machine had two major innovations for IBM: 
It used characters, and it had 
a PMS structure that permitted 
more flexible 
and voluminous input/output. The latter feature 
was immediately incorporated 
into scientific computers, e.g., into the 
709, and then into all large 
scientific computers as separate inpnt/output control (either Kio or Pio), for it was realized that there were also demands on input/ output for scientific 
calculation. Thus the bifurcation was tempo- rarily halted. The specialization to 
characters as a basic type (as opposed 
to long words) was 
already present 
in the IBM 702 but did not have its effect until 5 years later with 
the development of the IBM 1401 (Chap. 18). The latter 
machine was adapted to 
business, both in being character-based 
and in being small enough so that small businesses could afford it. It was extremely successful 
(many thou- 
sands were produced) and 
certainly represents a 
successful func- ‚Such feelings are still extant, but we are concerned here 
not with 
the validity of the feelings but with what they 
led to at a particular 
period of computer development. 

48 Part 1 I The structure of computers tional specialization 
for business. However, 
it is interesting that the specialization has not 
been maintained, for the IBM Sys- tem/360 (Chaps. 
43 and 44) is again 
a single machine, although 
it has in essence two internal 
ISP™s, one centered around characters and the other around 
floating point data-types, that is, a business and a scientific specialization residing side by 
side.l Control. The third functional value is a computer used for 
control in real time. 
Examples are process-control computers, aerospace 
computers, and laboratory instrument-control computers. 
The role of the computer is to act as a sophisticated control 
(K) in some larger physical process, 
and thus it plays a subordinate 
role. Their relatively late arrival was due to the 
high cost 
and unreliability of early computers, as well 
as to the lack of necessary interface equipment. The functional specialization is seen most strongly 
in the word size, which reflects 
the appropriate numerical data-type. The 
numbers used in control 
processes are generated by 
physical de- vices and 
are rarely better than 0.1 percent accurate. 
Since elab- orate arithmetic 
calculations are not 
called for, the numbers, and 
hence the 
word size, 
can be around 12 bits. Most control com- puters have 
been 12 to 18 bits/word. A second specialization, again reflecting appropriate data-types, is that all control computers are binary and have boolean 
operations. This arises 
because many of the external conditions to be 
sensed and effected are 
binary in nature. About the only other functional specialization of control com- puters is the interrupt2 capability to 
allow them to respond to many potentially 
simultaneous external conditions in real 
time. This provides 
apparent parallelism, though 
still using a sequential 
processor. This 
is another possible example 
of functional speciali- zation leading 
to reunification rather than 
divergence, for it has again been widely accepted that all general-purpose computers 
must have good interrupt capabilities. However, in actuality, interrupts, though not 
existing in early 
computers, were developed 
to obtain good input/output facilities, not for control computers. Chapters 7 and 29 give examples 
of aerospace computers, 
and Chap. 33 describes the IBM 1800, which 
is specifically designed 
for process 
control. As these examples show, 
a complex ISP 
is not lThe story above has been told 
exclusively in terms of IBM machines. Although this does not distort the picture too strongly in terms of total movements of the field, since IBM dominated the market, concurrent 
developments were taking place throughout 
the field. UNIVAC I was the first computer built by a manufacturer and did 
not have the idiosyncrasies we ascribe to IBM; on the other hand, 
the marketing effort for it was nil. *Apparently introduced 
in the UNIVAC 1103. necessarily required. This in part reflects the fact that control computers may retain their 
programs over 
their whole lifetime, so that programming and reprogramming is less important. (It is not absent, however, and so this is not a very strong functional 
adaptation.) Communication. The functional specialization of communication could be taken as a subfunction 
of a control computer. The function is mainly to behave as a switch. In a message-switching application the computer transfers messages from 
terminals (and links) into primary (and 
sometimes secondary) memories and then transfers them to other terminals (and links). In message switching, messages are first stored and then 
forwarded. The computer in a telephone 
exchange functions as a very sophisticated switch 
control. Here the computer reads the off-the-hook signal, 
detects the 
dialed numbers, rings the dialed parties, and finally sets 
the switches to connect the telephones together. In some instances, 
when it an- swers information inquiries about new telephone numbers or re- 
routes calls to other phones, it functions as a memory. Thus a 
communications computer is functionally a switch 
or a control for a switch. The main distinction between control computers and 
commu- nications computers is that the 
task environment of the latter, since it consists of digitally encoded messages (even in the case of the voice telephone exchange), can 
be handled directly by the communications computer. That 
is, the communications computer can do the work of transshipment and storage as well as control. There are 
no pure examples of communications computers 
in this book. 
However, the Pio™s serve essentially the same function within a 
single computer (Part 4, Sec. l), and they can profitably be examined from this viewpoint. 
File Control. We list this as 
a separate specialization only because a number 
of computers have 
been built 
to do exactly this task. 
The specialization is easily described: It is a communication 
com- puter with the messages being characters (since they are built for business), and with the large memory 
(the file) being considered 
to be 
part of the system. There are no examples 
of file-control computers in this book, 
but the early IBM 305 and UNIVAC file computers serve this function. 
An IBM 
1800 is used as the control for a 1012-bit photo-optical memory, for example. 
Terminal. Since 
it is possible to obtain a 
separate computer 
system whose only function is to run a display, we have 
listed this as a separate functional specialization. In fact, 
it is better viewed (and almost always occurs) as 
a component 
of a larger computer system, 
Chapter 3 1 The computer 
space 49 i.e., as a 
special Pio. 
The DEC 338 is such a P.display and is described both later 
in this 
chapter and 
in detail 
in Chap. 25. Time-sharing. The requirement to have 
a large number of users in simultaneous conversational 
interaction with 
a single 
large machine has bred a new specialization, that of the time-sharing computer. All the computers described above 
can be 
time-shared (even if they do not have interrupts 
or inherent multiprogram- ming). However, the emphasis on this mode 
of operation with the particular timing and flexibility requirements of human users doing general computing at consoles in 
multiple software systems has led to a number of innovations in design. 
The most important is the virtual-memory techniques for achieving multiprogramming 
(described in Part 
3, Sec. 6). There is also substantially increased 
complexity of PMS structure to handle the integration of large files, swapping memories, and the huge software systems that seem to be endemic to time-sharing systems. It is still too early 
to tell whether any 
of the design responses will 
produce permanent 
spe- cialization or 
will again simply be the 
first instigation of design features that will become universally used. 
In summary, we 
see that there 
is functional specialization 
and that it translates mostly into total size of the machine and into 
the data-types available. Many 
of the other design aspects created in response to functional 
specialization have instead become the common property of all machines. 
Performance For a device that does a complex job, it is meaningless to ask for a single 
precise index 
of performance. It is like asking for 
the average speed of a given model 
of car over its lifetime 
without specifying who will own it, where 
he will drive it, and 
what sort of terrain he will encounter along the way. Notice that the diffi- culty is as much in the complexity of the task environment as in the complexity of the internal workings of the machine. Specify everything about the environment, and the performance can often be given in a single 
figure. It may be hard to determine, but at least it is well defined. 
If you know 
the terrain and road conditions 
perfectly and how the car 
was driven, 
then from the structure of the car it is possible to figure out the instantaneous velocity 
and from this to construct the average speed. To put this in 
terms of computers, given a 
particular configura- tion for a 
computer system, given a 
particular program, and given a particular set of input data, 
it is possible to determine all 
aspects of the performance: how long it took, how much 
space was used, 
whether it was correct, and so on. But we are 
not interested in 
such specifics. We want to know how well the computer system performs, given some 
vague notion 
of the kind of task-programs and data-that will 
be used with it. Although we know that we cannot have adequate measures, we believe 
that there is something that can be said about the performance-that tells us that a CDC 6600 is many times more powerful 
in actual 
performance than a PDP-8. An interesting way 
to look at the problem of specifying perform- 
ance is to play a simple game: We 
will give you a 
number, say 4. You are to give the best description of computer systems involv- 
ing only that many parameters (equivalently, dimensions 
or attri- butes). That is, what is the best description 
of a computer that 
can be stated in four numbers? 
The game is easier to play if we speak of the dimensions, rather than the information content of the description (in bits, say).™ \lie have still not defined ﬁbest,ﬂ of course. It can be taken to mean the best prediction 
of the relative ordering 
of the computer system; better on the index means better on the same task.2 To start at the beginning, what single number would you give 
to characterize a computer™s power? Such 
a question makes most 
people uncomfortable, since 
strong feelings exist 
for at least two kinds of numbers, dealing 
with speed and memory, respectively. 
If forced, we would probably 
settle for something related to proc- essing speed. The cycle time of the primary memory 
is a possibility 
because for simple machines 
it determines 
(limits) the operation rate. It is a structural parameter, 
but that 
is no reason to avoid it as a 
performance index. The average number of instructions per second, or operations per second, is a better indicator. Since the latter does not take into account 
the size of the word being proc- essed, perhaps average bits 
processed per second is the best single number. (We measure this 
number at the processor, and it may include both the 
instruction and data streams.) To take an 
average we must adopt some weightings. The sim- plest scheme 
is simply to add all the instruction (or 
operation) times and divide by their number. 
This is equivalent to weighting them equally, the rare ones and the 
common ones. If we want to do better than 
that we need some data. Several sets of relative frequencies, of instruction types, called 
ﬁmixes,ﬂ have been 
used in the literature. Table 2 gives four examples. The Gibson mix is ‚It is not fair, of course, to invent tricks to encode many conceptually independent dimensions into a single one, 
just to beat the limit. 
On the other hand, 
composite dimensions, 
such as average operation time, are perfectly acceptable. 
2Definitional precision is not appropriate, since we are not attempting to deal seriously with 
the technical questions of indices, only to illustrate the issues. 
50 Part 1 I The structure 
of computers Table 2 Instruction-mix weights for evaluating 
computer power 
Arbuckle [1966] Gibson‚ Knight (scientijic) Knight (commercial) Fixed + / - X Floating + / - Floating x Floating + Load/store - ... ... 9.5 5.6 2.0 28.5 6 3 1 25 (move) Indexing 22.5 Conditional branch 
13.2 20 
Compare ... 24 Branch on character ... 10 Edit ... 4 1/0 initiate ... 7 Other 18.7 ... ‚Published reference unknown. ‚Extra weight for either indirect addressing or index registers. probably the best known. 
The best source 
for such 
data comes from instruction counts of running programs. Knight takes the view (Fig. 3) that a single 
number can be used to indicate power, and his formula has been evaluated 
for some 300 computers [Knight, 19661. His formula is the product of three factors: processing time, memory size (in words), and word length. The formula was derived (roughly) 
to measure power 
so that technological change could be modeled. Applying 
the formula is like measuring automotive-vehicle power 
as a product of speed, weight, and the 
number of wheels. (Such 
an indicator 
is roughly proportional to a car™s 
momentum.) Thus, 
although it is a reason- 
able single-number 
indication for power, a computer buyer could not use it directly. Taking averages, as 
in the case of mixes, suggests a 
more sophis- ticated approach. A collection of programs, called a ﬁbench mark,ﬂ is developed that does a 
variety of different tasks. Then the one number is the time it 
takes to do 
this collection. Such 
a bench mark generates its own frequencies of occurrence of the primitive instructions. It brings in a number of additional dimensions that affect performance: the instruction code, 
the size of Mp, pro- gramming skill, input/output devices, etc. It also carries with it an implicit frequency of different kinds of task demands (how much of the set involves compiling, how much number 
crunching, how much 
I/O, etc.). There are 
severe practical problems in carrying out such meas- urements on many computers, since the problems must 
be coded and run on all the systems. It is somewhat easier 
if the task set 10(25)* 6 2 10 25(45)2 1 72 74 is restricted to programs coded in a 
procedure-oriented language, 
such as FORTRAN, where all computers accept FORTRAN. Nevertheless, although it has often 
been done 
to compare two systems, only occasionally 
has it been done 
for even 
a modest 
number. We feel that for a 
general-purpose computer the com- piler-derived bench mark is a reasonable single-performance 
number. Much actual use will 
be with the compiler, and good compilers produce code to 
rival hand coding, so that special fea- tures of the machine are 
utilized. Cox [1968] compares several, 
using hand coding and compilers for several tasks. There is a difficulty 
with the bench-mark scheme that is inher- ent in its strongest 
advantage, that of doing a total problem and thus integrating 
all features 
of the computer. The number obtained depends not 
only on the type 
of computer, for example, an IBM 704, but on the exact configuration, for 
example, 16 kwords of Mp versus 32 kwords, and even on the operating system and the soft- ware (which 
version of FORTRAN). Thus, 
although the number perhaps comes 
closest to an adequate single-performance figure, it becomes much 
less of a parameter characterizing 
the structure of the computer than one characterizing 
a contingent total system. Let us underscore again 
the distinction between the computer type and 
the particular configuration (possibly 
including basic software) assembled in a particular installation. Computer systems are designed with certain forms of variability. To speci~ a CDC 1604 is to specify many things, such as the ISP of the Pc, the cycle time of Mp, the K™s used to control secondary memories 
(Ms), and interfaces to the external world. 
But it leaves open many other 
Chapter 3 I The computer space 51 ~'iiriiihli~s~nttrihutes of each computing 
system P L T t, = the computing 
power of the nfh computing system = the word lengths (in bits) 
= the total number of words in memory = the time for the 
Central Processing 
Unit to 
perform 1 million operations = the time the 
Central Processing 
Unit stands idle waiting for 
1/0 to take A, = the time for 
the Central Processing 
Unit to perform 1 fixed point addition 
A, = the time for the Central 
Processing Unit to perform 1 floating point addition 
M = the time 
for the Central Processing 
Unit to 
perform 1 multiply D = the time 
for the Central Processing Unit to perform 1 divide L = the time 
for the Central Processing Unit to perform 1 logic operation 
B = the number of characters of 1/0 in each word KI1 = the Input transfer rate (characters per 
second) of 
the primary 1/0 system Kol = the Output transfer rate (characters per 
second) of the primary 
I/O system KIP = the Input transfer rate (characters per 
second) of 
the secondary 1/0 system KO2 = the Output transfer rate 
(characters per second) of the secondary 1/0 SI = the start time of the primary 
1/0 system not overlapped with compute HI = the stop time of the primary 
1/0 system not overlapped with compute Sz = the start time of the secondary 1/0 system not overlapped with compute Ha = the stop time of the secondary 1/0 system not overlapped with compute R1 = 1 + the fraction 
of the useful primary 1/0 time that 
is required for non- place system overlap rewind 
time CP c3 c4 CS P WIl WOl ~ ~ Semi-constant factors Values Scientific Commercial 
Symbol Description computation computation WF the word factor a. fixed word length memory 1 1 memory 2 2 
b. variable word length c1 weighting factor representing the percentage of the fixed add operations a. computers without 
index registers or 
indirect addressing 10 25 b. computers with index registers or 
indirect addressing 25 45 Fig. 3. Knight's functional model algorithm 
to calculate P for any com- 
puter system. (Courtesy 
of Datamation, vol. 12, no. 9, September, 1966, page 42.) weighting factor 
that indicates the percentage of floating additions the percentage of multiply operations the percentage of divide operations 
the percentage of logic operations 
percentage of the 1/0 that uses the primary 
1/0 system a. systems with only a primary 1/0 system b. systems with a 
primary and secondary 1/0 system weighting factor 
that indicates weighting factor that indicates weighting factor 
that indicates number of input words per million internal 
operations using the primary 
1/0 system a. magnetic tape 1/0 system b. other 1/0 systems number of output words per million internal 
operations using the primary 
1/0 system per million internal operations using the 
secondary 1/0 system number of times separate data 
is read 
into or out of the computer per million operations overlap factor 
1-the fraction of the primary 1/0 system's time not overlapped with compute a. no overlap-no 
buffer b. read or write 
with com- pute-single buffer c. read, 
write and com- pute-single buffer d. multiple 
read, write and compute-several buffers e. multiple read, write and compute 
with program interrupt - several buffers overlap factor 2-the fraction of the secondary 1/0 system's time not 
over- lapped with compute number of input/output words the exponential memory weighting factor 
10 0 6 1 2 0 72 74 1 .O 1.0 variable variable 20,000 100,000 2,000 10,000 the values are 
the same as those given 
above for WI1 the values are 
the same as those given 
above for WI1 4 20 1 1 .85 .a5 .7 .7 .60 .60 .25 .55 values are 
the same as those given 
above for OL1, a through 
e .5 ,333 
52 Part 1 I The structure of computers things, e.%., 
the types and sizes of Ms and the size of Mp. On some computers it can even leave open 
part of the ISP (e.g., 
the multiply/divide options 
on many small machines), 
or the speed of the Pc and Mp 
(e.g., in 
the IBM System/360). When we ask questions about computer systems, we should be clear whether we are talking about a computer ﬁtype,ﬂ 
such as CDC 1604, or 
whether we are talking about a particular 
installa- tion, with all the variability specified. It is possible to describe either with PMS and ISP, provided 
we recognize that the 
diagrams for the types represent 
maximal possibilities for assembling 
par- ticular systems. This 
is how almost all 
the PMS and ISP diagrams in this book 
were prepared. From the point of view of our ﬁnumber 
game,ﬂ if we are talking about computer types, we 
might prefer numbers that do not depend on the particular configuration. If two numbers were 
available for describing performance, 
what would they be? Clearly there are 
several directions to 
go. One could fractionate the bench mark, so that one has a bench mark for 
arithmetic-rich tasks and a bench mark for 
others (a composite of compiling and data 
processing). One could decom- 
pose the processing rate into, say, operations per 
second and word size (from which 
bits per second can be recaptured approximately). Alternatively, one could retain only a single number for processing 
rate and add 
a measure of the memory available, 
e.g., size of Mp (in bits). 
Of the three 
we would choose 
the latter, 
especially if we were talking about a particular installation rather than com- puter types, for which Mp size remains variable. We can continue 
this game 
through several numbers. Table 
3 shows some 
of our choices. Various parameters drop out or change only when they are decomposed into other 
parameters from which they can be recovered. Thus, 
initially Mp must be measured 
into bits, but when the word size is given, Mp is more reasonably measured in words. One of the reasons for exposing such 
a list is to emphasize its judgmental and approximate 
character. There 
is as yet no way to validate 
such proposals for brief 
descriptions. Table 3 Performance parameters 
specification (as a 
function of an allowable number of parameters) If we had bench marks, which 
are themselves only approximations 
at measuring performance, we might look at how well the param- eters in Table 3 predict the bench marks. But 
there remain the difficulties of how to take into account the additional aspects of the total 
system (e.g., compiler efficiency) that are implied in the 
bench mark. Alternatively, one might want to construct a 
mixed description of bench-mark numbers 
and measurements of the kind in Table 3. Then the relationship between bench marks and these 
other measurements would become an indirect measure of the efficiency of the rest of the system. We have discussed 
performance in a crude and 
cavalier way, but this accurately reflects the state of the art. There 
are no precise measures for 
performance. There are 
precise structure and 
per- formance measures of individual components (e.g., memory size, and speed and word length, and processor instruction times). When designers (and users) are faced with obtaining a 
certain total 
performance for a given cost, the only method is that of the bench mark, because the task is such a significant variable. If performance is to be increased, unless the task is sufficiently trivial, it 
is difficult to predict what 
effect changing even 
the most direct structural 
variables will 
have (e.g., memory speed). Structure We now turn from function and performance, which 
provide design constraints and objectives, to the 
dimensions of structure, which provide the space in which the design is actually cast. 
A structural dimension is one in which the designer can attain any of the values along the dimension by 
relatively direct means. Thus a machine 
is completely specified by listing all its values along the structural dimensions. From this, 
the system™s function and 
its performance within 
that function can be determined. What dimensions should 
be selected for structure? The view- point is distinctly different from 
that of performance, where one 
Number of parameters allowed: 1 2 3 4 5 Parameters: Pc(i.rate:(b/s)), - Pc(operation-rate:(op/s))-+ Mp(size:(b))- Pc(i.width(b)) > -Ms(i.(words)) BT > > 
Ms(size:(b)) Mp(i .(words)) 
Chapter 3 I The computer 
space 53 averages and combines many features to 
summarize effective out- put. This tends to obscure structure. For structure, one wants maximally independent aspects which are easily obtained if se- lected as a design choice. For example, if the computer designer had only a single dimension to describe a 
computer, he would undoubtedly select the logic technology used 
in the Pc and Ks. This tells him a good deal about 
many aspects of the computer's structure. In fact, 
the technology and the average bits processed per second by the Pc 
are correlated, and so each can be used to predict the other, though only imperfectly. If one is interested in performance, effective bits per second 
is preferred; if one is interested in design, technology is preferred. The computer space in Table 1 presents our choice 
of the major structure dimensions. There is even less means to validate 
the choice of dimensions here than there is for performance. Never- theless, there are 
a few hallmarks. 
Perhaps the most important is redundancy (the opposite side of the coin from independence, mentioned above). Several dimensions 
of structure may covary, so that giving any one 
of them is tantamount to giving the others. This covariation need 
not come from physical 
dependence; it may arise from 
the nature of an appropriate design and good engineer- ing practice. Such a cluster of covarying dimensions 
is likely to indicate an important dimension (which one among 
the correlates is to be 
used is a secondary 
matter). Table 1 is organized in terms of such clusters, with one of each selected as the main representa- 
tive and placed at the 
left. A second hallmark derives from the hierarchical nature of computer systems. Generally a description 
of a system consists 
of the union of the description of its parts, plus a description 
of the interconnections. This is the basic style 
of PMS, for example. But 
there are a few features that affect the total system, Le., affect many components. These 
are usually rather important. 
Technology is a prime example. Yet a third clue is that the 
dimensions discriminate the actual population of computers. If all machines 
had single-address in- 
structions, for instance, there would be no sense 
in using number of addresses per instruction as a dimension. Any computer engineer who had studied 
machines at all would 
know this to 
be true of all computers. Thus 
one looks for dimensions 
that spread the machines out evenly 
into a substantial number of categories. If the dimensions of the space are known, a computer is sup- posed to be 
defined by a 
single point. For most existing 
computers this is actually the case. However, if a computer system were complicated enough, 
say consisting 
of several processors, 
each built with different technologies and having a different number of ad- dresses per instruction, then such a representation 
would not be possible. For instance, the Rice University 
computer uses vacuum tubes, transistors, and integrated-circuit logic. But such complexi- 
ties are rare; 
time and 
good engineering practice work against 
it. If it were necessary to consider such cases, 
then additional dimensions (e.g., for secondary and 
tertiary logic) could be 
added, or several 
points in 
the space for a given computer could be used. The computer-structure space 
is thus our choice 
of the seven most important dimensions. It is our response, so to speak, to playing the number game, given only seven 
descriptors. They are arranged in order of importance, although 
clearly no simple 
way exists to validate such an 
order. But, if we were to have only three attributes to describe the structure of a computer system, we would pick logic technology, word 
size, and PMS structure (i.e., what processors exist with what 
functions). At this point we 
are ready to proceed through 
the space, de- scribing the various dimensions 
and discussing how the computer systems in this book 
illustrate various points along them. We take up each major dimension 
separately. A few of the correlated 
dimensions are accorded separate sections, but most are discussed along with 
the main dimension. 
Technology Computers are constrained by 
the physical technology from which 
they are constructed. It is not just that new technologies provide greater speed, size, and reliability at less cost, although of course they do that. But technologies 
dictate the kinds of structures that can be considered and thus come 
to shape our 
whole view 
of what a computer is. For instance, the emergence of the PMS system level is due to advances in technology. Prior to transistor technol- ogy, it did not 
make sense 
to think of elaborate PMS structures. The costs of the various parts were too 
high and the reliabilities were too low. 
When, occasionally, such a machine 
was in fact designed, it invariably proved too far 
ahead of its time to succeed. An example in this book might be the RW-40, described in 1960 (Chap. 38). A more classic example 
is the Analytic Engine of Babbage, which 
he designed in 1844 and was never able to 
com- p1ete.l The technology of the time was entirely mechanical, and its crude state 
accounts for a large share of the failure. Thus the technology is by all odds the most important single attribute to 
know about the computer system. Many technologies go into making up a computer. Each type of component typically uses a different one. 
In current 
(so-called 'Thus, the first real digital computer 
established the precedent of failing by a large margin 
to meet the expected dates 
of completion and 
full operation. 
54 Part 1 I The structure of computers third-generation) machines the Pc may use hybrid- and 
inte- grated-circuit technology for its logic, thin-film technology for 
the Pc generalized registers, core technology for 
the Mp, electro- mechanical technology 
for tapes and disks (with integrated circuits for logic), 
mechanical technology 
for card punches and type- writers, and even manual 
technology for mounting tapes and disk packs. The existence of all these technologies poses major issues 
of systems balance, issues which are only imperfectly resolved. For example, it remains true in the current 
generation that input/ output is not in balance with 
the internal structures. This is due to the crude state of terminal technology, so that it 
appears to cost too much 
to provide an appropriate 
solution.™ The heterogeneity of technologies is not a consequence 
of cost/benefit analysis; rather, each 
represents the forefront tech- 
nology for 
the type 
of device shown. (There is, of course, cost/ performance exchange for 
any component, 
but this is usually within a 
technology.) Thus there is a sense in which the leading technology can be used to represent them 
all. This 
is the technol- ogy used for the logic level and is the one listed in the computer 
space. If it is known that transistor logic is used in the Pc of a computer, it is a safe prediction that Ms is electromechanical, Mp is core, Tio is electromechanical printers and punches, etc. This reflects 
the fact that technology develops and 
hence be- comes locked 
with calendar time. Thus a prediction 
is from logic technology to date 
and then to all other things known 
to be current at that date. 
This correlation of date with technology is given in the com- puter space along 
with the generation. It can also be seen in the time chart. 
The correspondences must be taken as very rough only. 
The technologies are listed in increasing power (and decreasing cost). The dates run 
in exactly the same order. The one exception 
is fluidics, which has 
been introduced very recently and is a special technology for ruggedness, reliability, and direct external 
coupling in certain control systems. (Small fluidic 
computers are at the early prototype stage.) Alongside the technology dimension 
we list the dimensions: Pc speed 
(operations per second), and cost (dollars 
per million op- 
erations), all of which vary 
directly (or inversely) with logic tech- nology. In 
general, costs are extremely difficult to determine, espe- Although beside the point of the current discussion, one reason why these imbalances appear to be ﬁpermanentﬂ is that the 
time constant for change in the technology is of the same order as the time constant for human beings 
(i.e., systems analysts, programmers, 
and users) to understand the 
imbal- ance. Before system imbalance is diagnosed and solved, 
the terms of the problem change, inducing new imbalances. cially when 
technological costs are of interest rather than market costs (which reflect numerous other factors). Nevertheless the effect of technology on costs has 
been so striking (while simulta- neously pushing 
up performance along 
all other dimensions) that it seemed necessary to give a measure of cost in Table 1, no matter how crude. We have indicated 
only a few of the dimensions that are corre- lated with technology. In 
fact, the only dimensions 
in Table 1 that are independent 
of technology are the word length and the Pc addresses/instruction. All the rest show 
dependence on technol- ogy. For some, such as 
memory speed and 
size, there is a direct correlation. For others, such as PMS structure and 
Pc concurrency, 
the development of more complex 
versions-the leading edge, 
so to speak-depends on technology, but there is free use of all versions that are 
in existence at any given time. There are 
still other dimensions of importance, not 
shown in Table 1, that have also changed with 
technology, e.g., 
electric-power consumption. 
One way to 
see both what 
varies and what 
is independent of technology is to compare selected machines. 
For instance, Whirl- wind (Chap. 
6), a first-generation system, 
and the IBM 1800 (Chap. 33), a third-generation 
system, have reasonably similar ISP 
descrip- tions, if one ignores index registers, which 
were not invented 
at the time of Whirlwinds design. However, they have 
very different PMS structures. In Whirlwind, the early system, transferred infor- mation between Tio™s and Ms was under program control of the Pc. The existing Pc registers and transfer gates were used because it was too expensive to have separate 
ones. In the 1800, which uses hybrid circuits, it is economical to have additional subsystems devoted to special functions; hence there are 
many Pio™s operating independently of the main Pc. It was not cost alone that limited the complexity of first-generation vacuum-tube systems. The large physical size 
of tubes introduced substantial transmission delays; 
their large power consumption 
added dependency 
on a cooling system; and their limited life and deteriorating nature 
constrained the number of tubes that could be 
used in a system requiring high reliability. The IBM 700 scientific series (701, 704, 709, 7090, 7040, 7044, 
7094 I and 11) offers another comparison, where there is an evolv- ing structure over time, hence across technologies, 
but where for reasons of compatibility the ISP™s have remained 
almost constant (except for the 701). Again we see radical increases 
both inperform- 
ance (Pc speed increases by a factor 
of 5 from the 701 to the 704 and another 
10 to the 
7094 11) and PMS complexity. But various 
other features, though not affecting compatibility, 
were locked in with the ISP and remained fairly constant. For example, Mp size went to 32 kw (kilowords) early in 
the series with the 704; and 
Chapter 3 I The computer 
space 55 it took a jerry-rigged modification 
to get 64 kw on a 7094 toward the end of the lifetime of the series (see 
Chap. 41, page 517). Throughout this section we 
have referred to technology as the dominant factor in 
the computer. Does this 
mean that computer development waits upon new fundamental 
windfalls? We have been lucky 
in getting the transistor and, to a lesser degree, the integrated circuit 
from external efforts. However, core 
memories were invented for the computer and 
resulted because of need. Read-only memories 
have also resulted both from development at the circuit level and from pressure above, 
requiring the mem- ories to be developed. 
All the electromechanical secondary 
mem- ories (Le., magnetic tape, drums, disks, and photostores) have 
resulted from the computer's needs. Thus, 
although technology is dominant, the computer often forces 
the development. The Pc operation 
rate is strongly correlated with 
logic tech- nology, as 
we have indicated in the computer space. Our discussion about technology and generations is also about operation rate. The 
principal reason for 
the higher operation 
rate is because of faster logic technology. Technology also has 
a secondary 
effect on in- 
creasing speed. 
More reliable devices allow 
large computers to be built. Smaller devices allow 
higher device 
densities, thus de- 
creasing stray capacitance and inductance and 
shortening trans- mission delays. Smaller 
components also allow 
increased inter- connection density. Operation rate is also relatively highly correlated with total performance. If we hold the structure and 
concurrency constant, the simplest way to increase performance 
is by increasing the clock rate. The 
increase in the performance/cost ratio over the past two decades of computer evolution has made 
their primary gains through higher operation 
rates. The two 16-bit computers already 
mentioned, Whirlwind 
(Chap. 6) and the IBM 1800 (Chap. 33), provide a nice comparison 
of the evolution. With a difference of 10 years and two generations, their cost ratio is -1O:l whereas performance is -1:5 and the internal 
clock rates are also -1:5.l 
Znformation structure: word length, information base, 
and data-types All computers structure their 
information in a 
hierarchy of units, which we defined as an i-unit 
in Chap. 2. For example, the IBM System/360 starts with the bit; then the 
byte, which is 8 bits; then the word, which is 4 bytes; then the record, which is a variable number of words. In between, playing minor roles, 
are decimal 'However, it is not as dramatic an example 
as we could find. By picking a better third-generation example we might get 
a cost ratio of -1OO:l and a performance ratio 
of -1:lO. digits (4 bits), the halfword, and 
the double word. A number of features of the design are related 
to this hierarchical organization of data. Before we consider them, we need to characterize the 
organization itself. One characteristic of this organization, the word length (in bits), 
gives most 
of the information, the rest of the hierarchy adding 
only a little. Let us see why this 
is so. At the bottom there is the bit, 
encoded in two-state devices. Although 
other numbers of states are possible, and ternary (three-state) 
machines have been 
proposed occasion- 
ally, digital technology has 
developed exclusively to handle binary information. There are 
several reasons for this. 
The first is the requirement for high 
reliability and high signal-to-noise 
ratios in the basic devices. 
Generally a 
basic n-state device (that is, one not built up from other k-state devices) 
is realized by breaking a continuous 
physical dimension, such as voltage, 
current, or magnetic flux, into n discrete levels or regions. Reliability 
and signal-to-noise ratio then depend 
on keeping adequate separation. This is easiest to do with two 
states (e.g., in 
the limit they become on-off devices) and becomes progressively 
more difficult as 
n in- creases. The second reason is the simplicity of the logical design 
for binary representations. A basic device for combining two ternary digits must deal with 3 x 3 = 9 configurations, rather than 
2 x 2 = 4 configurations for 
the binary case. This also 
gets worse as n increases. A final reason-the 
coup de grace, so to speak-is that no one 
has ever found 
striking advantages for the resulting processing structure in having more 
than two 
states. Thus there are 
no com- 
pelling reasons to suffer the first two disadvantages. 
In short, what might have been an important 
dimension on which 
to distinguish computers, namely, 
the number of states in 
the basic encoding, turns out instead 
to be 
one of the great uniformities in digital 
technology. Information base. That the physical devices 
deal ultimately 
in bits does not imply 
that the information processing must 
be organized in terms of bits. It is possible to select 
an arbitrary base (one with any number 
of states) and construct the entire ISP in its terms. 
A base unit is represented physically, of course, as a set of bits. If one wanted a base 13 machine, for example, 
one would have 
to use at least 4 bits (with 16 
states) to encode it. But no operations at the ISP level would refer 
to anything but base units and data structures built up from sets 
of base units, and there 
would be no way to 
manipulate directly the bits that represented the base. Thus, using a base other than 
binary obtains whatever advantages 
might accrue to n-state units, 
without any of the disadvantages at the device level. 
56 Part 1 1 The structure 
of computers Computers have 
been built with a 
variety of different bases, the main ones being binary, decimal, and character. 
The character has shifted between 
a 6-bit character and an 8-bit character (byte).™ The arguments for bases other than 
binary (which 
repre- sents the natural base of the computer) all hinge on the alphabets used externally by human 
beings and the desire to avoid conver- sions into a different representation inside the computer. With universal acceptance of higher languages, such as 
FORTRAN and ALGOL, this argument has also lost 
much of its force. In fact, all third-generation machines 
are binary. Nevertheless, in the fifties there was much controversy over which base 
to use, and the 
machines presented in this book exhibit all 
three bases. There is little difference between binary and decimal com- puters in their ISP organization. However, there is a great differ- ence between 
these two and character 
machines. The latter are designed for 
handling text and are 
constructed to deal with varia- 
ble-length strings of characters. Correspondingly, they deempha- 
size numerical computation. Both these decisions affect 
the ISP considerably. Thus, 
in the computer space we 
indicate the 
base dimension along with the word-length dimension. The two 
to- gether make up a single dimension. 
Word length. Let us now examine the role of word length. The word is the first major information 
unit above the base. It is defined as n bits for a binary 
computer or n digits for a decimal 
computer (character machines being excluded as not having a fixed word length). Sometimes there are intermediate 
units, but they always play a minor role 
and we 
can disregard them at this stage. As we noted earlier, the main determinant of word length has been the function of the total system: large word lengths for arithmetic systems, small word 
lengths for control systems (and character 
strings for business). Thus, only 
within narrow 
limits is the word length a 
free design choice. However, the interesting thing about 
word length is not so much its determinant as the way it 
affects other aspects of the total system design. This 
starts with a design decision 
that the 
unit of information transfer between components will be a word. 
As soon as this becomes the case, then registers in various com- 
ponents must hold a word, 
since that is what arrives or 
is to be transmitted. Thus the word becomes 
the information unit of the Mp, and 
most of the registers of the Pc hold one word. The instruc- tion is designed to fit into one word, since 
that is the number of bits that is obtained ﬁat onceﬂ and hence can 
be used to effect the next time increment 
of processing. ‚Seven bits have been 
proposed for communication purposes 
but have never been made the basis of a machine, as far as we know. Once these basic features are set, others follow. An integer number of any smaller units, such as the character, should fit into a word, since otherwise a set 
of words will 
not provide a 
homoge- neous sequence of subunits. (That is, only five 6-bit characters fit into 32 bits, 
so that a set of 32-bit words filled 
with 6-bit characters has a number 
of 2-bit holes 
in it. This can complicate algorithms that deal with long 
character strings.) The constraint of compati- bility is not so strong with 
Ms, since speeds 
are slow enough 
to permit conversion algorithms (either hardware 
or software). Still, 
the system is simpler (and therefore usually will work 
better) if incommensurabilities of information units do not exist. Thus, to pick an example, the number of parallel tracks 
on magnetic tapes tends to divide evenly into the word length. IBM tapes for the 700 series of 36-bit machines have six data tracks; for 
the Sys- tem/360, which 
has a 32-bit word, 
the tapes have 
eight data tracks. There is an interesting correlation 
between the word length of a computer and the number of data-types that it 
makes availa- 
ble. As we saw in Chap. 2, the operations in a 
computer can 
be classified according to the type 
of data they operate 
upon. Each data type 
tends to have a 
certain set of operations appropriate to it 
(for example, 
+ , -, X, and / for numbers) and the decision to include a data-type carries with it the decision to include its operations, 
Thus the number of operations tends 
to grow with the number of data-types. The total 
amount of hardware in a computer grows as 
the word size 
(because data paths are word- parallel2) and also as 
the number of operations. Thus machines with large word 
size tend to be large machines and have 
many data-types and 
many operations, 
(ﬁLargeﬂ as an adjective for machines invariably means big 
and expensive, hence-given eco- 
nomics-capable of doing large amounts of processing.) There are two 
additional, somewhat independent, features that support the relationship between word size, number of data-types, and size of computer. First, with a 
large system there will already be available many 
of the pieces necessary to add additional oper- ations. That is, the marginal cost 
of a new operation goes down as the system grows. Therefore, given a large system, there is a tendency to add more operations, The number of operations per 
data-type is not easy to increase; rather, one adds 
new data-types. 
Second, with small word 
lengths, one cannot define many worth- while data-types that will fit into a word, and multiple-word 
data- types are left to the programmer to define with software. With large word 
lengths there are 
many different 
worthwhile data-types 
that fit into the word, for instance, decompositions of the word into partial words, or into character strings. Each of these requires The issue of bit-serial versus bit-parallel is discussed subsequently. 
Chapter 3 I The computer space 57 additional operations, since 
the initial data-types involve the entire 
word or some large part of it (i.e., the word, address, 
and integer operations). In sum, the word length stands as an indicator of many aspects of the machine. It not only tells something about the basic organi- 
zation of many components but indicates how big 
the computer is, both in number of data-types and number 
of operations. Figure 2 shows time lines of well-known computers with 
their word length, with a 
special time line for the ones in this book. 
Five groups are suggested in the figure which classify these c0mputers.l 
The classes overlap, and to separate a computer into 
one of two classes requires more knowledge (e.g., 
the number of data-types). For example, the 24-bit SDS 9300 and CDC 3200 appear in the 
same class with the 36-bit IBM 7090 just because both machines have floating point hardware and, 
in fact, perform comparably for arithmetic tasks. The one 
design choice that makes word 
length have 
few of the consequences just 
described is making a computer bit-serial rather than bit-parallel. In many machines information 
transfers are con- ducted on a single bit stream (especially Pc-Mp transfers). 
Coinci- dent with 
this is the construction of operations on a bit-by-bit basis. This 
works well for 
arithmetic and 
logical operations. Time is traded for hardware. The cost of the system becomes 
independ- ent of word length, but the processing rates go down correspond- ingly. This design decision was 
an extremely important one when 
logic was expensive 
and unreliable. It has become less so in the current era, where 
processors and transfer paths are relatively 
few in number while 
both the cost and the reliability of components have improved. However, as large parallel 
processors are con- sidered (- lo3 P™s), bit-serial processors again 
become a 
serious design alternative. (See the serial computers of Part 3, Sec. 2.) In summary, word length is an important 
dimension, and we find many characteristics 
either proportional to or inversely pro- 
portional to it. 
To be sure, these relations hold only for 
current design practice, as we have seen with the bit-serial designs. The main-line computers in Part 2 are ordered according 
to increasing word length. Data-types. We have presented the 
number of data-types as being correlated with 
word length and 
also with computer size through the effect on 
number of operations. Although far from 
perfect, there is a rough order in which specific 
data-types are included in a computer. We have listed the main types in such an order in the data-type dimension of the computer space. (See Chap. 2 ‚The class number is essentially [log,(Mp word length) - 21. for their definitions.) To be located at a point 
on this dimension (say at floating point) means to have all the data 
types below 
it on the dimension, (i.e., word, address, 
integer, boolean.) Occa- 
sionally machines which violate this have 
arisen. Decimal ma- chines do not 
generally have 
boolean data-types, and there 
has been some attempt at machines with only floating 
point, i.e., without a separate integer type (e.g., the CDC G202). The reason behind this cumulation of data-types in a fixed order is that certain general tasks must 
be performed by any computer. 
It must transmit data between the Pc 
and Mp, and this trans- mission has nothing to do with the meaning or content of the data; thus there is always the ﬁunit of transmission,ﬂ which 
is the word (except on character machines). Next, all 
computers manipulate addresses to achieve generality (e.g., 
to compile), providing 
for a second data-type. Next come integers, since 
almost all 
algorithms make use of arithmetic (this could conceivably 
be absent in some communications computers), and 
on up to 
floating point numbers, 
multiple precision, and vector and string operations. 
At each stage the uses are more specialized so that lower ones 
cannot be elimi- nated, except for a few cases such as 
handling addresses as 
regular integers. Addresses per instruction and processor state The number of addresses in an instruction has been a 
traditional way of describing processors (i.e., 
their ISP™s) and hence the com- puter systems containing these  processor^.^ We use it in 
Parts 2 and 3 to separate the different processors. 
Originally the dimension was simple: one-, two-, three-, and four-address machines 
were constructed. 
It has become somewhat 
more complex. A ﬁone plus oneﬂ machine 
has one address for 
data and one for 
determining the next instruction, and is to be distin- guished from 
a two-address machine, which uses both addresses for data. Index registers 
and so-called general registers provide instruction schemes which 
lie somewhere between one- and two- address organizations. 
When processors admit several instruction formats or variable-length instructions, matters become even 
more complicated. A correlated dimension in the computer 
space is the amount of processor state, that is, the number 
of bits that exist in the 
processor, as 
described in the ISP. This 
is the amount 
of informa- tion that can be 
held at the end 
of one instruction to 
provide the processing context for the next instruction. It consists of a number 
of status and mode bits (in modern machines packaged 
into regis- Originally the Bendix G-20. 3Although used mostly 
to describe Pc™s, the description applies 
to any processor. 
58 Part 1 I The structure 
of computers ters, but in earlier machines simply scattered around in the proc- essor), the next instruction address, the accumulator and other arithmetic registers, the index registers, 
and other general registers making up a ﬁscratch-pad™ memory. It is a simpler 
descriptor of the ISP than addresses per instruction, since it is independent of the number and variety of instruction formats. It is easy to define processor state generally for any ISP, but difficult to define ad- dresses per instruction. The processor state is not the total number of bits in 
the proc- essor, since there may be registers in the physical system 
that are used within the interpretation 
of one instruction but which carry no information 
between instructions. Address registers for 
obtain- ing operands from Mp are the 
most common such ﬁundergroundﬂ or ﬁtemporaryﬂ registers, but there can 
be others. We implied this distinction by defining processor 
state in terms of the ISP rather than the physical processor. 
The correlation between the processor state and the number of addresses per instruction is not simple, since 
it rests on two separate issues. For the first, note that larger programs perform 
transformations on the state of Mp (or even Ms or Tio™s) and are 
not concerned with 
the state of the processor. Processor 
state enters only because, in decomposing the total algorithm into a series of small steps, it is not possible (or efficient) to make each step a transformation from Mp to Mp. Basically, this 
happens because the instruction does not hold enough information 
to spec- 
ify the Mp-to-Mp transformations. For 
example, if one wants 
to add two numbers, two operands 
are required, and an 
instruction must contain at least two addresses; if it does not, then an inter- 
mediate state (i.e., processor 
state) must be created to hold the information while 
the additional instructions 
are fetched. Thus, one-address organizations require 
the most processor 
state, with 
less for two- and three-address organizations, This consideration stops at three (two 
operands and a result) because only a few elementary operations 
are more than binary. The processor state cannot be eliminated entirely, however, since there must be at least an instruction address (a program register) to 
maintain con- tinuity of the program. ~ The second source of correlation between processor state and 
instructions per address comes from differential access 
time to processor registers 
and to Mp. As long as there is an 
appreciable differential, substantial gain, 
processing power 
can be obtained from increasing 
processor state. This derives, 
again, from the struc- ture of algorithms which generate intermediate 
results that are used almost 
immediately afterward and 
then are 
of no further interest. Rapid temporary storage and retrieval are beneficial under these conditions. Thus, working against 
higher address organization is the extra time to store in 
Mp results that need only temporary storage. Thus, also, index 
registers and general registers almost always 
imply increased 
processor state, although they 
need not do so logically (that is, the registers could exist in Mp and 
still have their effect on the instruction format). 
With interrupts and 
multiprogramming the processor state gains additional significance, since it is the amount of information that has to be saved and restored when 
switching programs. For example, in the Honeywell H-800, an early three-address computer, the processor state per program consisted only 
of the program counter and index registers, 
and when io-halts occurred during processing, the Pc was switched immediately 
to another program. Eight programs could 
run concurrently (by 
having a 
total processor state of 64 program registers). 
In present computers with general-register state, often 25 - 100 words must 
be stored, which implies an appreciable time 
for switching contexts. We can now consider briefly 
the different organizations accord- 
ing to addresses per instruction. To show the common similarities, we give in Fig. 4 a 
state diagram that can be used for all processors. 
In common is the basic idea of the stored 
program: Fetch an instruction, determine what the 
instruction is to do, 
then execute it (the 
fetch-execute cycle). Other than 
this, only a part of the state diagram will 
be applicable to a given processor 
type. As shown in the computer 
space, the addresses-per-instruction dimension starts with zero 
addresses, then one address, then one plus indexing, one plus general 
registers, and on up to two, three, and variable addresses. However, from an expository viewpoint one should follow 
a different course, starting with single-address machines, then indexing, then two- and three-address machines, 
then general registers, and finally the zero-address and variable- address organizations. This not only 
puts the 
more common organizations first but makes it easy to relate the organizations to each other. P(l address) and P(l + index address). These Pc™s constitute most first-, second-, and simple third-generation computers. 
The earliest outline of the structure 
was the IAS computer (Chap. 
4), which has come to be 
known as the von Neumann computer. 
Although fundamentally like the IAS computer, EDSAC™s adaptation ap- 
pears to be the closest prototype to this class. Although 
EDSAC is not described, it influenced M.I.T.™s Whirlwind I significantly (Chap. 6). A significant change to the 
IAS machine was the addition of the index register 
(called B-tubes) in the Manchester University machine in the early 1950s. The evolution can 
be seen by 
compar- ing the first and third generations using Whirlwind (Chap. 6) and 
Chapter 3 I The computer space 59 fetch operand fetch (read) lav. r I \ 4 Request Determines the Request instruction operand q from Mp instruction q trom Mp store (write) (av. w) operation opera1 specified address CaIcuIa t ion q (0) (0v.w) Return for string or vector data fetch next instructiok Multiple results PC2 'Mp controlled 
state 'PC controlled state Note: Any state may be 
null State name soq/oq saq/aq sa. o/o.o sov.r/ovr sav.r/av r solo SOV.W/O" w sav.w/av w Time in a state toq taq to.0 tov. r tav. r to tov. w tav. w Meaning Operation to determine the instruction q Access (to Mp) for the instruction q Operation to decode the operation of 
q Operation to determine the variable address v Access (to Mp) read the variable v Operation specified in q Operation to determine the variable address v Access (to Mpl to write variable v Fig. 4. ISP interpretation state 
diagram. the IBM 1800 (Chap. 33) or looking at the IBM 701-7094 evolution in Part 6, Sec. 1. Index registers are motivated 
by the frequent occurrence, in 
1 address systems, of circuitous address calcula- 
tions that involve first computing the address (e.g., the index of an array in Mp) and then 
planting it 
just ahead in the instruc- tion stream in order to make use of it as an address. Providing a set of index registers introduces a second address 
into the 
in- struction, even though 
of extremely limited function. Thus we classify processors 
with indexing as having (1 + x) addresses per instructi0n.l 
An alternative view of index registers suggests that they double 
the number of data-types by allowing operations 
on vector 
data elements rather than 
just scalars. 'Indirect addressing, on the other 
hand, does not add 
to the addresses per instruction; rather, it 
introduces a second operation per instruction. For the 1 address processor, 
the processor state (Mps) typically 
consists of the program counter (instruction location counter), 
an Accumulator/AC, a Multiplier-Quotient register/MQ (the exten- sion of AC), and one or more Index 
registers/X/XR. With only one address 
in the instruction, the one arithmetic 
register, A, must be used for 
temporary results. Thus an effective- address integer (z) is computed as a 
function of the address part (v part) of the instruction (9) and the index registers. This process is typically z := v + X[j] where X[j] is the jth index registers as specified 
in the instruction. There are 
several forms for 
the transmission operators between A and Mp. 
60 Part 1 I The structure of computers Atz loud immediate A + MpExI load direct A t Mp[Mp[x]] load indirect M[x] c A store direct Mp[Mp[z]] t A store indirect In indirect 
operations a convention 
may be required to determine what address 
in Mp[z] is to be 
used. Similarly, the binary operations 
(+, -, X, /, A, V, 0, con- catenation, etc.) are 
generally of the form™ AtAbMp[z] Rarely do we find the symmetrical operation 
form For unary operations 
(-,, -, abs, sin, cos, 
etc.) the most com- 
mon forms are AtuA A tu Mp[z] Rarely do we find MP[ZI + MP[ZI Mp[z] tu A In both 
the above cases, exclusion 
of the operations that place results in Mp[z] stems from the added cost of including the sym- metrical function 
and the marginal utility of such a function, 
which stems from the result of applying u not being available for further processing. The transmission, unary, and binary operators account 
for al- most all 
operations in these computers. If we allow A to stand for any 
part of the Mps, rather than 
just the accumulator, then the instructions not included above are input/output 
data trans- mission, e.g., 
MpcT and TcMp and conditional execution 
(branch if zero AC) -+ ((AC = 0) -+ (P c z)) Having index registers 
requires operations to process them. At a minimum they must be loaded and 
stored (usually from 
and to Mp), Le., Mp[z] t X store index 
X c Mp[z] load index register 
Any of the addressing modes suggested above can be used for an operand: that is, I immediate, Mp[z] direct, and Mp[MP[z]] indirect. But simple 
operations on an X are also desirable; for example, 
XtX+ 1 Here X is used to point to (access) the next element in a vector. More complex 
operations can 
be carried out by placing X in the A register, via the program steps: 
AtX load A with I< A c f(A) manipulate A XtA load X with A An operation to add k to X would then be AtX; next AcA + k; next 
XtA instead of Mp[z] t X; next A +- Mp[z]; next 
A t A + k; next Mp[z] c A; next x +- Mp[zI which assumes no transmission 
paths between X and A. Ideally we would like 
to perform any 
operation directly on X as simply XcX+k From this begins 
the idea that X should look like the main arith- metic register, A. This is, no doubt, one evolutionary 
path to general-register processors. 
Part 2, Sec. 1 is devoted entirely to 1 address computers in the first three generations. They were 
the ﬁmain lineﬂ of computer development. P(2 address) and 
P(3 address). The computers 
in Part 3, Sec. 1 have instructions which contain multiple 
addresses per instruc- tion. The addresses (v) specify 
operands in Mp (Fig. 4). The Mps decreases as the number of addresses per instruction increases, since the operands need not be 
held temporarily between instruc- tions (Le., 
each instruction performs a complete operation). The instruction form for 
the 3 address computer is where b is a binary 
operator, and vl, v2, and vB are the 
addresses specifying the operands. In the case of unary operations, u, v2 is usually blank. In 
the case of a binary operation and a three-address 
computer, the states are 
oq, aq, 00, ov.r, av.r, ov.r, 
av.r, 0, ov.w, 
Chapter 3 I The computer space 
61 av.w (Fig. 4). MIDAC (Chap. 14) and Strela (Chap. 15) are typical three-address computers. 
A 2 address computer does not necessarily require more proc- 
essor state than 
a 3 address computer, since the operations can correspond to and However, sometimes extra Mps is usual. 
The RW-400 (Chap. 38) has an accumulator, and operations generally 
terminate with results both in primary memory, Mp[v,], and in the accumulator. The branch on accumulator instructions allows results 
to be 
checked directly without referring to Mp. An especially nice instruction in 2 address computers is the transmission instruction (a special-case unary operation): Mp[v,] t Mp[vl]. The IBM 1401 (Chap. 18) has two registers, Laddress and B-address, which hold 
v1 and v2 and can 
be loaded by 
the v1 and v2 parts of the instruction. These registers point to (address) oper- ands and 
do not contain 
data. The remaining processor state is the Instruction-address. The 1401 has instructions with no address parts, and these instructions take as operand addresses the values of Laddress and B-address as 
of the previous in- 
struction. The 1401 instruction-interpreter state 
diagram is given in Chap. 18 (Fig. 3). The state-diagram specialization (Fig. 
4) is roughly: oq, aq, 00 {ov.rl,av.r1,0v.r2,av.r2,0,0v.w2,av.w2}. . . { ov.rl,av.r1,0v.r2,av.r2,0,0v.w2,av.w2} where the sequence delimited by 
the {. . .} is the operation on a character; because the 1401 operates on variable-length strings, it is repeated until 
the end of the string. P(n + 1 address). Processors with n + 1 addresses deviate only slightly from 
the u-address processors 
above. The final, or +1, address explicitly specifies 
the address of the next instruction. As such, it can be used with any instruction set. 
There are two 
reasons why +1 addressing is used. First, freedom is provided in the placement of each instruction within the program address 
space. Second, the next instruction address can be calculated in parallel with the execution of the current 
instruction. For computers with 
cyclic memories 
(Part 3, Sec. 2), the +1 address allows both data 
and the next instruction to be 
specified independently, providing the opportunity to arrange the program and data in an optimum fashion. Since 
each instruction completion time depends on the location of data, it is desirable that the 
next instruction location 
be variable rather than 
the implicit next ad- 
dress used for most processors. This 
is almost universal 
practice in computers with 
Mp.cyclic (see 
LGP-30 in Chap. 16 for an 
exception). Microprogrammed processors 
may use the + 1 address to locate the next instruction, and there 
may be several such next addresses. 
Microprogram subroutines tend to be short (intrinsic to 
interpret- ing an instruction set), 
and there are 
many jump addresses. The increased speed 
from not having 
to compute the next instruction address is worth the added 
space cost. The IBM System/360 Model 
30 (Chap. 32) shows the use of multiple (+1) addresses and if classified according to our scheme 
would be at least a •'(micro- program; 3 + 1 address). P(generaZ register). The general register processor has 
a small array of registers that can be used for 
multiple functions. These have 
fast access 
compared with the Mp, so that it 
pays to do as much processing as possible 
within them. Since the general register array is small, it requires only 
a small address 
(3 to 8 bits). Thus the instruction format contains fields for one (or more) general regis- ters. There must still exist addressing 
for Mp, though this never exceeds a single address. 
Thus we 
classify general registers ma- chines as (1 + g) addresses 
per instruction. The organization of a (1 + g) system 
can vary from 
something very close to a (1 + x) organization, in 
which essentially every instruction involves some 
Mp information, 
to an organization in which the only Mp instructions are transfers between Mp and Mps (the processor state holding the general registers), and there 
is a two- or three-address instruction set 
involving only 
Mps (see the CDC 6600 in Chap. 39). That is, from a data point of view the Mps acts like a directly addressable Mp. 
The processor state of a general 
register processor 
is invariably held entirely within the general register array (rather than 
having additional independent registers). This 
is due in part to an already available mechanism (the array) and in part to the need for pro- 
gram switching, which is somewhat simplified by having 
all the Mps held in a single homogeneous memory. 
The general registers typically perform a variety of functions: 1 Arithmetic registers (accumulator and the accumulator ex- tension for 
the multiplier-quotient). 2 Index registers. 
3 A second index register or base 
register; if the program addresses (v) 
are short, 
a base register 
is needed to address any area of Mp. 4 Subroutine linkage registers. 

62 Part 1 1 The structure of computers 5 Program flag (sense) registers 
for boolean variables. 
where 6 Stack pointer (P may have multiple 
simultaneously active b are binary operators (+ - ,, , l A , l etc,) 7 8 u are unary operators (7 I - I ahs( ) 1 -ab( ) I etc.) G is the general-register array g, g,, g,, g, are instruction parts specifying a general register, G v, vl, v,, v3 
are Mp addresses specified as a function of instruction and general registers (for example, v := (address + G[g]) or v := (ad- stacks). Address pointers to data arrays and lists. Temporary data storage for intermediate results. 9 Temporary program storage for short program loops. 
The power of a general 
register processor 
is obtained because 
the registers can serve many functions. 
Thus the operations on these registers can be 
extensive, because the operations need 
not be duplicated in other parts of the structure. For 
example, special operations for index registers 
are not necessary because the opera- tions for integers apply universally to both the accumulator and 
index registers. 
Of course, such 
generality requires 
compromises. The stack computer is faster for problems which 
can utilize stacks, whereas the general register Pc must utilize Mp for the stack(s) and does not have the encoding efficiency of a pure stack processor (see below). In addition, the assignment (and reassignment) of general registers is most crucial, since they are a scarce 
resource with many uses. A general register organization allows processors 
with a high degree of parallelism to be constructed, since several instruction subsequences can be executed concurrently. 
The actual number of registers is rather critical and depends 
not only on the algorithms of tasks coded but also on 
the technol- ogy. In multiprogramming and 
interrupt computers, the program switching time increases with the number of registers. Thus the upper bound on the number of registers is both cost and program switching time. We would expect to find instructions which produced the fol- lowing affects. 
Addresseslinstruction dress + G[g,] + G[g,]) in the IBM System/360). General registers can be thought of as an outgrowth (generali- zation) of the 1 + x processors, as we have already 
suggested. Alternatively, they can be thought of as evolving from 
a 2 or 3 address structure. The 
UNIVAC 1103A, 
a 2 address processor (Chap. 13), was no 
doubt a forerunner 
of the general register UNIVAC 1107 and 1108. Pegasus 
(Chap. 9) is, we think, about the earliest computer to use general registers (1956). In Part 2, Sec. 2 we discuss four 
general registers computers. P.stack (0 addresses per instruction). From a 
PMS viewpoint the P.stack is 
built around having 
a first-in-last-out memory (Mstack) as part of the processor state. Conceptually, it is built around the fact that computations can often be sequenced 
so that no explicit names (Le., addresses) 
are required for temporary results. All operations are performed on the top of the stack. As each partial result is computed, it is pushed down in the 
stack and appears 
again to participate as an operand at exactly the appropriate point in later calculation. Thus the stack operates as an 
implicit memory 
for all 
intermediate products and not only are transfers between P and Mp 
avoided but space in the instruction for Mp addresses is eliminated. Instructions in such a system consist only 
of operations, since 
all their operands are in the stack. Thus the instruction format is that of zero addresses per instruction. There must, of course, be some addressing 
of Mp (just as 
in a general-register organiza- tion). However, the addresses for 
Mp themselves sit 
in the stack so that the 
instruction contains only the transfer (load 
or store) operation, not the address. There still must exist some way of getting fresh data in the stack, and all P.stacks 
have at least one operation that loads an address written in the program stream onto the top 
of the stack. Why there should be this happy correspondence between cal- culations and memory to be performed and stack memories re- 
quires a little explication. It rests fundamentally on the phrase structuring of calculation in which each partial result is required at one and only one point, so that each subcomputation can be nested in the program (and hence 
its result 
nested in the stack) 
Chapter 3 I The computer 
space 63 in the same order as it will occur as operand to 
the one operation that uses it. There are 
several arguments against a Pstack. Multiple stacks are often required. 
Part of the power of a P.stack 
is derived from having higher-speed 
Mps for the stack. Yet only the top 
few (2 - 8) registers of the stack can be 
in Mps. When M.stack overflows into Mp, the speed of operations can become much 
worse than not having a stack at all. A simpler implementation, for example, P.general,registers, is as fast and perhaps more general. Another 
difficulty with the stack is the inability to access other than the 
top. If full addressing is provided, then the organization has be- 
come almost general register. 
Yet another difficulty arises from inhomogeneity of data-types, especially 
if several of them are packed into a single 
word (the width of the stack). Thus, 
for in- 
stance, in one stack machine (the Burroughs B 5000 in Chap. 22) there is a completely separate nonstack ISP for string manipula- tion. A simple numerical 
computation is given in 
Table 4 as a com- parison of the P.stack, P.l address, and P.general,registers. Here, the Pstack is probably shown at its best as there are no array- indices calculations or 
program-flow manipulations involving testing, etc. The criteria we measure are the algorithm encoding 
space and the problem running time. 
The kinds of instructions interpreted by 
a P.stack 
are typically: Interpreter state 
Operation sequence 
Example Load oq, aq, 00, ov.r, av.r 
M.stack-top t Mp[v] Store oq, aq, 00, ov.w, av.w Mp[v] t M.stack-top Unary operation oq, aq. 00. o(u) M.stack-top t u M.stack-top Binary operation Oq, aq, 00, o(b) M.stack-top c M.stack-top b M. stack-top- 1 Variable numbers of addresses per instruction. Although there are a few operations 
that require the specification of three or more addresses, these are of such low frequency 
that no machine 
has ever been built 
(or seriously proposed, for that matter) that has more than three data 
addresses and one next-instruction address. (Some of the microprogrammed processors have more 
than one next-instruction address, 
and they often do several operations 
in parallel in one 
instruction.) However, there have been developed processors that can have a variable number of operands. Most of these involve the use of an instruction that is larger than a single Mp word. Thus, bringing 
in the first word of an instruction, which contains the operation code, determines how many 
additional operands are needed 
and hence how many additional words to obtain 
from Mp. (In a char- acter-based system this may 
require several reads 
per operand; 
in a 
word-based system this may 
be one or two operands per 
read.) The gain in such a system 
is the higher average density 
of opera- tions per instruction, bought at the price of extra Mp accesses. Most such 
variable-address processors have a mixture of one, two, and three addresses per instruction-simply a 
mix of the types already considered. The fundamental limit to such variability 
is the processor state (plus the additional within-instruction 
tempo- rary state). This, of physical necessity, must 
be finite, and the 
number of addresses must yield 
an amount of information that is less than this total state. 
Otherwise the processor cannot hold onto 
it to process it.l Thus the various processors 
which claim to operate from a higher language 
(see the P.languages of Part 4, Sec. 4) must in fact either translate into another simpler programming lan- 
guage, as does 
the FORTRAN machine (Chap. 
31), or become an interpreter which processes a small 
amount of a language state- ment before the rest. PMS structure The idea that there 
is significant higher organization 
to computers 
is relatively new. 
Texts on logical design 
of computers develop 
a model based on an arithmetic section, input/output devices, a 
memory for holding instructions 
and data, and 
a single control 
to force the other components 
to interact. A PMS diagram of an early model is given in Fig. 5 (X represents an external agent, usually a 
man). The Whirlwind I manual-model figure (page 10) used in Chap. 1 was rather highly developed because 
it had a secondary memory 
and switching. Figure 6 is a PMS diagram which reflects this more 
accurate model. Often computer designers lump the 
devices at the periphery and call them all input/output; these devices 
are both input/output 
terminals (T) and secondary memories (Ms). 'If it processes a large amount 
of information, but 
in pieces (i.e., sequen- tially in real time), it is not really executing 
a single instruction based on all the addresses but has decomposed the total computation, just as a single address organization has. Fig. 5. Early model of a stored 
program digital computer 
PMS diagram. 
64 Part 1 I The structure of computers Table 4 Comparison of stack, 
general registers, and accumulator 
Pc for evaluating 
the expression: f = (a - b)/(c - d x e) Pcstuck [stack contents] Pcgeneral register Pc. 1 address Push a [a] Load G[1], a Load d Push b [.a, b] Subtract G[1], b Multiply e Subtract [a - b] Load G[2], d Inverse subtract c1 Push c [a - b, c] Push d [a - b, c, d] Push e [a - b, c, d, e] Multiply [a - b, c, d x e] Subtract [a - b, c - d x e] Divide [(a - b)/(c - d x e)] Pop f [ ] - stores stack at Multiply G[2], e Inverse subtract G[2], c1 Divide G[1], G[2] Store G[1], f Store temporary 
Load a Subtract b Divide temporary Store f location, f Program size: Address integer/ai Operation parts/o 6 ai 40 Number of Mp refer- ences for data: Program size for hypothetical example 4x6 6 x (18 + 1) machines: 138 Program size in bits among specific C™s: B850 1 3: 168 6 ai + 8 ai(gr) 70 8 ai 80 6 x (18 + 6 + 42) 1 x (6 + 2 x 49 182 192 
IBM System /360:208(above1) IBM 7090:288(above1) :224(actual) 360(actual) 
+ base register 
overhead 8 x (18 + 6) (0 - 192)* ‚Not an instruction in the specific.example machines. 2Assume 16 general registers. 3The Burroughs Corporation 88501 Pc.stack (discontinued) 4Not completely true, since Systern/360 has 
only a 
12-bit address and uses base registers. Some overhead should be assumed. Worst case 
(but not 
unreasonable) IS 6 x 32 or 192-bit 
overhead. If we separate each component according to its function, 
assign control (K) to each element, 
and finally introduce the processor (P), we get 
the structure of Fig. 7. Of course, a large part of P is a data operator (D). The processor has the behavioral properties 
attributed to the structure of Fig. 5. If we include the control within each component, 
we get Fig. 8 from Fig. 
7. To consider larger 
structures, consisting of several Mp™s, P™s, Ms™s, and T™s, one might think 
to expand the system as shown 
in Fig. 9, in which 
we connect everything through 
a single 
switch. If the central S has sufficient power for 
multiple conversations, this indeed provides maximum 
generality. However, although 
Fig. 6. Early computer model 
(with Ms and S) PMS diagram. Fig. 7. General computer model 
(with distributed control) 
PMS diagram. 
Chapter 3 1 The computer space 
65 designs have 
been proposed for such 
a system, technology and economics have 
so far prohibited their actual 
realization. Instead, 
there has developed 
the general latticelike structure shown in Fig. 10. Each switch in this structure connects components 
on one side with components 
on the opposite side (the S interconnecting the P™s being the exception). The lattice structure 
of Fig. 10 is hierarchical in the sense that the Mp™s form the inner core 
and one travels out toward the periphery in moving from 
left to right. With this movement 
there is a general decrease 
in data rate, 
being highest through the Mp-P switch and lower as one moves to the 
right. The model has 
five switches (S). One switch connects 
the com- puter™s peripheral devices with the external environment (human 
beings, other processes, etc.). Three 
switches appear alike in the way they interconnect Mp-P, P-K, and K-(T I Ms), respectively. However, they 
are usually quite different. We would expect 
any P to connect with any 
Mp. We probably would 
expect to have only one 
or two Pio™s connected to a given set of K™s. Most cer- tainly one or 
two K™s would manage 
a given set of Ms™s or T™s. Thus the structure nearest the periphery becomes more like 
a tree, rather than 
a lattice (examples are provided in Figs. 11 and 12). The last switch 
in Fig. 
10, unlike the above four, provides 
inter- communication among the processors. In any multiprocessor struc- ture (even 1Pc-nPio) 
there must be communication among 
the processors. A switch of this type is organized as a nonhierarchy 
and appears like a conventional 
telephone exchange, since 
any P 
can call another. On 
the other hand, 
the amount of communica- tion (measured in bits) is rather low. The P™s and (usually) Mp™s have their controls associated with them, and we have not bothered to show such 
K™s in the diagram. The K™s that are 
shown provide 
control for the T™s and Ms™s. These are separated 
in the figure because they are separated 
in current computer systems and made into identifiable physical 
components. Under current technology they are expensive devices, 
so that one K per T or Ms is not economical. Therefore, each 
K needs to be P I P P P... T-X Ms M5 Ms.. . I X Fig. 9. General computer model 
(with multiple 
components) PMI diagram. U periphery lX(hurnan /computer /network lrnechanical process) where Pi0 := -Pia- 1 - Kio- K := ~~III-K- I-K-K- T ;= -T-l-K-T- Ms := kMs- 1 -K-Ms- Fig. 10. General computer model (multiprocessors) 
PMS diagram. ~T.console - Mp-Pc- K-Strn -•::- K-Sfx rT- Fig. 8. General computer model 
(without K) PMS diagram. Fig. 11. Tree-structured computer 
(1Pc) PMS diagram. 
66 Part 1 1 The structure 
of computers shared among a set of T™s and Ms™s. (That is, one purchases a single magnetic-tape controller for, say, four 
magnetic tapes.) The shared K also explains 
why only one of a given class 
of devices (e.g., 
magnetic tapes) can operate 
at a time. 
As technology changes (especially costs), these separate Ks may disappear. Nearly all the computers discussed in this book 
fit the lattice 
model of Fig. 10. However, it is not unlikely that structures will be or have 
been built that do 
not conveniently 
fit it. For 
example, NOVA (Chap. 26) does not fit the model nicely, although the more complex ILLIAC IV arithmetic-computer portion 
(Chap. 27) does. 
The values along 
the PMS structure dimension of the computer 
space have been 
generated from the general model and laid out in the order of their evolution. This 
evolution is strictly from less complex to more. The seemingly more complex 
network structures, such as the duplexed computers, are not necessarily as complex as a single multiprocessor 
computer. Duplex computers have 
been used for some 
time. The slow evolution to the 
parallel processor structure is due primarily to limitations in 
technology. A struc- tured computer with a 
distributed control 
is more expensive 
than a tightly integrated design with 
shared function. In addition, multiprogramming-a question 
of software-must be present to allow multiprocessing. 
The PMS structure plays only 
a minor role 
in obtaining multi- processing and parallel processing. The classical debate about building large computers has always 
been resolved by building 
a single large processor 
(e.g., the CDC 6600 and Stretch, Chaps. 39 and 34). Proponents of multiprocessors say 
that one can always add several large processors to a structure and increase the per- Mp MP ~™~~~- S IK --S r TT: Kio Ms Pi0 u I att i ce rnernorv-Drocessor 
T- X I .. / switching computer boundary 
(periphery) Fig. 12. Tree-structured computer (1Pc-2Pio and lattice Mp-P 
switch) PMS diagram. 
formance of a one-processor structure. In Part 6, Sec. 3, when we discuss the IBM System/:360, we advocate 
multiprocessing. Today there is no parallel processing in the form suggested 
in Chap. 37. We include a 
discussion of parallel processing on the bet that it will come 
in the future. Part 
5 is dedicated to moving along the PMS structure dimension. The simple 1 Pc structure shown in Fig. 11 is a tree. Although there are 
no values 
on the information rates, the nature 
of the fixed1 and time-multiplexed switches 
indicates that perhaps the top 
two T™s, one Ms, and one of the bottom T™s can all be active at a given time. In Fig. 
12 a 1 Pc, 2 Pi0 computer is given. Here we note that the 
control of one secondary 
memory is by a Kio rather than the 
Pio. (The Kio cannot fetch 
its next instruction from Mp and must rely 
on Pc for control.) Note that there is necessarily a lattice connection between the 2 Mp 
and the 
Pc, 2 Pio, and Kio. The special cases of P.displays multiprocessors, 
P(array I wired algorithm), and parallel processing are all realized 
from the general model of Fig. 10. Switching A principal issue of a computer design at the 
PMS level is switch- ing (as 
we indicated 
in the preface). Unfortunately, we do not illuminate switching 
problems in this book 
except to provide examples. The switching dimension of the computer 
space is cor- related with PMS structure, as we have 
just seen. To have a 
more complex structure, more complex 
intercommunication (switching) is required. Figure 13 shows the various logical switches, 
together with some of the more common implementations. The switch parameters are also given 
in the Appendix of this book. 
Each of the switching issues will be discussed in turn as they apply 
to various parts of the structural model (Fig. 10). The reader should note that Fig. 13 has relatively primitive switches. More complex 
switches can be formed by cascading 
(connecting) the primitives together. (A noncomputer example is the manner in which tele- phone exchanges are constructed and 
interconnected together.) Processor-memory switching. Only recently, with the advent of multiple processors, has memory-processor switching become 
an important problem. But the Mp-P switch 
makes multiprocessing 
possible, and it is a determining factor in both performance and reliability. The structure 
of the processor-memory switch 
for computers which have multiple 
memories and multiple 
processors is a lattice if simultaneous memory/processor dialogues 
are allowed. A cross- ‚A relative value for the attribute that denotes the time a switch is closed. Fixed usually 
denotes a time 
duration such that more than 1 i-unit is transmitted. 
Chapter 3 I The computer 
space 67 Group I. Hierarchical switches for connecting am comDi to bn components for 2-way conversations. The logica structures are first given, followed by common physic realizations, For the physical 
realizations links ar required between pairs of components. Not all physic realizations are given; it is assumed the roles of th and b's can be 
interchanged. al-L-S-b I .la  gate; switching at b) 1 al- 5- L - b ,]b  gate; switching at a) al- S-L - S -bl .lC  gate; switching at a,b) a - s(duplex) 1 n .2 (duplex I a: n b: concurrency:l; n S.gate) al[~~~~b2 bl L- S-b .za  duplex; radial; switching at b) I alf?- L- bp S- L-b LS-L-b ,2b  duplex; radial; switching at a) 1 a-L 1 --I-S-b L S-b .2 L. -S-b I il n 3 .2c S duplex; bus/chain; comonZy used for k'-T, c P-K interconnection n ,3  dual-duplex; 2a; n b; concurrrncy:L; 2 n S.gate) / 7"' "7"' L- L- 8 'Jbn L-s .3a S(dual-duplex; radial; switching at b, duolez version of .%a) S- Ls-s - L .3b S(dual-duplex; radial; switching at a, duplez version of LL Hi'"' LL i' :s hn .3c S( dual-duplex; bus/chain; 
duplex version of .Zc) rn .4 S time-multiplex. cross-point: m a: n b. concurrency:l; + s.~~~~; c'ascale of :? dupZes 1 S-L-b i a - L-S n Fig. 13. Logical and physical switch structures 
PMS diagrams. 
68 Part 1 I The structure 
of computers a -S I1 L L "- si L amI .4b S (t irne-mu1 t i plex; cross-paint: bus/chai n) ;:a, s (crass -pa i n t )-, am b" 1 m a; n b: cancurrency:rnin(m,n) 
m x n S.gate ;::% a- L LLL Ill bl bp...bn .5a S(cross-point: radial: Links to a or b ma!/ he null: a-L I YLIL-i bn b _.. bl 2 .5b S(crass-paint: bus/chain. use? for Vp-P interconnect rual-duplex cross- point 1 aJ a; n 0; concurrency: aTS4 s- n .6a s (dual-duplex; cross-poi 
nt; radial ) a rn a; n b; concurrency:l bl b2 bn Group 11. Non-hierarchical switching for interconnecting a components for 2-way conversations. S(duplex; non-hierarchical) ai ,8 s( duplex; non-hierarchical; concurrency:l) a-L-S a -L-S l a-L-S .9a S(duplex: non-hierarchical; central) Fig. 13. (Continued) 
Chapter 3 I The computer 
space 69 L rer'undant, use? to keep constant .8b S(duplex; non-hierarchical; bus/chain) 3 .9 S cross-point: non-hierarchical; 
m a; concurrency:m/Z m Y (m-l)/2 S.gate c al- L1 central) non-hierarchical; radial; 
m x (m-l)l2 all nodes have links to alZ other 3 .IO Sk-trunk; non-hierarchical; 
rn a; concurrency:min(rn/2 k); F x rn S.gate; T's mau not be extemai! Fig. 13. (Continued) t, tg . . . 
tk .loa S(k- trunk; 
central; non-hierarchical) I Fig. 13. (Continued) point switch provides redundancy and 
is used to form the lattice structure. To vary from 
the full-duplex/duplex switch (for m-memories and one 
processor, or p-processors 
and one 
memory) requires more components to be devoted to the switching, to 
buffering, and to arbitration control. Hence duplex switches 
are used on most multiprocessor 
computers. The processor-memory switching possibilities can be seen nicely in Fig. 13. The im- portant switch parameters 
are the number of memories, the num- ber of processors, and the number of simultaneous processor- 
memory dialogues. In current 
designs P always originates the dialogue, which is generally taken 
to mean the reading or writ- ing of a given word 
in Mp. The range of complexity is roughly S(nul1; 1M; 1P; concurrency: 1)I S(simplex1 I half-duplex2 I full-duplex3; (mM; 1P)I(lM; pp); concurrency: 1) I S(time-multiplex cross-point; mM; pP; concurrency: 1) I S(cross-point; mM; pP; concurrency: min(m,p)) 
An %duplex can 
be used to increase the number of processors which can be 
connected to the memory system while 
not having 
to provide additional switch points on each memory. For example, in the CDC 3600 [Casale, 19621 a basic S(8M; 4P; concur- rency: 4) is expanded by placing 
another S(1M; 6P; concurrency: 
1) in series to give a possible overall 
S(8M; 24P; concurrency: 4). This scheme was used 
to provide multiple processor accesses 
to the memories. Processor-control switching. The first switching problem developed with the need to communicate with several input/output devices. This switching is hierarchical in 
nature; one (or two) processors 'A switch which allows communication 
in one direction between 
two ports. ZA switch which 
allows communication in 
either direction but only one direction at a time. 3A switch which allows 
concurrent communication between two 
ports. 
70 Part 1 I The structure of computers maintain control of many Ks by giving 
a K a single instruction task. At the completion of the task the K signals the processor that the task has been completed. The switch provides a link between processor and controls for the secondary memory or the terminals and is parameterized by the number of processors, the number of controls, the number of simultaneous conversations, and who originates 
the dialogue. In these switches 
the control of information transmission 
is always by the processor. The evolution has been approximately 
as follows: 
1 S(nul1; 1P; 1K; concurrency: 1; initiator: P) 
P and K are connected during 
data transfers. S(simp1ex I half-duplex I full-duplex/duplex; 1P; 1K; concurrency: 1; initiator: P, K) Each K operates independently because it can return 
or request communication 
with P when control 
task is com- pleted. S(dua1-duplex; 2P; 1K; concurrency: 2; initiator: P, K) Duplex paths from dual P™s to each K for reliability. S(cross-point; pP; kK; concurrency: min (p,k) initiator: P,K) General case of multiple P™s and K™s with communication 
among the components. 2 3 4 The early machines used 
the first structure, and concurrent 
operation of controls was possible only 
by starting several controls and by very 
carefully programming the timing for the data trans- fers. Two conditions occurred to cause this: 
The buffering for 
a T or an Ms was associated 
with the processor, and the control could not 
signal the processor. Although 
rather trivial to imple- ment, the idea (item 2 above) of allowing a K to signal the proc- essor did not occur until after the 
idea of arithmetic processor traps were incorporated 
into processors. The interrupt was used 
as the method 
by which a K communicated its desire to converse with a 
P. The early IBM 709 provided a separate, independent 
processor for 
handling the communication with 
input/output equipment. Simultaneous processor-to-input/output or secondary- 
memory dialogues could take place (provided 
the devices were connected to the 
right processor). 
In most of the early computers, part of the control function (data buffering) was associated 
with the Pc, and, as such, only one device could 
operate at a time. This stemmed from the comparatively high cost 
of registers, so that links were established for a fixed period of time during 
a com- plete block transfer 
of data. In some of the military computers a 
duplicate set of K™s is provided for reliability. The more elaborate switching structures (types 3 or 4 above) are rarely used 
between Pio™s and K™s; thus to work on a peripheral requires the use of the rest of the com- puter. The S. dual-duplex is becoming more common; it provides a method 
of off-line operation for maintaining better component utilization and a more 
reliable structure. Control-terminal and control-secondary-memory switching. 
The switches which link 
a control 
with a particular terminal or second- ary memory are generally fairly straightforward. Normally, a fixed duplex switch 
is used. However, 
a dual-duplex 
switch is used if multiple access paths to the 
component are required. The switch links a secondary 
memory to a control during the transmission of relatively long information units (e.g., records). 
A typical ex- ample of such a switch 
is the bus structure used when magnetic tape units connect to a common control. 
Only one of the units operates at a time (although 
all can be rewinding simultaneously). The switches are far less interesting than those above. Because they are nearer the periphery, failure in them does not imply a failure in 
the complete 
system. Processor function The emergence of complex PMS structures is coincident with the 
development of functionally specialized 
processors. In the 
simple computers of Figs. 5 to 9 there is place only for Pc. In 
the general lattice there can 
be a Pc 
specialized to perform no 
input/output operations; one or more Pio™s specialized to communicate with 
the T™s and Ms™s and even to organize information in Mp for transshipment; additional Pio™s specialized to handle graphic dis- plays (hence P.display); and even P™s specialized to work on spe- 
cific data-types (for example, P.array) or specific algorithms (e.g., 
the fast Fourier transform). In addition, any of these processors may be realized by microprogramming, which 
is to say, by having 
its ISP interpreted by a specialized 
P.microprogram. Although the existence of various functionally specialized 
processors is coupled most closely 
with the PMS structure dimen- sion, the processors themselves 
are defined primarily 
by the data- 
types they 
can process. In this they agree 
entirely with the com- puter-system-function dimension. Possibly the processor-function dimension should 
be considered simply an extension of the com- puter-system-function dimension. 
On the other hand, the inclusion of microprogrammed processors really extends 
the PMS structure dimension to where a 
P can be seen as 
a cascade 
of two P™s. The processor-function dimension 
in the computer space is laid out in an evolutionary way, so that its correspondence with 
PMS structure is clear. P.microprogram is put at the beginning of the dimension ahead of Pc, not because it occurs earlier in 
evolu- tionary development, 
but because it extends the PMS dimension 
Chapter 3 1 The computer space 71 down into 
the processor. Any of the P™s along the dimension can be attained 
by a 
P.microprogram. As an actual dimension characterizing a total computer 
it must be viewed cumulatively (similarly to the data-type dimension). Thus, if a computer has a Pio, it also has a Pc, and if it has a P.array 
it also ha5 the prior ones. 
There are 
numerous exceptions 
to this, such as small 
Pc™s with •‚.displays (hence with no 
Pio™s). This evolutionary ordering 
does not correspond 
to complexity or num- 
ber of data-types in 
the P. 
Pc and P.array are the 
most complex; Pi0 and 
P.vector,move are least. We will make a few 
brief comments on each functional type, 
taking them in 
the order of the dimension. Microprogram processor 
(P.microprogram). The term microprogram- ming was introduced initially in 
ﬁThe Best Way to Design an Automatic Calculating Machineﬂ 
(Wilkes, 1951~). We use ﬁmicro- programmedﬂ to mean that an 
ISP is defined by an interpreter program residing in an internal Mp, processed by 
an internal processor (the •‚.microprogram). Thus the structure 
is really an external processor (ISP) 
being defined by the computer formed as P : = Mp(interna1; read-only)-P.microprogram The operations that microprogram processors perform are primitive in comparison with other processors. The task of the microprocessor is to interpret 
the instructions of the ISP it is realizing. This involves mostly 
data transfers among the registers of the processor state (Mps) plus simple boolean tests. Although 
it must handle all the data-types of the larger ISP, it does 
so only as bit fields to be extracted and transferred from one register to another. The complex data operations (e.g., multiplication) are carried out 
by other 
units (D™s). In fact, if a complex 
instruction set were 
to be used for the P.microprogram, the external processor might as well be implemented 
directly in hardware. 
In very minimal P™s, for example, C(PDP-8) in Chap. 5, the ISP is essen- tially already 
at the level of a microprogram ISP, 
as shown by 
the inclusion of instruction that can be 
microcoded. The long lag 
between the idea of microprogramming and its more widespread adoption 
is due to several reasons. Early ISP™s were comparatively straightforward, 
so that a microprogram ap- proach was not economically justified. The interpretation overhead time is higher than with 
the hardwired approach, and unless complex functions are realized this time becomes objectionable. In addition, suitable 
read-only memories 
were not developed 
until the mid 1060s (though it is imclear whether this is came or effect). 
An additional feature of using a P.microprogram is the ability to realize several ISP™s within a single physical processor. 
IBM has exploited this 
feature extensively in the System/360 (Part 6, Sec. 3), which is by far the most ambitious use 
of microprogram- ming. One can argue 
that without the additional payoff, which was used to ease the transition to a new incompatible 
computer system by providing 
emulation of the old system, 
the micropro- gramming would be marginal. Several P.microprogram design 
approaches have emerged: 
Kampe (Chap. 29) presents a design based on a short word; 
the internal processor is very much like a 
conventional processor. At the other extreme, 
the IBM System/360 (Chap. 32) is based on a long 
word which 
allows multiple operations 
to be coded in parallel. (The parallel operations 
are necessary to gain an accept- able performance 
level.) Thompson Ram0 Wooldridge 
called their AN/UYK a ﬁstored logicﬂ computer, and 
it provided the ability to use primary memory for 
defining the ISP. The IBM System/36O Model 25 (page 567) also iises this approach. The Hewlett-Packard desk calculator (Chap. 20) shows 
the use of microprogramming on a relatively 
circumscribed, but complex, task. Central processors (Pc). These processors interpret an instruction 
set for manipulating arithmetic, 
logical, and symbolic data-types. In all simple systems it is the only processor and thus does all tasks. The growth of processor specialization can be described in terms of relieving the Pc of simpler functions 
that require sub- stantial processing time but do not 
make full 
use of the devices within the Pc, such as 
the arithmetic 
units. Crucial 
to this issue is the time it takes the Pc to switch from one task to another 
(recall the discussion on Mps, 
the processor state), since many of the jobs that are extracted to specialized processors are demand jobs, such as input/output. With the removal of tasks from 
the Pc, it 
becomes more spe- 
cialized. A very 
pure example of this is the Pc 
of the CDC 
6600 (Chap. 39), which has no input/output instructions of any kind in the Pc. That is, not only has the control and management of communication and transmission with the 
T™s and Ms™s been re- moved from the Pc, but the act 
of initiation has been removed as well 
and placed in the Pio™s. Thus, the 6600 Pc is just an 
engine for working on the arithmetic, 
logical, and symbolic (ad- dress) data-types. The mixture of operations to be performed in most complex algorithms prevents specialization 
of the Pc from going very far, 
e.g., from 
there being a P.arithmetic, for with every switch 
be- tween capabilities distributed in distinct 
P™s there must be inter- communication of the components, which introduces an overhead 
cost in processing time. 
72 Part 1 I The structure of computers lnput/output processors (Pio). The Pi0 specializes in the manage- ment of peripherals (secondary memories and terminals). They are also called peripheral 
processors, data channels, and channels1 
The tasks a Pi0 
and its subordinate peripherals perform 
are the 
transmission of information between Ms and Mp; the transmission of information between some extra computer real-time system (e.g., human); and the transmission of information outside 
the C, 
via a T to some other information media 
(e.g., a 
card reader, card punch, line printer, etc.). 
All the above tasks are similar and often are considered the same, though in principle they 
can be 
quite different. A task in this environment 
is the management of some quanta of information, whether it 
be one bit or character, a voice 
message, or a record or 
file from magnetic disk or 
magnetic tape. 
Thus a Pi0 
does not usually change any information; 
it is merely an interpreter for moving 
information. There are three exceptions: Computation is required for error and correction and/or detection; 
computation is required if recoding and reformatting are done; and computation is required when search operations 
are carried out on Ms without Pc intervention. 
To accomplish the above tasks requires a fairly 
simple instruc- 
tion set. 
Typically it contains jump (branch); data transmission within Mp to initialize process variables; 
simple counting ability, e.g., to control 
error retries; subroutine calling; 
interrupt process handling; initializing 
KMs or KT; testing the state of KMs or KT; and sometimes code 
conversion (data in one code format is con- verted to another code). Thus substantial arithmetic and logic facility is not needed. Part 
4, Sec. 1 provides a 
detailed discussion of Pio's. Display processors (P.display). The P.display is a complex Pi0 
that processes information for display terminals. The data-type is a representation of a complex graphic object, e.g., lines, 
points, curves, and spatially localized text. 
The representations vary con- siderably from 
system to system, using various list 
pointers and vector encodings. 
The operations on the data-types include the maintenance of the display (due to the short-term persistence 
of the CRT); the selective modification of the representation under commands from the T.display or the Pc, such 
as adding or deleting a line, inserting text, etc.; 
the control of T.inputs such as key- boards, light 
pens, joysticks; 
and the performance of more complex 
spatial transformations, such 
as translation, rotation, scale change, 
and determination of hidden lines. 'These terms 
are usually used 
without distinguishing between a Pin and a Kin, that is, whether the device interprets a sequential program (and thus is capable of sustained independent activity) 
or only decodes a single 
instruction. The •'.display is a good 
example of a highly 
complex but spe- cialized data-type for which there are substantial local operations 
to perform, that is, where no interaction 
is needed with 
a complex algorithm (that requires the Pc). 
Users of displays wish to correct, modify, and transform the display in geometrically simple 
ways (in effect, edit and 
view) between processing of the graphic infor- mation by complex algorithms. 
Thus the graphic display is a prime candidate for the development of a specialized processor. The DEC 338 (Chap. 25) is typical of these processors, being neither the simplest nor 
the most complex (e.g., it does not have 
rotation or hidden line elimination instructions). 
Array processors (P.array). The array processor might be considered a more general 
Pc. It has been proposed or discussed in 
the litera- ture for some time. (See bibliography for Chap. 27, page 329.) The information unit processed is an array 
of one (vector) 
or two (matrix) dimensions. Instructions are provided to operate on these data. The specification of algorithms for a P.array is based on the assumption that an operation can be carried out 
in parallel for array elements. 
Actually, both serial (sequential) and parallel (concurrent) execution can be implemented. Both structures have 
the same logical characteristics, from an ISP viewpoint, 
and may differ only in execution rate. The three array 
processors, ILLIAC IV (Chap. 27), NOVA (Chap. %), and the 
IBM 2038 (page 577), are discussed in 
Part 4, Sec. 2 (page 315). Vector-moue processors. The vector-move processor is a special-case P.array. It is capable only of moving a word vector 
at some loca- tion in Mp to some other location within Mp. 
Because of its limited instruction set, such a P is found only 
in computers which require constant Mp shuffling. This condition 
arises either because of a hierarchy of Mp speeds or because the programs must have a particular structure before they can 
be interpreted by the 
proc- essor. A time-shared computer might require such a processor for 
multiprogram memory management. 
It is therefore common 
to find block (vector) 
transmission instructions in a Pc. The IBM Sys- tem/360 has Pio(Storagc 
channel) for this function 
(page 577). Special algorithm processors (P.aZgorithm). Only a small 
number of special algorithm 
processors have been 
specified and/or imple- mented. High performance 
is almost guaranteed by hardwiring and through specialization. The time to fetch the algorithm (instruc- 
tion fetch time) 
and many of the references to Mp for temporary data are eliminated by hardwiring. A hardwired algorithm can easily outperform a stored program by a factor of 10 - 100. The lack of these processors in systems stems mainly 
from lack 
of market demand. 
Chapter 3 I The computer 
space 73 It is not clear that the 
special algorithm 
processors meet our criteria for being a processor, 
because of the rather limited func- tions they perform. In 
fact, some so-called processors are just K™s, or D™s since they have no instruction location counter 
and inter- pret only a single 
instruction at a time, requesting each new instruction from a 
superior component. Algorithms which have been hardwired 
(or proposed) include 
the fast Fourier transform using 
the Cooley-Tukey algorithm; cross-correlation, autocorrelation, and convolution processing; polynomial and power-series evaluation; floating-point array processing; and neural network simulation.™ Language processors (P.Zanguage). Laqguage P™s interpret a lan- guage that has been designed to some external 
criteria, such as a procedure-oriented language 
(ALGOL or FORTRAN) or 
a list language (IPL-VI). 
Thus complexity takes 
the form of a complex data-type for the ﬁinstruction,ﬂ rather than a complex data-type for processing 
(e.g., floating complex numbers). 
If such processors were extended to 
do all the things a Pc also does, then they 
would become more 
complex than a Pc. However, 
to date, most of them are experimental and focus exclusively on 
language interpretation. In Part 
4, Sec. 4, several examples are presented. It is worthy of note that of the three 
P.1anguage.s only EULER (chap. 32) has been implemented in hardware 
using a P.microprogram. Memory access The most useful classification 
of memories is according to their accessing algorithm.2 These are queue 
(i.e., access 
according to first-in-first-out discipline); stack 
(i.e., access 
according to first- in-last-out discipline); linear 
(e.g., a tape with forward 
read and rewind); bilinear 
(e.g., a tape with forward 
and backward read); cyclic (e.g., a 
drum); random (e.g., core); and content and associa- tive. All these memories 
are explicitly addressed except 
the stack and queue, which 
deliver an implicitly specified i-unit on each read. Memory size and basic operation times (Le., the time constants in the access algorithm) are important 
too, of course. But 
once a distinction is made between Mp and Ms, then for any given technological era there 
have existed characteristic sizes and speeds ‚Chasm: A Macromodular Computer for Analog Neuron Models [Molnar, 19671. ‚Access for writing should be distinguished from access for reading. Mem- ories are conceivable with 
arbitrarily different read and write access algo- rithms (e.g., random read and cyclic write). However, in general, the two access algorithms 
are tightly coupled, and 
normally only the read access algorithm is given. for memories 
of a specified access 
algorithm. Where there 
has been variation, either it has 
been linear with 
size (e.g., buying two boxes of magnetic core 
Mp versus buying one) or 
there has been a narrow range of cost/performance tradeoff (as in data rate 
for magnetic tapes, in which modest increases 
in density and tape 
speed can be bought for substantially increased 
dollars). Table 5 shows the relative price, 
size, and performance of various mem- ories. The memory-size versus 
information-rate plot (Fig. 14) shows the clustering of memories and their suitability for a 
particular function. From a technology standpoint, Mp™s have been 
constrained to either cyclic- or random-access memories (although 
one can easily construct any type 
from random-access 
memories). In 
Part 2, Sec. 1 we have not separated the machines according to whether they 
used cyclic- or random-access memories. 
The early first-generation computers used cyclic-access memories. 
Part 3, Sec. 2 presents only the cyclic-access memories. 
Similarly, Ms™s have been 
constrained to be cyclic or linear, 
although quasi-random 
access has 
been achieved with 
some disks and magnetic-card memories (random 
by block and linear or cyclic within a block). Any Ms™s can be part 
of almost any computer 
structure. Thus there 
is no large effect of Ms structure on the main design features of computer systems, and they are not discussed to any extent in the remainder of the book. Our discussion of memory type below deals 
exclusively with Mp and Mps. Stack and queue memories (M.stack, M.queue). Data elements in a stack and queue are not accessed explicitly, as we noted above. The stack has 
some rather unique properties 
that aid in 
the com- pilation and evaluation of nested arithmetic expressions. Although there are 
no machines employing stacks 
exclusively for 
primary memory, there are stacks in some arithmetic processors. Part 3, Sec. 5 is devoted to processors with stack memories 
(i.e., with stacks in the processor state). The IPL-VI machine (Chap. 30) is the only computer in the book to have 
its entire memory organized 
as a list 
of stacks. Although no hardware exists that inherently behaves as a 
stack or queue,3 
it can 
be simulated by 
a random-access memory. 
A shift register capable of shifting in either 
of two directions is a stack. Cyclic-access memories (Mp.cyclic). Nearly all the first-generation (vacuum tube) computers 
had Mp.cyclic. The Mpxyclic 
acoustic, magnetostrictive delay line, 
and magnetic drum provided an in- 3Small (10 - 1,000 word) queue- and stack-accessed memories are espe- cially easy to build with 
large-scale integrated-circuit technology. 
74 Part 1 I The structure of computers Table 5 Memory characteristics Memory size Memmy performance 
Module Modules/ Access Data 
Access size computer time 
rate Memy module Function method (bits) sec (bits/sec) Cost/bit( $)I Punched paper card 
Magnetic card Magnetic tape Moving-head disk 
pack Fixed-head disk Drum Bulk core memory High-speed core or Integrated circuit thin-film memory (scratch-pad memory) Integrated circuit Read only 
(content addressable) (capacitor, inductor) 
permanent, random + (500 - 1,000)/ archival linear 
card; - 1,000 card/unit secondary, linear + 3 x 109 archival constant + secondary, archival secondary, files swapping 
secondary, files swapping 
secondary, swapping primary and/or secondary, swapping primary primary, processor state primary, cache 
processor instruction-set definition cyclic linear linear + cyclic cyclic cyclic random random random content, random random 2 x 10s 2 x 108 5 x 101 (1 - 5) x 107 107 105 - io6 103 - 105 2 x 105 (1 - 5) x 105 1-2 1-4 1 - 16 1 - 16 1 - 40 1 - 10 1-8 1 - 16 1 1-2 1 io0 - 103 104 2 x 10-6 + 2 x 10-1 1.5 x 10-8 + 0.4 x lo6 10-1 - 100 5 x 10-5 100 - 102 0.4 - 4 x 106 2 x 10-7 + 10-4 10-1 - 100 2.5 x 106 3 x 10-6 + 10-4 10-3 106 - 101 -10-2 (5 - 30) x 10-3 io6 - 107 10-3 0.02 - 0.05 (2 - 10) x 10-6 106 - 108 0.05 - 0.25 (0.2 - 2) x 10-6 107 - 10s -10-7 109 0.25 - 1.0 1-3 109 -10-7 10-6 - 10-7 10s - 109 10-3 - 10-2 'The first 
component is the memory media (e.g., a disk pack), 
and the second component is the transducer (e.g., a disk drive) expensive, simple, producible memory. 
By the second generation 
the cost of Mp.random (though still more 
expensive than an Mp.cyclic) was 
about equal 
to the 
processor logic. 
The incremental cost for 
an Mp.random in a 
large system was 
then small, whereas the performance gain could 
be a factor of up to 3,000 (access 
time of 10 microseconds versus 
30 - 30,000 microseconds). Some 
of the first-generation machines were reimplemented 
using transistors 
(the LGP-30 became the LGP-21). Only 
a few new cyclic access machines were introduced in 
the second generation. 
Most notable was the low-cost Packard-Bell 
PB-250 using transistor logic and magnetostrictive delay lines 
(a derivative of the Bendix G-15 and NPL ACE). Nearly all these 
computers use some 
form of n + 1 addressing. The memory is organized on a digit-by-digit 
serial basis for a 
word (e.g., ZEBRA with binary 
and IBM 650 with decimal). 
Hence, the arithmetic or logic function hardware is implemented for only a single digit. 
An operation is done for the entire word by 
iterating over all 
digits in time; thus the cost of a serial computer is nearly independent of its word 
length. Because of the cyclic and synchronous nature of these Mp's, it is difficult to synchronize them with secondary memories 
and terminals (which are also synchronous). 
The very early machines had no large secondary 
memories. In some cases, where magnetic tape was used, it was added at very low performance (low density, 
low speed, and, 
therefore, low 
data rates) so that synchronization was not a problem. In other cases a small random-access 
core 
Chapter 3 1 The computer 
space 75 IOﬂ 1 0™0 109 108 I 07 r 106 J3 m 105 2. 0 al E 3 104 1 o3 102 10™ 100 11-21 Magnetic --- card L / \ /- I Moving head disk (6-8) ™ ™,l ----‚\ super conductive integrated an I Moving head disk-pak (1 unit 1 M=( processor definition 
1, read card only, capacitive Drum, fixed head disk 
(321 Ms -tape, drums,disc,magnetic (128) Content addressed. integrated M(terrnina1 M(working) IBM card (and card reader) -_~ - -~ ~ ~ 
- - ~~- ~ - ~ - ~-~ ~ M(Logic) \ Stepping switches Transistor circuits \ I ,Integrated transistor / \ /Mechanical Relays Fluid io5 lo6 10™ io8 io9 iolo ioﬂ 
™(x) indicates width 
of informotion,In bits Effective information rate! in bits/sec Fig. 14. Memory size versus 
effective information rate. memory was added to 
provide synchronization 
between the two memories (for example, IBM 650). Rundoni-uccess memories (Mp.randon~). Random-access memories 
were used late in the first generation, and they have remained 
the predominant memory during the second and third generations. It is unlikely that their popularity 
will decline unless content- addressable memories 
can be constructed sufficiently cheaply (if then). The earliest first-generation random-access memories 
were electrostatic and depended on maintaining 
a charge on plates 
of an array of capacitors. The most common was the Williams tube (invented by F. H. Williams at the University of Manchester) which works in essence like a 
CRT, with the beam used to charge a capacitor array 
at the tube 
face [Williams 
and Kilburn, 19491. Other schemes included an 
array of capacitors which 
were selected by digital 
logic (Pilot, Chap. 35). Late in the first generation Forrester [1951] invented the core memory, which rapidly 
became the predominant primary-memory 
76 Part 1 I The structure of computers component. It is unlikely that it will be replaced in the near future; the most likely candidate is large-scale integrated-circuit arrays of flip-flops. The random-access memory seems 
nearly perfect 
for the Mp™s of present computers. 
Of course, enthusiasm for this memory may 
be based on not 
knowing how 
computers would have developed if we had not had them. 
However, with 
little or no effort an M.random can be 
a stack, a queue, a linear, a cyclic, and even (within limits) a content or associative memory. It is an organiza- tion which 
is very hard to beat. Content-addressable and associative memories. It is posdde to conceive of many exotic accessing 
capabilities, and numerous proposals have been 
made involving either theoretical structures 
or experimental prototypes. 
Since no particular 
varieties have become widespread, terminology 
is still variable. 
Content- addressable memories are usually taken to mean a collection of cells of predetermined size (i.e., a fixed i-unit) such that if one presents as ﬁaddressﬂ the contents of a predetermined part of the cell (the tag or content address) then the 
contents of the entire cell will be retrieved. An associative memory is usually taken to mean a system 
such that, when presented with 
an item of informa- tion, it delivers one 
or more ﬁassociatedﬂ items 
of information. The principle of association is variable, yielding different kinds 
of associative memories. 
Content-addressable memories provide 
a form of association, as 
do all memories, in 
fact. Thus 
the term ﬁassociative memoryﬂ tends to denote forms of association different from familiar 
ones-forms that presumably have less sharp con- straints imposed by 
the structure of memory (as opposed to the structure of the information in the memory). No examples exist of a computer with 
a content-addressable memory as its primary-memory 
structure. However, both the IBM 360 Models 67 (page 571) and Model 85 (page 574) use 8 and -1,000-word content-addressable memories, respectively, to in- crease performance (in 
both cases they are transparent to the program). The CDC 6600 instruction buffer 
is in effect a small 
content-addressable memory. In 
the above three cases, the con- tent-addressable memories vary 
in size and position in the struc- ture; however, the pattern 
of use is common. There is a large but slower Mp.random behind the content-addressable memory. 
The purpose of the fast small 
content-addressable memory 
is to hold local, current data so that an access will 
not have 
to be made to the random-access memory. Small prototype associative addressable M™s have been con- structed, but they are normally based 
on random-access 
memories nnder the 
control of special hardware. There are immediate uses for content-addressable memories with a large information-content 
address. For example, the read-only memories for microprogram processors use 
long words principally because content-addressable 
memories are not available. Ideally 
a microprogrammed processor would like 
to look at a fairly large 
processor state to determine 
what action 
is to be taken in the microprogram. It is interesting to speculate about 
the evolution of computers if a content- addressable memory 
had been 
developed in place of the random- access memory. 
Mp concurrency Multiprogramming is the simultaneous existence of multiple, independent programs within Mp being processed sequentially or 
in parallel by 
one or more processors. Multiprogramming provides each user program with 
a memory space independent 
of other users. It may provide, 
in addition, the sharing by 
several users 
(for independent use, not for communication) of a block 
of Mp, which 
thus does not have to 
be duplicated. For 
example, operating sys- tems software, including compilers, 
assemblers, loaders, and edi- tors, can be usefully shared. The ability to have multiple programs gives rise to a corre- sponding problem 
of communication between programs. We have defined this as a correlated dimension in the computer 
space (interprogram communication) 
and will discuss 
it in the next sec- 
tion. The issues it raises are just the opposite from 
those raised 
by the 
requirement for multiple programs, which are discussed in this section. Here we are concerned 
with protecting one 
pro- gram from another-with assuring 
that no unjustified communica- tion will occur-and with obtaining appropriate space 
in Mp so that multiple programs can run. The requirement for protection is obvious. If two independent 
programs are to be resident in Mp at the same time, they must not have 
access to each other™s space. Not only 
would such 
access (especially for writing) have 
disastrous consequences when the programs are running, but they would be entirely unpredictable 
and undebuggable from the viewpoint of the programmer of each individual program. 
Thus this requirement is absolute; i.e., it must be highly reliable. This implies 
a hardware solution, although 
purely software schemes 
are possible in special cases. The requirement for appropriate space 
is somewhat more 
sub- tle. Certainly there 
must be enough space in 
Mp for all the pro- grams that are 
to be resident simultaneously. 
It must be possible to find that space, assign it to a new program, and make it available again when that program is finished. But what kind of space will do? Must 
it be a single interval of Mp, large enough 
for the total program with 
data? Arid if the program is assembled or compiled 
Chapter 3 I The computer 
space 77 in Mp and is removed temporarily to make room for another program, must it be 
brought back 
into the exact same addresses into which it was originally assembled? 
The key issue resides in the kind of intercommunications that hold within a 
program and its data, for these 
determine how and in what way a program is interconnected and depends 
on the specific Mp addresses that it 
occupies. These 
connections are of two kinds: explicit addresses present in the program and data and 
implict relations between addresses due to addressing algorithms (e.g., that programs are laid sequentially in 
Mp, or 
that the 
ele- ments of an array are to be accessed by indexing 
and hence must occupy consecutive 
addresses). Again, 
although some purely soft- ware solutions to the 
space issue exist, hardware is involved in a fundamental 
way. Thus, the two main questions 
of program concurrencyl- 
protection and space 
assignment-imply basic design 
features of a computer system. It might seem that they imply separate fea- tures and should be separate dimensions in the computer space. In fact, each proposal for 
how to solve the space-assignment prob- 
lem also 
contains a 
particular proposal for 
the protection 
problem. Thus we treat them as a single dimension. 
Virtual-address space 
and Mp mapping. Before considering various 
solutions to Mp 
concurrency (Le., the values along 
the dimension), let us introduce two concepts in terms of which all current solu- tions can be 
understood. Consider 
a particular program, PRO- GRAM-1, one of many that might wish 
to reside in the Mp. PRO- GRAM-1 assumes a set of addresses, some explicitly 
and some implicitly, in 
the addressing algorithm it uses. PROGRAM-1 re- 
quires a memory space that has addresses 
that satisfy all 
these requirements, the implicit and explicit ones. 
Other than 
that it 
does not care how these addresses are realized. Let us call this 
address space required 
by PROGRAM-1 its virtual memory, Mv. Thus, each program has 
its own virtual 
memory. (You might think of this as 
having its own 
Mp, except, as we shall see, this 
Mp may be many times bigger 
than any actual Mp and still be entirely feasible.) Actually to run PROGRAM-1 requires 
that it be placed in the real Mp in such a way that the real addresses of Mp containing 
it satisfy all the requirements, that is, that it be a faithful image of the virtual memory. Thus there must be some memory mapping 
that maps the actual addresses into the 
actual memory. Once PROGRAM-1 is placed in Mp 
there must be some process 
that takes each virtual address (as 
it occurs to be 
processed in an ‚See also Randell and Kuehner [1968]. instruction) and finds the actual 
address in Mp, so that the correct contents can 
be obtained. 
This might seem simply 
a complicated 
and abstract 
way to view matters, but it 
becomes essential 
as soon as we realize that the 
computer can have hardware memory mappings other than the 
familiar direct-addressing structure of Mp. Furthermore, if this mapping is given the right properties, 
it may solve some 
of the space-assignment and protection problems for 
Mp concurrency. What we have 
really done is to divorce the addressing required by the programs from 
that provided by the physical computer, so that we can redesign it (via the memory mapping) to meet new 
design requirements that were not 
apparent when the original random-addressing schemes 
were created. 
Let us make the notion of memory mapping more precise. 
The program contains 
virtual addresses, z (that is, symbols in the pro- gram that denote addresses are taken to denote addresses in Mv). During the execution of the program, whenever 
there is a refer- ence to an address 
z (either explicitly via an address 
calculation or implicitly via, say, 
getting the next instruction), a computation 
occurs on z to obtain the actual address in Mp. This 
computation is part of the Pc, just as 
is an automatic 
indexing or 
indirect- addressing calculation. It takes as input not just the virtual address z but information on 
where the program is located in Mp. The latter information is called the map, and a program™s map infor- mation is determined when it is placed into Mp on a given run. Thus, using our ISP notation, and calling the address calculation f, we get 
Mv[z] : = Mp[f(z,map)] That is, the information in virtual memory at virtual address z is the same as the information in actual memory at address This whole 
scheme is built to permit programs to be placed 
in Mp™s in various ways, 
e.g., relocated or scattered around, and still make it possible to run the program. Any such scheme brings a solution to the protection problem, namely, 
that for some values 
of z the above calculation cannot take place 
or is invalid (i.e., 
there is no mapping for z). 
This can correspond 
to a violation of protec- tion, which can then be prevented. All calculations may 
even be permissible, but f is so arranged that it 
never produces an address 
in anyone else™s part of Mp. The memory map is part of each user™s program. With many users, it must reside 
in Mp, since there will not be enough space in Mps to hold a large amount of mapping information. However, when a program 
is being executed, 
some part of the mapping information becomes part of the Mps (Le., at least the Mp address f(z,map). 
78 Part 1 I The structure 
of computers of the rest of the map). In addition, the map 
may contain special access control information, such 
as whether a part may be read, read as data, written, 
or read as program. The map 
can also collect statistical information 
concerning whether 
a part of the program has been used or has 
been changed (written). 
Random-access memories for 
Mp constrain the mapping by requiring linear 
addresses of the form Mp[O:p], 
since the mapping calculation must be economical (as it is performed with very high frequency). We would 
not consider a map structure which provides every word 
in Mv to be mapped into 
an arbitrary word in Mp, for this would 
require a map exactly the same size as Mv. 
With many programs 
in Mp, there would be little room for 
anything but maps. Similarly, 
the amount of processing in f, the calculation, must be very minimal. These two 
aspects constrain 
the mapping scheme strongly. 
The constraint to linear addresses appears to force the structure of virtual memory to consist of a multidimensional array. This 
can Table 6 Memory-allocation methods 
be one-dimensional, Mv[O:n], or two-dimensional, Mv[O:s][O:m]. It could be of higher dimension, but the 
need seems not to have been 
felt (since 
within any 
single dimension one can have 
multi- dimensional arrays 
as one normally does 
in a regular Mp). 
How- ever, the two-dimensional array, which 
also is called segmented 
addresses, since it can 
be taken as a 
discrete collection 
of s + 1 segments each of m + 1 linear addresses, has advantages 
in terms of the mappings; namely, segments 
can be placed disjointly in 
Mp without fear that virtual-address calculations 
will cross from one 
segment to another. 
With this introduction to the problems of multiprogramming we will look at some of the hardware schemes. Table 6 provides a summarization of them, including a brief 
description of how each scheme operates. 
No special mapping hardware. If no hardware 
exists in the Pc to accomplish a memory address mapping, 
then when the address Hurrlioare designution (cinaizged in order uf tncrmwig hardiLcire coinplerity) Method of™ memory allocation among multiple users Limits of particular method (example of use) Xo relocation Mr 5 ,Mp; Conventional computer-no memory-al- No special hardware. 
Completely done by inter- location hardware pretive programming. 1 + 1 users. Protection bit for each A protection bit is added to each memory 
cell. The bit specifies whether 
the cell can be written or accessed. memory cell 1 + 1 users. Protection bit for each A protection bit is added for each page. (See 
memory page. above 
scheme.) Page-locked memory Each block of memory has a user number which must coincide 
with the currently active user 
number. Completely interpretive programming 
required. Very high cost in time is paid for generality. (JOHNNIAC interpret- ing JOSS). Only 1 special user 
+ 1 other user is al- lowed. User programs must 
be writ- 
ten at special locations 
or with special conventions. or loaded or assembled into place. 
The time to 
change bits if a user job is changed makes the method nearly useless. No memory allocation by hardware. (IBM 1800) No memory allocation by hardware. (SDS Sigma 2) Not general. Expensive. Memory reloca- tion must be done by conventions or by relocation software. 
A fixed, small number of users are permitted by the hardware. No memory allocation 
by hardware. A program cannot be moved until it is run to completion. 
(IBM System/360) 
Chapter 3 I The computer space 79 Relocation and protection: Mu 5 Mp: One protection count and 
one field reg- ister (addresses formed and checked by logical operations) 
All programs are written as though their origin were location 0. The count register deter- 
mines the 
number of high-order bits to 
be examined. The 
field register is then com- pared for identity with 
the requested address. One set of protection 
and relocation reg- isters (base address 
and limit regis- ters). Also called boundary registers. Two sets 
of protection 
and relocation reg- isters. Two segments. n 2 3 sets of protection and relocation 
registers. Mapping, Mu 2 Mp: Memory page mapping Memory page/segmentation 
mapping Indirect references through a descriptor table to segments. All programs written as though their origin 
were location 0. The relocation register 
specifies the actual location of the user, and the pro- tection register specifies 
the number of words allowed. Similar to above. Two 
discontiguous physical areas of memory can 
be mapped into a homo- geneous virtual memory. Similar to above. More similar to page mapping. For each page (26 to 21' words) in a user's vir- tual memory, corresponding 
information is kept concerning the actual physical location 
in primary or seconaary 
memory. If the map is in primary memory, 
it may be desir- able to have "associative registers" 
at the processor-memory interface to remember previous reference 
to virtual pages, and their actual locations. Alternatively, 
a hardware map may be 
placed between 
the processor and memory 
to transform processor virtual 
addresses into physical addresses. 
Additional address 
space is provided beyond a virtual memory above by providing a seg- ment number. This segment 
number ad- 
dresses or selects the page tables. 
This al- lows a user an almost unlimited set of ad- dresses. Both segmentation and 
page map look-up is provided in hardware. May be thought of as two-dimensional addressing. All data 
are considered 
part of a descriptor array which 
is referred to by a number. A descriptor table indexed by 
the descriptor number is used 
to locate the array in Mp and give its size. Memory allocation blocks 
must be in power of 
2. Unless blocks are the same size, the memory 
utilization can be poor. Although faster 
than the 
fol- lowing scheme (which requires 
a hard- ware adder), 
the inflexibility of loca- tion and size makes it restrictive. (IBM 7040) As users enter and leave, primary-mem- ory holes form, requiring the 
moving of users. Pure procedures can 
be im- plemented only 
by moving impure part adjacent to pure part. 
(CDC 6600, PDP-6) Similar to above. Simple, pure proce- dures with one data 
array area can be 
implemented. (UNIVAC 1108, PDP-10) Has not been used 
in any conventional 
computer. Relatively expensive. 
Not as general as following method 
for implementing pure procedures. (Atlas, 
CDC-3500, SDS-940) Expensive. Little experience to judge effectiveness. (GE 645, IBM 
360/67) An indirect reference must be made to the description table in Mp. (B 5500) 
80 Part 1 1 The structure 
of computers z is encountered in the program, the information at Mp[z] will be obtained. There are 
still, however, two different ways 
to obtain the effect of a virtual memory. First, one can operate 
interpretively, with a software system 
taking the place of hardware. That is, the programs of all the users are in a 
nonmachine language (e.g., a higher procedure-oriented language), and 
each access in the language is processed by the software interpreter before an access is made to Mp. It is clear that all the logical power of a memory mapping is available with this scheme. The only drawback is the loss of efficiency from 
the interpretation, which may range from a factor of 5 to 100. Conse- quently this scheme 
is used only 
in special 
circumstances, such as multiuser time-shared 
conversational algebraic languages. The second scheme is to modify the code at the time it is placed in the Mp for a given run, so that all addresses in the code corre- spond to the actual 
Mp addresses used. 
That is, an assembly or 
translation operation is performed each time the 
program is placed in Mp. The advantage of this scheme 
is that no further address calculations are necessary. There are three disadvantages. Assem- bly operations are expensive so that, although the scheme is tolera- ble if the program is brought in once and run to completion, it is not tolerable if programs are continually being swapped 
in and out of Mp. In 
addition, the program must 
be laid into continuous intervals of Mp corresponding to predetermined segments of the program, for assembly 
occurs on a static representation of the program and cannot unravel 
the potential effect of address algo- 
rithms. Finally, the 
size of Mv (i.e., the addresses used 
externally) must be not 
greater than Mp. Relative to these software schemes-one interpretive and 
very expensive and one involving assembly (Le., 
compilation) and 
load- ing-the hardware schemes to be described 
appear as address 
interpreters, where the cost of continuous interpretation has been made tolerable. Protection for words or pages hardware. There are three schemes in Table 6 that provide a means of protecting one part of Mp against references from 
other programs. The rationale for these 
designs is that there will be only two users (or user classes), 
one user being superior and 
assumed perfect (its program debugged). References to Mp 
via the imperfect program to a perfected 
and superior part of Mp are forbidden. These 
schemes provide no method of hardware mapping, 
and physical addresses 
are the same as virtual addresses. In 
the simplest scheme, as in the IBM 1800 (Chap. 33), a protect bit 
is added to every word in Mp, that is, Mp[O: 2lS - 1](0: (w - l), protect-bit) Every reference 
Mv[z] takes 
place as Mv[z] : = (7Mp[z](protect,bit) + Mp[z]; Mp[z](protect-bit) + protection violation t 1) That is, any reference 
to a word with a 
protect bit 
causes an 
error. The other two schemes protect on the basis of blocks of words. Protection and relocating register(s) hardware. A protection and relocation register mechanism 
is used in four schemes 
of Table 6. These provide either one concatenated, one additive, two addi- tive, or n additive register pairs for 
mapping a 
single program 
into one, one, two, or n nonadjacent 
blocks in Mp. The authors know of no schemes where more than three 
registers are used; this would 
really be akin to using a more general page map. Generally, 
these schemes restrict Mv 5 Mp. An additive protection and relocation register pair is shown in Fig. 15 in which four users 
are occupying a 
Mp[0:7999]. Each user program 
is written to occupy a continuous 
address space in a virtual Mv. Thus in ISP, when Pc is running programs for user-j, which address Mv[z], 
with z varying from 
0 to vj 
- 1 the map- ping uses actual memory. The action is Mv[z] : =((z < Protection) -+ Mp[z + Relocation]; z 2 Protection + (Protection violation t 1)) Protection and 
Relocation are the two registers that specify map- 
ping. The implementation of this scheme generally 
takes the form of adding the contents of the relocation register after all address 
calculations have taken 
place. Thus, in PMS we might think of the structure Mp-K(ad&ess translation)-Pc. M(l Protection,Relocation) Page-map hardware. Figure 16 shows the memory allocation 
using a page map. Note 
that, of the 4,096 words it is possible to define by the map, the range 1,024 to 2,047 is actually undefined. Along with the map containing the addresses to words in actual Mp, it is desirable to have accessor protection control information. Such information might specify: 
1 No restrictions (any form of reading or writing can take 2 Read only as 
data. 3 Read only 
as a program. 4 Writing. 5 Undefined. place). 
Chapter 3 I The computer 
space 81 6 Defined but located in Ms. 7 This page has been written in (to know whether a copy 
in Ms has to be updated). 8 This page has been accessed. This scheme 
is essentially a generalization of n protect/relocate registers hut includes more 
control bits, 
suggested above, 
and restricts each block to he the 
same size. Note 
that Mv can he greater than Mp. In 
addition, parts of the virtual memory may 
remain unused. There are 
two ways the above scheme 
is usually implemented: 1 A complete map 
is first considered as a conventional, ex- plicitly addressed M whose addresses 
correspond to the 
virtual-address pages. 
At a given page-memory address 
the contents of the map specifies the address in Mp. The map is similar to an indirect reference. 
However, the map 
is usually about 10 times faster and about 
1/1,000 the size, since it keeps track only of pages, not words. The PMS structure is Mp-M.map-Pc 2 The map 
is retained in Mp and referenced 
by a protection and relocation register which 
are set for the particular active user. In 
order to avoid making 
references to Mp for each word reference to Mv by a Pc, a small, fast 
M(content ad- dress) is placed between Pc and Mp. The PMS structure is L(data) - t K(address translation) 
t L(addresses) t I M(content address; 
8 - 16 words) Pc MenLorii-segmentation hardware. Figure 9 (page 574) in the intro- duction to the IBM System/360 shows 
the logical mapping process for a segmented 
memory. There is provision for a very large two- dimensional virtual-address 
space. This scheme is discussed exten- 
sively in the literature 
[Arden et al., 1966; Dennis, 1965; Gibson, 
19661. The physical implementation is similar to that 
of paging. Note that two levels of mapping are provided: the segment map and the 
page maps. 
The two levels facilitate the sharing of a single segment by two jobs. The Hurroughs R 5000 (Chap. 22) and the later 
R 8500 have a mapping that is more closely 
integrated into the Pc because 
they Relocation r ' E3 - ,/ k--Z-21 Protection H + uq Table of user location information 3 0i 2 2 User-memory'' addresses in 1,000s of words Hardware registers,I7 
----'I ,, when user 2 is running \ "Absolute memory'' addresses in 1,000s of words Fig. 15. Memory allocation using a boundary (relocation and protection) 
register. provide a 
variable-sized address space 
(not paged) within 
a seg- ment. The segments are named, and a 
large number of segments exist. lntetprogram communication 
The dimension of interprogram communication 
is completely cor- related with the multiprogramming dimension as 
we have previ- ously noted. To have a problem 
of intercommunication, there must be a structure of components that require communication. 
At the simplest level 
the dimension is represented by a single program, and there is no need for intercommunication. Variables of the r,,,,,, (2-4) 2048-4095 for Map locoting user,~jk virtd memory in "absolute" memory 0-1023 for y Absolute memory 
Fig. 16. Memory allocation using a page allocation map. 
82 Part 1 I The structure of computers program are completely accessible to the 
whole program, and 
the address space is essentially uniform. 
The second value 
of the dimension, subroutine calling, produces a hierarchy 
of communication contexts. There is not a 
fixed num- ber of levels to the hierarchy, since 
each subroutine 
may call others ad izuuseum. When subroutines are present, address names 
and values within the subroutine become 
addresses which 
are local to that part of the subprogram. Such a structuring is apparent when looking at the higher-level languages such as 
FORTRAN, ALGOL, and PL/I, where there are 
explicit statements for con- 
trolling the names (addresses) 
that are available to 
each of the parts of the program. The concept 
of subroutine structure has been with us almost from 
the first programs. The next value 
of the dimension relates to signaling within a 
single process. 
It is akin to subroutines embedded 
in hardware. These are called 
extracodes and were perhaps 
first suggested for 
the Atlas (Chap. 23). Each extracode 
can be looked at as just a call to a specific subroutine. The variables of the user (caller™s) 
program are made available 
to the called (extracode defined) program. The calling usually is accompanied by a 
context shift, 
in which a completely 
different program (one that is used by 
any number of calling programs) takes 
command to interpret the in- struction. This scheme is used in systems which 
are controlled by 
a special 
software monitor. 
When a function 
such as the input or output of a file is 
required, the main program 
issues a call to 
the monitor to make the transfer. (In theory, 
the monitor knows 
about conditions in the system and has the capability to perform the complex function.) A central monitor control can then begin to run another program if the request is one which would normally 
halt the computer. 
This form 
of communication is useful to supply extra facilities to users and to have a method of knowing what the users are doing (e.g., 
so that equipment will be better utilized). As more complex program 
structures are directly 
represented by the hardware, the 
intercommunication complexity also 
in- creases beyond 
the simple subroutine call. If a segmented-memory scheme is used, the problem of communicating between the 
seg- ments can be 
solved in a range of ways. The value of the range would be somewhere 
between ignoring the problem with 
the hardware and providing methods 
for naming of addresses between the communicating segments. In the above cases, 
the communication among the various programs or parts of programs is done explicitly by one 
program to another program. The instruction trap does not fit this view so nicely. Here, conditions occurring within a 
single process which 
are not explicitly 
called cause another part 
of the program to be 
called. Typical conditions which cause 
traps are arithmetic 
results outside expected range 
or erroneous program conditions (e.g., 
trying to call someone 
else™s program). The trap causes a change in context that is synchronized with 
the process causing 
it. Trap- ping is a form of program interruption; a trap is an intraprocess interrupt as distinct from interprocess interrupts. Intercommunication between two independent processes (being carried out by two 
independent components) is usually accom- 
plished by using the program interrupt. The interrupting 
process requests that a program interrupt occur in a component 
(inter- ruptee). The interrupter™s 
request is acknowledged by 
the inter- 
ruptee, and a change 
of process state occurs in the interruptee; a new 
process is then run in the interruptee 
on behalf of the interrupter. The 
program interrupt is used among processors 
in a multiprocessor system 
and between 1Pc 
and nPio™s. A control K may also use the program-interrupt request 
to communicate with its superior Pi0 or Pc. For example, a Pi0 does 
not usually have the logical capability to execute an algorithm which would 
decide that action is to be 
taken for various error conditions. Usually the interruptee 
is equipped with certain 
logic which 
is capable of arranging priorities of requesting interrupters. The 
typical kinds of interrupt requests are component faults (e.g., 
parity error), a timer has counted 
down, and various task 
comple- tions (e.g., a 
program has 
completed, a 
tape unit has rewound, a disk arm has stopped moving, a certain record has 
been found on tape, a buffer is full). State diagrams would show 
how each of the communication methods above are similar to one another. A typical interrupt state 
diagram is shown in Fig. 17. There are 
four states: normal process 
interpretation, process state saving, interrupt process interpreta- tion, and process state restoration. The sequence is as follows: Normal instruction interpretation is occurring in the inter- ruptee. The interrupter requests an interrupt. After some delay, t.acknowledgment, a 
state is reached in which part of the interruptee™s process state is saved. After t.acknowledgment + tsave, a program 
is running in the interruptee in response to the interrupter. The interrupt program is run for 
t.interrupt. At the completion of the interrupt program, the original process state is restored in the interrupter. 
After t.restore, normal processing resumes 
in the inter- rupter. 
Chapter 3 I The computer 
space 83 The significant attributes of the system are the 
various times 
re- quired to move from state to state. These times 
are directly 
related to the amount of process state which must 
be saved (and restored) when switching context. The intercommunication problem is probably the least under- stood dimension 
in the computer space. It is rather intimately 
related to the ISP, in that the 
various calling methods (implicitly and explicitly) depend on the ISP. Also, the amount of processor state (a 
function of the ISP) affects 
the response time for making 
context transitions. Most interrupt systems allow several 
inde- pendent classes and/or sources of interrupters. The 
classes are arranged in priority so that lower-level interrupters are ignored until higher-level interrupt programs are run to completion (see Chap. 42 on the SDS 910-9300 series). The design problems 
as- sociated with intercommunication 
are not those 
of implementa- tion but of knowing what should be implemented. The PMS structure part and 
the corresponding register-transfer implementa- tions for intercommunication are, by comparison, 
straightforward. Processor concurrency Concurrency (parallelism) in 
the processor is the number of events or logical operations that are happening at a given time. If the basic logic technology 
is held constant, decreasing the processing time (increasing the power) requires increasing the number of parallel operations. 
An exact measure 
of parallelism can be made in terms of the number of n-bit operations made 
per clock pulse. 
The parallelism in a structure is also a measure of its complexity; to have a 
highly parallel structure implies control structure to- gether with multiple 
data paths (and 
operations) which can be concurrently evoked. Processor parallelism 
is also necessary 
to overcome Mp speed 
technological boundaries. Thus 
it is difficult to isolate completely the processor from 
the memory. Flynn [ 19661 categorized high-speed processors by 
whether there are 
single or multiple instruction streams and whether 
each stream has single 
or multiple data streams. The CDC 6600 and IBM Stretch are examples of a single instruction stream and a 
single data stream. An ILLIAC IV processor has 
a single instruc- tion stream with multiple data streams. Thus, the single instruction stream and multiple data stream are a form of array processing in which an 
instruction performs an operation on multiple data elements. The CDC 6600 main processor has 
multiple instructions of a single stream in the fetch, 
buffering, and decoding process at a given time. In addition, instructions 
are being executed 
in parallel Interrupt request from interruptor, Interpret instruction in Mp (interpretation in interrupted state1 No interrupt request t restore Interrupt program execution Fig. 17. State diagram 
for the interrupt process. by the 10 parallel data-operations. 
The 6600 has functionally differ- ent data 
operators, although a 
system could exist in which these operators are the same, or, if the operator were 
much faster, 
a single unit could be used sequentially. Depending on the utiliza- tion of the 10 data units, there could be a computer with several 
processors which 
share a 
common set 
of data-operations. The 6600™s peripheral processors are implemented in a mode whereby several instructions streams are processed in parallel by a single processor. 
The simplicity of the shared processor for multiprocessing or 
parallel processing thereby provides still 
another form of parallel- ism. The following subsections discuss 
particular forms of paral- lelism. At one end of the dimension there is the most primitive structure, a serial processor, and at the other end there are 
pipe- line processors. Serial processors. At the most elementary level only 
one bit of an n-bit word is operated on at a given time. There is no concurrency, and even the most trivial operations on n bits requires 
a time 
of n. The bit-serial processor was used in the first generation because 
the cyclic primary memories to which it connected were funda- 
mentally bit-serial (see page 73). Although the processor memory could be made 
to operate on a parallel basis where words were available in one unit of time, such a tradeoff was not worthwhile 
because of the relatively long access 
time to Mp. The word lengths for serial processors 
tended to be relatively long, 
because the cost is independent of word length (see page 216). Parallel-b y-word processors. The simple parallel-by-word 
processor is the most common processor of the first to third generation. This occurred in part because Mp became 
parallel by word. 
Within 
84 Part 1 1 The structure of computers the processor we assume that almost every internal register- transfer operation requires 
one or more clock times. 
(A simple multiply operation 
usually takes between n/2 and 2n clock times.) 
We do not 
mean to rule out multiple simultaneous internal opera- tions within 
the processor, but they are exceptions. With only a view of a processor's registers, it is easy to tell if multiple opera- 
tions are possible. Most of these processors do only one operation 
at a time. As a rule, the simple processor is locked to the 
primary- memory cycle 
time (usually core). Approximately 
2 - 10 events (clock times) are available within the processor. For example, the PDP-8 (Chap. 5) has four events, 
and the IBM 7090 (Chap. 41) has 10 events. A precise measure 
of parallelism would count the number of operations per clock time for given 
program conditions. 
Multiple instruction streams, 1 Pc. The only example of this structure in 
the book is the CDC 6600. Opportunities for such a structure are 
possible with the 
parallel computer suggested by Lehmann (Chapter 37). Multiple datu streams. The most obvious 
implementation of multiple data streams with one or more 
instruction streams is the array processor. Part 4, Section 2 is devoted to 
these struc- tures. 1-Instruction buffer. The 1-instruction buffer 
is a form 
of looking ahead in the instruction-interpretation cycle and is about the simplest form of parallelism in a parallel-by-word processor. A 
single register is assigned the role of holding the next instruction to be interpreted. The IBM 7094 Instruction Backup Register (Chap. 41) is typical of this case. In the 7094 two instructions are fetched at 
a time. More generally the next instruction would 
be fetched during 
the execution of the current instruction. n-Instruction buffering. 
Multiple instruction 
buffering is a general- ization of the 1-instruction buffer above. It can take several 
forms depending on the algorithms used to fetch 
the next instruction (i.e., the look-ahead) and the organization of the memory holding 
the instructions. Stretch (Chap. 
34) and the CDC 
6600 (Chap. 39) use instruction buffers. A small, restricted content-addressable memory holds a block 
of instructions. In 
the simplest case 
of these computers a block 
of memory, relative 
to the instruction counter, is kept in the local instruction buffer memory. Look-aside buffering (sluve) 
memories. Look-aside is a more general form of instruction buffering because 
both instructions and com- monly accessed 
data tend to migrate to the faster look-aside memory. This 
scheme is discussed for 
the IBM System/360 Model 
85 (page 574). The look-aside memory suggested 
by Wilkes [1965] is a content-addressable memory 
for retaining the active (most recently used) memory 
words. Pipeline processing. 
Pipeline (assembly-line) concurrency is the name given to a system 
of multiple functional units, 
each of which is responsible for 
partial interpretation 
and execution of the in- struction stream. 
A pipeline processor has several partially com- 
pleted instructions in process at one time. 
Each processor stage operates on a specific part of the instruction, e.g., instruction fetch, effective-address calculation, operand fetching, execution 
of opera- tion specified by the instruction, and results storing. A PMS dia- gram for a 
pipeline processor is given in 
Fig. 19. Thus there 
is a separate functional unit for each state suggested by the state diagram of Fig. 4. There must be interlocks so that sequence is preserved, i.e., so that results are not used until they are available. Figure 18 shows a 
time/function diagram 
of a pipeline processor. There are 
at least three instructions being 
interpreted simultane- ously. Although 
we have not extended Fig. 18, we would expect 
the processor in the sketch to operate 
on about eight instructions 
1 -J I * = tq3 Instruction 3 toq Operation time to determine instruction q toq Access time to determine instruction q to" Ooeratlan tme to determine dotum 
v tov to Operation tlme for instruction too Operation time to determine operation 
of instruction tq Total instruction time Access time to 
determine datum 
v Fie. 18. Time-function diagram 
for a pipeline 
processor. .. I .. 
Chapter 3 I The computer 
space 85 M.data M. instructions t4.data instruction fetch 
data setup execution data restore - IU- L Fig. 19. Example of processor parallelism by spatially independent control function (pipeline 
processing) PMS diagram. at one time. Note 
that the 
processor sometimes 
completes later instructions first. In this model there is only one instruction fetch- 
ing, one operand 
fetching, and one operand 
storing unit, while there are multiple data operation units. 
The particular number of each type 
of unit is obviously not fixed for all structures but depends heavily on the memory system, 
the number of instruction streams, and the ISP. A processor may require many data-operation units 
in order 
to avoid bottlenecks. Each unit is independent and may be functionally capable of carrying out only selected tasks. Multiple data-operations are normally desirable 
in a 
pipeline processor so that several operations can be carried out at a time, since 
most of the processing time within the processor is spent on the operations (e.g., multiplication, division, shifting, 
etc.) Conclusions You now have our view 
of the important aspects of the stored- program computer. We have tried to 
organize the parameters as dimensions so that a computer can be viewed as a 
point (or points) 
in a multidimensional space. The previous discussion 
has enumer- 
ated the 
values of one dimension, while (in effect) holding 
the values of other dimensions constant. The dimensions are highly correlated, especially with cost and evolutionary time. We 
have been brief in presenting the dimensions because 
the book is pri- marily about computer 
examples. However, 
one should he able to recognize the dimensions and values when they are 
encountered within the context of a particular computer. 
The remainder of the book is organized around these dimen- 
sions. The examples lose 
the identity of dimensions because they are descriptions of points in the space (computers). Furthermore, the descriptions themselves 
are not especially organized 
around these dimensions but are based on the designer™s own view of his machine. References AdamA60,66,67,68; AdamC6O; ArbuR66; ArdeB66; 
BowdB53; CampR50; CasaC62; ChasC52; CoxJ68; DennJGS; FlynM66; ForrJFjl; 
GibsC66; KnigK66; MolnC67; NiseN66; RandB68; RoseS69; SamuA57; SerrR62; WeikM55,61,64; 
WilkMSla,65; WillF49. 

PART 2 The Instruction-set 
Processor: main-line computers To have a "main line" of computers 
is to have a family that predominates through the generations. Predominance 
can probably best 
be measured 
by the percentage of distinct computers 
produced within the family, as opposed to outside it. Members of the family need not all be identical; especially evolution over time can be tolerated. But it must be the case that there is at any moment a "standard" design which 
is seen as emerging from the just prior 
"standard" design. Within these definitions there indeed 
has been 
a main line in computer systems. It is based 
on the 
Burks, Goldstine, and von Neumann memorandum, 
reprinted as Chap. 4. The most striking 
characteristic is 
the evolution from 1 address organization (l), through index-register (1 + x) to general-register (1 + g) organization. 
Left outside the main 
line have been multiple-address organizations, character machines, 
and stack machines. This seems to be an appropriate description, 
even though a character machine (variable-length character string), 
the IBM 1401, probably holds 
the record for number of machines produced 
(when each model of the IBM 
Sys- tem/360 is counted as a separate computer). 
A second characteristic feature has been the PMS structure, which has 
evolved from a single P to a Pc-nPio structure. 
This has not been uniform within the family, since it applies only to the 
larger members; the small machines, such as the PDP-8 (Chap. 5), have no separate Pio's. It might seem that all computer systems, both within and without 
the family, have evolved in this 
same way. But this disregards the history of computer development. For a while, in the 
early fifties, there were seen to be two main lines of potential 
development: scientific computers, 
featuring large computation and small 
input/output, and business computers, featuring small 
computation and 
large input/output. The latter started 
to develop into the Pc-nPio structure (with 
the IBM 
702) but, instead of a separate line developing, scientific computers (with the IBM 
704 and UNIVAC computers) adopted 
the more powerful input/output structure. Again, despite its success, the 1401 has not bred a 
new generation of computer systems in its image, either 
within IBM 
(where one might argue that the overriding consideration was to have a uniform series) or by 
IBM's competitors. A third characteristic of the main line is 
the use of binary as opposed to decimal as the basic radix of the machine. This affects both the 
arithmetic and 
whether logi- cal processing 
(on bit vectors) can be done. 
The issue seems almost settled in the third generation, with smaller machines being 
binary and 
larger machines having multiple data-types. The last serious venture into a large pure decimal 
machine was the UNIVAC LARC, delivered in 1960. In retrospect, the difference in organizations between binary and 
decimal machines 
seems small enough 
so that we have included them all in the 
same section. 
There are 
a number 
of striking features that are characteristic 
of the main 
line but do not differentiate it from 
any of the alternatives that have actually been produced. These features include 
the stored-program 
concept; the use of sequential a7 
88 Part 2 1 The instruction-set processor: main-line computers 
instructions of the operator-operand 
variety; the use of the word as an information 
unit, within the range of 12 to 64 bits; and a processor state of 
less than 100 words. Alternative organizations 
are conceivable, 
though they have clearly 
not seemed practical to computer designers. For instance, in the 
early fifties there was an at- tempt to 
construct an electronic plugboard machine, 
after the fashion of the ENIAC and the 
IBM CPC (Card Programmed Calculator). And we see in the new programmed desk calculators (Part 
3, Sec. 4) yet another organization that is rather far from the main line (but 
because of low cost may yet be a part of 
the future main 
line). These desk 
calculators, by the way, are decimal, rather 
than binary. 
Section 1 Processors with one address per instruction This section 
is principally concerned 
with the ISP. It is the largest section in the 
book, reflecting the dominance of 
the one-address organization during the first two 
generations. Machines with index registers 
are included, but not machines with general registers, which are discussed 
in Sec. 2. Some processors store two single-address instructions per word, fol- lowing the pattern of 
the IASl (von Neumann) machine (Chap. 4). In machines with short 
word lengths, one single-address 
instruction is stored 
in one or 
two words, for example, in the 
16-bit IBM 1800 (Chap. 33) and in the 
12-bit PDP-8 (Chap. 5). The evolution of these machines 
can be seen by comparingfirst- and third-generation 
machines (e.g., Whirlwind and the IBM 1800). In general, the section is arranged 
by increasing word 
length, alternatively complexity 
and performance. Preliminary discussion of the logical design 
of an electronic computing instrument This article 
(Chap. 4) is important for 
historical as well as 
tech- nical reasons. It is one 
of a 
series' written in 1946 prior to building the first 
fully stored-program 
computer. Although 
its authors were not engineers, it is written with 
the caution of 
those responsible for the implementation of 
a rather significant 
development task. The major problems for the 
computer are identified, the alternatives analyzed, and a rationale for each decision is given. If computer designers were all required 
to analyze and describe their machines 
in such a 
fashion prior to building them, 
there would be fewer, 
but better, computers. 
Some of the especially enjoyable aspects of 
the discussion in- clude: Institute for Advanced Study, Princeton University, Princeton, N.J. 'The articles in the series were: 1 Selection of word length and number 
base. 2 Discussion of the instructions needed. 3 Concern for the 
input/output structure and 
the idea of displays (now almost a reality). 4 Rationale for not including floating-point arithmetic (caution about the technology). 5 The lack of necessity for the 
rather trivial binary-decimal conversion hardware 
and the 
idea of cost effectiveness. 6 Analysis 
of the addition, multiplication, and 
division hardware implementation. (This description includes 
a nice, one-page discussion 
of the average carry length for addition.) It is difficult to say which machines have been influenced by this memorandum since the idea of data and instructions 
stored together 
in a homogeneous 
primary memory is so basic to all computers. The idea of the single-address instruction set and format 
is at the heart of all the machines discussed in this section. However, it did not have index registers. 
Many of the machines with long 
word length, like IAS, use 
the two-instruc- tions-per-word format. Subsequent machines built with only 
minor variations in- clude ORDVAC; ILLIAC I at the 
University of Illinois with 
a 40-bit electrostatic memory 
and vacuum-tube 
logic; AVIDAC, ORACLE, 
MANIAC I, WEIZAC, SILLIAC, BESK, DASK, 
CSIRAC, and JOHNNIAC at the 
RAND Corporation with a 40-bit core memory and transistor logic [Gruenberger, 
19681. Other similar com- puters include the IBM 701 with a 36-bit word, electrostatic memory and vacuum-tube 
logic; and the CDC 1604, with a 48-bit word, core memory, 
and transistor 
logic (possibly 
in- fluenced by 
MANIAC 11). On the Principles of Large Scale Computing Machines (1946) 
[Goldstine and von Neumann, 1963al. 
Preliminary Discussion 
of the Logical Design of an Electronic Computing The DEC pDp-8 The PDP-8 is included as Chap. 5 to illustrate the effects Of Instrument, pt. I, vol. l(1946) [Burks, Goldstine. and von 
Neumann, 19631. Planning and 
Coding of Problems for an Electronic Computing Instrument, Dt. 11. vols. 1.2.3 (1947-19481 rGoldstme 
and von Neumann, 19636, 
1963c, a 12.bit word length, it is given in detail using a ~yop.down~~ approach in Order that the student may thoroughly understand 1963dl. it by simulating it, interpreting it, writing microprograms that 89 
90 Part 2 1 The instruction-set processor: main-line computers Section 1 I Processors with one address per instruction emulate it, making 
incremental modifications 
to it, 
and com- pletely redesigning it.' 
The PDP-8, 
although not the first 12-bit 
computer, achieved a status that made it the first 
standard for 
small, low cost dedicated computers. There is an active market now for com- 
puters in this size and price range 
to which the marketing 
culture has responded 
with the names microcomputer, 
mini- computer and midicomputer 
for 8- to 12-, 12- to 16- and 16- 
to 24-bit word-length 
computers, respectively.2 The PDP-8 
has a nearly 
minimal processor state because the address and ISP integers are 12 bits. Twelve bits is just large 
enough to represent data from external physical process 
environments (analog 
signals) and also just right to address a 
4096 word memory. 
System software (editors, 
assemblers, compilers, etc.) can surprisingly all 
fit into this sized mem~ry.~ The processor 
state is only 26 bits, and the 
predecessor PDP-5 had a hardwired state of only 14 bits. The PDP-8 is also discussed 
in Part 5, Sec. 2, page 396. K-T(CRT; display; area:S2ilO2 in2)-, K-T(I ight; pen)> K-T(film; camera),' 64 bs/w; 12 n 2048 w; 800-IO00 ft; 30 in/sec; (2+1 index) b/char; 
100 char/in 'M(toggle switch; 
8 ~s/w; 32 w; 16 b/w) 'Pc(50 kop/s; 16 b/w; I instruction/w; 1 address/instruction; M.processor state(3 w); technology: vacuum tube; 1948- 
1966) 38(fixed; from: Pc; to: 8 K; concurrency: 1) 4Mp(*O:I; core; 8 vs/w; 1024 w; 16 b/w; taccess: 2 as) The Whirlwind I computer Whirlwind I is based on Wilkes' EDSAC at Manchester Univer- 
sity. Chapter 6 describes 
the computer and 
gives a brief 
descrip- tion of vacuum-tube logic and electrostatic storage-tube tech- 
nology. The PMS structure of Whirlwind I with core memory is 
given in Fig. 1. The Memory 
Test Computer (MTC) of M.I.T.'s Lincoln Labora- 
tory was the first 
computer to use a core 
memory. MTC was 
built to test the memory which Whirlwind 
I received in August, 1953. Subsequent 
modifications included the addition of 
an- other 2,048-word 
magnetic-core memory 
in September, 1953. The machine's construction 
and technology are 
outstanding. It has effective marginal checking and preventive-maintenance test facilities. At the time the 
machine was dismembered and moved from M.I.T., it had a use time availability of greater than 95 percent. Although Whirlwind 
I left M.I.T. in 1960, the ma- 
chine was reassembled and was operational as late as 1966. The machine's PMS structure is a simple 1 Pc. The K to Mp block transfers are via the Pc on a one-at-a-time, programmed basis. A single 
data transfer can be initiated to a particular device, thus providing some opportunity for 
input/output and processing concurrency. The simple 
structure is due to the high 'Perhaps also because of one of the author's 
(GB) obvious attachment *See the computers in this size range Chapter 
3, Flgure 2, page 43. 3Conce~vably a corollary to Parkinson's law: Programs expand to fill every word in the primary memory of a computer. Fig. 1. Whirlwind I PMS diagram. register costs 
of the vacuum-tube technology; thus only a single central processor register is provided to hold (or buffer) data during a K transmission to a T 
or Ms. Appendix 
1 of Chap. 6, which is from the programming manual, 
gi,ves its instruction 
set. The IBM 1800 The IBM 1800(Chap. 33) 
is a third-generation, 16-bit computer. It is discussed in Part 5, Sec. 2, page 396. Some aspects of the logical design 
of a control computer: a case 
study Chapter 7 presents the aerospace computer Apollo designed 
by M.I.T.'s Instrumentation Laboratory. It is presented 
in contrast to the general-purpose 16-bit computers, Whirlwind 
(Chap. 6) and the IBM 1800 (Chap. 33). The Apollo 
computer uses a M(read only) because it is obviously a 
problem to reload pro- grams. Kampe's SD-2 (Chap. 29) and Apollo (Chap. 7) are both controllers and 
have other similar 
design constraints. The IBM 1800 is also used for control purposes. In fact, the 
computers in this 
section up to 
and including 
the 24-bit 
SDS 910-9300 series are all designed for control environments. 
However, all the latter machines have a 
goal of generality not present in the Apollo. 
Section 1 I Processors with one address per instruction 91 The SDS 910-9300 series The SDS 910-9300 computers are illustrative of 
typical, second- generation, 24-bit computers. The computers are discussed 
in Part 6, 
Sec. 2, page 542. Chapter 42 also attempts to show how implementation affects performance for 
the series. The LGP-30 and LGP-21 The LGP-30 and later LGP-21 
is presented in Chap. 16 and dis- 
cussed in Part 3, Sec. 2, page 216. IBM 650 instruction logic The IBM 650 (Chap. 17) is a one plus one 
address computer. Its attributes as a cyclic-memory 
computer, though 
hardly ap- parent at the 
ISP level, are discussed 
in Part 3, Sec. 2, page 216. The IBM 7094 I, II Part 6, 
Sec. 1 shows the evolution of the IBM 36-bit scientific computers. The IBM 7094 II (Chap. 41) 
is presented for many reasons (page 517). 
Among them 
are its effect on the 
later IBM System/360 and 
its position as the standard 
large scientific computer of the 
late fifties and early sixties. 
The UNIVAC system The YNIVAC 
system, first delivered in March, 1951, 
was later known as UNIVAC 
I. UNIVAC (UNIVersal AutomaticComputers) was the second computer1 to be manufactured by the Eckert- Mauchly Computer Corporation, subsequently a division 
of Remington-Rand.2 UNIVAC is 
a single-address, decimal computer 
with 12 digits/ word. Two instructions are stored per word. In effect, UNIVAC is a decimal 
version of the IAS computer. The Mp consists of 1,000 words, made up of 10 words/delay line. Each delay 
line requires 404 microseconds to recirculate. UNIVAC is significant because it was the most important 
computer during the 
early 1950s. Its performance record is discussed in Chap. 8. The UNIVSERVO 
magnetic-tape system was rather advanced for 1950, considering performance, error 
checking, and buffering. 
Particularly nice 
is the ability to parti- tion the input/output system for off-line printing 
and key punching. One-level storage 
system The 48-bit Atlas was developed at Manchester University 
and subsequently manufactured by Ferranti Corp. (now part of Inter- national Computers and Tabulators). The development began 
about 1960, 
and the paper was written in 1962. The importance of Atlas with respect to current and 
future machines is dis- cussed in Part 3, Sec. 6, page 274. 
The engineering 
design of the Stretch computer 
The IBM Stretch (also called 
the IBM 
Model 7030) single- 
address computer (Chap. 34) is one 
of the earliest computers 
built to provide maximum computing 
power subject 
to no ap- parent cost, size, and producibility constraints. A discussion of its importance is given 
in Part 5, Sec. 2, page 
396. lThe Eckert-Mauchly BINAC 
was apparently the first computer to be manu- * Eckert.Mauchly Computer Corporation was 
initially independent of Remington- factured by a corporation. Rand. 
Chapter 4 Preliminary discussion of the logical design of an electronic computing instrument1 Arthur W. Burks / Herman H. Goldstine / John von Neumann PART I 1. 1.1. Inasmuch as the completed device will be a general-purpose computing machine it should contain certain 
main organs relating to arithmetic, memory-storage, control 
and connection with the human operator. It is intended that the 
machine be fully automatic in character, 
i.e. independent of the human 
operator after 
the computation starts. A fuller discussion of the implications of this remark will be given in Sec. 3 below. It is evident that the machine must 
be capable of storing in some manner not only the digital information needed in 
a given 
computation such as 
boundary values, tables of functions (such 
as the equation of state of a fluid) 
and also the intermediate results of the computation (which may 
be wanted for varying lengths 
of time), but also the instructions which govern the actual routine to be performed on the numerical data. In a special-purpose machine these instructions 
are an 
integral part of the device and constitute a part of its design structure. For an all-purpose machine 
it must be possible to instruct the device to carry out any 
compu- tation that can be formulated in numerical terms. Hence there must be some organ capable of storing these program orders. 
There must, moreover, be a unit which can understand 
these instructions 
and order their 
execution. Conceptually we have discussed above two different forms of memory: storage of numbers and storage of orders. If, however, the orders to the machine are reduced to a numerical code and if the machine can in some fashion distinguish a 
number from an order, the memory organ 
can be used to store both 
num- Principal components of the machine 1.2. 1.3. ‚From A. H. Taub (ed.), ﬁCollected Works of John von Neumann,ﬂ vol. 5, pp. 34-79, The Macmillan Company, 
New York, 1963. Taken from report to U. S. Army Ordnance Department, 1946. See also Bibliography Burks, Goldstine and von Neumann, 1962a, 1962b, 1963; and Goldstine and von Neumann 1963a, 1963h, 1963c, 1963d. 
bers and orders. The coding of orders into numeric form is dis- 
cussed in 
6.3 below. If the memory for orders is merely a storage organ there must exist an organ which can automatically 
execute the orders stored in 
the memory. We 
shall call this organ 
the Control. Inasmuch as the device is to be a computing machine there must be an arithmetic organ in it which can perform certain of the elementary arithmetic 
operations. There will be, therefore, a unit capable 
of adding, subtracting, multiplying 
and dividing. It will be seen in 6.6 below 
that it can also perform additional 
operations that occur quite frequently. The operations that the machine will view 
as elementary are 
clearly those 
which are wired into the machine. To illustrate, the operation of multiplication could 
be eliminated from the device as an elementary 
process if one were 
willing to view it as a 
prop- erly ordered series of additions. Similar remarks apply to division. In general, 
the inner economy of the arithmetic unit 
is determined by a compromise between the desire for speed of operation-a non-elementary operation 
will generally take a long time to per- form since 
it is constituted of a series 
of orders given by the control-and the desire for simplicity, or cheapness, 
of the ma- chine. 1.6. Lastly there must exist devices, the input and output 
organ, whereby the human operator and the machine can com- municate with each other. 
This organ will 
be seen below 
in 4.5, where it is discussed, to constitute 
a secondary form 
of automatic memory. 1.4. 1.5. 2. 2.1. It is clear that the 
size of the memory is a critical considera- 
tion in 
the design of a satisfactory 
general-purpose computing First remarks on the memory 92 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing 
instrument 93 machine. We proceed to discuss what quantities 
the memory should store for various types of computations. In the solution of partial differential equations the storage requirements are likely to be 
quite extensive. In general, one 
must remember not 
only the initial and boundary conditions 
and any arbitrary functions that enter the 
problem but also an extensive number of intermediate results. 2.2. a For equations 
of parabolic or hyperbolic type in two inde- pendent variables the integration process is essentially a 
double induction. 
To find the values of the dependent vari- ables at time t + At one integrates 
with respect to x from one boundary 
to the other 
by utilizing the data at 
time t as if they were 
coefficients which contribute to defining the problem of this integration. 
Not only must the memory have sufficient room 
to store these intermediate data but 
there must be provisions whereby these 
data can later be 
removed, i.e. at the end of the (t + At) cycle, and replaced by the 
corresponding data for the (t + 2At) cycle. This process of removing data from the memory and of replacing them with new informa- tion must, 
of course, be done quite 
automatically under the 
direction of the control. For total differential equations the memory requirements are clearly similar to, 
hut smaller than, 
those discussed in (a) above. Problems that are 
solved by iterative 
procedures such as systems of linear equations 
or elliptic partial differential equations, treated by relaxation 
techniques, may 
be ex- pected to require quite extensive memory capacity. 
The memory requirement for such problems 
is apparently much 
greater than 
for those problems in 
(a) above in which one 
needs only to store information corresponding to the in- stantaneous value of one variable 
[tin (a) above], while now 
entire solutions (covering all values 
of all variables) must 
he stored. This 
apparent discrepancy in magnitudes can, 
however, be somewhat overcome by the use of techniques which permit the use of much coarser 
integration meshes in this case, 
than in the cases under (a). 
b c 2.3. It is reasonable at this time to build a machine that can conveniently handle 
problems several orders 
of magnitude more 
complex than are 
now handled by existing machines, 
electronic or electro-mechanical. We consequently plan 
on a fully 
automatic electronic storage 
facility of about 4,000 numbers of 40 binary digits each. This 
corresponds to a precision 
of T40 - 0.9 x i.e. of about 12 decimals. We 
believe that this memory 
capacity exceeds the capacities required 
for most 
problems that one deals 
with at present by a factor of about 10. The precision is also safely 
higher than what is required for the great majority of present day 
problems. In addition, we propose that we have 
a subsidiary 
memory of much larger capacity, which 
is also fully automatic, on some medium such 
as magnetic wire 
or tape. 3. First remarks 
on the control and 
code 3.1. It is easy to see by formal-logical 
methods that there exist codes that are in abstracto adequate to control and cause the execution of any sequence 
of operations which are individually available in the machine and which are, in their 
entirety, con- ceivable by the problem planner. The really decisive considera- 
tions from the present point 
of view, in selecting a code, are more of a practical nature: 
simplicity of the equipment demanded by 
the code, and the clarity of its application 
to the actually impor- tant problems together with 
the speed of its handling 
of those problems. It would take us much too far 
afield to discuss these questions at all generally or 
from first 
principles. We will therefore restrict ourselves to analyzing only the type of code which we now envisage for 
our machine. 
There must certainly be instructions for performing the fundamental arithmetic 
operations. The specifications for these orders will not be completely given until the arithmetic unit 
is described in 
a little more detail. It must be possible to transfer data from the memory to the arithmetic organ and back again. In transferring information from the arithmetic organ back into the 
memory there are 
two types we must distinguish: Transfers 
of numbers as such 
and trans- fers of numbers which are 
parts of orders. The first case is quite obvious and needs no further explication. The second case 
is more subtle and serves to illustrate the generality and simplicity of the system. Consider, by way 
of illustration, the problem of interpola- tion in the system. Let us suppose that we have formulated 
the necessary instructions for performing 
an interpolation 
of order n in a sequence of data. The exact location 
in the memory of the (n + 1) quantities that bracket the desired functional value 
is, of 
course, a 
function of the argument. This argument probably 
is found as the result of a computation in the machine. We thus need 
an order which can 
substitute a number into a given 
order-in the case of interpolation the location of the argument or the group of arguments that is nearest in our 
table to the desired value. By means of such an order the results of a computation can be 
in- troduced into the 
instructions governing that or a 
different com- putation. This 
makes it possible for 
a sequence of instructions to be used with different sets of numbers located in different 
parts of the memory. 3.2. 3.3. 
94 Part 2 1 The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
To summarize, transfers 
into the memory will be of two sorts: Total substitutions, whereby the quantity previously stored is cleared out 
and replaced by a new number. Partial substitutions in which that part of an order containing 
a memory location- number-we assume the various positions 
in the 
memory are enumerated serially by memory 
location-numbers-is replaced by a new memory location-number. It is clear that one must be able to get numbers from 
any part of the memory at any time. The treatment 
in the case of orders can, however, 
be more methodical since one can 
at least partially arrange the control instructions 
in a linear sequence. 
Consequently the control will be so constructed that it will nor- mally proceed from place n in the memory to place (n + 1) for its next 
instruction. The utility of an automatic computer 
lies in the possi- bility of using a given 
sequence of instructions repeatedly, the number of times it is iterated being either 
preassigned or depend- ent upon the results of the computation. When the 
iteration is completed a different sequence of orders is to be followed, so we must, in most 
cases, give two parallel trains 
of orders preceded by an instruction as to which routine is to be followed. This choice 
can be made to depend upon the sign of a number (zero being reckoned as plus 
for machine purposes). Consequently, we 
intro- duce an order 
(the conditional transfer order) 
which will, depend- ing on the sign of a given 
number, cause the proper one 
of two routines to be executed. Frequently two 
parallel trains 
of orders terminate in a 
common routine. It is desirable, therefore, 
to order the control in either 
case to proceed to the beginning point 
of the common routine. This unconditional transfer can be achieved either by the artificial use of a conditional transfer 
or by the introduction of an explicit order for such a transfer. Finally we 
need orders which 
will integrate the input- 
output devices with 
the machine. These 
are discussed briefly 
in 6.8. We proceed now 
to a more detailed discussion of the machine. Inasmuch 
as our experience has 
shown that the moment one chooses a given 
component as the elementary memory unit, one has also more or less determined upon much 
of the balance of the machine, we 
start by a consideration of the memory organ. In attempting an exposition of a highly 
integrated device like 
a computing machine we do not 
find it possible, however, to give an exhaustive discussion 
of each organ before 
completing its description. It is only in 
the final block diagrams that anything approaching a complete unit 
can be achieved. 3.4. 3.5. 3.6. 3.7. The time units to 
be used in what 
follows will 
be: 1 pec = 1 microsecond = 10F seconds 1 msec = 1 millisecond = lop3 seconds 4. The memory 
organ 4.1. Ideally one would desire 
an indefinitely large memory ca- pacity such that any 
particular aggregate of 40 binary digits, or word (cf. 2.3), would be immediately available-Le. in a time which is somewhat or considerably shorter than the operation time 
of a fast electronic multiplier. This may 
be assumed to be practical at the level of about 100 psec. Hence the availability time for a word in the memory should be 5 to 50 psec. It is equally desirable 
that words may be replaced with new 
words at about the same rate. It does not seem possible physically 
to achieve such 
a capac- ity. We are therefore forced 
to recognize the possibility of con- structing a hierarchy of memories, each of which has greater 
capacity than the 
preceding but which is less 
quickly accessible. The most common forms of storage in electrical 
circuits are the flip-flop or trigger 
circuit, the gas tube, and the electro- mechanical relay. To achieve a memory of n words would, of course, require about 
40n such elements, 
exclusive of the switching elements. We 
saw earlier (cf. 2.2) that a fast memory 
of several thousand words 
is not at all unreasonable 
for an all-purpose instru- 
ment. Hence, about 
lo5 flip-flops or analogous elements would be required! This would, 
of course, be entirely impractical. 
We must therefore seek 
out some more fundamental method of storing electrical information than has been suggested above. 
One criterion for such a storage medium 
is that the 
individual storage organs, which accommodate only one binary 
digit each, should not be macroscopic components, 
but rather 
microscopic elements of some suitable organ. 
They would then, of course, not be identified and switched to 
by the usual macroscopic wire 
con- nections, but by some functional 
procedure in manipulating 
that organ. One device which 
displays this property to 
a marked degree is the iconoscope tube. In its conventional form 
it possesses a linear resolution of about one 
part in 500. This would correspond 
to a (two-dimensional) memory 
capacity of 500 x 500 = 2.5 x lo5. One is accordingly led to consider the possibility of storing elec- 
trical charges on a 
dielectric plate 
inside a cathode-ray tube. Effectively such a tube is nothing more than a myriad of electrical capacitors which can be connected into the circuit by means of an electron beam. 
Actually the above mentioned high resolution 
and concomitant memory capacity are only realistic 
under the 
conditions of tele- vision-image storage, which are much less exigent in respect to 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
95 the reliability of individual markings than what one 
can accept 
in the storage for a computer. In 
this latter case resolutions 
of one part in 20 to 100, i.e. memory capacities 
of 400 to 10,000, would seem 
to be more reasonable 
in terms of equipment built essentially along familiar lines. 
At the present time 
the Princeton Laboratories 
of the Radio Corporation of America are engaged in the development of a storage tube, the Selectron, of the type we have mentioned above. This tube is also planned to have a non-amplitude-sensitive switch- ing system whereby the electron beam can be directed to a given spot on the plate within a quite small fraction of a millisecond. Inasmuch as the storage tube is the key component of the machine envisaged in this 
report we 
are extremely fortunate in having secured the cooperation of the RCA group in this as well as 
in various other developments. An alternate form of rapid memory organ is the acoustic feed- back delay line described in various reports on the EDVAC. (This 
is an electronic computing machine being developed 
for the Ordnance Department, 
U.S. Army, by 
the University of Pennsyl- vania, Moore School of Electrical Engineering.) Inasmuch as that device has been so clearly reported in those papers we 
give no 
further discussion. There are 
still other physical and chemical 
properties of matter in the presence of electrons or photons 
that might be considered, but since none 
is yet beyond 
the early dis- cussion stage we shall not make further mention of them. We shall accordingly assume 
throughout the balance of this report that the 
Selectron is the modus for 
storage of words at electronic speeds. As now planned, this tube will have a capac- 
ity of 2™* = 4,096 =: 4,000 binary digits. To achieve a 
total elec- tronic storage of about 4,000 words we propose to use 40 Selec- trons, thereby achieving a 
memory of 212 words of 40 binary digits each. (Cf. 
again 2.3.) There are two possible means for 
storing a particular word in the Selectron memory-or, in fact, in either a delay line memory or in a storage tube with amplitude-sensitive 
deflection. One method is to store the entire 
word in a given tube and then to get 
the word out by picking out its respective digits in a serial fashion. The other 
method is to store in 
corresponding places 
in each of the 40 tubes one digit of the word. To get a word from 
the memory in 
this scheme requires, then, one switching 
mech- anism to which all 
40 tubes are connected in parallel. Such a switching scheme 
seems to us to be 
simpler than the technique needed in the serial system 
and is, of course, 40 times faster. 
We accordingly adopt the 
parallel procedure and thus 
are led 
to con- sider a so-called parallel machine, as contrasted with the serial principles being considered 
for the EDVAC. (In the EDVAC the 4.2. 4.3. peculiar characteristics of the acoustic delay line, as well as various 
other considerations, seem to justify a serial procedure. For 
more details, cf. the reports referred 
to in 4.1.) The essential difference 
between these two systems lies 
in the method of performing an addition; in 
a parallel machine all corresponding pairs of digits are added 
simultaneously, whereas 
in a serial one 
these pairs are added serially in time. To summarize, we 
assume that the fast electronic memory 
consists of 40 Selectrons which 
are switched in parallel 
by a com- mon switching arrangement. 
The inputs of the switch are con- trolled by the control. Inasmuch as 
a great many 
highly important classes of problems require a 
far greater 
total memory than 212 words, we now consider 
the next stage in our storage 
hierarchy. Although the solution of partial differential equations frequently 
involves the manipulation of many thousands of words, these 
data are generally required 
only in blocks which 
are well within the 212 capacity of the electronic memory. Our second form 
of storage must therefore be a medium which feeds these blocks of words to the 
electronic memory. It should be controlled by 
the control of the computer and 
is thus an integral part 
of the system, not requiring human intervention. There are 
evidently two 
distinct problems raised above. 
One can choose a given medium for storage such 
as teletype tapes, magnetic wire 
or tapes, movie film or similar media. 
There still remains the problem of automatic integration of this storage medium with the 
machine. This integration is achieved logically by introducing 
appropriate orders into the 
code which can instruct the machine to read or write on the medium, or to move it by a given amount or to a place with given characteristics. We discuss this question a 
little more fully 
in 6.8. Let us return now to 
the question of what properties the sec- ondary storage 
medium should have. 
It clearly should be able to store information for periods 
of time long enough so that only a few per 
cent of the total 
computing time is spent in re-registering 
information that is ﬁfading off.ﬂ It is certainly desirable, 
although not imperative, 
that information can be erased and replaced by 
new data. The medium should be such that it can be 
controlled, i.e. moved forward 
and backward, automatically. 
This considera- 
tion makes certain media, such as 
punched cards, undesirable. 
While cards can, of course, be printed or read by appropriate orders from some 
machine, they 
are not well 
adapted to problems in which the output data are fed directly back into the machine, and are 
required in a sequence 
which is non-monotone with re- spect to the 
order of the cards. The medium should be capable of remembering very large numbers of data at a much 
smaller price 4.4. 4.5. 
96 Part 2 1 The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
than electronic devices. It must be fast enough so that, even when it has to be 
used frequently in a problem, a 
large percentage of the total solution time is not spent 
in getting data 
into and out 
of this medium and achieving the desired positioning on 
it. If this condition is not reasonably well 
met, the advantages of the high electronic speeds of the machine will be largely lost. 
Both light- or electron-sensitive 
film and magnetic 
wires or 
tapes, whose motions 
are controlled by servo-mechanisms 
inte- grated with the control, would seem 
to fulfil our needs 
reasonably well. We have tentatively decided 
to use magnetic wires since we have achieved 
reliable performance with them 
at pulse rates of the order of 25,00O/sec and beyond. Lastly our memory hierarchy requires a vast quantity of dead storage, is. storage not integrated with 
the machine. This 
storage requirement 
may be satisfied by 
a library of wires that can be introduced into the machine when desired and at that time become automatically 
controlled. Thus our 
dead storage is really nothing but an extension 
of our secondary 
storage medium. 
It differs from 
the latter 
only in its availability to 
the machine. We impose one additional requirement on our secondary 
memory. It must be possible for 
a human 
to put words on to the 
wire or other substance used and to read the words put on by the machine. In this manner the human can 
control the machine's functions. It is now clear that the secondary storage medium 
is really nothing other than 
a part of our input-output system, cf. 
6.8.4 for a description of a mechanism for 
achieving this. There is another highly important part 
of the input- 
output which we merely mention at this time, namely, some 
mechanism for viewing 
graphically the results of a given compu- tation. This can, of course, be achieved by a 
Selectron-like tube which causes its screen to fluoresce when data are 
put on it by an electron 
beam. For definiteness in the subsequent discussions we assume that associated with the output of each Selectron is a flip-flop. This assemblage 
of 40 flip-flops we term 
the Selectron Register. 4.6. 4.7. 4.8. 4.9. 5. The arithmetic organ 
5.1. In this section 
we discuss the features we now consider desirable for the arithmetic part 
of our machine. 
We give our tentative conclusions as to which of the arithmetic 
operations should be built into 
the machine and which should be 
pro- grammed. Finally, a 
schematic of the arithmetic unit is described. In a discussion of the arithmetical organs of a computing machine one is naturally led to a consideration 
of the number system to be 
adopted. In spite 
of the longstanding tradition of 5.2. building digital machines in the decimal system, we feel strongly 
in favor of the binary system for 
our device. Our fundamental unit of memory is naturally adapted to the 
binary system since we do not attempt to measure gradations of charge at a particular point in the Selectron but are content 
to distinguish two states. The flip-flop again is truly a binary 
device. On magnetic 
wires or 
tapes and in acoustic delay 
line memories one is also content to recog- nize the presence or absence of a pulse or 
(if a carrier frequency is used) of a pulse train, or of the sign of a pulse. (We will not discuss here the ternary possibilities of a positive-or-negative- or-no-pulse system 
and their 
relationship to questions of reliability and checking, nor the very interesting possibilities of carrier fre- quency modulation.) 
Hence if one contemplates 
using a decimal 
system with either 
the iconoscope or delay-line memory 
one is forced into a binary coding 
of the decimal system-each decimal digit being represented 
by at least a tetrad of binary digits. Thus an accuracy of ten decimal digits requires at least 40 binary digits. In a true binary representation 
of numbers, however, 
about 33 digits suffice 
to achieve a 
precision of lolo. The use of the binary system is therefore somewhat more economical of equipment than 
is the decimal. The main virtue of the binary system as 
against the decimal is, however, the greater simplicity and speed with 
which the elementary operations 
can be performed. To illustrate, consider multiplication by repeated addition. In binary multiplication 
the product of a particular digit of the multiplier by the multiplicand is either the 
multiplicand or null according as the multiplier digit 
is 1 or 0. In the decimal system, however, this product has ten possible values 
between null and nine times 
the multiplicand, inclusive. Of course, a decimal number 
has only 
log,,2 - 0.3 times as many digits as a binary number of the same accuracy, but even so multiplication in the decimal system is considerably longer than in the binary system. One can accelerate 
decimal multiplication by complicating the circuits, but this fact is irrelevant to 
the point just made since binary multiplication can 
likewise be accelerated by adding 
to the equipment. Similar remarks 
may be made about 
the other operations. An additional point that deserves emphasis 
is this: An important part of the machine is not arithmetical, but logical in nature. Now logics, being a 
yes-no system, 
is fundamentally binary. Therefore a binary arrangement 
of the arithmetical organs contributes very significantly towards 
producing a more 
homogeneous machine, which can be better 
integrated and 
is more efficient. 
The one disadvantage of the binary system from 
the human 
point of view is the conversion problem. Since, however, it is completely known how to convert numbers 
from one base 
to 
Chapter 4 1 Preliminary discussion 
of the logical design of an electronic computing instrument 
97 another and since this conversion 
can be 
effected solely by 
the use of the usual arithmetic processes there is no reason why the computer itself cannot carry out 
this conversion. 
It might be argued that this is a time consuming operation. This, 
however, is not the case. (Cf. 9.6 
and 9.7 of Part 11. Part I1 is a report issued under the title 
Planning and Coding of Problems for an Electronic Computing Instrument.™) Indeed a general-purpose 
computer, used as a scientific research tool, 
is called upon to do a very great number of multiplications upon a 
relatively small 
amount of input data, and hence the 
time consumed in the decimal to binary conversion is only a trivial percentage of the total computing time. A similar remark is applicable to the output data. 
In the 
preceding discussion we have tacitly assumed the de- sirability of introducing and 
withdrawing data in the decimal system. We feel, 
however, that the 
base 10 may not even be a permanent feature 
in a scientific instrument and consequently will probably attempt to 
train ourselves to use numbers base 
2 or 8 or 16. The reason for 
the bases 8 or 16 is this: Since 8 and 16 are powers of 2 the conversion to binary is trivial; since 
both are 
about the size of 10, they violate many 
of our habits less badly than base 2. (Cf. Part 11, 9.4.) Several of the digital computers being 
built or planned in this country and England 
are to contain a 
so-called ﬁfloating 
decimal pointﬂ. This is a mechanism for expressing each word as a characteristic and a mantissa-e.g. 123.45 would be carried in 
the machine as (0.12345,03), where the 3 is the exponent of 10 associated with the number. There appear 
to be two 
major pur- poses in 
a ﬁfloatingﬂ 
decimal point 
system both of which arise from 
the fact that the number of digits in 
a word is a constant, fixed by design considerations 
for each particular machine. The first of these purposes is to retain in a sum or product as many significant digits as possible 
and the second of these is to free 
the human 
operator from the burden of estimating and inserting into 
a prob- 
lem ﬁscale factorsﬂ-multiplicative 
constants which 
serve to keep numbers within the limits of the machine. There is, of course, no 
denying the fact 
that human time is consumed in arranging for the introduction of suitable scale fac- tors. We only argue that the 
time so consumed is a very small 
percentage of the total 
time we 
will spend in preparing an inter- 
esting problem for 
our machine. 
The first advantage of the floating point is, we feel, somewhat illusory. In order to have such 
a floating point one 
must waste memory capacity which could otherwise 
be used for 
carrying more 
digits per word. It would therefore seem 5.3. lSee Bibliography [Goldstine 
and von Neumann, 1963b, 1963c, 1963dI. 
References in this chapter are 
all to this report. to us not at all clear whether the 
modest advantages of a floating binary point 
offset the loss of memory capacity and the increased complexity of the arithmetic 
and control circuits. 
There are 
certainly some problems 
within the scope of our device which really 
require more than 2-40 precision. To handle such problems 
we wish to plan in terms of words whose 
lengths are some fixed integral multiple of 40, and program the machine in such a manner as to give the corresponding aggregates 
of 40 digit words the proper treatment. We must then consider an addi- tion or multiplication as a complex operation programmed 
from a number 
of primitive additions or multiplications (cf. $9, Part 11). There would seem 
to be considerable extra difficulties in the way of such a procedure 
in an instrument with a floating binary point. The reader may remark upon our 
alternate spells of radicalism and conservatism in deciding upon 
various possible 
features for our mechanism. We hope, however, that he will agree, on closer 
inspection, that we are guided by a 
consistent and sound principle in judging the merits of any idea. We 
wish to incorporate into the machine-in the form of circuits-only such logical 
concepts as are either 
necessary to have a complete system or highly con- 
venient because 
of the frequency with which 
they occur and the 
influence they exert in the relevant mathematical 
situations. On the basis of this criterion we definitely wish to build into the machine circuits which will 
enable it to form the binary sum of two 40 digit numbers. We make this decision not because 
addition is a logically basic 
notion but rather because it would slow the mechanism as well 
as the operator down enormously if each addition were 
programmed out of the more simple operations 
of ﬁandﬂ, ﬁorﬂ, and ﬁnotﬂ. The 
same is true for the subtraction. Similarly we reject the desire to form products by programming 
them out 
of additions, the detailed motivation being 
very much the same as 
in the case of addition and 
subtraction. The cases for 
division and square-rooting are much 
less clear. It is well known 
that the 
reciprocal of a number 
a can be 
formed to any desired accuracy 
by iterative schemes. One such scheme consists of improving an estimate X by forming X™ = 2X - ax2. Thus the new error 1 - uX™ is (1 - ax)?-, which is the square of the error in 
the preceding estimate. We notice that in the formation of X™, there are two 
bona fide multiplications-we do not 
consider multiplication by 
2 as a true product since we will have a 
facility for shifting right 
or left in one or two pulse times. If then we somehow could guess l/a to a precision of 2-5, 6 multiplications-3 iterations-would 
suffice to give a final result good to 2-40. Accordingly a small table of Z4 entries could be 
used to get the initial estimate of l/a. In this 
way a reciprocal l/a 5.4. 
98 Part 2 1 The instruction-set processor: main-line computers 
Section 1 I Processors with 
one address per instruction could be formed in 
6 multiplication times, and hence 
a quotient b/a in 7 multiplication times. Accordingly 
we see that the 
question of building a divider is really a 
function of how fast it can be made to operate compared to the 
iterative method sketched 
above: In order to justify its existence, 
a divider must perform a division 
in a good deal less than 7 multiplication times. We 
have, however, 
conceived a divider 
which is much faster than these 7 multipli- cation times and therefore feel justified in building it, 
especially since the amount of equipment needed above 
the requirements of the multiplier is not important. It is, of course, also possible 
to handle square 
roots by 
iterative techniques. In fact, if X is our estimate 
of all2, then X' = yz(X + a/X) is a better estimate. We see 
that this scheme involves one division per iteration. As will be seen below in our more 
detailed examination of the arithmetic 
organ we 
do not include 
a square- rooter in our plans because such 
a device would involve more 
equipment than 
we feel 
is desirable in afirst model. 
(Concerning the iterative method of square-rooting, cf. 8.10 in Part 11.) The first part of our arithmetic 
organ requires 
little dis- cussion at this point. It should be a parallel storage 
organ which can receive 
a number and add it to the 
one already in it, which is also able to clear its contents 
and which can transmit what it contains. We will call such an organ 
an Accumulator. It is quite conventional in principle in past 
and present computing 
machines of the most varied types, e.g. desk 
multipliers, standard IBM counters, more 
modern relay machines, 
the ENIAC. There are of, course, numerous 
ways to build such a 
binary accumulator. 
We distinguish two broad types of such devices: 
static, and dynamic or pulse-type 
accumulators. These 
will be discussed in 
5.11, but it is first 
necessary to make a 
few remarks concerning the arith- 
metic of binary addition. In 
a parallel accumulator, 
the first step in an addition 
is to add each digit of the addend to the corre- sponding digit of the augend. The second step is to perform the carries, and this must 
be done in sequence since a 
carry may produce a carry. In 
the worst case, 
39 carries will occur. Clearly 
it is inefficient to allow 39 times as much time for the second step (performing the carries) as 
for the first step (adding the digits). Hence either 
the carries 
must be accelerated, or use must 
be made of the average number 
of carries or both. We shall show 
that for a sum of binary words, each of length n, the length of the largest carry sequence 
is on the average not in excess 
of 210g n. Let p,(o) designate the probability that a carry sequence 
is of length u or greater in the 
sum of two binary 
words of length n. Then clearly p,(o) - p,(o + 1) is the proba- bility that the largest carry sequence 
is of length exactly o and the weighted average 
5.5. 5.6. n a, = 1 o[pn(4 - p,(o + 111 L'=o is the average length 
of such carry. Note that r,=O since p,(o) = 0 if o > n. From these it is easily inferred that n a, = 2 p,(o) u=l We now proceed 
to show that p,(v) 5 min[l, (n - o + 1)/2"+l]. Observe first that Indeed, p,(o) is the probability that the 
sum of two n-digit 
numbers contains a carry sequence 
of length 20. This probability obtains 
by adding the probabilities of two mutually exclusive alternatives: First: Either the 
n - 1 first digits of the two numbers 
by them- selves contain a carry sequence 
of length zo. This has the proba- bility P,-~(G). Second: The n - 1 first digits of the two numbers by themselves 
do not contain 
a carry sequence 
of length 20. In this case any 
carry sequence 
of length 20 in the total numbers 
(of length n) must end with the last digits 
of the total 
sequence. Hence these must form 
the combination 1, 1. The next v - 1 digits must propagate the carry, hence each of these must form the combination 1, 0 or 0, 1. (The combinations 1, 1 and 0, 0 do not propagate a carry.) The probability of the combination 1, 
1 is x, that one of the alternative combinations 1, 0 or 0, 1 is '/. The total probability 
of this sequence 
is therefore y4('/2)"-' = (%)"+l. The remaining n - o digits must 
not contain 
a carry sequence 
of length 20. This has the probability 1 -p,-"(o). Thus the probability of the second case is [l - p,-,(~)]/2"+~. Combining these two cases, the desired relation obtains. The observation that p,(u) = 0 if II > n is trivial. We see 
with the help of the formulas proved above 
that p,(v) - p,-,(v) is always S1/Zv+l, and hence that 
the sum 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
99 is not in excess of (n - o + l)/2v+1 since there are 
n - o + 1 terms in the sum; since, 
moreover, each p,(o) is a probability, it is not greater 
than 1. Hence we have 
Finally we turn to the question of getting an 
upper bound on a, = ~;=~p,(v). Choose K so that 2K 5 n 5 eK+l. Then This last 
expression is clearly linear 
in 1~ in the interval 2K 5n52K+1, and it is =K for n = ZK and =K + 1 for n = 2K+1, i.e. it is Z21og n at both ends 
of this interval. 
Since the function 210g n is everywhere concave from below, 
it follows that our expression is s210g n throughout this interval. 
Thus a, 5 210g n. This holds for all K, i.e. for all 
n, and it is the in- equality which we wanted 
to prove. For our case 
n = 40 we have a, 5 log,40 - 5.3, i.e. an average length of about 5 for the longest carry sequence. (The actual 
value of u4(, is 4.62.) Having discussed the addition, we 
can now go on 
to the subtraction. It is convenient to discuss at this point our 
treatment of negative numbers, 
and in order to do 
that right, it is desirable to make some observations 
about the treatment of numbers in general. Our numbers are 40 digit aggregates, 
the left-most digit being 
the sign digit, and the other digits genuine 
binary digits, with positional values 2-l, 2-*, 
. . . 
, 2-39 (going from left to right). 
Our accumulator will, however, treat the sign digit, too, 
as a binary digit with the positional value 
2O-at least when it functions as an adder. For 
numbers between 0 and 1 this is clearly all right: The left-most digit will then be 0, and if 0 at this place is taken to represent 
a + sign, then the number is correctly expressed with its sign and 39 binary digits. Let us now consider one 
or more unrestricted 40 binary digit 
numbers. The accumulator will add them, with 
the digit-adding and the carrying mechanisms functioning normally 
and identically in all 40 positions. There is one reservation, however: 
If a carry originates in the left-most position, then it has nowhere to 
go from 
there (there being no further 
positions to the 
left) and is ﬁlostﬂ. This means, 
of course, that the addend and the 
augend, both numbers between 0 and 2, produced a sum 
exceeding 2, 
and the accumulator, being unable to 
express a digit with a positional value 2l, which would now 
be necessary, omitted 2. That is, the 5.7. sum was formed correctly, excepting 
a possible 
error 2. If several such additions 
are performed in succession, 
then the ultimate error 
may be any integer multiple 
of 2. That is, the accumulator is an adder which allows errors that are 
integer multiples of 2-it is an adder modulo 2. It should be noted that our convention 
of placing the binary point immediately to the 
right of the left-most digit has 
nothing to do 
with the structure of the adder. In order to make this point clearer we proceed to discuss the possibilities of positioning the binary point in 
somewhat more 
detail. We begin 
by enumerating the 40 digits of our numbers (words) 
from left to right. In doing this we use 
an index h = 1, . . 
. , 40. Now we might have 
placed the binary point 
just as 
well between digits j and i + 1, i = 0, . . . , 40. Note, that i = .0 corresponds to the 
position at the extreme left 
(there is no digit h = i = 0); j = 40 corresponds to the position at the extreme right (there is no position h = i + 1 = 41); and j = 1 corresponds to our above 
choice. Whatever our choice 
of j, it does not affect the correctness of the accumulator™s addition. (This 
is equally true for subtraction, cf. below, but not for multiplication and division, cf. 5.8.) Indeed, we have merely multiplied all numbers by 
2i-I (as against our 
previous convention), and such a 
ﬁchange of scaleﬂ has 
no effect on addition (and subtraction). However, now 
the accumulator is an adder which 
allows errors that are integer multiples of 2i it is an adder modulo 2j. We mention this because 
it is occasionally convenient to 
think in terms of a convention which places 
the binary point at the right end of the digital aggregate. 
Then j = 40, our numbers 
are integers, and the 
accumulator is an adder modulo 24ﬂ. We must emphasize, however, 
that all of this, i.e. all 
attribu- tions of values to j, are purely convention-Le. it is solely the mathematician™s interpretation of the functioning of the machine and not a physical 
feature of the machine. This convention 
will necessitate measures that have to be made effective by 
actual physical features of the machine-i.e. the convention will become a physical 
and engineering reality 
only when we come to the organs of multiplication. We will use 
the convention i = 1, i.e. our numbers lie 
in 0 and 2 and the 
accumulator adds modulo 
2. This being so, these numbers 
between 0 and 2 can be used to represent all numbers modulo 
2. Any real number x agrees modulo 
2 with one 
and only one number X between 0 and 2-0r, to be quite precise: 0 5 X < 2. Since our addition functions modulo 
2, we see that the accumulator may be used to represent and to add numbers modulo 
2. This determines the representation of negative numbers: 
If x < 0, then we 
have to 
find the unique integer multiple of 2, 2s 
100 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
(s = 1, 2, . . .) such that 0 5 T < 2 for F = x + 2s (Le. 
- 2s 5 x < 2(1 - s)), and represent x by the digitalization of X In this way, 
however, the sign digit character of the left-most digit is lost: It can be 0 or 1 for both x 2 0 and x < 0, hence 0 in the left-most position 
can no longer be associated with the + sign of x. This may seem a bad deficiency of the system, but it is easy to remedy-at least to 
an extent which suffices for our purposes. This 
is done as follows: We usually work 
with numbers 
x between -I and 1-or, to be quite precise: - 1 x < 1. Now the X with 0 5 X < 2, which 
differs from 
x by an 
integer multiple of 2, behaves 
as follows: If x 2 0, then 0 x < 1, hence X = x, and so 0 s X < 1, the left- most digit 
of X is 0. If x < 0, then - 1 s x < 0, hence Z = x + 2, and so 1 5 X < 2, the 
left-most digit of li: is 1. Thus the left-most digit (of 3 is now a precise equivalent of the sign (of x): 0 corre- sponds to + and 1 to - . Summing up: The accumulator may be taken to represent all real numbers modulo 2, and it adds them modulo 2. If x lies between - 1 and 1 (precisely: -1 5 x < 1)-as it will in almost all 
of our uses of the machine-then the left-most digit represents the sign: 0 is + and 1 is - . Consider now a negative number 
x with -1 5 x < 0. Put x = -y, 0 < y 1. Then we 
digitalize x by representing it as x + 2 = 2 - y = 1 + (1 - y). That is, the left-most (sign) 
digit of x = -y is, as it should be, 1; and the remaining 39 digits are those of the complement of y = -x = 1x1, i.e. those 
of 1 - y. Thus we 
have been 
led to 
the familiar representation of negative numbers by complementation. The connection between the digits of x and those of -x is now easily formulated, for any x 5 0. Indeed, -x is equivalent to 2- x = ((21 - 2-39) - 39 i=O x} + 2-39 = (2: 2-i - + 2-39 ) (This digit 
index i = 1, . . . 
, 39 is related to our previous 
digit index h = 1, . . . 
, 40 by i = h - 1. Actually it is best to treat 
i as if its domain included the additional value i = 0-indeed i = 0 then corresponds to h = 1, i.e. to the 
sign digit. In any case i expresses the positional value 
of the digit to which it refers more simply than h does: This positional value 
is 2-i = 2-‚h-1™. Note that if we had positioned the binary point 
more generally between i and i + 1, as discussed further above, this positional value would 
have been 
2-(h-j). We now have, as pointed out 
previously, j = 1.) Hence its digits 
obtain by subtracting every digit of x from 1-by complementing each 
digit, i.e. by 
replacing 0 by 1 and 1 by 0-and then adding 1 in the right-most position 
(and effecting all the carries that this may cause). 
(Note how the left-most digit, interpreted as a sign digit, gets inverted by this 
procedure as it should be.) A subtraction x - y is therefore performed by 
the accumulator, Ac, as 
follows: Form x + y™, where y™ has a digit 0 or 1 where y has a digit 
1 or 0, respectively, and then add 1 in the right-most position. The last operation can 
be performed by injecting a carry 
into the 
right-most stage of Ac-since this stage can never receive a carry 
from any other source (there being no further positions to the 
right). In the light of 5.7 multiplication requires special 
care, because here the entire 
modulo 2 procedure breaks down. Indeed, assume that we want to compute a product xy, and that we had to change one 
of the factors, say x, by an integer multiple of 2, say by 2. Then the product (x + 2)y obtains, and this differs from 
the desired xy by 2y. 
214, however, will not in general be an integer multiple of 2, since y is not in general an integer. We will therefore begin 
our discussion 
of the multiplication by eliminating all such difficulties, 
and assume that both factors x, y lie between 0 and 1. Or, to be quite precise: 0 5 x < 1, To effect such 
a multiplication we 
first send the multiplier x into a register AR, the Arithmetic Register, 
which is essentially just 
a set of 40 flip-flops whose characteristics will be discussed below. We place the multiplicand y in the Selectron Register, SR (cf. 4.9) and use the accumulator, Ac, to form and store the partial 
prod- ucts. We propose to multiply the entire 
multiplicand by 
the successive digits of the multiplier in a serial fashion. 
There are, 
of course, two possible ways this 
can be done: 
We can either 
start with the digit in the lowest position-position 
2-39-0r in the highest position-position 
2-1-and proceed successively to the left or right, respectively. There are 
a few advantages from our 
point of view in starting with the right-most digit of the multiplier. We therefore describe that scheme. The multiplication takes place in 39 steps, which correspond 
to the 39 (non-sign) digits 
of the multiplier x = 0, El,&, . . 
. , [39 = (0&c2, . . . 
, &9), enumerated backwards: (39, . . . , &&. Assume that the k - 1 first steps (k = 1, . . . 
, 39) have already 
taken place, involving multiplication of the multiplicand y with the k - 1 last digits 
of the multiplier: [39, . . . 
, &,; and that we are now at the 
kth step, involving multiplication with 
the kth last 
digit: [40-k. Assume furthermore, that Ac now contains the quantity 
p,-,, the result of the k - 1 first steps. [This is the (k - 1)st partial product. For k = 1 clearly p, = 0.1 We now form 2p, = pk-l + 5.8. 05y<l1. &-,y, i.e. 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing 
instrument 101 That is, we do nothing or add y, according to whether .$40--k = 0 or 1. We can then 
form p, by halving 
2p,. Note that the 
addition of (1) produces no carry beyond 
the 2" position, i.e. 
the sign digit: 0 5 p, < 1 is true for h = 0, and if it is true for h = k - 1, then (1) extends it to h = k also, since 0 y, < 1. Hence the 
sum in (1) is 20 and <2, and no carries 
beyond the 2" position arise. 
Hence p, obtains from 2p, by a 
simple right shift, 
which is combined with 
filling in the sign digit (that is freed by this shift) 
with a 0. This right 
shift is effected by an electronic 
shifter that is part of Ac. Now Thus this process produces the product xy, as desired. Note that this xy is the exact product of x and y. Since x and y are 39 digit binaries, 
their exact product xy is a 78 digit binary (we 
disregard the sign digit throughout). How- ever, Ac will only hold 39 
of these. These 
are clearly the left 39 digits of xy. The right 39 digits of xy are dropped 
from Ac one by one 
in the course of the 39 steps, or to be 
more specific, of the 39 right shifts. We will see later that these right 39 digits of xy should and will also 
be conserved (cf. the end 
of this section and the end 
of 5.12, as well as 
6.6.3). The left 39 digits, which remain in Ac, should also 
be rounded off, but we will not discuss this matter here 
(cf. loc. cit. above and 9.9, Part 11). To complete the 
general picture of our multiplication tech- 
nique we must consider how we sense 
the respective digits of our multiplier. There are two 
schemes which come 
to one's mind in this connection. One is to have a 
gate tube associated with each 
flip-flop of AR in such a fashion 
that this gate 
is open if a digit 
is 1 and closed if it is null. We would then need a 39-stage 
counter to act as a switch 
which would successively stimulate these gate tubes to react. A more efficient scheme is to build into AR a shifter 
circuit which enables 
AR to be 
shifted one stage 
to the right each time Ac is shifted and to sense the value of the digit in the right- most flip-flop 
of AR. The shifter itself requires one gate tube per stage. We need in addition a counter to count out the 
39 steps of the multiplication, but this can be achieved by a six stage binary 
counter. Thus the 
latter is more economical of tubes and has one additional virtue 
from our point of view which we discuss in the next paragraph. The choice of 40 digits to a word 
(including the sign) is prob- ably adequate for most 
computational problems but situations certainly might 
arise when we desire 
higher precision, i.e. words of greater length. A trivial illustration 
of this would 
be the 
com- putation of T to more places 
than are now known 
(about 700 decimals, i.e. 
about 2,300 binaries). More important instances are the solutions of N linear equations in 
N variables for large values of N. The extra precision becomes probably necessary 
when N exceeds a 
limit somewhere 
between 20 and 40. A justification 
of this estimate has to be 
based on 
a detailed theory of numerical matrix inversion 
which will be given in a subsequent report. 
It is therefore desirable to be able to handle numbers 
of 39k digits and signs by means 
of program instructions. One way to achieve this end is to use k words to represent a 39k digit number with 
signs. (In this way 39 digits in each 40 digit word are used, but all sign digits excepting the first one, are apparently 
wasted; cf. however the treatment 
of double precision numbers in Chapter 9, Part 11.) It is, of course, necessary 
in this case 
to instruct the machine to perform the elementary operations 
of arithmetic in a manner that conforms with this interpretation of k-word com- plexes as 
single numbers. (Cf. 9.8-9.10, 
Part IT.) In order 
to be 
able to treat numbers in 
this manner, it is desirable to keep not 
39 digits in a product, but 78; this 
is discussed in 
more detail in 6.6.3 below. To accomplish this 
end (conserving 78 product digits) we connect, 
via our 
shifter circuit, 
the right-most digit 
of Ac with the left-most non-sign digit of AR. Thus, when in 
the process of multiplication a shift 
is ordered, the last digit 
of Ac is transferred into the 
place in AR made vacant when 
the multiplier was shifted. 5.9. To conclude our 
discussion of the multiplication of posi- tive numbers, we note this: As described thus far, 
the multiplier forms the 78 digit product, xy, for a 39 digit multipler x and a 39 digit multiplicand y. We assumed x 2 0, y 2 0 and therefore had xy 2 0, and we will only 
depart from these assumptions 
in 5.10. In addition 
to these, how- ever, we also assumed x < l, y < l, i.e. the x, y have their binary points both immediately 
right of the sign digit, which implied the same for xy. One might question the necessity of these additional 
assumptions. Prima facie they may seem 
mere conventions, which affect only the mathematician's interpretation of the functioning of the ma- chine, and not a physical 
feature of the machine. (Cf. the cor- responding situation in addition 
and subtraction, in 
5.7.) Indeed, if r had its binary point between digits 
and i + 1 from the left (cf. the discussion of 5.7 dealing with this 
j; it also applies to k below), and y between k and k + 1, then our above 
method of multiplication would still give the correct result xy, provided that 
102 Part 2 
1 The instruction-set 
processor: main-line computers 
Section 1 1 Processors with one address per instruction 
the position of the binary point in xy is appropriately assigned. Specifically: Let the binary point 
of xy be between digits 1 and I + 1. x has the binary point between digits i and i + 1, and its sign digit is 0, hence its range 
is 0 5 x < 2i-l. Similarly y has the range 0 5 y < ek-l, and xy has the range 0 5 xy < 2z-1. Now the ranges of x and y imply that the 
range of xy is necessarily 0 I - xy < 21-l ek-l = 21+k-2. Hcnce 1 = i + k - 1. Thus it might seem that our actual positioning of the binary point-immediately right of the sign digit, i.e. i = k = 1-is still a mere convention. 
It is therefore important to realize 
that this is not so: The choices of i and k actually correspond to very real, physical, engi- neering decisions. The reason for this is as 
follows: It is desirable to base the running of the machine on a sole, consistent mathe- matical interpretation. It 
is therefore desirable 
that all arithmeti- cal operations be performed with 
an identically conceived posi- tioning of the binary point 
in Ac. Applying this 
principle to x and y gives i = k. Hence the position of the binary point 
for xy is given by j + k - 1 = 2j - 1. If this is to be the same as for 
x, and y, then 21 - 1 = 1, i.e. i = 1 ensues-that is, our above 
positioning of the binary point immediately 
right of the sign digit. There is one possible escape: To place into 
Ac not the left 39 digits of xy (not counting the sign digit 0), but the 
digits i to i + 38 from the left. 
Indeed, in this 
way the position of the binary point 
of xy will be (2j - 1) - (j - 1) = j, the same as for 
x and y. This procedure means that we drop the left i - 1 and right 40 + i digits of xy and hold the middle 39 in- Ac. Note- that posi- tioning of the binary point-means that x < 2i-l, y < 2i-l and xy can only be used if xy < 21-l. Now the assumptions secure only xy < 223-2. Hence xy must be 2j-l times smaller 
than it might be. This is just the thing 
which would 
be secured by 
the vanishing of the left i - 1 digits that we had 
to drop from Ac, as shown 
above. If we wanted to use such a procedure, 
with those 
dropped left i - 1 digits really existing, 
i.e. with j# 1, then we would have to make physical 
arrangements for their conservation elsewhere. 
Also the general mathematical planning for the machine would be definitely complicated, due to the physical fact that Ac now holds a rather arbitrarily picked middle 
stretch of 39 digits from 
among the 78 digits of xy. Alternatively, we might fail 
to make such arrangements, but this would 
necessitate to see to it in 
the mathematical planning 
of each problem, that all products turn out to be 2i-l times smaller 
than their a priori maxima. Such an 
observance is not at all impossible; 
indeed similar things 
are un- avoidable for the other operations. [For example, 
with a factor 2 in addition (of positives) or 
subtraction (of opposite sign quanti- ties). Cf. also the remarks in the first part of 5.12, dealing with keeping ﬁwithin rangeﬂ.] However, it involves a loss of significant digits, and the choice i = 1 makes it unnecessary in multiplication. We will therefore make our choice 
i = 1, i.e. the positioning of the binary point immediately 
right of the sign digit, binding for all 
that follows. We now pass to the case where the multiplier x and the multiplicand y may have either 
sign + or -, i.e. any combi- 
nation of these signs. It would not do simply to extend the method of 5.8 to include the sign digits 
of x and y also. Indeed, we assume - 1 5 x < 1, - 1 s y < 1, and the 
multiplication procedure 
in question is defi- nitely based on the 20 interpretations of x and y. Hence if x < 0, then it is really using 
x + 2, and if y < 0, then it is really using 
y + 2. Hence for x < 0, y 2 0 it forms 5.10. (x + 2)y = xy + 2y for x 2 0, y < 0 it forms x(y + 2) = xy + 2x (x + 2)(y + 2) = xy + 2x + 2y + 4 for x < 0, x < 0, it forms or since things 
may be taken modulo 2, xy + 21 + 214. Hence correction terms 
-2y, -2x would be needed for x < 0, y < 0, respectively (either or both). This would 
be a 
possible procedure, but there 
is one difficulty: As xy is formed, the 39 digits of the multiplier 
x are gradually lost from 
AR, to be replaced 
by the right 39 digits of xy. (Cf. the discussion at the end of 5.8.) Unless we are willing to build an additional 40 stage register to hold x, therefore, x will not be 
available at the end of the multiplication. Hence we cannot use it in the correction 2x of xy, which becomes necessary 
for y < 0. Thus the case x < 0 can be handled along 
the above lines, 
but not the case y < 0. It is nevertheless possible 
to develop an adequate procedure, and we now proceed to do this. 
Throughout this procedure we 
will maintain the assumptions - 1 5 x < 1, - 1 5 y < 1. We proceed in several successive steps First: Assume that the 
corrections necessitated by the possi- bility of y < 0 have been 
taken care of. We 
permit therefore y $ 0. We will consider the corrections necessitated by the possi- bility of x < 0. Let us disregard the sign digit of x, which is 1, i.e. replace it by 0. Then x goes over into x™ = x - 1 and as - 1 x < 0, this d will actually behave like (x - 1) + 2 = x + 1. Hence our multiplication procedure will produce x‚y = (x + l)y = xy + y, 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
103 and therefore a correction -y is needed at the end. (Note 
that we did not use the sign digit of x in the conventional way. 
Had we done so, then a correction -2y would have 
been necessary, as seen above.) 
We see therefore: Consider 
x 5 0. Perform first all necessary steps for forming x'y(y 5 0), without yet reaching the sign digit of x (i.e. treating x as if it were 
20). When the 
time arrives at which the digit to of x has to become effective-Le. immediately after became effective, after 39 shifts (cf. the discussion near the end of 5.8)-at which time 
Ac contains, say, jZl (this corresponds to the p,, of 5.8), then form This is xy. (Note the difference between this last 
step, forming p, and the 39 preceding steps in 5.8, forming p,, p,, . . . 
, p39.) Second: Having disposed of the possibility x < 0, we may now 
assume x 2 0. With this assumption we 
have to treat all y 0. Since y 2 0 brings us back entirely 
to the familiar case 
of 5.8, we need to 
consider the case y < 0 only. Let y' be the 
number that obtains by disregarding 
the sign digit of y' which is 1, i.e. by 
replacing it 
by 0. Again y' acts not like 
y - 1, but like (y - 1) + 2 = y + 1. Hence the multiplication procedure of 5.8 will produce xy' = x(y + 1) = xy + x, and there- 
fore a 
correction x is needed. (Note that, quite 
similarly to what we saw in the first case above, 
the suppression of the sign digit of y replaced the previously recognized correction 
-2x by the present one - x.) As we observed earlier, this correction -x cannot be applied at the end to the completed xy' since at that time x is no longer available. Hence we must apply the correction -x digitwise, subtracting every digit 
at the time when it is last found 
in AR, and in a 
way that makes it effective with the proper posi- tional value. 
Third: Consider then x = 0, tl, t,, . . . 
, t39 = (E1, t2 . . . 
t3J. The 39 digits c1 . . . 
t39 of x are lost in the course of the 39 shifts of the multiplication procedure of 5.8, going 
from right to left. Thus the operation 
No. k + 1 (k = 0, 1, . . . , 38, cf. 5.8) finds 
t39-k in the right-most stage of AR, uses it, and then 
loses it through its concluding right 
shift (of both Ac and AR). After this step 39 - (k + 1) = 38 - k further steps, i.e. shifts follow, 
hence before its own 
concluding shift there are still 39 - k shifts to come. Hence the 
positional values are 23y-k times higher 
than they will be at the end. <39-k should appear at the end, in the correcting term -x, with the 
sign - and the positional value 
2--(39-k3. Hence we may inject 
it during 
the step k + 1 (before its shift) with the - - sign - and the 
positional value 1. That is to say, -t3,-k in the sign digit. This, however, is inadmissible. Indeed, <39-k might cause carries 
(if t39-k = l), which would 
have nowhere 
to go from the sign digit (there being no further 
positions to the left). This error 
is at its origin an integer multiple 
of 2, but the 39 - k subsequent shifts reduce its positional 
value 239-k times. Hence it might contribute 
to the end 
result any integer multiple of 2-(38-kJ-and this is a genuine error. 
Let us therefore add 1 - &-, to the sign digit, i.e. 0 or 1 if &-k is 1 or 0, respectively. We will 
show further below, that with this procedure there 
arise no carries of the inadmissible kind. Taking this momentarily for granted, let us see what the total effect is. We are correcting not 
by -x but by cz?l 2-i - x = 1 - - 
x. Hence a final correction by - 1 + 2-39 is needed. Since this 
is done at 
the end (after all shifts), 
it may be taken modulo 
2. That is to say, we must add 1 + 2-39, i.e. 1 in each of the two extreme 
positions. Adding 
1 in the right-most position has 
the same effect as in the discussion at the end of 5.7 (dealing with the subtraction). It is equivalent to injecting a carry into the right-most stage of Ac. Adding 1 in the left-most position, 
i.e. to the sign digit, 
produces a 1, since that digit was necessarily 
0. (Indeed, the last operation ended in 
a shift, thus 
freeing the sign digit, cf. below.) 
Fourth: Let us now consider 
the question of the carries that may arise in the 39 steps of the process described above. In 
order to do this, let us describe the kth step (k = 1, . . . , 39), which is a variant of the kth step described for a positive 
multiplication in 5.8, in the same way in which we described the original kth 
step loc. cit. That is to say, let us see 
what the formula (1) of 5.8 has become. It is clearly 2p, = p,-, + (1 - (40-k) + t4"-,y', i.e. That is, we add 
1 (y's sign digit) or y' (y without its sign digit), according to whether <4n-k = 0 or 1. Then p, should obtain from 2p, again by 
halving. Now the addition of (2) produces 
no carries beyond the 2" position, as we asserted 
earlier, for the same reason as 
the addition of (1) in 5.8. We can argue in the same way 
as there: 0 5 p, < 1 is true for h = 0, and if it is true for h = k - 1, then (1) extends it to h = k also, since 0 5 Y'~ 5 1. Hence the sum in (2) is 20 and <2, and no carries beyond 
the 2" position arise. Fifth: In the three last observations we assumed y < 0. Let us now restore 
the full generality of y 5 0. We can then describe 
104 Part 2 1 The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
the equations (1) of 5.8 (valid for y 2 0) and (2) above (valid for 
y < 0) by a single formula, 
2pk = pk-1 + Yp (3) = y™s sign digit for 540-k = 0 = y without its 
sign digit for 540-k = 1 YL [ Thus our verbal formulation 
of (2) applies here, too: We add y™s sign digit or 
y without its sign, 
according to whether 
<40-k = 0 or 1. All pk are 20 and < 1, and the 
addition of (3) never originates 
a carry beyond the 2O position. pk obtains from 2p, by a right shift, filling the sign digit with a 0. (Cf. however, 
Part 11, Table 2 for another sort of right shift 
that is desirable in explicit form, 
i.e. as 
an order.) For y 2 0, xy is p,,, for y < 0, xy obtains from p,, by injecting 
a carry into 
the right-most stage of Ac and by placing 
a 1 into the sign digit in Ac. Sixth: This procedure applies for x 2 0. For x < 0 it should also be applied, since it makes use 
of x™s non-sign digits only, but at the end 
y must be subtracted from the result. This method of binary multiplication 
will be illustrated in 
some examples in 5.15. 5.11. To complete our discussion of the multiplicative organs 
of our machine we 
must return to a consideration of the types of accumulators mentioned 
in 5.5. The static accumulator operates 
as an adder 
by simultaneously applying static 
voltages to its two 
inputs-one for each of the two 
numbers being added. When steady-state operation 
is reached the total sum is formed complete with all 
carries. For such an accumulator the above discussion is substantially complete, except 
that it should be remarked that such a circuit requires at most 39 rise times to complete a carry. Actually it is possible that the 
duration of these successive rises is proportional to a lower power 
of 39 than the first one. Each stage of a dynamic accumulator consists of a binary counter for registering the digit and a flip-flop for 
temporary storage of the carry. The counter receives a pulse if a 1 is to be added in at that place; if this causes 
the counter to go from 
1 to 0 a carry has occurred and hence 
the carry flip-flop will be set. It then 
remains to perform the carries. Each 
flip-flop has associated with it a gate, the output 
of which is connected to the next binary counter to 
the left. The carry is begun by pulsing all 
carry gates. 
Now a carry may produce a carry, so that the 
process needs to be repeated until all carry 
flip-flops register 0. This can be detected by means of a circuit involving a 
sensing tube con- nected to each carry flip-flop. It was shown in 5.6 that, on the average, five pulse times (flip-flop reaction times) are required for the complete carry. An alternative scheme is to connect a gate tube to each binary counter 
which will detect whether an incom- ing carry pulse would 
produce a carry and will, under this 
cir- cumstance, pass the incoming carry 
pulse directly to the next stage. This 
circuit would require at most 39 rise times for the completion of the carry. (Actually 
less, cf. above.) At the present time the development of a static accumulator 
is being concluded. From preliminary tests 
it seems that it will add two numbers in about 5 psec and will shift 
right or left in about 1 psec. We return now to the multiplication operation. In 
a static accumulator we order simultaneously an addition 
of the multi- plicand with 
sign deleted or 
the sign of the multiplicand (cf. 5.10) and a complete carry 
and then 
a shift for 
each of the 39 steps. In a dynamic accumulator of the second kind just 
described we order in succession 
an addition 
of the multiplicand with 
sign deleted or the sign of the multiplicand, a complete carry, and a shift for each of the 39 steps. In 
a dynamic accumulator of the first kind we can avoid losing the time required for completing the carry (in 
this case an 
average of 5 pulse times, cf. above) 
at each of the 39 steps. 
We order an addition by 
the multiplicand with sign deleted or the sign of the multiplicand, then order one 
pulsing of the carry gates, and finally shift the contents 
of both the digit counters 
and the 
carry flip-flops. This process is repeated 39 times. 
A simple arithmetical analysis which may 
be carried out in a 
later report, 
shows that at each one 
of these intermediate stages a single 
carry is adequate, and 
that a complete set of carries is needed at 
the end 
only. We then carry out 
the complement corrections, still without ever ordering 
a complete set of carry operations. When all these corrections are completed and after round-off, described below, we then 
order the complete carry 
mentioned above. It is desirable at this point in 
the discussion to consider rules for 
rounding-off to n-digits. 
In order to assess the charac- teristics of alternative possibilities for 
such properly, 
and in par- ticular the 
role of the concept of ﬁunbiasednessﬂ, it is necessary to visualize the conditions under which rounding-off is needed. Every number x that appears in 
the computing machine 
is an approximation of another number x™, which would 
have appeared 
if the calculation had been performed absolutely 
rigorously. The approximations to which we 
refer here are not 
those that are caused by the 
explicitly introduced approximations of the numeri- cal-mathematical set-up, 
e.g. the replacement of a (continuous) differential equation by a (discrete) difference 
equation. The effect of such approximations should 
be evaluated mathematically 
by the person who plans 
the problem for the machine, and should not be a direct concern 
of the machine. Indeed, it has to be handled 5.12. 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
105 by a mathematician and cannot be handled by the machine, since 
its nature, complexity, and difficulty may be of any kind, depend- ing upon the problem under consideration. The approximations which concern us here are 
these: Even 
the elementary operations of arithmetic, to which the mathematical approximation-formula- tion for the machine 
has to reduce the true (possibly transcenden- tal) problem, are not 
rigorously executed by the machine. The machine deals 
with numbers of n digits, where n, no matter how large, has to be a fixed quantity. (We assumed for our machine 40 digits, including the sign, i.e. n = 39.) Now the sum and differ- ence of two n-digit numbers are again n-digit numbers, 
but their product and quotient (in general) 
are not. (They 
have, in general, 2n or w-digits, respectively.) Consequently, multiplication 
and division must unavoidably be replaced by the machine by two different operations which must produce 
n-digits under all 
condi- tions, and which, subject 
to this limitation, should lie as close as 
possible to the results of the true 
multiplication and division. One might call 
them pseudo-multiplication and pseudo-division; how- ever, the accepted 
nomenclature terms them as multiplication and division with round-off. (We are now creating the impression that addition and subtraction are entirely 
free of such shortcomings. 
This is only true inasmuch as they do not 
create new 
digits to the right, as multiplication and division do. However, they can create new digits to the left, i.e. cause the numbers to ﬁgrow out of rangeﬂ. This complication, which 
is, of course, well 
known, is normally met by the planner, by mathematical arrangements and 
estimates to keep the numbers ﬁwithin rangeﬂ. 
Since we propose to have our machine deal 
with numbers 
between -1 and 1, multiplication can never cause 
them to ﬁgrow out of rangeﬂ. Division, of course, might cause 
this complication, too. The plan- ner must therefore 
see to it 
that in every division the absolute value of the divisor exceeds 
that of the dividend.) Thus the round-off is intended to produce 
satisfactory n-digit approximations for 
the product xy and the quotient x/y of two n-digit numbers. 
Two things are wanted of the round-off: (1) The approximation should be good, i.e. its variance from the ﬁtrueﬂ xy or x/y should be as small as practicable; (2) The approximation should be unbiased, i.e. its mean should 
be equal to 
the ﬁtrueﬂ xy or x/y. These desiderata must, however, 
be considered in conjunction with some further comments. Specifically: (a) x and y themselves are likely to be the results of similar round-offs, directly or in- directly inherent, 
i.e. x and y themselves should be viewed as unbiased n-digit approximations of ﬁtrueﬂ x™ and y™ values; (b) by talking of ﬁvariancesﬂ and ﬁmeansﬂ we 
are introducing statistical concepts. Now the approximations which we 
are here considering are not really of a statistical nature, but are due to 
the peculiarities (from our point of view, inadequacies) of arithmetic and of digital representation, and are therefore actually 
rigorously and uniquely determined. It seems, however, in the present state of mathe- matical science, rather hopeless to try to deal with 
these matters rigorously. Furthermore, a certain statistical approach, while not truly justified, has always given 
adequate practical results. This consists of treating those digits which one 
does not wish to use individually in subsequent calculations 
as random variables 
with equiprobable digital values, and of treating any 
two such digits as statistically independent (unless this is patently false). These things being understood, 
we can now undertake to dis- cuss round-off procedures, realizing 
that we will have to apply them to the multiplication and to the 
division. Let x = (.t1 . . . 
t,) and y = (.ql . . . q,) be unbiased approxi- mations of x™ and y™. Then the ﬁtrueﬂ xy = (.tl . . . 
. . . t2,) and the 
ﬁtrueﬂ x/y = (.a1 . . . 
W,W,+~W,+~ . . . 
) (this goes on ad infinitum!) are approximations of x™y™ and x™/y™. Before we discuss how to round 
them off, we must know 
whether the ﬁtrueﬂ 
xy and x/y are themselves unbiased approximations 
of x™y™ and x™/y™. xy is indeed an unbiased approximation 
of x™y™, i.e. the mean of xy is the mean of x( = x™) times the mean of y( = y™), owing to the independence assumption which we 
made above. However, 
if x and y are closely correlated, e.g. for 
x = y, i.e. for 
squaring, there is a bias. 
It is of the order of the mean square of x - x™, i.e. of the variance of x. Since x has n digits, this variance 
is about 1/22n (If the digits of x™, beyond n are entirely 
unknown, then our original assumptions give the variance 1/12.22n.) Next, x/y can be written as x.y-l, and since we have already 
discussed the bias of the product, it 
suffices now to consider the reciprocal y-™. Now if y is an unbiased estimate of y™, then y-l is not an 
unbiased estimate of y™-™, i.e. the mean of y™s reciprocal is not the reciprocal of y™s mean. The difference is -Y-~ times the variance of y, i.e. it is of essentially the same order 
as the bias found above 
in the case of squaring. It follows from all this 
that it is futile to 
attempt to avoid biases 
of the order of magnitude 1/22n or less. (The factor Y12 above may 
seem to be changing the order of magnitude in question. However, 
it is really the square root of the variance which 
matters and d(Y12 - 0.3 is a moderate factor.) Since we propose to use n = 39, therefore 1/278(-3 x is the critical case. Note that this possible bias level is l/23y(-2 x 10-12) times our last 
significant digit. Hence we will look for round-off rules to n digits for the ﬁtrueﬂ xy = (.tl . . . t,[,+l . . . 
t2,) and x/y = (.wl . . . 
W,W,+~W,+~ . . 
. ). The desideratum (1) which we formulated 
previously, that the variance should 
be small, is still valid. The 
106 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
desideratum (2), however, that the 
bias should 
be zero, need, according to the above, only 
be enforced up to terms of the order The round-off procedures, which we 
can use in this connection, fall into two 
broad classes. The first class is characterized by its 
ignoring all digits beyond 
the nth, and 
even the nth digit itself, which it replaces by 
a 1. The second class is characterized by the procedure of adding one unit in 
the (n + 1)st digit, performing 
the carries which this may 
induce, and then 
keeping only the n first digits. When applied to a number of the form (.vl . . 
. V,V,+~U,+~ . . . 
) (ad infinitum!), the effects of either procedure 
are easily estimated. In the first case we may say we are dealing 
with (.vl, . . . , u,-™) plus a random number of the form (.O . . . 
, Ov,~,+~v,+~ . . 
. ), i.e. random 
in the interval 0, 1/2%-™. Comparing with the rounded off (.v1v2 . . . z~,-~l), we therefore 
have a difference random 
in the interval - l/2%, 1/2,. Hence its mean is 0 and its variance 
y3 22n. In the second case 
we are dealing with (.vl . . . v,) plus a random number of the form (.O . . 
. 00~,+~v,+~ . . . 
), i.e. random in the interval 0, 1/2,. The ﬁrounded-offﬂ value will be (.v, . . . 
v,) in- creased by 
0 or by 1/2,, according to whether 
the random number in question lies in the interval 0, 1/2,+™, or in the interval 1/2,+l, 1/2,. Hence comparing with the ﬁrounded-offﬂ value, 
we have a difference random 
in the intervals 0, 1/2,+™, and 0, --1/2,+l, i.e. in the interval - 1/2,+l, 1/2,+™. Hence its mean is 0 and its variance ( y12)22n. If the number to be rounded-off has the form (.vl . . . 
V,V,+~V,+~ . . . 
v,,,) (p finite), then these results are somewhat affected. The order of magnitude of the variance remains the same; indeed for large p even its relative 
change is negligible. The mean difference may 
deviate from 0 by amounts which are easily esti- mated to be of the order 1/2, * 1/2P = 1/2ﬂ+P. In division we have the first situation, x/y = (.wl . . . W~W,+~W,+~ . . . 
), i.e. p is infinite. In multiplication we have the second one, xy = (.& . . . 
[n.$n+l . . . 
&,), i.e. p = n. Hence for the division both methods are applicable without 
modification. In multiplication a bias of the order of 1/22n may be introduced. We have seen that it is pointless to insist on 
removing biases of this size. We will therefore use the unmodified methods in this 
case, too. It should be noted that the bias in the case of multiplication can be 
removed in various ways. 
However, for the reasons set forth 
above, we shall not complicate 
the machine by introducing such corrections. Thus we have two standard 
ﬁround-off ﬂ methods, both unbiased to the 
extent to which we need this, and with 
the variances 1/22,, 1/3 - 22n, and (1/2)22n, that is, with the dispersions (1/~‚3)(1/2~) = 0.58 times the last digit 
and (1/2~‚3)(1/2,) = 0.29 times the last digit. 
The first one requires no carry 
facilities, the second one requires them. 
Inasmuch as we propose to form the product x™y™ in the accu- mulator, which has carry 
facilities, there is no reason 
why we should not adopt the 
rounding scheme described above which has 
the smaller dispersion, i.e. the one which may 
induce carries. In 
the case, however, of division we wish to avoid schemes leading 
to carries since 
we expect to form the quotient in 
the arithmetic register, which does not permit 
of carry operations. 
The scheme which we accordingly 
adopt is the one in 
which w, is replaced by 1. This method has the decided advantage 
that it enables us to write 
down the approximate quotient as soon as we know its first (n - 1) digits. It will be seen in 5.14 and 6.6.4 below that our procedure for forming the quotient of two numbers will always 
lead to a result that is correctly rounded in accordance with 
the decisions just 
made. We 
do not 
consider as serious the fact that our rounding scheme 
in the case of division has a dispersion 
twice as large as that in multiplication since 
division is a far less frequent operation. A final remark should 
be made in connection with the possible, occasional need of carrying more 
than n = 39 digits. Our logical control is sufficiently flexible to permit treating 
k (=2, 3, . . . 
) words as 
one number, 
and thus effecting n = 3%. In this case 
the round-off has to be handled differently, cf. Chapter 9, Part 11. The multiplier produces all 
78 digits of the basic 39 by 39 digit multi- 
plication: The first 39 in the Ac, the last 39 in the AR. These must then be manipulated in an appropriate manner. (For 
details, cf. 6.6.3 and 9.9-9.10, Part 11.) The divider works for 
39 digits only: 
In forming x/y, it is necessary, even if x and y are available to 39k digits, to use only 39 digits of each, and a 39 digit result 
will appear. It seems most 
convenient to use this result 
as the first step of a series 
of successive approximations. The successive improve- ments can then be obtained by various means. 
One way consists of using the well known iteration formula (cf. 5.4). For k = 2 one such step will be needed, for k = 3, 4, two steps, for 
k = 5, 6, 
7, 8 three steps, etc. An alternative procedure 
is this: Calculate the remainder, using the approximate, 39 digit, quotient and the 
complete, 39k digit, divisor and dividend. Divide this again by 
the approximate, 39 digit, divisor, thus obtaining essentially the next 39 digits of the quotient. Repeat 
this procedure until 
the full 39k desired digits 
of the quotient have been obtained. 
We might mention 
at this time a complication which 
arises when a floating 
binary point is introduced into the machine. The operation of addition which 
usually takes at most ylo of a 5.13. 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
107 multiplication time becomes much longer 
in a machine with 
floating binary since 
one must perform shifts and round-offs as 
well as additions. It would seem reasonable in this case 
to place the time of an addition as about y3 to '/z of a multiplication. At this rate it is clear that the number of additions in a problem is as important a factor in the total solution time as are the number of multiplications. (For further details concerning the floating binary point, 
cf. 6.6.7.) We conclude our discussion of the arithmetic unit with 
a description of our method for handling the division operation. To perform a division 
we wish to store the dividend in SR, the partial remainder in 
Ac and the 
partial quotient 
in AR. Before proceeding further let us consider the so-called restoring and non-restoring methods of division. In order to be able to 
make certain comparisons, we will do this for a general base 
m = 2, 3, .... Assume for the moment that divisor and dividend are both 
positive. The ordinary process of division consists of subtracting from the partial remainder (at 
the very beginning 
of the process this is, of course, the dividend) the divisor, repeating this until the former becomes smaller 
than the latter. 
For any 
fixed positional value in the quotient in 
a well-conducted division this need be done at most m - 1 times. If, after precisely k = 0,1, . . . 
, m - 1 repetitions of this step, the partial remainder 
has indeed become less than the 
divisor, then the digit k is put in the quotient (at 
the position under consideration), the partial remainder 
is shifted one place 
to the 
left, and the 
whole process is repeated for the next position, 
etc. Note that the above comparison 
of sizes is only needed at k = 0, 1, . . . 
, m - 2, i.e. before step 1 and after steps 
1, . . 
. , m - 2. If the value k = m - 1, Le. the point after 
step m - I, is at all reached in 
a well-conducted division, then it may be taken for granted without 
any test, 
that the partial remainder 
has become smaller than the divisor, and the operations on the position under consideration can therefore be concluded. (In 
the binary system, m = 2, there is thus only one step, 
and only one comparison of sizes, before this step.) 
In this way this scheme, 
known as the restoring scheme, requires 
a maximum 
of m - 1 com- parisons and utilizes the digits 0, 1, . . . 
, m - 1 in each place 
in the quotient. The difficulty of this scheme 
for machine purposes is that usually the only economical 
method for comparing two numbers as to size is to subtract one from the other. If the partial remainder 
r, were less than the dividend d, one would then have to add d back into r, - d in order to 
restore the remainder. Thus 
at every stage an unnecessary operation would be performed. A more sym- metrical scheme 
is obtained by not restoring. In this 
method (from here on we need not 
assume the positivity of divisor and dividend) 5.14. one compares the signs of rn and d; if they are of the same sign, the dividend is repeatedly subtracted 
from the remainder until 
the signs become opposite; 
if they are 
opposite, the dividend is repeatedly added 
to the remainder until 
the signs again become like. In this scheme 
the digits that may occur in 
a given 
place in the quotient are evidently kl, k2, . . . 
, k(m - l), the posi- tive digits corresponding 
to subtractions and the 
negative ones to additions of the dividend to 
the remainder. Thus we have 2(m - 1) digits instead 
of the usual m digits. In the decimal system this would 
mean 18 digits instead 
of 10. This is a redundant notation. 
The standard form of the quotient must therefore be restored by 
subtracting from the aggregate of its positive digits the aggregate of its negative 
digits. This requires 
carry facilities in the place where the quotient is stored. We propose 
to store the quotient in AR, which has 
no carry facilities. Hence we could not use this scheme 
if we were to operate in the decimal system. The same objection applies 
to any 
base m for which the digital representation in question is redundant-i.e. when 2(m - 1) > m. Now 2(m - 1) > m whenever m > 2, but 2(m - 1) = m for m = 2. Hence, with 
the use of a register which we 
have so far contemplated, this division scheme is certainly excluded 
from the start unless the binary system is used. Let us now investigate 
the situation in the binary system. We inquire if it is possible to obtain a quasi-quotient by using 
the non-restoring scheme 
and by using 
the digits 1, 0 instead of 1, -1. Or rather we have to 
ask this question: Does this quasi- 
quotient bear 
a simple relationship 
to the true 
quotient? Let us momentarily assume this question 
can be 
answered affirmatively and describe the division procedure. We store the divisor initially in Ac, the dividend in SR and wish to form the quotient in 
AR. We now either add or subtract the contents of SR into Ac, according to whether the signs in Ac and SR are opposite or the same, and insert correspondingly 
a 0 or 1 in the right-hand place of AR. We then shift both Ac and AR one place 
left, with 
electronic shifters that are 
parts of these two aggregates. At this point we 
interrupt the 
discussion to note this: multipli- 
cation required an 
ability to shift right 
in'both Ac and AR (cf. 5.8). We have now found that division similarly 
requires an ability to shift left in both Ac and AR. Hence both organs must 
be able to shift both ways electronically. Since these abilities 
have to be present for the implicit needs 
of multiplication and division, it is just as well 
to make use of them explicitly in the form of explicit orders. 
These are the 
orders 20,21 of Table 1, and of Table 2, Part 11. It will, however, turn out to be convenient to arrange some details 
in the shifts, when they occur 
explicitly under the 
control of those orders, 
108 Pari 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with 
one address per instruction 
differently from when they occur 
implicitly under the control of a multiplication or a division. (For these 
things, cf. the discussion of the shifts near the end 
of 5.8 and in the third remark below 
on one hand, and in the third remark in 7.2, Part 11, on the other hand.) 
Let us now resume the discussion of the division. The process described above 
will have to be 
repeated as many times as 
the number of quotient digits that we consider appropriate to produce in this 
way. This 
is likely to be 39 or 40; we will determine the exact number further below. In this process 
we formed digits .$ = 0 or 1 for the quotient, when the digit should actually have been 
ti = - 1 or 1, with 5; = 2[: - 1. Thus we have a 
difference between the true 
quotient z (based on the digits ti) and the quasi-quotient z' (based on the digits ti), but at the same time a 
one-to-one connection. It would be easy to establish the algebraical expression for this 
connection between z' and z directly, but it seems better to do this 
as part of a discussion which clarifies all 
other questions connected with the process of division at the same time. We first make some 
general remarks: First: Let x be the dividend and y the divisor. We assume, of course, - 1 x < 1, - 1 5 y < 1. It will be found that our pres- ent process of division is entirely unaffected by the signs of x and y, hence no further restrictions on that score are required. On the other hand, the quotient 
z = x/y must also 
fulfil - 1 5 z < 1. It seems somewhat simpler although this is by no means necessary, 
to exclude for 
the purposes of this discussion 
z = - 1, and to demand I z I < 1. This means 
in terms of the dividend x and the divisor y that we exclude x = - y and assume Second: The division takes 
place in n steps, which correspond to the n digits ti, . . . , .$; of the pseudo-quotient z', n being yet to be determined (presumably 39 or 40). Assume that the 
k - 1 first steps (k = 1, . . . 
, n) have already taken 
place, having produced 
the k - 1 first digits: ti, . . . , ek-l; and that we are now at the kth step, involving production of the kth digit; &. Assume furthermore, that Ac now contains the quantity rk-l, the result of the k - 1 first steps. (This is the (k - 1)st partial remainder. For k = 1 clearly r,, = x.) We then form rk = 2rk-1 7 y, accord- ing to whether the signs of rk-l and y do or do not agree, i.e. 
rk = 2rk-,By is - if the signs of rk-l and y do agree ' [ is + if the signs of rk-, and y do not agree 
1x1 < Y. Let us now see 
what carries may originate 
in this procedure. We can argue as follows: lrhl < /yl is true for h = 0( Ir,l = I x I < I y I), and if it is true for h = k - 1, then (4) extends it to 
h = k also, since rk-l and 0 y have opposite 
signs. The last point may be elaborated a 
little further: 
because of the opposite signs Hence we have 
always I rk 1 < I y I ,and therefore afortiori I rk I < 1, i.e. -1 < rk < 1. Consequently in equation (4) one summand is necessarily > -2, <2, the other is 21, <1, and the sum is >-1, <l. Hence we 
may carry out the operations of (4) modulo 2, disregarding any possibilities of carries beyond the 2O position, and the resulting rk will be automatically 
correct (in the 
range >-1, <1). Third: Note however that the 
sign of rk-l, which plays an 
important role in (4) above, is only then correctly determinable from the sign digit, if the number from which it is derived is 2 - 1, <l. (Cf. the discussion in 5.7.) This requirement however 
is met, as we saw above, by 
rk-l, but not necessarily by 
2rkpI. Hence the sign of rk-l (Le. its sign digit) as required by 
(4), must be sensed before rk-l is doubled. This being understood, 
the doubling of rk-l may be performed 
as a simple left shift, in which the left-most digit (the sign digit) is allowed to be lost-this corresponds to the 
disregarding of carries beyond the 2O position, which 
we recognized above as being permissible in (4). (Cf. however, Part 
11, Table 2, for another sort of left shift that is desirable in 
explicit form, 
i.e. as an order.) Fourth: Consider now the precise implication of (4) above. 5; = 1 or 0 corresponds to LE = - or +, respectively. Hence (4) may be written rk = 2rk-1 + (1 - 2t;)y i.e. l)rk-l + (2-k - 2-(k-1)[!) 2-kr - 2-(k- ky k- Summing over k = 1, . . . , n gives i.e. This makes 
it clear, 
that Z = - 1 + + 2-" corre- sponds to true quotient z = x/y and 2-"rn, with an absolute value <2-" I y I 5 2-", to the remainder. Hence, if we disregard the term -1 for a moment 
<i&, . . . 
, <A, 1 are the n + 1 first digits of what may be used as a true quotient, the 
sign digit being part of this sequence. 
Chapter 4 1 Preliminary discussion 
of the logical design of an electronic computing instrument 
109 Fifth: If we do not wish to get involved in 
more complicated round-off procedures which exceed 
the immediate capacity 
of the only available adder Ac, then the above result 
suggests that we should put n + 1 = 40, n = 39. The ti, . . . , ti9 are then 
39 digits of the quotient, including the sign digit, but not including 
the right-most digit. The right-most digit is taken care of by placing a 1 into the 
right-most stage of Ac. At this point an additional argument in 
favor of the procedure that we have adopted here 
becomes apparent. The procedure coincides (without a need for any further 
corrections) with the second round-off procedure that we 
discussed in 5.12. There remains the term -1. Since this applies 
to the final result, and no right shifts are to follow, carries which might 
go beyond the 2O position may 
be disregarded. Hence this amounts simply to changing the sign digit of the quotient 3: replacing 0 or 1 by 1 or 0, respectively. This concludes our 
discussion of the division scheme. We 
wish, however, to re-emphasize two very distinctive features which 
it possesses: First: This division scheme applies 
equally for any combina- tions of signs of divisor and dividend. This 
is a characteristic of the non-restoring division schemes, but it is not the case for any simple known multiplication scheme. 
It will be remembered, in 
particular, that our multiplication 
procedure of 5.9 had to contain special correcting steps for the cases where either 
or both factors are negative. Second: This division scheme is practicable in the binary sys- tem only; it has no analog for any other base. This method of binary division will 
be illustrated on 
some examples in 5.15. 5.15. We give below some illustrative examples of the opera- tions of binary arithmetic which were discussed in the preceding sections. Although it presented no difficulties or ambiguities, it seems best to begin with an example of addition. Binary notation Decimal notation (fractional form) 
Augend 0010110011 179/5 12 Addend 0011010111 215/512 Sum 0110001010 394/512 (Carries) 1111 111 
In what follows we will not show 
the carnes 
any more. We form the negative of a number (cf. 5 7). Complement: Binary notation 0.101 110100 1.010001011 1 A subtraction (cf. 
5.7): 1.01000 1 100 Binary notation Subtrahend 0011010111 Minuend 0110001010 Decimal notation (fractional form) 372/512 -1 +140/512 Decimal notation (fractional form) 
21 5/51 2 394/5 12 Complement of subtrahend 1.1001 01000 I -1 +297/512 Difference 0.0101 1001 
1 179/512 
110 Pari 2 I The instruction-set processor: main-line computers 
Some multiplications (cf. 5.8 and 5.9): Binary notation 
Mu It i pl icand Multiplier 0.011 0.101 Section 1 I Processors with one address per instruction 
Decimal notation (fractional 
form) 5/8 3/8 0101 0101 0 Product 0001111 Binary notation 
Mu It i pl icand 1.101 Multiplier 1011 0101 0101 1 101111 Correction lt 11 1.1 10111 
Correction 2$ (Complement of the multiplicand). 0.010 1 0.001 11 1 A division (cf. 5.14): Binary notation Divisor 1011000 Dividend 0001111 0011110 1011000 1.1 101 10 1.101 100 0.100111 1 0.010100 0.101000 1.01 1000 0.000000 0.000000 1.011000 1.011000 0.110000 0.1001 11 1 1.01 1000 Quotient (uncorrected) 0 10011 ﬂ (corrected) 1100111 Q.D.3 0 1 ~ 15/64 Decimal notation (fractional form) - 3/8 - 5/8 Decimal notation (fractional 
form) 15/64 - 5/8 1 -1 + 39/64 = -25/64 t For the sign of the multiplicand $ For the sign of the multiplier. 5 Quotient digit 
Chapter 4 I Preliminary discussion 
of the logical design of an electronic computing instrument 
111 Note that this deviates by YG4, i.e. by 
one unit of the right-most position, from 
the correct result -"/. This is a consequence of our round-off rule, which forces 
the right-most digit to 
be 1 under all conditions. This occasionally produces results 
with unfamiliar and even annoying aspects 
(e.g. when quotients 
like 0:y 
or y:y are formed), but it is nevertheless unobjectionable and 
self- consistent on the basis of our general 
principles. 6. The control 6.1. It has already been 
stated that the 
computer will contain an organ, called the control, which can automatically execute 
the orders stored 
in the Selectrons. Actually, for 
a reason stated in 6.3, the orders for this computer are 
less than half as long as 
a forty binary digit number, and 
hence the 
orders are stored in the Selectron memory in pairs. Let us consider the routine that the 
control performs in direct- 
ing a computation. 
The control must know the location in the Selectron memory of the pair 
of orders to be 
executed. It must direct the Selectrons to transmit this pair of orders to the Selectron register and then 
to itself. It must then direct 
the execution of the operation specified in the first of the two orders. Among these orders we can immediately describe 
two major types: 
An order of the first type begins by causing 
the transfer of the number, which is stored at a specified memory location, 
from the Selectrons to the 
Selectron register. Next, 
it causes the arithmetical unit 
to perform some 
arithmetical operations on this number (usually in conjunction with 
another number which is already in the arith- metical unit), and to retain 
the resulting number in the arith- metical unit. The 
second type order causes the transfer of the number, which is held in the arithmetical unit, into the 
Selectron register, and from there to a specified memory location in 
the Selectrons. (It may also be that 
this latter operation will permit a direct transfer from 
the arithmetical unit into 
the Selectrons.) An additional type of order consists of the transfer orders 
of 3.5. Further orders control the inputs and 
the outputs 
of the machine. The process described at the 
beginning of this paragraph must then be repeated with the second order of the order pair. This entire routine is repeated until 
the end of the problem. It is clear from what has just 
been stated that the 
control must have 
a means of switching to a specified location 
in the Selectron memory, for 
withdrawing both numbers for the compu- tation and pairs of orders. Since the Selectron memory (as tenta- tively planned) will hold 
212 = 4,096 forty-digit words 
(a word is either a number 
or a pair of orders), a twelve-digit 
binary number 
suffices to identify 
a memory location. 
Hence a switching mecha- 
6.2. nism is required which will, 
on receiving a twelve-digit binary number, select the corresponding memory location. 
The type of circuit we propose to use for this purpose 
is known as a decoding 
or many-one function table. It has been developed 
in various forms 
independently by 
J. Rajchman [Rajchman, 
19431 and P. Crawford [Crawford, 
Is??]. It consists of n flip-flops which register an n-digit binary number. 
It also has 
a maximum of 2n output wires. The flip-flops activate a matrix in which the inter- 
connections between input 
and output wires are made in such a way that one and 
only one of 2" output wires is selected (Le. has a positive voltage 
applied to it). 
These interconnections 
may be established by means 
of resistors or 
by means of non-linear ele- ments (such as diodes or rectifiers); all these various methods 
are under investigation. The Selectron is so designed that four such 
function table switches are required, each with a 
three digit entry and eight (23) outputs. Four sets of eight wires each are brought out of the Selectron for switching purposes, and a 
particular loca- tion is selected by making one wire positive 
with respect 
to the remainder. Since all 
forty Selectrons are switched in parallel, 
these four sets 
of wires may be connected directly to the four function table outputs. Since most 
computer operations involve at least one number located 
in the Selectron memory, it is reasonable to adopt a code 
in which twelve binary 
digits of every order are assigned to the 
specification of a Selectron 
location. In those orders which 
do not require a number 
to be taken out 
of or into the Selectrons these digit positions will 
not be used. Though it has not been 
definitely decided how many 
operations will be built into 
the computer 
(Le. how many different orders the control must be able to understand), it will be seen presently that there will probably be more than Z5 but certainly less than 26. For this reason 
it is feasible to assign 6 binary digits for the order code. 
It thus turns out that each order 
must contain eighteen 
binary digits, the first twelve identifying a memory location 
and the remaining six specifying an 
operation. It can 
now be explained why orders are stored in 
the memory in 
pairs. Since 
the same memory organ is to be 
used in this 
computer for both orders and numbers, it is efficient to make the length of each about 
equivalent. But numbers of eighteen binary 
digits would 
not be sufficiently accurate for problems which this 
machine will solve. 
Rather, an 
accuracy of at least or 
2F3 is required. Hence it is preferable to make the numbers long 
enough to accommodate two orders. As we pointed out 
in 2.3, and used in 4.2 et seq. 
and 5.7 et seq., our numbers will 
actually have 40 binary digits each. This allows 20 binary digits for each order, i.e. the 12 digits that specify a memory location, 
and 8 more digits specifying the nature of the 6.3. 
112 Part 2 I The instruction-set processor: main-line computers Section 1 I Processors with 
one address per instruction operation (instead 
of the minimum of 6 referred to above). It is convenient, as will be seen in 
6.8.2. and Chapter 
9, Part 11, to group these binary digits 
into tetrads, groups of 4 binary digits. Hence a 
whole word consists 
of 10 tetrads, a 
half word or order of 5 tetrads, and of these 3 specify a memory location 
and the remaining 2 specify the nature 
of the operation. Outside the machine each tetrad can be expressed by 
a base 16 digit. (The base 16 digits are best designated 
by symbols 
of the 10 decimal digits 0 to 9, and 6 additional symbols, e.g. the letters a to f. Cf. 
Chapter 9, Part 11.) These 16 characters should appear in the typing for and the printing from the machine. (For further details 
of these arrangements, cf. Zoc. cit. above.) The specification of the nature 
of the operation that is involved in an order 
occurs in binary form, 
so that another many-one or 
decoding function 
is required to decode the order. This 
function table will have six input flip-flops (the two remaining digits 
of the order are not needed). Since there will not be 64 different orders, 
not all 64 outputs need 
be provided. However, 
it is perhaps worthwhile to connect the outputs corresponding to unused order possibilities to a checking circuit which will give 
an indication 
whenever a code word unintelligible 
to the control is received in the input flip-flops. The function table 
just described energizes 
a different output wire for each different code 
operation. As will be shown later, many of the steps involved in executing different orders overlap. 
(For example, addition, multiplication, 
division, and going from the Selectrons to the register all include transferring a number from the Selectrons to the Selectron register.) For this reason it is perhaps desirable to have an additional set 
of control wires, each of which is activated by any 
particular combination of different code digits. These may be obtained by taking 
the output 
wires of the many-one function table 
and using them 
to operate 
tubes which will in turn operate 
a one-many (or coding) function table. 
Such a function 
table consists of a matrix as before, but in this case only one of the input 
wires are activated. This particular table 
may be referred to as the recoding function table. 
The twelve flip-flops operating the four function tables used 
in selecting 
a Selectron position, 
and the six flip-flops operating the function table 
used for decoding the order, are referred to as the Function Table Register, FR. Let us consider next 
the process of transferring a pair 
of orders from the Selectrons to the control. These orders 
first go into SR. The order 
which is to be used next may be transferred directly into FR. The second order of the pair must 
be removed from SR (since SR may be used when 
the first order is executed), but cannot as yet be placed in FR. Hence a temporary 
storage 6.4. is provided for it. The storage means 
is called the Control Register, 
CR, and consists of 20 (or possibly 18) flip-flops, capable of re- ceiving a number from SR and transmitting a 
number to 
FR. As already stated (til), the control must know 
the location of the pair of orders it is to get 
from the Selectron memory. Normally 
this location will be the one following 
the location of the two orders just executed. That is, until it receives an 
order to do otherwise, the control will take its orders from the Selectrons in 
sequence. Hence 
the order location may be remembered in a twelve stage binary 
counter (one capable 
of counting 212) to which one unit is added whenever a pair 
of orders is executed. This counter is called the Control Counter, CC. The details of the process of obtaining a pair 
of orders from 
the Selectron are thus as follows: 
The contents 
of CC are copied into FR, the proper Selectron location 
is selected, and the contents of the Selectrons are transferred to SR. FR is then cleared, and 
the contents of SR 
are transferred to it 
and CR. CC is advanced by one 
unit so the control will be 
prepared to select the next pair of orders from 
the memory. (There is, however, an exception from this last 
rule for the so-called transfer orders, 
cf. 3.5. This may 
feed CC in a different manner, 
cf. the next paragraph below.) First 
the order in FR is executed and then the order in CR is transferred to FR and executed. It should be noted that all these operations 
are directed by the control itself-not only 
the operations specified in the control words sent 
to FR, but also the automatic operations required to get 
the correct orders there. Since the method by means 
of which the control takes order pairs in sequence 
from the memory has 
been described, it only remains to consider how the control shifts itself from one 
sequence of control orders 
to another in accordance with 
the operations described in 
3.5. The execution of these operations 
is relatively simple. An order calling for one of these operations contains 
the twelve digit specification 
of the position to which the control is to be switched, and 
these digits will 
appear in the left-hand twelve 
flip-flops of FR. All that is required to shift the control is to transfer the contents of these flip-flops to CC. When the control goes to the Selectrons for the next pair of orders it will then go to the location specified 
by the number so transferred. In the case of the unconditional transfer, 
the transfer is made automatically; in the case of the conditional transfer it 
is made only 
if the sign counter of the Accumulator registers zero. 
In this report 
we will discuss 
only the general method by means of which the control will execute specific orders, leaving 
the details until 
later. It has already 
been explained (5.5) that when a circuit 
is to be designed to accomplish a particular elementary 
operation (such as addition), a 
choice must 
be made between a 
6.5. 
Chapter 4 I Preliminary discussion 
of the logical design 
of an electronic computing instrument 
113 static type and a dynamic 
type circuit. 
When the design of the control is considered, this same choice arises. The function of the control is to direct a sequence 
of operations which take place in 
the various circuits of the computer (including the circuits of the control itself). 
Consider what is involved in directing an operation. The control must signal for 
the operation to begin, it must supply whatever signals are required to specify that particular operation, and it 
must in some way know when 
the operation has been completed so that it may start the 
succeeding operation. 
Hence the control circuits 
must be capable of timing the operations. It should be noted that timing is required whether the 
circuit per- 
forming the operation is static or dynamic. In the case of a static type circuit 
the control must supply static control signals for 
a period of time sufficient to allow the output 
voltages to reach 
the steady-state condition. 
In the case of a dynamic 
type circuit the control must send various pulses 
at proper intervals 
to this circuit. If all circuits of a computer are static 
in character, the control timing circuits 
may likewise be static, and no pulses are needed 
in the system. However, though some 
of the circuits of the com- puter we are planning 
will be static, they will probably not 
all be so, and hence 
pulses as well 
as static signals must 
be supplied 
by the control to the rest of the computer. There are many advan- tages in deriving these 
pulses from 
a central source, called 
the clock. The timing may then be done either by means 
of counters counting clock pulses or 
by means of electrical delay 
lines (an RC circuit is here regarded as a simple delay line). 
Since the timing of the entire computer 
is governed by a 
single pulse 
source, the computer circuits will be said to operate as a synchronized 
system. The clock plays 
an important 
role both in detecting and in 
localizing the errors made by 
the computer. One method 
of check- ing which 
is under consideration 
is that of having two identical 
computers which operate in parallel and automatically compare 
each other™s results. Both machines would 
be controlled by the same clock, 
so they would operate in absolute 
synchronism. It is not necessary to compare 
every flip-flop of one machine with 
the corresponding flip-flop of the other. Since all numbers and control 
words pass through either the 
Selectron register or 
the accumu- lator soon before or soon 
after they are used, it suffices to check the flip-flops of the Selectron register and the flip-flops of the accumulator which hold 
the number registered there; in fact, it seems possible 
to check 
the accumulator only (cf. the end of 6.6.2). The checking circuit would stop the 
clock whenever a 
difference appeared, or stop the machine in a 
more direct manner if an asynchronous system 
is used. Every flip-flop of each computer will be located at a convenient place. 
In fact, all neons will 
be located on one panel, 
the corresponding neons of the two machines being placed in parallel 
rows so that one can tell 
at a glance (after the machine has been stopped) where the discrepancies are. 
The merits of any checking 
system must 
be weighed against its cost. Building two machines may 
appear to be expensive, but since most 
of the cost of a scientific computer lies in development 
rather than 
production, this consideration is not so important as it might seem. 
Experience may show 
that for most problems 
the two machines need not be 
operated in parallel. 
Indeed, in most cases purely mathematical, external 
checks are possible: Smooth- 
ness of the results, behavior of differences of various types, validity 
of suitable identities, 
redundant calculations, etc. All of these methods are usually adequate to disclose the presence or absence of error in toto; their drawback is only that they may not allow the detailed 
diagnosing and locating 
of errors at all or with ease. When a problem is run for the first time, so that it requires special 
care, or when an error is known to be present, and has to be located-only then will it be 
necessary as 
a rule, to 
use both machines in parallel. Thus they can 
be used as separate machines most of the time. The essential feature of such a method 
of check- ing lies in the fact that it checks the computation at every point (and hence detects 
transient errors 
as well as 
steady-state ones) and stops the machine when the error occurs so that the process of localizing the fault is greatly simplified. These 
advantages are 
only partially gained by duplicating 
the arithmetic part 
of the computer, or by following one operation with 
the complement operation (multiplication 
by division, 
etc.), since this fails 
to check either the 
memory or 
the control (which 
is the most complicated, though not 
the largest, part of the machine). The method of localizing errors, 
either with or without a dupli- 
cate machine, needs further discussion. It is planned to 
design all the circuits (including 
those of the control) of the computer so that if the clock is stopped between 
pulses the computer will retain all its 
information in flip-flops so that the 
computation may proceed unaltered when 
the clock is started again. This principle has already demonstrated its 
usefulness in the ENIAC. This makes 
it possible for 
the machine to compute with the clock operating at any speed 
below a certain maximum, as long 
as the clock gives out pulses of constant shape 
regardless of the spacing between 
pulses. In particular, the spacing between 
pulses may 
be made indefinitely large. The clock will 
be provided with a 
mode of operation in 
which it will emit a single pulse 
whenever instructed 
to do so by the operator. 13y means of this, the operator can 
cause the machine to go through an operation step by step, checking the results by means of the indicating-lamps connected to the 
flip-flops. It will be noted that this design principle does not exclude the use of delay lines 
to obtain delays as 
long as these 
114 Part 2 I The instruction-set processor: 
main-line computers Section 1 I Processors with one 
address per 
instruction are only used to time the 
constituent operations 
of a single step, and have no part in determining the machine™s operating repeti- 
tion rate. Timing coincidences by 
means of delay lines 
is excluded since this requires a constant pulse 
rate. The orders which the control understands 
may be divided 
into two 
groups: Those 
that specify operations which are per- formed within 
the computer and those that specify operations involved in getting data into and 
out of the computer. At the present time the internal operations are more completely planned 
than the input and output operations, and 
hence they will be discussed more 
in detail than the latter 
(which are treated 
briefly in 6.8). The internal 
operations which have been tentatively 
adopted are 
listed in 
Table 1. It has already been 
pointed out that not all of these operations 
are logically basic, but that 
many can be programmed by 
means of others. In the case of some of these operations the reasons for 
building them into 
the control have 
already been 
given. In this section we will give reasons for 
building the other operations into the control and 
will explain 
in the case of each operation 
what the control must do 
in order to 
exe- cute it. In order to have the precise mathematical meaning of the symbols which 
are introduced in what follows clearly in mind, the reader should consult 
the table at the end of the report 
for each new symbol, 
in addition 
to the 
explanations given 
in the text. Throughout what 
follows S(x) will denote the 
memory location No. x in the Selectron. Accordingly the x which appears in S(x) is a 12-digit binary, in the sense of 6.2. The eight addition 
operations [S(x)+ Ac+, S(x)+ Ac--, S(x)+ Ah+, S(x)-t Ah-, involves the following possible four 
steps: 6.6. 6.6.1. S(X)+ Ac + M, S(X)-+ Ac - M, S(X) + Ah + M, S(X)+ Ah - MI First: Clear SR and transfer into it the number at S(x). Second: Clear Ac if the order contains 
the symbol c; do not clear Ac if the order contains 
the symbol h. Third: Add 
the number in SR or its negative 
(Le. in 
our present system its complement with respect to 
2l) into Ac. If the order does not contain 
the symbol M, use the number in 
SR or its negative according to whether the order contains 
the symbol + or - . If the order contains 
the symbol M, use the number in 
SR or its negative according to 
whether the 
sign of the number in 
SR and the symbol + or - in the order do or do not agree. 
Fourth: Perform a complete carry. 
Building the last four addi- tion operations (those containing 
the symbol M) into the 
control is fairly simple: It calls only for 
one extra 
comparison (of the sign in SR and the + or - in the order, cf. the third step above), and it requires, therefore, 
only a few tubes more than required for the first four addition operations (those not containing 
the symbol M). These facts would seem 
of themselves to justify adding the opera- tions in question: plus 
and minus the absolute value. But it should be noted that these operations 
can be programmed out of the other 
operations of Table 1 with correspondingly few orders 
(three for absolute value and five for minus 
absolute value), 
so that some further justification for 
building them in 
is required. The absolute value order is frequently in connection with 
the orders L and R (see 6.6.7), while the minus absolute value 
order makes the detec- tion of a zero very simple 
by merely 
detecting the sign of - J NJ . (If - JNI 2 0, then N = 0.) The operation of S(x) .+ R involves the following two steps: 6.6.2. First: Clear SR, and transfer S(x) to it. Second: Clear AR and add the number in 
the Selectron register 
into it. The operation of R + Ac merits more 
detailed discussion, since there are alternative 
ways of removing numbers from 
AR. Such numbers could 
be taken directly to 
the Selectrons as well as into Ac, and they 
could be transferred 
to Ac in parallel, 
in sequence, or in sequence parallel. 
It should be recalled 
that while most of the numbers that go into AR have come from 
the Selec- trons and thus need not 
be returned to them, the result of a division and the right-hand 39 digits of a product 
appear in AR. Hence while 
an operation for withdrawing a number 
from AR is required, it 
is relatively infrequent and therefore need not be 
particularly fast. We 
are therefore considering the possibility of transferring at least partially 
in sequence and of using the shifting properties of Ac and of AR 
for this. Transferring the number to the Selectron via the accumulator is also desirable if the dual machine method 
of checking is employed, for it means that even if numbers are only checked in their transit through 
the accumu- lator, nevertheless every number going into the 
Selectron is checked before being placed 
there. 6.6.3. The operation S(x) x R --f Ac involves the following six steps: First: Clear SR and transfer S(x) (the multiplicand) into 
it. Second: Thirty-nine steps, each 
of which consist 
of the two following parts: (a) Add (or rather shift) the sign digit of SR into the partial product 
in Ac, or add all but the sign digit of SR 
into the partial product in 
Ac-depending upon whether 
the right-most digit in 
AR is 0 or 1-and effect the appropriate carries. (b) Shift Ac and AR to the right, fill the sign digit of Ac with a 0 and the 
digit of AR immediately right of the sign digit (positional 
value 2-l) with the previously right-most 
digit of Ac. (There are 
ways to save time by 
merging these 
two operations when 
the right-most digit in 
Ar is 0, but we will 
not discuss them here 
more fully.) Third: If the sign digit in SR is 1 (Le. -), then inject a carry 

Chapter 4 I Preliminary discussion 
of the logical design 
of an electronic computing instrument 
115 into the 
right-most stage of Ac and place a 
1 into the sign digit of Ac. Fourth: If the original sign 
digit of AR is 1 (Le. -), then sub- tract the 
contents of SR from Ac. Fifth: If a partial carry system was 
employed in 
the main process, then a complete carry is necessary at the end. Sixth: The appropriate round-off must be effected. (Cf. 
Chapter 9, Part 11, for details, where 
it is also explained how 
the sign digit of the Arithmetic register is treated as part of the round-off process.) It will be noted that since any number held in 
Ac at the begin- ning of the process is gradually shifted into 
AR, it is impossible to accumulate sums of products in 
Ac without storing the various products temporarily in 
the Selectrons. While this is undoubtedly a disadvantage, 
it cannot be eliminated without constructing 
a.n extra register, 
and this does not at this moment seem worthwhile. On the other hand, 
saving the right-hand 39 digits of the answer is accomplished with 
very little extra equipment, since it 
means connecting the 2-39 stage of Ac to the 2-1 stage of AR 
during the shift operation. The advantage of saving these 
digits is that it simplifies the handling 
of numbers of any number 
of digits in the computer (cf. the last part of 5.12). Any number of 39k binary digits (where 
k is an integer) and sign can be divided 
into k parts, each part 
being placed in a 
separate Selectron position. Addition 
and subtraction of such numbers may be programmed out of a series of additions or subtractions of the 39-digit parts, the 
carry- over being programmed by means 
of Cc+ S(x) and Cc'+ S(x) operations. (If the 2" stage of Ac registers negative after the 
addi- tion of two 39-digit parts, a 
carry-over has taken place 
and hence 
2-39 must be added to the sum of the next parts.) A similar proce- dure may be followed in multiplication 
if all 78 digits of the product of the two 39-digit parts are kept, 
as is planned. (For the details, cf. 
Chapter 9, Part 11.) Since it would greatly complicate 
the computer to make provision for 
holding and 
using a 78 digit dividend, it is planned to program 39k digit division in one 
of the ways described at the end of 5.12. The operation of division Ac i S(x) + R involves the following four steps: 
6.6.4. First: Clear SR and transfer S(x) (the divisor) into it. Second: Clear AR. Third: Thirty-nine steps, each 
of which consists 
of the following three parts: (a) Sense the signs of the contents 
of Ac (the partial remainder) and 
of SR, and sense whether they agree 
or not. (b) Shift Ac and AR left. In 
this process the previous sign 
digit of Ac is lost. Fill 
the right-most digit of Ac (after the shift) with a 
0, and the 
right-most digit of AR (before the shift) with 0 or 1, depending on whether there was disagreement or agreement in 
(a). (c) Add or subtract the contents of SR into Ac, depending on the same alternative as above. 
Fourth: Fill the right-most digit of AR 
with a 
1, and change 
its sign digit. For the purpose of timing the 39 steps 
involved in division a six-stage counter (capable 
of counting to 26 = 64) will 
be built into the control. This same 
counter will also 
be used for 
timing the 39 steps of multiplication, and possibly for 
controlling Ac when a number 
is being transferred 
between it and a 
tape in either direction (see 6.8.). 
The three 
substitution operations 
[At -+ S(x), Ap -+ S(x), and Ap' + S(x)] involve transferring all or part of the number held in Ac into the Selectrons. This will 
be done by 
means of gate tubes connected to the registering flip-flops of Ac. Forty such tubes are needed for the total substitutions, At + S(x). The partial substitu- tion Ap -+ S(x) and Ap' -+ S(x) requires that the left-hand twelve 
digits of the number held in Ac be substituted in 
the proper places in the left-hand and right-hand 
orders, respectively. This may 
be done by 
means of extra gate tubes, or by 
shifting the number in 
Ac and using the gate tubes required 
for At -+ S(x). (This scheme 
needs some 
additional elaboration, 
when the order directing and 
the order suffering the substitution are the two successive halves 
of the same word; i.e. when 
the latter 
is already in 
FR at the time 
when the former becomes operative in CR, 
so that the 
substitution effected in the Selectrons comes 
too late to 
alter the order which has already reached CR, to become operative 
at the next step in FR. There are various ways to take care of this complication, either by some 
additional equipment or by appropriate prescriptions in coding. We will not discuss them here in more detail, since the decisions in this respect are still open.) 
The importance 
of the partial substitution operations 
can hardly be overestimated. It has already been pointed out 
(3.3) that they allow the computer 
to perform operations it 
could not other- wise conveniently perform, such as making 
use of a function 
table stored in 
the Selectron memory. 
Furthermore, these operations remove a very sizeable burden from the person coding problems, 
for they make possible 
the coding of classes of problems in contrast to coding each individual problem separately. 
Because Ap -+ S(x) and Ap' + S(x) are available, any 
program sequence may be stated in general form (that is, without Selectron location designations 
for the numbers being 
operated on) and the 
Selectron locations of the numbers to be 
operated on substituted whenever 
that se- quence is used. As an example, consider 
a general code 
for nth order integration 
of m total differential equations 
for p steps 
of independent variable t, formulated in advance. Whenever a 
prob- 6.6.5. 
116 Part 2 I The instruction-set 
processor: main-line computers 
Section 1 I Processors with one address per instruction 
lem requiring this rule 
is coded for the computer, the 
general integration sequence can 
be inserted into the statement 
of the problem along with coded instructions 
for telling the sequence where it 
will be located in 
the memory [so that the proper S(x) designations will 
be inserted into such orders 
as Cu + S(x), etc.]. Whenever this sequence is to be 
used by the computer it will automatically substitute the correct values of m, n, p and At, as well as the locations of the boundary conditions and 
the descrip- tions of the differential equations, into 
the general sequence. (For 
the details of this particular procedure, cf. Chapter 13, Part 11.) A library of such general sequences 
will be built up, and 
facilities provided for convenient insertion 
of any of these into the coded statement of a problem (cf. 6.8.4). When such a scheme is used, only the distinctive features 
of a problem need be coded. The manner in 
which the control shift operations [Cu + S(x), Cu' + S(x), Cc -+ S(x), and Cc' + S(x)] are realized 
has been discussed in 6.4 and needs no 
further comment. One basic question which must 
be decided before a computer is built is whether the machine is to have a 
so-called floating binary (or decimal) point. 
While a floating binary point 
is undoubtedly very convenient in coding 
problems, building it into the computer adds 
greatly to its complexity and hence a choice in 
this matter should receive very careful attention. How- ever, it should first be noted that the alternatives ordinarily 
con- sidered (building a machine 
with a floating binary point 
vs. doing all computation with a 
fixed binary point) 
are not exhaustive and hence that the arguments generally advanced 
for the floating binary point 
are only of limited validity. Such 
arguments overlook the fact 
that the choice with respect to 
any particular 
operation (except for certain basic ones) 
is not between building it into the 
computer and not 
using it at all, but rather between 
building it 
into the computer and programming it out 
of operations built into 
the computer. (One short reference to 
the floating binary point 
was made in 
5.13.) Building a floating binary point 
into the 
computer will not only complicate the control but will also 
increase the length of a num- ber and hence increase the size of the memory and the arithmetic unit. Every number 
is effectively increased in size, even though 
the floating binary point 
is not needed in 
many instances. 
Further- more, there is considerable redundancy 
in a floating binary point 
type of notation, for each number carries with it a 
scale factor, while generally speaking 
a single scale 
factor will suffice for a possibly extensive 
set of numbers. By means of the operations already described in 
the report a floating binary point 
can be programmed. While additional 
memory capacity is needed for this, it is probably less than that required by a built-in 
floating binary 
6.6.6. 6.6.7. point since a different scale 
factor does not need 
to be 
remembered for each number. 
To program a floating binary point 
involves detecting where the first zero occurs 
in a number in 
Ac. Since Ac has shifting facilities this can best be done by 
means of them. In 
terms of the operations previously described this 
would require taking 
the given number out of Ac and performing a suitable 
arithmetical operation on it: For a (multiple) 
right shift a multiplication, 
for a (multiple) left shift either one 
division, or as many doublings 
(Le. additions) as the shift has stages. However, 
these operations 
are inconvenient and time-consuming, so we propose 
to introduce two 
operations (L and R) in order that this (i.e. 
the single left and right shift) can be accomplished directly. These operations make use of facili- ties already present in 
Ac and hence add very little equipment 
to the 
computer. It should be noted that in many instances 
a single use of L and possibly of R will suffice in programming a 
floating binary point. For 
if the two factors in a multiplication have 
no superfluous zeros, 
the product will have at most one superfluous zero (if '/z Y < 1, then y4 5 XY < 1). This is similarly true in division (if '/4 5 X < y2 and y2 _I Y < 1, then y4 < X/Y < 1). in addition and subtraction 
any numbers 
growing out of range can be treated similarly. Numbers which 
decrease in these cases, 
i.e. develop a sequence 
of zeros at the beginning, are really (mathematically) losing precision. 
Hence it is perfectly proper to omit formal 
readjustments in this event. 
(indeed, such a true loss of precision cannot be obviated by any formal proce- dure, but, 
if at all, only by 
a different mathematical formulation 
of the problem.) Table 1 shows that many of the operations which the control is to execute have 
common elements. Thus addition, 
sub- traction, multiplication and 
division all involve transferring a 
number from the Selectrons to SR. Hence the control may be simplified by breaking 
some of the operations up into more basic 
ones. A timing circuit 
will be provided for 
each basic operation, and one or more such 
circuits will be involved in the execution of an order. The exact choice of basic Operations will 
depend upon how the arithmetic unit is built. In addition to the timing circuits needed 
for executing the orders of Table 1, two such circuits are needed for the automatic 
operations of transferring orders from 
the Selectron register to CR and FR, and for transferring an order from CR to FR. In normal computer operation these two circuits are 
used alternately, so a binary counter is needed to remember which is to be 
used next. 
in the operations Cu' -+ S(x) and Cc + S(x) the first order of a pair 
is ignored, so the binary counter must be altered accordingly. The execution of a sequence 
of orders involves using 
the various X < 1 and '/z 6.7. 
Chapter 4 1 Preliminary discussion of the logical 
design of an electronic computing instrument 
117 Table 1 Symbolization Complete Abbreviated Operation 
1 2 3 4 5 6 7 9 10 11 a 12 13 14 15 16 17 la 19 20 21 S(x) 4 Ac+ S(x) 4 Ac- S(x) + AcM S(x) 4 Ac - M S(x) 4 Ah+ S(x) + Ah- S(x) 4 AhM S(x)-t Ah - M S(x) 4 R R+A S(x) x R + A A i S(x) + R cu + S(X) Cu™ + S(X) cc + S(x) CC™ + S(X) At -+ S(x) Ap + S(x) Ap™ + S(x) L R X X- xM x-M xh xh - xhM x - hM xR A xx Xi xc XC XCC XCC‚ XS XSP xSp™ L R Clear accumulator and add number located at position 
x in the Selectrons into it. 
Clear accumulator and subtract number 
located at position x in the Selectrons into it. Clear accumulator and add absolute 
value of number located at 
position x in the Selectrons Clear accumulator and 
subtract absolute value of number located at position x in the Selec- Add number located at position 
x in the Selectrons into the 
accumulator. Subtract number 
located at position x in the Selectrons into the accumulator. Add absolute value of number located at position x in the Selectrons into the accumulator. Subtract absolute 
value of number located at 
position x in the Selectrons into the 
accumulator. Clear register? and add number located at position 
x in the Selectrons into it. Clear accumulator and 
shift number 
held in register into it. 
Clear accumulator and multiply the number 
located at position 
x in the Selectrons by the num- ber in the register, placing the left-hand 
39 digits of the answer in the accumulator and 
the right-hand 39 digits of the answer in the register. Clear register and divide the number 
in the accumulator by the number 
located in position x of the Selectrons, leaving the remainder in the accumulator and placing 
the quotient in the register. Shift the control to the left-hand 
order of the order 
pair located 
at position 
x in the Selectrons. Shift the control to 
the right-hand 
order of the order pair 
located at position x in the Selectrons. If the number in the accumulator is 2 0, shift the control 
as in Cu 4 S(x). If the number in the accumulator is 2 0, shift the control as in Cu™ 4 S(x). Transfer the number 
in the accumulator to position x in the Selectrons. Replace the left-hand 12 digits of the left-hand 
order located 
at position x in the Selectrons by Replace the left-hand 12 digits 
of the right-hand 
order located at position 
x in the Selectrons Multiply the 
number in the accumulator by 2, leaving 
it there. Divide the number in the accumulator by 2, leaving it there. into it. trons into 
it. the left-hand 12 digits 
in the accumulator. by the left-hand 
12 digits in the 
accumulator. t Register means arithmetic register. 
timing circuits in sequence. When a given timing circuit has completed its operation, it emits a pulse which should go to the timing circuit to be used next. Since this depends upon the partic- ular operation being executed, these pulses 
are routed according to the signals received from the decoding and recoding function 
tables activated by the six binary digits specifying an order. In this section we will consider what must be added to the control so that it can direct 
the mechanisms for getting data 
into and out of the 
computer and also describe the mechanisms themselves. Three different kinds of input-output mechanisms are 
planned. First: Several 
magnetic wire storage units operated by servo- mechanisms controlled by the computer. 6.8. Second: Some viewing tubes for graphical portrayal of results. Third: A typewriter for feeding data 
directly into the com- puter, not to be 
confused with the equipment used for preparing 
and printing from magnetic wires. As presently planned the latter 
will consist of modified Teletypewriter 
equipment, cf. 6.8.2 and 6.8.4. Since there already 
exists a way of transferring numbers 
between the 
Selectrons and Ac, therefore Ac may be used for transferring numbers 
from and to a wire. The latter transfer will be done serially 
and will make use of the 
shifting facilities of Ac. Using Ac for this purpose eliminates the possibility of computing 
and reading from or writing on the wires simultaneously. However, simultaneous operation of the computer 
and the input-output 
6.8.1. 
118 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
organ requires additional 
temporary storage and introduces a syn- chronizing problem, 
and hence it is not being 
considered for the first model. Since, at the beginning of the problem, the computer is empty, facilities must be built into the control for reading a set of numbers from a 
wire when the operator presses a 
manual switch. As each number is read from a 
wire into Ac, the control must transfer it to its proper location in the Selectrons. The CC may be used to count off these positions in sequence, since 
it is capable of trans- mitting its contents to 
FR. A detection circuit 
on CC will stop the process when the specified number of numbers has been placed in the memory, and the control will then be shifted to the 
orders located in the first position of the Selectron memory. It has already 
been stated that the 
entire memory facilities of the wires should 
be available to the 
computer without human 
intervention. This 
means that the 
control must be able to select the proper set of numbers from 
those going by. 
Hence additional 
orders are required for the code. Here, 
as before, 
we are faced with two 
alternatives. We can 
make the control capable of exe- cuting an order 
of the form: Take 
numbers from positions 
p to p + s on wire No. k and place them in 
Selectron locations 
u to 0 + s. Or we can make the control capable of executing some less complicated operations which, 
together with 
the already given control orders, 
are sufficient for programming the transfer opera- 
tion of the first alternative. Since the latter scheme is simpler we adopt it tentatively. 
The computer must have some way of finding a 
particular number on a 
wire. One method 
of arranging for this is to have each number 
carry with it its own location designation. A method more economical of wire memory capacity is to use the Selectron memory facilities to remember the position of each wire. For example, the computer would hold the number t, specifying which number on the wire is in position 
to be read. If the control is instructed to read the number at position p, on this wire, 
it will compare p, with t,; and if they differ, cause the wire to 
move in the proper direction. As each number 
on the wire passes by, one unit 
is added or subtracted to t, and the comparison repeated. When p, = t, numbers will be transferred from the wire to the 
accumulator and then 
to the 
proper location in the memory. Then both t, and p, will be increased by 1, and the transfer from the wire to accumulator to memory repeated. This will be iterated, until t, + s and p, + s are reached, at which time the control will direct the wire to stop. Under this system the control must be able to execute the following orders with regard to each wire: Start 
the wire forward, start the wire in reverse, stop the wire, transfer 
from wire to Ac, and transfer from Ac to wire. In 
addition, the wire must signal the control as each digit is read and when the end 
of a number has been reached. Conversely, 
when recording is done the control must have a means of timing the signals sent from Ac to the wire, and of counting off the digits. The 26 counter used for multiplica- tion and division may be used for 
the latter purpose, but other timing circuits 
will be required for 
the former. If the method of checking by means 
of two computers operating 
simultaneously is adopted, and each machine is built so that it can operate independently of the other, then each will have a separate input-output 
mechanism. The process of making wires 
for the computer must then be 
duplicated, and in this way the work of the person making a wire can be checked. Since the wire servomechanisms cannot be synchronized by the central clock, a 
problem of synchronizing the two computers when 
the wires are being used arises. It is probably not practical 
to synchronize the wire feeds to within a given 
digit, but this is unnecessary since the numbers coming 
into the two organs Ac 
need not be 
checked as the individual digits arrive, 
but only prior to being deposited 
in the Selectron memory. Since the computer operates in the binary system, some 
means of decimal-binary and binary-decimal conversions is highly desirable. Various alternative ways of handling this problem have been considered. In general we recognize two broad classes of solutions to this problem. 
First: The conversion problems can be regarded as simple arith- metic processes and programmed as sub-routines out of the orders already incorporated in 
the machine. The details of these programs together with 
a more complete 
discussion are given fully 
in Chap- 
ter 9, Part 11, where it 
is shown, among other 
things, that the conversion of a word takes about 5 msec. Thus the conversion time is comparable to the reading or withdrawing time 
for a word- about 2 msec-and is trivial as compared to the solution time for problems to be handled by the computer. It should be noted that the treatment 
proposed there presupposes only 
that the decimal data presented to or received 
from the computer are in tetrads, each tetrad 
being the binary coding 
of a decimal digit-the infor- mation (precision) represented by a decimal digit being actually 
equivalent to 
that represented by 3.3 binary digits. The coding of decimal digits 
into tetrads of binary digits 
and the printing of decimal digits from 
such tetrads can be accomplished quite simply and automatically by slightly modified Teletype equipment, cf. 6.8.4 below. Second: The conversion problems can be regarded as unique problems and handled by separate conversion equipment incor- porated either in the computer proper 
or associated with the 6.8.2. 
Chapter 4 1 Preliminary discussion 
of the logical design 
of an electronic computing instrument 
119 mechanisms for 
preparing and printing from magnetic wires. Such 
converters are 
really nothing other than 
special purpose digital computers. They 
would seem 
to be justified only for those com- 
puters which are primarily intended for solving problems 
in which the computation time 
is small compared to the 
input-output time, to which class 
our computer does not belong. It is possible to use various types 
of cathode ray tubes, and in particular Selectrons for the viewing tubes, in which case 
programming the viewing operation is quite simple. The viewing Selectrons can be switched by the same function tables 
that switch the memory Selectrons. 
By means of the substitution operation 
Ap -+ S(x) and Ap' + S(x), six-digit numbers specifying the abscissa and ordinate 
of the point (six binary digits represent a 
precision of one part in 26 = 64, i.e. of about 1.5 per cent which seems 
reasonable in such a component) can 
be substituted in 
this order, which will specify 
that a particular one of the viewing Selectrons is to be activated. As was mentioned above, the mechanisms used for 
preparing and printing 
from wire for 
the first model, at least, will be modified Teletype equipment. 
We are quite fortunate 
in having 
secured the full cooperation of the Ordnance Development 
Divi- sion of the National Bureau of Standards in 
making these modifi- cations and in 
designing and building 
some associated 
equipment. By means of this modified Teletype equipment an 
operator first prepares a checked 
paper tape and then 
directs the equipment to transfer 
the information from 
the paper tape to the magnetic wire. Similarly 
a magnetic wire can 
transfer its contents to a paper 6.8.3. 6.8.4. tape which can be used to operate a teletypewriter. (Studies are being undertaken 
to design equipment that will eliminate the necessity for using 
paper tapes.) As was shown in 6.6.5, the statement of a new problem 
on a wire involves 
data unique to that 
problem interspersed with data found on previously prepared paper tapes 
or magnetic wires. The equipment discussed in the previous paragraph makes it possible for the operator to combine conveniently 
these data on to a 
single magnetic wire ready for insertion 
into the computer. It is frequently very convenient to introduce 
data into 
a com- putation without producing a 
new wire. 
Hence it is planned to 
build one simple 
typewriter as an integral part of the computer. By means of this typewriter the operator can 
stop the 
computation, type in a 
memory location (which 
will go to the 
FR), type in a 
number (which 
will go to Ac and then 
be placed in the first mentioned location), 
and start 
the computation again. There is one further order that the control needs to execute. There 
should be some means by which 
the computer 
can signal to the operator when a computation has been concluded, 
or when 
the computation has reached a previously determined point. Hence 
an order is needed which will 
tell the computer to stop and to flash a light 
or ring a bell. 
6.8.5. References BurkA62a, BurkA6Zb; Craw•'??; GoldHGSa, b, c, d; RajcJ43 
The DEC PDP-8 Introduction' The PDP-8 is a single-address, 12-bit-word 
computer of the second generation. It is designed for 
task environments with minimum 
arithmetic computing 
and small Mp requirements. 
For example, 
it can be used to control laboratory 
devices, such as gas 
chromoto- graphs or sampling oscilloscopes. Together 
with special 
T's, it is programmed to be a laboratory instrument, 
such as 
a pulse height analyzer or a spectrum analyzer. These applications are typical 
of the laboratory and 
process control requirements 
for which 
the machine was designed. As another example, it can 
serve as a message concentrator by controlling telephone 
lines to which typewriters and Teletypes are attached. 
The computer occasion- ally stands alone as 
a small-scale general-purpose computer. Most recently it was introduced as a small-scale general-purpose time- 
sharing system, based 
on work 
at Carnegie-Mellon University 
and DEC. It is used as a KT(disp1ay) when it has a P(disp1ay; '338); this C 
is discussed in Chap. 25. The PDP-8 has achieved a produc- 
tion status formerly 
reserved for 
ZBM computers; about 
5,000 have been constructed. 
PDP-8 differs from the character-oriented 8-bit computer in Chap. 10; it is not unlike the 16-bit computers, such as the IBM 1800 in Chap. 33. The PDP-8 is typical of several 12-bit 
computers: the early CDC-160 series (1960), CDC-6600 
Peripheral and Con- trol Processor (Chap. 39), the SDS-92, M.I.T. Lincoln Laboratory's 
Laboratory Instrument 
Computer LINC (1963), Washington 
Uni- versity's Programmed Console (1967), 
and the SCC 650 (1966). 
The PDP-5 (transistor, 1963), PDP-8 (l965), PDP-8/S (serial, 
1966) and PDP-8/1 
(integrated circuit, 
1968), PDP-R/L 
(integrated circuit, 1968) constitute a 
series of computers based on evolving technology. All of these have 
identical ISP's. Their PMS structures are nearly identical, and 
all components other than 
Pc and Mp 
are compatible throughout 
the series. The LINC-8-338 PMS struc- ture is presented in Fig. 
1. A cost performance tradeoff took 
place in the PDP-8 (parallel-by-word 
arithmetic) and PDP-8/S (serial- 
by-bit arithmetic) implementations. A PDP-S/S is one-fifteenth of a PDP-8 
at one-half the cost. The performance factors can be attributed to 8/1.5 or 5.3 for Mp speed and a factor 
of about 3 for logical 
organization, even though 
the same 2-megahertz logic clock is used in both cases. The PDP-8 is about 6.7 
times a PDP-5. 'The initials in the title 
stand for Digital Equipment Corporation 
Pro- grammed Data Processor. The ISP of the PDP-8 Pc 
is about the most trivial in 
the book. It has only a few data operators, namely, 
+-, +, - (negate), 7, A, / 2, x 2, (optional) x , /, and normalize. 
It operates on words, integers, and boolean vectors. 
However, there are 
microcoded instructions, which allow 
compound instructions 
to be formed in a single instruction. The computer is straightforward and illustrates 
the levels dis- cussed in Chap. 1. We can easily look at it from the "top down." The C in PMS notation is C('PDP-8; techno1ogy:transistors; 12 b/w; descendants:'PDP-8/S, 'PDP-8/1, 'PDP-8/L; antecedents: 'PDP-5; Mp(core; #0:7; 
4096 w; 
tc:1.5 p/w); Pc(Mps(2 - 4 w); instruction length:lI2 w address/instruction: 1; operations on data/od:(t, +, 7, A, -(negate), x 2, / 2, +1) optional operations:( x , /, normalize); data-types:word, integer, 
boolean vector; 
operations for data access:4); P(disp1ay; '338); P(c; 'LINC); 
S('I/O BUS; 1 Pc; 64 K); Ms(disk, 'DECtape, magnetic tape); T(paper tape, card, 
analog, cathode-ray tube)) ISP The ISP is presented in Appendix 1 of this chapter (including the optional Extended Arithmetic Element/EAE). 
The 212-word Mp is divided into 32 fixed-length pages 
of 128 words 
each. Address calculation is based on references to the first page, Page-0, or to the current 
page of the Program Counter/PC. The 
effective- address calculation procedure 
provides for 
both direct 
and indirect reference to 
either the current 
page or the first page. This 
scheme allows a 7-bit address 
to specify local 
page addresses. A 215-word Mp is available on the PDP-8, but addressing greater than 212 words is comparatively inefficient. In the extended range, two 
3-bit registers, the Program Field and Data 
Field Registers, select which of the eight 212-~ord blocks are being actively addressed as program and data. 
There is an array of eight registers, called the Auto-index registers, which resides 
in Page-0. 
This array (Auto,index[O: 11](0:7): = M[108:178](O:11)) possesses the useful property that whenever an indirect reference 
is made to 
it, a 1 is first added 120 
Chapter 5 I The DEC PDP-8 121 K- S- K(#0:63; Teletype: 110, 180 b/s)- (12,l parity) b/w T(#0:3; CRT: display: area: IO x IO inz)-) T(#0:3; light: pen)> T(#0:3; push buttons; console)+ 
T. conso I e Ms #0:1; LINCdape: addressable 
magnetic tape: 
- -= P (display: '338) T.consol e - Mp fJ'0;7) !- Sz-S-/c~S4 10 char/s; 8 b/char; 64 char)- I K-T paper tape; (reader; 
300 char/s)) (punch: - 100 char/s): 8 b/char 3 3 c "16b cha r/co I 3 "1 30 us/point: .01 1.005 in/point 3 K-T incremental point plot: 300 point/s; .01 + c in/point K-T(card; reader: 2001800 card/min) t K-T(card; punch: 100 card/min)+ line: printer; 
300 line/min: 120 col/line: + CRT: display: area: IO x IO in215 x 5 in2; + K- T(1iqht: pen)> K- T(Dataphone; I .2 4.8 kb/s)- ~(P~:~~)-~(analog; output; 0 - -10 volts)+ K-SS-L(#0:63; analog: input: 0 - -10 volts)+ 'Mp(core; 1.5 p/w; 4096 w: (12 + I)b) S ( ' Memory Bus) 3Pc(1 -2 w/instruction: data: w, i,bv: 12 b/w: M.prowssor statef2i -11) w: technolooy: transistors; 4S('l/0 Bus; from: Pc: to: 64 K) "KII - 4 instructions; M.buffer(l char-2 
w)) 2 antecedents: PDP-5: descendants; PDP-BS, PDP-81, PDF-L) 
Fig. 1. DEC LINC-8-338 PMS diagram. 
122 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
to its contents. (That is, there is a side effect 
to referencing.) Thus, 
address integers 
in the register can select the next member of a vector or string for accessing. The instruction-set-execution definition can also be presented as a decoding diagram or 
tree (Fig. 2). Here, each 
block represents 
an encoding of bits in the instruction word. 
A decoding diagram 
allows one more descriptive dimension 
than the conventional, linear ISP description, revealing 
the assignment of bits to the 
instruction. Figure 2 still requires 
ISP descriptions for Mp, Mps, the instruction execution, 
the effective-address calculation, and the interpreter. Diagrams such 
as Fig. 2 are useful in the ISP design to determine 
which instruction numbers 
are to be assigned to names and operations and instructions which 
are free to be assigned (or encoded). There are 
eight basic instructions encoded by 3 bits, that is op(O:2) : = i(0:2), where instruction/i(O:ll). Each 
of the first six 
instructions (where 0 5 op < 6) have the 4 address operand deter- 
mination modes (thus yielding essentially 24 instructions). 
The first six instructions are: 
data transmission: deposit and clear-accumulator/dca tor/tad two™s complement add to the accumula- Principle oddressable 
instructions OP+ 0 ond - I Operate, opr 
Operate microcoded 
instructions opr-1 A i<j> A time 11,2,3,41 6 7 8 9 in 11 rtl- IKI - I Tal- time 1 ufiq ~1; clo- sma- szo- snl- invert next 0%- hlt- \ EAE A I<]> A time [1,2,31 instruction) instruction i<O:ll> ! = op ib p page,oddress Instruction word format Extended arithmetic element, EAE, inst ructions 
i Fig. 2. DEC PDP-8 instruction-decoding diagram. 

Chapter 5 I The DEC PDP-8 123 binary arithmetic: 
two's complement add 
to the accumu- lator/tad binary boolean: 
program control: jump/set 
program counter/jmp and to the accumulator/and jump to subroutine/jms index memory 
and skip if results are zero/isz Note that the 
add instruction, 
tad, is used for 
both data 
trans- mission and arithmetic. The subroutine-calling instruction, 
jms, provides 
a method 
for transferring a 
link to the beginning (or 
head) of the subroutine. In this way 
arguments can be accessed indirectly, and a return is executed by a 
jump indirect 
instruction to 
the location storing 
the returned 
address. This 
straightforward subroutine-call mecha- 
nism, although inexpensive to implement, requires reentrant and recursive subroutine calls to be interpreted by software, rather than by hardware. A stack, as in the DEC 
338 (Chap. 25), would 
be nicer. 
The input-output instruction/iot 
(:= op = 6) uses the re- maining 9 bits of the instruction to specify instructions to input/ 
output devices. The 6 io-select bits select 
1 of 64 devices. 
The 3 bits, io-pl-bit, 
io-p&-bit, io,p4,bit, command the selected device by conditionally providing 
three pulses in sequence. The instructions to a typical 
io device are: 
io-pl-bit -+ (IO,skip,flag[io select] + (PC t PC + 1)) testing a condition of an IO device output 
to a device input 
from a device 
io,p4,bit + (Output,data[io select] 
t AC) io-p2,bit + (AC c Input,data[io select]) There are three 
microcoded instruction 
groups selected by op = 7. The instruction decoding diagram (Fig. 
2) and the ISP description (Appendix 1 of this chapter) show the microinstruc- tions which 
can be combined in a single instruction. These instruc- 
tions are: operate 
group 1 (: = (op = 7) A 1 i(3)) for operating on the processor state; operate 
group 2 (: = (op = 7) A (i(3,ll) = 10,)) for testing the processor state; and the extended arithmetic element group 
(:= ((op = 7) A (i(3,ll) = 11,))) for multiply, divide, etc. 
Within each 
instruction the remaining bits, (4:lO) 
or (4:11), are 
extended instruction (or opcode) bits; 
that is, the bits are microcoded to select instructions. In this way 
an instruction is actually programmed 
(or microcoded). For 
example, the instruc- tion set-link +L tl is formed by 
coding the two microinstruc- tions, clear link, 
next, complement link. 
opr- 1 + (i(5) + L t 0; next i(7) -+L t 1L) Thus, in operate group 1, the instructions clear 
link, complement link, and set 
link are formed by coding instruction(5,7) 
= 10, 01, and 11, respectively. The operate group 2 instruction is used for 
testing the condition of the Pc state. This instruction uses bits 5, 6, and 8 to code tests for 
the accumulator. The AC skip conditions 
are coded (0 - 7) as never, always, 
=0, #0, <0, 20, 50, and >O. If all the nonredundant and useful variations 
in the two operate groups were available 
as separate instructions in 
the manner of the first seven (dca, tad, etc.), there would be approximately 7 + 12(0pr-l) + lO(0pr-2) + 6(EAE) = 35 instructions in the The optional Extended Arithmetic 
Element/EAE includes additional Multiplier 
Quotient/MQ and 
Shift Counter/SC regis- ters and 
provides the hardwired operations multiply, divide, 
logi- cal shift left, arithmetic shift, and normalize. 
The EAE is defined on the last page of Appendix 1. The interrupt scheme External conditions in the input/output devices can request 
that Pc be interrupted. Interrupts are 
allowed if (Interrupt-state = 1). A request to 
interrupt clears Interrupt-state (Interrupt-state 
t 0), and Pc behaves as though 
a jump 
to subroutine 0 instruction, jms 0, had been 
given. A special iot instruction (instruction 
= 6001,) followed by 
a jump to subroutine indirect 
to 0 instruction (instruction = 5200,) returns Pc to the interruptable state 
with Interrupt-state = 1. The program time to save M(processor 
state/ps) is 6 Mp 
accesses (9 microseconds), and the time to restore Mps is 9 Mp accesses (13.5 microseconds). Only one interrupt level is provided in the hardware. If multi- ple priority levels are desired, programmed 
polling is required. Most io devices 
have to interrupt 
because they 
do not have a program-controlled enable switch 
for the interrupt. For multiple 
devices approximately 3 cycles (4.5 
ps) are required to poll each interrupter. PDP-8. PMS structure The PMS structure of the LINC-8-338 consisting of a Pc('LlNC), Pc('PDP-8), and P.display('338) is shown in Fig. 1. The PDP-8 is just a single Pc. 
The Pc('L1NC) is a very capable Pc with more 

124 Part 2 
1 The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
instructions than 
the main Pc. 
It is available in the structure 
to interpret programs written for the C('LINC), 
a computer devel- oped by M.I.T.'s Lincoln 
Laboratory as a laboratory instrument 
computer for biomedical and laboratory applications. 
Because of the rather limited ISP in Pc, one would hardly expect 
to find all the components present 
in Fig. 1 in an actual 
configuration. The S between the Mp and the 
Pc allows eight Mp's. This S is actually S('Memory Bus; 8 Mp; 1 Pc; (P requests); time-multi- plexed; 1.5 ps/w). Thus the switch makes Mp logically equivalent to a single Mp(32768 
w). There are two 
other L's which are con- nected to the Pc, excluding 
the T.console. They are 
L('I/O Bus) and L('Data 
Break; Direct Memory Access). These links 
become switches when we 
consider the physical structure. Associated with each device is a switch, and 
the bus links all 
the devices; the L('I/O Bus) is really an 
S('I/O Bus). Each time a 
K connects to 
it, the 
S is included in 
the K. A simplified PMS diagram (Fig. 
3) shows the structure and the logical-physical transformation. 
Thus, the 1/0 Bus is S('I/O Bus; duplex; bus; 
time-multiplexed, 1 Pc; 64 K; Pc controlled, K requests; t:4.5 ps/w) 
The S('I/O Bus) is the same for 
the PDP-5, 8, 8/S, 8/I, and 8/L. Hence, any K can be used on any of the above C's. The 1/0 Bus is the link to the 
K's for Pc-controlled data transfers. Each word transferred is designated by a 
Pc instruction. However, the 1/0 Bus allows a K to request Pc's 
attention via the interrupt request signal. The Pc polls the K's to find the requesting K if multiple interrupt requests occur. 
A detailed structure 
of the Pc-Mp (Fig. 4) shows these L('I/O Bus, 'Data Break) connections to the 
registers and control 
in the notation used by 
DEC. This diagram 
is essentially a functional 
block diagram. The S('I/O Bus) in 
Fig. 1 is only an abstract 
representation of T.console- I I L('Data Break) L --PK- Mp(#O: core)- S- L-Pc-L (I I/O BUS) I L I. MP (k'71-S 2 1. u S('Mernory Bus) L LS-K- U S('I/O Bus) Fig. 3. DEC PDP-8 PMS 
diagram (simplified). 
the structure. Since it is a bus 
structure, the 
S can be expanded into L's and simple S's as shown in Fig. 3. The termination of the L in Pc is given in Fig. 3. The corresponding logic at a K is given in Fig. 5 in terms 
of logic design 
elements (AND's and OR's). (Fig. 5 also shows 
the S('I/O Bus) structure of Figs. 1 and 3). The operation of S('I/O Bus) shown in Fig. 5 starts when Pc 
sends a signal to select (or 
address) a particular K, using the IO-select (O:5) signals to form a 6-bit code 
to which K responds. Each K is hardwired to respond to a unique code. 
The local control, Kb], select signal is then used to form three local commands when 
ANDed with the three 
iot command lines from Pc, 
io-pl-bit, io,p2,bit, and io,p4,bit. Twelve data bits are transmitted either 
to or from 
Pc, indirectly under 
Ks control. This is accomplished by using the AND-OR gates in K for data input 
to Pc, and 
the AND gate for data input to K. The data 
lines are connected to AC as shown in Fig. 4. A single skip input is used so that Pc can 
test a status 
bit in K. A K communicates to Pc 
via the interrupt request line. 
Any K wanting attention simply ORs its request signal 
into the interrupt request signal. Program 
polling in 
Pc then selects the specific interrupter. Normally, the K signal causing an inter- rupt is also connected to the skip input. The L('Data Break; Direct Memory 
Access) provides a direct access path for a P 
or K to Mp via Pc. The number of access ports 
to memory can be expanded to eight by using 
the S('DMO1 Data Multiplexer). The S is requested from a P 
or K. The P or K supplies an Mp address, a read 
or write access request, and then either 
accepts or supplies data for the Mp accessed word. In 
the config- uration (Fig. 
l), P('L1NC) and P('338) are connected 
to S('DMO1) and make requests to Mp for 
both their instructions and 
data in the same way as 
the Pc. The global control of these processor 
programs is via the S('I/O Bus). The Pc issues start and stop com- mands, initializes their state, 
and examines their final state when a program in the other 
P halts 
or requires assistance. 
When a K is connected to L('Data Break) or to S('DMO1 Data Multiplexer), the K only accesses 
Mp for data. The most complex 
function these 
K's carry out 
is the transfer of a complete block of data between the Mp and an 
Ms or a T, for example, 
K('DECtape, disk). A special mode, the three-cycle data break, is controlled by Pc so that a K may request the next word from 
a queue in Mp. In this mode 
the next word 
is taken from the queue (block) in Mp, and a counter is reduced each 
time K makes a request. With this scheme, a 
word transfer takes 
three Mp cycles: one to add one to the 
block count, one to add one to 
the address pointer, and 
one to transmit the word. The DECtape was derived from M.I.T.'s Lincoln 
Laboratory LINCtape unit. Data are 
explicitly addressed 
by blocks (variable 
Chapter 5 I The DEC PDP-8 125 Skip Peripherol equipment C I/O Bus Data Address Switches =. I/O Bus AC k-24 Doto peripheral data (12) equipment - 4 using programmed t--- fronsfers Select c code Outpd Link (6) - Teletype drivers * 1 * ASR 4 bus model 33 - Accumulator Teletype control Ooto (a) 0 - 1 { *m Memory Peripheral equipment buffer control register I/O Peripherol equipment using the Data Break facilities Peripheral equipment- 1/0 Bus Program counter contro I Progrom counter * 12 4 register Inhibit current address count Transfer direction control ME Word count overflow 
Major stote Breok stote generotor C Address oaepted I. Memory address register 1; MA control Flow direction 4 DEC stondord positive 
pulse (-3 volts to ground 1 4 DEC standard negotive pulse (ground to -3 volt-) Port of ISP Transfer direction is into POP-8 when -3 volts, out of PDP - 8 when around DEC standard ground level signal 
DEC standard --3 volt level signal Oota break request is for 
three- cycle breok when ground or one- cycle break when -3 volts Fig. 4. DEC PDP-8 timing 
and control-element block diagram. (Courtesy of Digital Equipment Corporation.) 
126 Part 2 I The instruction-set processor: main-line computers 
iiiiii Section 1 1 Processors with one 
address per instruction 
- ~('10,pulse~pl,p2.~4; pulse; output)- L(~Io,s~I~c~<o:~>;ou~Pu~ )- ~~=lOOlOl=k _____ ~ kuselect.= (IO-select= 
k 1 M:%4G".e I I 
IO,pulae,pP A k-select (used for AC-Input-dot0 [k]) I0,pulseYp4 A h-select (used for outputudoto [k]-ACl To next K - 5s i Actual Bus Structure Logical Structure 
Fig. 5. DEC PDP-8 S('I/O Bus) logic and PMS diagrams. but by convention 128 w). Thus information in a 
block can be replaced or rewritten at random. This 
operation is unlike queue- 
accessed tape (conventional IBM format magnetic tape) in which data can be appended only to the 
end of a file. The control for the T(te1ephone) links 64 Teletypes or 
type- writers to the Pc. The final K which connects to a line 
is on a bit-serial basis. Since 
a telephone line 
sends and receives informa- 
tion serially by bit, there are 
special input/output instructions in the Pc to sample the line and to convert 
the sampled bits 
to coded characters. There are 
11 bits transmitted 
per character (although other codings use 7, 7.42, 7.5, and 10 bits per character). 
Of the 11 bits, there are 
3 control, 1 parity, and 7 information bits. 
The action of the Pc instruction, 
which is issued 5 x 11 (55) times for every character, is to control 
the line by forming 
the 7-bit charac- ters. The instruction is a good example of tradeoff in the hard- ware/software domain toward almost pure software; the only hardware state associated with a telephone line is a I-bit register to hold the state of the outgoing line, 
and a single AND 
gate to sample the incoming line state. This sampling 
process requires 
about 0.3 per cent of Pc-Mp capacity per 
active line (each 
of 10 - 15 char/s). In general, the PDP-8 hardware controls 
are minimal-in turn fairly elaborate control 
programs must 
be used as part of them. Computer levels In this section we 
describe all the systems levels 
in the PDP-8 computer from the top 
down. The reader should already have a 
sketchy knowledge of the PDP-8 because 
the registers and ISP have been exposed. Here, we wish to clarify how 
it operates. A map of the hierarchy is given in Fig. 6, starting from PMS to ISP and down through logic design 
to circuit electronics. These de- 
scription levels are subdivided to provide more organizational 
detail. For example, the register-transfer level has 
the more de- 
tailed registers, data operators, functional units, and 
macro logic 
of the processor, whereas the next logic level below has 
sequential and combinational networks, and the sequential and combinatorial 
elements. It should be apparent that the 
relationship of the various de- scription levels constitutes a 
tree structure 
where the organiza- tionally complex computer is the top node and 
each descending description level represents increasing 
detail (or smaller 
com- ponent size), until the final circuit element level is reached. For 
simplicity, only a few of the many possible 
paths through the structural description tree are 
illustrated. For 
example, the path 
showing mechanical parts is missing. 
The path shown proceeds from the PDP-8 computer to the 
processor and from there to the arithmetic unit 
or, more specifically, to the 
AC register of the arithmetic unit. Next, the macro logic implementing the register- transfer operations and functions 
for the jth bit of the AC is given; the flip-flops and gates needed for this 
particular implementation are shown. Finally, 
on the last segment of the path, 
come the electronic circuits and components 
of which flip-flops and NAND gates are constructed. 
Chapter 5 
1 The DEC 
PDP-8 127 R lpassive component) 1 [XI indicates figure number of instonce I Fig. 6. DEC PDP-8 hierarchy of descriptions. 
Abstract representations Figure 6 also lists some 
of the methods used 
to represent the physical computer abstractly at the different description 
levels. As mentioned previously, only 
a small part of the PDP-8 descrip- 
tion tree is represented here. 
The many documents, schematics, 
diagrams, etc., which constitute the 
complete representation 
of even this small 
computer include logic diagrams, wiring lists, 
circuit schematics 
and printed-circuit board 
layout masks, pro- duction description 
diagrams, production parts 
lists, testing speci- fications, programs for 
testing and diagnosing faults, and manuals for modification, production, maintenance, 
and use. As the discus- sion continues down the abstract description 
tree, the reader will observe that the 
tree conveniently represents 
the constituent ob- jects of each level and their interconnection at the next highest 
level. Each level in the abstract-description tree will be described in order. 
The PMS level The simplified PMS structure in Fig. 
3 has been reduced 
from Fig. 1. The computer 
is small enough 
so that the 
physical delinea- tion of the PMS components, such as Ks and S™s, is less pro- nounced than in larger 
systems. In 
fact, in the case of the S(™Memory Bus, II/O Bus), 
the S™s are actually within 
the K and Mp, as shown in Fig. 5. The implementation of these switches 
within the K and Mp was shown in Fig. 5. In Fig. 7 we present a more conventional functional diagram and 
the equivalent PMS diagram of the computer, with Pc 
decomposed into K, processor state (Mps), and D. The functional diagram 
has the same compo- nents of the characteristic elementary 
computer model, namely, 
K, D, M, and T(input, output). 
These figures give 
a somewhat general idea 
of what processes can occur 
in the computer, and how information flows, but it is apparent that at 
least another level is needed to describe the internal structure 
and behavior 
of the Mp and Pc. 
We should look at these primitives (although still 
together as a C) at the register-transfer level. Programming level 
(ZSP) The ISP interpretation is given in Appendix 1 of this chapter and 
is the specification of the programming machine. 
In addition, it constrains the physical machine™s behavior 
to have a particular ISP. The ISP has been discussed earlier in 
the chapter. Register-transfer level 
The C can also be represented at the register-transfer level by 
using PMS. Figure 4 (by DEC) shows the register-transfer level; 
Console Processor state Doto operations (arithmetic and Input-output, and memory logical ) secondary memory 
I I I L------- Fig. 7. DEC PDP-8 function block and PMS diagrams. (a) Processor functional block diagram. 
(b) Pc PMS diagram. 
128 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
L ('Memory Bus):= -L MB<O 11) data [wtpui broddms;;lZ br rTl'Senseuarnplifier <O'lD M ('Coreustack ;12b;40%w iI- 7--------- To MpIXl:71 I I pc:= 1 1 I -1 D r 'Link/L. operotions'll-0: L-1 1 I- I buffer/MB<O:ll>;flip flop1 ___ &M('Mernorv address/MA<O:ll> I I I / +blnstruction register decode) I ILOAC- LOAC x4 (rotate), ~~Ac-LOAC xz (rotate). 'LOAC- LOAC x4 [rotate). 'AC-ACeMB,AC-ACAMB, 'AC-Carry lAC,MEI. 'AC-ACV Dat~~switcherl [ 1 ! 1, I I I 1- I I I MpsVProgrorn caunter/PC<O 
Il>; flip flop1 
(lInstructlon regster /IR<O:Z> ., f lip flop 1 D ('IR; operations: 
?IR-O:IR-M[MA] <O:Z> 11 I' t-J I L"0 Bus1 := -L('AC; input,output, 1ZbI- --cL I'IOuselect<O:5>1:= ['MEh.E> - IOuskipl, (output; IO-pulre-pl,pP.p4 I I K (ISP,Mp.S('I/O bus1.T.console 'Data T (clock) I Inputs tm~} ~ [ break; M (working;IStoteuregister,l I T console ('Datauswitchesl- ~ L('requert;direction,cycle-select<0:1> 1- I -+ifaddress -occepted:word-oount_w: break-statel- 1 --CLl'ME~O'll~;outputl- I - Ll'DB-address<O.ll>; inputl- I I I T. console I lightsl- - L('DB-dato <0.11>; input)- Fig. 8. DEC PDP-8 register-transfer-level PMS diagram. only registers, 
operations, and 
L's are important 
at this level. 
We still lack information 
about the conditions under which operations 
are evoked. Figure 8 is a PMS diagram of Pc-Mp registers. Here we show considerably more 
detail (although we do not bother with electrical pulse voltages 
and polarities) 
than in Fig. 4. We declare the Pc state (including the temporary register) within 
Pc. The figure also gives 
the permissible data operations, D, which are permitted on the registers. It should be clear 
from this 
that the logical design level 
for the registers and the operators can 
easily be reached. The K logic design 
cannot be reached until we 
use the programming level 
constraints (ISP), thus defining the conditions for evoking 
the data operators. The core memory. The Mp structure is given in Fig. 8. A more detailed block diagram which shows 
the core stack with 
its twelve 64 x 64 1-bit core planes 
is needed. Such a diagram, 
though still 
a functional 
block diagram, takes on some of the aspects of a circuit diagram because a core 
memory is largely circuit-level 
details. The Mp (Fig. 9) consists of the component units: the two address decoders (which select 1 each of 64 outputs in the X and Y axis directions of the coincident current memory); selection switches (which 
transform a coincident 
logic address 
into a high- current path 
to switch the magnetic cores); the 12 inhibit drivers (which switch a 
high current or no current into 
a plane 
when either a 0 or 1 is rewritten); 12 sense amplifiers 
(which take 
the induced low sense voltage 
from a selected core 
from a plane being switched or not switched and transform it into 
a 1 or 0); and the core stack, 
an array M[0:7777,](0:11). Since this 
is the only time the Mp is mentioned, Fig. 9 also includes the associated circuit- level hardware needed in 
the core-memory operation, such as 
Chapter 5 I The DEC PDP-8 129 power supplies, timing, and 
logic signal level conversion amplifiers. 
The timing signals are generated 
within Pc(K) and are shown together with Pc™s clock in Fig. 10. The process of reading a 
word from memory 
is: 1 A 12-bit selection address is established on 
the MA(0:ll) address lines, which 
is 1 of 10000, (or 4096,,) unique num- 
bers. The upper 
6 bits, (0:5), select 1 of 64 groups of Y addresses and the lower 6 
bits, (6:11), select 1 of 64 groups of X addresses. The read logic signal 
is made a 
1. A high-current path flows via the X and Y selection switches. In 
each of the X and Y directions 64 x 12 cores 
2 3 have selection 
current. Only one core in each plane 
is selected since 
Ix = Iy = Iswitching/2, and 
the current 
at the selected intersection 
= Ix + Iy = Iswitching. 4 If a core 
is switched to 0 (by having Iswitching amperes 
through it), then 
a 1 was present and 
is read at the output 
of the plane (bit) sense amplifiers. 
A sense amplifier receives 
an input from a winding 
that threads every core of every bit within a core 
plane [0:7777,]. All 12 cores 
of the selected word are reset to 
0. The sense time at which the sense amplifier is observed is tms (memory 
strobe), and 
the strobe in effect creates hlB t M[MA]. 5 The read current is turned off. -X= Select - High current signals I I I (01 tIs/21-15/2) 1 Low level 
winding (Sense signals) 1 L-------- -_____-- 2 Current direction controls F~~~ To MB data inputsC0 112 Fig. 9. DEC PDP-8 four-wire coincident current (three dimensions) core-memory-logic 
block diagram. 
130 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
Clock pulses l(t2), I I (tm$ , , 
I 1 (tl) I , , I (tmd) I(t2) Read 1 I (p) 0 .5 1.0 1.5 time Write I I Inhibit I 1 Memory I (ME- M[MA]) strobe b- Memory cycle -4 Fig. 10. DEC PDP-8 clock and memory timing diagram. 6 The write and 
inhibit logic signals are turned 
on. The bit inhibit signal is present or not, depending on whether a 0 or 1, respectively, is written into 
a bit. A high-current path flows via the X and Y selection switches, but in an opposite direction 
to the read case (2 above). If a 1 is written, no inhibit 
current is present, and the net current 
in the selected core 
is --switching. If a 0 is written, the current is -1switching + (Iswitching/2) and the core remains reset. The inhibit and write 
logic signals are turned off, and the memory cycle 
is completed. 7 8 Registers and operations. As Fig. 8 shows, the registers in the Pc cannot be uniquely assigned to a single function. In a minimal 
machine such as the PDP-8, functional separation 
is not economi- 
cal. Thus there are not completely distinct 
registers and transfer paths for memory, arithmetic, and 
program and instruction flow. (This sharing 
complicates understanding 
of the machine.) How- ever, Fig. 
8 clarifies the structure considerably by 
defining all the registers in Pc (including temporaries). For 
example, the Memory Buffer/MB is used to hold the word being read 
from or written to Mp. MB also holds one of the operands for binary operations 
(for example, AC c AC A MB). MB is 
also used as an extension of the Instruction Register/IR during the 
instruction interpretation. The additional registers, not 
in the ISP, are: Memory Buffer/MB(O:ll) holds memory 
data, instruction, and oper- ands holds address 
of word in Mp being accessed Instruction Register/IR(O:2) holds the value of current instruction being performed 
Memory Address/MA(O:ll) State-register, Fetch/F : = (State-register = 0) Defer/D/Indirect := (State-register = 1) Execute/E := (State-register = 2) a ternary state 
register holding the major state of memory cycle 
being performed memory cycle to fetch instruction 
memory cycle 
to get address of operand memory cycle 
to fetch 
(store) operand and execute the instruc- tion Figure 8 has been concerned with 
the static definition (or 
declaration) of the information paths, the operations, and state. The ISP interpretation (Appendix 1) is the specification for the physical machine™s behavior. As the temporary hardware registers are added, a more 
detailed ISP definition could be given in terms of time and temporary registers. Instead, we give a state diagram (Fig. 11) to define the actual 
Pc which 
is constrained by both the ISP registers, 
the temporary registers implied by the implementa- tion, and time. The relationship among 
the state diagram, the ISP description, and the 
logic is shown in the hierarchy of Fig. 6. In the relationships of the figures, we observe 
that the ISP definition 
does not have all the necessary detail for fully 
defining a physical Pc. The physical Pc is constrained by actual hardware 
logic and lower-level details even 
at the circuit level. For example, a core memory is read by a destructive process and requires a 
temporary register (MB) to hold the value being rewritten. This 
is not repre- sentable within 
a single 
ISP language statement since we 
define only the nondestructive transfer t, but it can be 
considered as the two 
parallel operations 
MB t M[MA]; M[MA] 
c 0. The problem of explaining rewriting of core using ISP 
is also difficult, 
because explicit time is not in the ISP language (although 
we can define clock events, or 
at least relative time). 
The state diagram (Fig. 
11) describes the implementation be- havior using 
the registers and register operations (Fig. 
8) and the temporary registers declared above. The implementation is fundamentally Mp-timing-based, as we see from both 
the state 
diagram and the 
times when the four clock 
signals are generated 
(Fig. 10). Thus there 
are three (State-regis- ter = O,1,2) x 4 (clock), that is, 12 major states, in 
the implemen- tation. We use the IR to obtain two more states, F2b 
and F3b, 
Chapter 5 I The DEC PDP-8 131 "Fetch" instruction memory ms-I ME- M [MA] ; IR-IR V M [MA] <0:2> 1 ; ljmpvirnsl (IR=lO'#'I-; 5:11>- MB<5:ll>; E<4> -MA<0:4>-0 1 ; Defer"lindirect1 nddress 8 xecut:;;; ememory Memory cycle tms-(MB-M[MA]1. (tms A I isz vtod v and ))-I ME- M [MA] 1 ', t1-( t1-( 
It05 MArl7)-I ME-MB +'I 11 ; tmd4 I MA-ME); d AC-0)); ,MB9777)4 Pc-Pc+~lll; Tjmp .MB<3> 1-1 irns-( PC<S'lD-MB<5.1v; ME-PC; ~MB<4>-PC<0.9-01: state, (to EO1 O;Stote, register- 0 (to EO1 Note: State diagram does not include Doto Break, Interrupt, ond EA• Fig. 11. DEC PDP-8 Pc state diagram. for the description. The State-register values 0, 1, and 2 corre- 
spond to fetching, deferring (indirect 
addressing, i.e., 
fetching an operand address), and executing (fetching 
or storing data, then 
executing) the instruction. The state diagram does not describe 
the Extended Arithmetic Element/EAE operation, 
the interrupt state, and 
the data break states (these 
add 12 more states). The initialization procedure, including 
the T.console state diagram, is also not given. One should observe 
that when t2 occurs at the beginning of the memory cycle, a 
new State-register value 
is selected. The State-register value 
is always held for the remainder of the cycle; Le., only the sequences (FO + F1+ F2 + F3 or DO + D1+ D2 -+ D3 or EO -+ El + E2 -+ E3) are permitted. 
Figure 8 alludes to Pc(K), that is, the sequential network used 
for controlling Pc. 
The inputs and the present state (including clocks) determine the 
operations to 
be issued on the registers. Q /tM2Bfb;IR-O; Stoteuregister-Ol; (to FOI ps v dco v ISZI-LI M [MAI-MEN. Logic design level 
(registers and 
data operations) Proceeding from the register-transfer and ISP descriptions, the next level 
of detail is the logic module. Typical 
of the level is the 1-bit logic module for 
an accumulator bit, AC(j), illustrated 
in Fig. 12. 
The horizontal data inputs 
in the figure are to the logic module from AC(j), MB(j), IO Bus(j), and Data,switch(j). The vertical control 
signal inputs command 
the register operations (Le., the transfers); they are labeled by 
their respective ISP operations (for example, 
AC c MB A AC, AC c AC x 2 {rotate}). The 
sequential network Pc(K) (Fig. 
8) generates these control 
signal inputs. Logic design level 
(Pc control, Pc(K) sequential network) The output 
signals from 
the Pc(K) (Fig. 8) can be generated in 
a straightforward 
fashion by formulating 
the boolean expressions 

132 Pari 2 1 The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
Bus to each bit of AC r -AC<J> MB<J > AC<J> LAC-AC/2 (rotate }, 'AC- AC 
x 2 { rot a +e ), LAC-Carry (AC,MB) 
'AC-AC t 1 is formed by AC<12> 
carry input Fig. 12. DEC PDP-8 AC(J) 
bit register-transfer 
logic diagram. 'AC-0 := ( (tl A (1R = 111) A (7 MB<3> A MB<4> A 7 MB<6>) A (State,register=O)) v 
(tl A (IR = 111) A (MB<3> A _I MB<ll> A MB<4>) A (State-register=O)) v (tl A (IR =111) A (MB<3> n MB<ll> A MB<~>)A (Stateuregister=O))'v (tl A (IR =011) A (State-register. 2))) (tl A (((State-register = 0) A (IR =111) A MB<4> A (MB<3> v MB<~>))v ((State-register = 2) A (IR =OH)))) Logic equation for 
'AC- 0 ,IR<O> IR<1> R<2> (State-register = 2) Logic diagram 
for AC-0 'This term is derived from EAE and is 
not on the state diagram Fig. 13. DEC PDP-8 Pc(K) 'AC 
t 0 signal-logic equations 
and diagram. -15V Direct Direct clear DirectA set NOR ~ou~put Direct output clear - tlOV Flip-flop circuit Combinatorial logic equivalent of flip-flop Table of circuit input-output 1 0 Direct Direct 1 0 set clear outputs (at t) Inputs Outputs (at t+)' 0 -3 -3 -3 0 -3 -3 0 -3 -3 -3 0 -3 0 -3 0 -3 0 0 -3 -3 0 -3 0 -3 0 0 -3 0 -3 0 -3 0 -3 0 -3 set-clear flip- flop Direct set-clear flip-flop sequential logic element Table of flip-flop input-output 
1 0 0 Direct Direct 
1 set clear 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 4 0 Inputs outputs (at t+1' outputs (at f) _________ 'Note; This is not an "ideal" sequential 
circuit element, because there IS no delay in the output. Fig. 14. DEC PDP-8 sequential-element circuit 
and logic diagrams. 

Chapter 5 1 The DEC PDP-8 133 -3 000 -3 001 -3 01 0 -3 01 1 -3 100 -3 101 -3 110 0 111 -15 VOI tS 1 111 1 110 1 101 1 100 1 01 1 1 01 0 1 00 1 0 000 - 3volts -15vo1ts i Inputs lnoui NAND logic element 
Input NOR logic element 
Node Multiple input inverter circuit 
Table of circuit 
Table of NAND 
Table of NOR behavior behavior behavior Input 1 Output Input 1 Output Input I Output 123 123 123 000 0 0-3 0-3 0 0 -3 -3 
-3 0 0 -3 0-3 -3 -3 0 -3 -3 -3 Fig. 15. DEC PDP-8 combinational element circuit 
and logic 
diagrams. directly from the state diagram in Fig. 11. For example, the AC t 0 control signal is expressed algebraically and with 
a com- binatorial network in 
Fig. 13. Obviously these boolean 
output control signals are functions which 
include the clock, the State-register, and 
the states of the arithmetic registers (for 
example, A = 0, L = 0, etc.). The 
expressions should 
be factored and minimized so as to reduce 
the hardware cost of the con- trol for the interpreter. Although we 
are rather 
cavalier about 
Pc(K), it constitutes about 
one-half the logic within Pc. Circuit level The final level of description is the circuits which form 
the logic functions of storage (flip-flops) 
and gating (NAND gates). Figures 14 and 15 illustrate some of these logic devices 
in detail. 
In Fig. 14 a direct set and 
direct clear flip-flop, a sequential- 
logic element, is described in terms 
of circuit implementation, 
combinational logic equivalent, a 
table of its behavior, and its 
algebraic behavior. Note 
that this is not an ideal element, 
be- cause it has no 
delay and responds directly and immediately to 
an input. Some idealized sequential 
logic elements are 
used in the PDP-8 (but not illustrated), including 
the RS (Reset-Set), T(Trigger), JK, and D(De1ay). A delay in the flip-flops makes them behave in 
the same way 
as the ideal primitives in sequential- 
circuit theory. 
The outputs require a 
series delay, At, such that, if the inputs change 
at time t, the outputs will not change until 
t + At. In fact, the 
PDP-8 uses capacitor-diode gates at the 
flip- flop inputs to delay the inputs. Figure 15 illustrates the combinatorial logic elements used in 
the PDP-8. The circuit selection 
is limited to the 
inverter circuit 
with single or 
multiple inputs. 
These are more familiarly 
called NAND gates or NOR gates, depending on whether one uses posi- tive and/or negative logic-level definitions. 
Conclusion We could continue 
to discuss the behavior of the transistor as it is used in these switching-circuit primitives but will leave that to books on semiconductor electronics 
and physics. It is hoped that the 
student has gained a grasp 
of how to think about 
the hierarchical decomposition of computers into particular 
levels of analysis (and synthesis). 
134 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
APPENDIX 1 DEC PDP-8 ISP 
DESCRIPTION Pr Stnte ACd: I I>, L PCd: I I> Run I n te r rupt-s tate Io-putse-l ; IOSulseJ; IO,pulse,4 Appendix 1 OEC PDP-8 ISP Description Accnmulator Link hit/kC eriensioq ;'or overylcw and carry Progr'an Counter I i~hev. ?c Is intemreting instrurtions or "runn:ng" 1 ohen fc can be interrupted; under programmed control I3 pulses to IO ?evi?es I$ State Estended mernorg is not irclude j. M[O:777i8l<0:ll> Page,O[O: 17i81d: I I> := M[O: 177 Id: I I> Auto,index[O: 7l.a: I I> := Page-0 [IO , I7 Id: I I> 8' 8 smcial array of directlg addressed memory registers s-pecial arrap when a 
ldressed indirectly, is incrernented bg 8 Fc ('o~soie YCtnte Keys for start, step, coy,t-'nue, ezmiv;e (loa? frw memoc4), and deposii- (store in merory! are not included. Data switchesd:ll> Instmetion Format instruction/ii0:ll> op4 : 2; indi rect,b i t/i b page,O,bi t/p page-add ress<O : 6> th i s,page<O: 4> PC'<O: I I> IO,select<O:5> io,pl,bit i o,pZ,b i t io,p4,b i t s ma s za sn 1 data enterec' via console := i4:2; op code := i<3; 0, direct; : indirect rnemcry redfererce := i<4> 0 selects page 0; 1 selects this page := i<S:Il; := PC'd:4> := (PC<O:II> -1) := i<3:8> selects a 1" or ?.'s device := i<lI> := i<IO> := id> := i<5> w hit for> ski? on m".yus A?, operate 2 g~oup := i<6> hit ror skip on ze?o AC := i<7> bit .+'or skip ox nm zero Link these 3 bits control the selective generation of -3 volts, 0.4 1~s pulses to I/O devi.-es F-'.'ectiue _. Ai:.iress ('nlc~,Zatlcn ,Process z<O:II> := ( 7i b -> z"; ib A (lo8 c z" i 178) i (M[z"] +M[z"] + 1; next); i b 2 M[z"]) z'<O:ll; := (- ib iz"; ib -iM[z"]) z"<O:Il> := (page,O,bit i this,pageopage,address; ,page,O,bi t -) Onpage-address) p microcoded instruction or instruction bit(s) within an instruction 
Chapter 5 1 The DEC PDP-8 135 APPENDIX 1 DEC PDP-8 ISP 
DESCRIPTION (Continued) 
n i.rterpri.tati( n P'rocess Run A ((nterrupt,request h Interrupt-state) -> ( instruction <-M[PC]; PC <-PC + I; next instruction,execution); Run Interrupt-request A Interruptuslate -> ( M[O] <-PC; Interruptustale to; PC <- I) ir,stru..t;o?. .;?t am/' instr. tction 
Exer~~tior. rrocess Instr~ction~execution := ( and (:= op = 0) i (AC t AC A M[z]); tad (:= op = I) -) (LOAC c- LOAC + M[zl); isz (:= op = 2) -) (M[r'] <-M[rl + I; next (M[z'l = 0) + (PC tPC t I)): dca (:= op = 3) 4 (Mlrl c AC; AC t 0); jrns (:= op = 4) 4 (M[z] + PC; next PC z + I); jmp iot (:= op = 5) -) (PC t 2); (:= op = 6) 4 ( io,pl,bit -> IO,pulse,l c I; next io42,bit + IO,pulse,Z <- I; next io,pb,bit -> IO,pulse,b i- I); opr (:= op = 7) -,Operate,execution ) logical ad two Is complement aJd ir.iiex and skip if zero the operiitc insirwetion is k0.w I below end Instruction ewcution rate insti.uc: ions: operate grour I, operaie gmup 2, and csi e arit,*met;c ure ric'ined as a separate Operate,execution := ( cla (:= i<4> = I) i (AC c- 0); opr,l (:= i<3> = 0) + ( operate groun I c11 (:= i<C = 1) -> (L <. 0); next p clear. link cma (:= id> = I) -> (AC <-7 AC): u complernmt AC cml (:= i<7> = I) + (L <-? L); next IL compZernent L iac (:= i<II> = I) -> (LWC r-LWC + I); next u. ircrement PC ral (:= i<8:10> = 2) + (Ln4C +LmC x 2 {rotate)); u rotate left rtl rar (:= i<E:lO> = 4) --f (LOAC tLOAC / 2 (rotate)); u. rotate right rtr (:= i<8:lO> = 5) + (LOAC tLOAC / Z2 {rotate])); clear Lr. ('ov~n LO all operate <nstructions. (:= i<B:lO> = 3) -'(LoAC <-LOAC X 2' (rotate?); u rotate twice left u. rotate twice raight oprd (:= i<3,11> = 10) + ( operate group 2 li. PC',.- ship test 51.ip condition C (id> = 1) -, (PC tPC + 1); next skip condition := ((ma A (AC < 0)) v' (sza A (AC = 0)) \I (snl A L)) 11 ''rrr" switche? u halt or stop nsr (:= i<9 = 1) + (AC t AC V Data switches); hlt (:= i<IO:,= I) -> (Run e 0)): FAE (:= i<3,lI> = 11) + EAF,instruction,execution) optinvln7 FA 
136 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
APPENDIX 1 DEC PDP-8 ISP 
DESCRIPTION (Continued) 
KT and Ws Ztate Fach K map have ariy or all of the following registers. 
There can he UP to 64 optional PIS. I nputdata [O :77 814: 1 1> O~tput~ata[O:77~14:11> 64 outpul hu.f.fws IOdkipflag LO: 7781 IO,interrupt,reques t [O: 77 1 1 sipni,fies a reauest. 1.f interrunt,qtate = 1, then ai 64 innut buffers 64 test conditions interrunt occurs. 8 o,f all reouests .from each IO device ''or ,! 1 nter rupt ,reques t : = ( max( I0-i nterrupt,reques t [o: n83)) Extended Arithmetic Eiement, EAF (optional) Provides additional arithmetic instructions (or operators) inclucliw x, /, normalize, lopical .shi,Ct and arithmet?c shi.ft,. EAE State MQQ: 11> ?Qltirlier Quotien i SC<D:L> Shift rounter Instruction Format and Data mdsd: 11> sa:&> := rnds<7:ll> shift count Darameter Instruction Set 
for t'AE EAE,instruction&xecution := (next mqa (:= i<5>) + (AC tAC v MQ); P.0 into PC sca (:= i<6>) + (AC cAC V SC): sc into Pc rnql (:= i<7;) 3 (MQ tAC; AC to): next AC into pdO, clear IC Note only one of nmi, shl, asr, lsr, muy, or &Vi can he piven at a time. i<8:10> = oom + ; IO ooerntion 7 nmi "(mds 
t M[PC]; PC cPC + 1); next muy (:= i<8:10> = 2) --f (LOACOMQ t MQ x mds; SC to) mu 2 tip 7 !.I dvi (:= i<8: lO> = 3) + (MQ t LoACoMO/mds; div i r7e LoAC c LoACoMQ mod mds: SC c 0) : nmi (:= i<8:10; = 4) + ( ACoMQ tnormalize(ACOM@); nomaliae (AC,M0) ?Y to .T SC t normal ize-exponent (ACOMQ)) ; shl (:= i<8:lO> = 5) + (LoACoMQ tLoACoMQ x ZS+l: SC to); asr (:= i<8:10> = 6) - (LoACoMQ tLoACoMQ / 2'+l: SC <-O): Isr (:= i<8:10> = 7) + (LoACoMQ tLoACoMQ / 2s+1{loqical); shi,ft left sh?:,ft right loqical shi,ft sc +-0) 1 eniJ instruction execution 
Chapter 6 The Whirlwind I computer1 R. R. 
Everett Project Whirlwind is a high-speed computer activity sponsored at the Digital Computer Laboratory, formerly a part of the Servo- mechanisms Laboratory, of the Massachusetts Institute of Tech- nology (M.I.T.) by 
the Office of Naval Research (O.N.R.) 
and the United States Air Force. The project began in 
1945 with the assignment of building a high-quality real-time aircraft simulator. 
Historically, the project has always 
been primarily interested in the fields of real-time simulation and control; 
but since about the 7 most of its efforts have been devoted 
to the 
design and construction 
of the digital computer known as 
Whirl- wind I (WWI). This computer has been in operation for about 1 year and an increasing proportion 
of project effort now 
is going into application studies. Applications for 
digital computers 
are found in 
many branches of science, engineering, and 
business. Although any 
modern gen- eral-purpose digital 
computer can be applied to all these 
fields, a machine 
is generally designed to be 
most suited to some particu- lar area. Whirlwind 
I was designed for 
use in 
control and simula- tion work such 
as air traffic control, industrial 
process control, and 
aircraft simulation. 
This does 
not mean 
that Whirlwind will not be used on applications other than 
control. About one-half 
the available computing 
time for the next year will 
be assigned to engineering and 
scientific calculation including 
research in such uses supported by the O.N.R. through the M.I.T. Committee on Machine Methods for Computation. These control and simulation problems result 
in a specialized 
emphasis on computer design. Short register length WWI has 16 binary digits 
and the 
control problems are usually very simple mathematically. 
Furthermore, the computer 
is almost always part of a feedback rather than an open-ended system. Consequently, roundoff errors are seldom troublesome 
and the register length can be shortened to something comparable 
to the 
sensitivity of the physical quantities involved, perhaps five decimal places or less. WWI has a register length of 16 binary digits including 
sign or about four and one-half decimals. 
The register length was lAIEE-IRE Conf., 70-74 (1951) chosen as the minimum that would provide a 
usable single-address 
order, in 
this case 
five binary digits 
for instruction and 
11 binary digits for address. 
In a future machine we 
would probably 
increase this register 
length to 20 or 24 binary digits to get additional order 
flexibility; the increased numerical 
precision is less important. For scientific and engineering calculation, 
greater than 16-digit precision is often required. There is available a 
set of multiple- length and floating point subroutines 
which make 
the use of greater precision very easy. 
It is true that these subroutines 
are slow, bringing effective machine speed 
down to about that ob- tained by 
acoustic memory machines. 
It is much more efficient 
occasionally to waste computing 
time this way 
than continuously to waste a large 
part of the storage and computing equipment 
of the machine by providing an unnecessarily 
long register. High operating speed WWI performs 20,000 single-address 
operations per second. Con- trol and simulation problems require very high speeds. 
The neces- sary calculations must be carried out 
in real time; 
the more com- 
plex the controlled system is, the faster the computer 
must be. There is no practical upper 
limit to 
the computing speed 
that could be used if available. Where the 
problems are large enough, and these 
problems are, one high-speed 
machine is much better than two simpler 
machines of half the speed. Communication 
between machines presents 
many of the same problem that communication between human 
beings presents. 
Great effort was put into 
WWI to obtain high speed. The target 
speed was 50,000 single-address 
operations per second, and all parts of the machine except storage meet 
this requirement. The actual WWI present operating speed 
of 20,000 single-address 
operations per 
second is on the lower edge of the desired speed range. Large internal storage 
WWI now has 1,280 registers. 
A large amount 
of high-speed in- 
ternal storage is needed since it is not in general 
possible to use slow auxiliary storage 
because of the time factor. 
In many cases 
a magnetic drum can 
be useful since its 
access time is short com- 137 
138 Part 2 
1 The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
Order type Numbers Basic pulse repetition frequency pared to the response times 
of real systems. Even 
with a drum 
there is considerable loss of computing and 
programming efficiency 
due to shuffling information back and forth 
between drum and computer. WWI is designed for 2,048 registers 
of storage. Until recently 
there has been available 
only about 300 registers. This 
number, while small, 
has been adequate 
for much useful work. 
Very re- cently a 
second bank 
of new-model storage tubes has been added. 
These new tubes 
operate at 1,024 spots 
per tube bringing the total 
WWI storage to 1,280 registers. These 
tubes have been in 
the computer and under test 
for 2 months and in active 
use for about 2 weeks. In 
the next few 
months the tubes in 
the first bank will 
be replaced by 
new-model storage 
tubes bringing 
the total 
storage to 2,048. This 
number is on the lower end of what the project considers desirable. 
What the 
computer business needs, has 
needed, and 
will probably always need is a bigger, better, and faster storage device. 
Extreme reliability In a system where much valuable property and 
perhaps many human lives are dependent 
on the proper operation 
of the com- puting equipment, failures must 
be very rare. Furthermore, check- ing alone, however complete, 
is inadequate. It is not enough merely to know that the equipment 
has made 
an error. It is very unlikely that a man, 
presumably not too well 
suited to the 
work during normal conditions, 
can handle the 
situation in an emer- gency. Multiple machines with majority rule seem to be 
the best answer. Self-correcting machines 
are a possibility but appear 
to be too complicated to compete, especially as they provide no standby protection. 
The characteristics of the Whirlwind I computer may be re- capitulated as follows: Register length Speed 20,000 single-address 
operations per 
16 binary digits, parallel second Storage capacity Originally 256 registers 
Recently 320 registers 
Presently 1,280 registers 
Target 2,048 registers 
Single-address, one order per 
word Fixed point, 9™s complement 1 megacycle 2 megacycles (arithmetic element 
only) Tube count 5,000, mostly single 
pentodes Crystal count 
11,000 There are 
32 possible 
operations, of which about 27 are as- signed. They are 
of the usual types: addition, subtraction, 
multi- plication, division, shifting by 
an arbitrary number of columns, transfer of all or parts of words, subprogram, and conditional 
subprogram. There are 
terminal equipment control orders and there are 
some special orders for 
facilitating double-length 
and floating-point operations. One way to increase the effective speed of a machine 
is to provide built-in facilities 
for operations that occur frequently 
in the problems of interest. An example is an automatic 
co-ordinate transformation order. 
The addition of such facilities does 
not affect the general-purpose nature of the machine. The machine retains 
its old flexibility 
but becomes faster and 
more suited to a certain class of problems. From March 
14, 1951, at which time we began to keep 
detailed records, until November 22, 1951 
a total of 950 hours 
of computer time were scheduled 
for applications use. The machine has been running on two shifts or 
a total of about 3,000 hours 
during this interval. The two-thirds time not 
used for applications has been used for 
machine improvement, adding 
equipment, and 
preventive maintenance. Of the 950 hours 
available, 500 have been 
used by the scientific and engineering calculation group, 
the rest for 
control studies. The limited storage available 
until recently has been admittedly a serious handicap to the scientific and engineering applications 
people. There has not been room in storage 
for the lengthy sub- routines necessary for convenient use of the machine. The largest part of their time 
has been spent in training, in 
setting up pro- cedures, and in preparing a library 
of subroutines. A partial list of the actual problems carried out 
by the group includes: An industrial production 
problem for 
the Harvard Eco- nomics School Magnetic flux density study 
for our magnetic storage 
work Oil reservoir 
depletion studies Ultra-high frequency television channel allocation investi- gation for Dumont Optical constants 
of thin metal 
films Computation of autocorrelation coefficients Tape generation 
for a digitally-controlled 
milling machine 
Chapter 6 1 The Whirlwind 
I computer 139 A 1h \I I, I1 \I The scientific and engineering applications time 
on Whirlwind I has been organized in a manner 
patterned after 
that originated by Dr. Wilkes at EDSAC. The group of programmers and mathe- maticians assigned to WWI assist users in setting up their own problems. Small problems 
requiring only a few seconds 
or minutes of computer time are encouraged. Applications time is assigned in 1-hour pieces 
two or three times a day. 
No program debugging is allowed on the machine. Program errors are deduced 
by the programmer from printed lists of results, storage contents, or order sequences as previously requested from the machine operator. 
The programmer then corrects his program which 
is rerun for him 
within a 
day or perhaps within a 
few hours. 
Every effort 
is made to reduce 
the time-consuming job 
of print- ing tabulated results. In many cases 
a user desires large 
amounts of tabulated data only because he doesn™t really know 
what an- swers he wants and so asks for everything. Such users 
are encour- aged to ask only for 
pertinent results in the form of numbers or curves plotted by the machine on a cathode-ray 
tube and auto- matically photographed. 
If these results prove 
inadequate or the user gets a 
better idea of his needs, he is allowed to rerun his program, again asking only for 
what appear to be 
significant re- 
sults. Figure 1 shows a sample curve 
plotted by the computing machine showing calibrated axes and decimal intercepts. 
*™ DIGIT TRAWFER BUS Fig. 1. Sample computer output. 
\I INPUT ~ 
~ 
Fig. 2. Simplified computer 
block diagram. - d bLi c13 v1 6 OUTPUT WWI system layout Figure 2 shows the major parts of any computer such as 
WWI. The major elements of the computer 
communicate with each other 
via a central bus system. 
WWI is basically a simple, straightforward, 
standard machine of the all-parallel type. Unfortunately, the simple concept 
often becomes complicated in execution, and this is true here. WW™s control has been complicated 
by the decision to keep 
it completely flexible, the arithmetic 
element by the need for high 
speed, the storage by 
the use of electrostatic storage tubes, 
the terminal equipment by 
the diversity of input and output media needed. Control The WW control is divided into 
several parts, as shown in Fig. 3. Central control 
The central 
control of the machine is the master source of control pulses. When necessary the central control allows one of the other 
controls to function. 
In general there is no overlapping of control operation; except 
for terminal equipment control, 
only one of the controls is in operation at any one time. 
Storage control 
Storage control generates 
the sequence of pulses and gates that operate the storage tubes. Central control instructs 
the storage control either to read or to write. 
Arithmetic control Arithmetic control 
carries out the 
details of the more complex 
arithmetic operations such as multiplication and division. The 
140 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
CONTROL r--------I I CENTRAL CONTROL I I ARITHMETIC TERMINAL EOUIP. Fig. 3. Control. setup of these operations plus 
the complete controlling of the simpler operations 
such as addition are carried out by 
central control. Terminal equipment control Terminal equipment control generates 
the necessary control pulses, delay times, and interlocks for the various terminal equip- ment units. Program counter The program counter which keeps 
track of the address of the next order to be carried out is considered as part of control. This is an 11-binary counter with provision for 
reading to the 
bus. Most of the functions of these subsidiary controls could be 
combined with the central control. The major reason 
they are not 
is that they were 
designed at different times. The arithmetic 
ele- ment and its control came 
first, followed 
by central control. At the time central 
control was designed, the necessary characteristics of storage control were unknown. 
In fact, the machine was de- signed so that any parallel high-speed storage could 
be used. The form of terminal equipment control was also unknown 
at this time. Since flexibility 
was a prime 
specification, it was felt preferable to build separate flexible controls for the various parts of the computer than to try 
to combine all the needed flexibility in one 
central control. In a new machine we 
would attempt to 
combine control 
func- tions where possible, hoping to have enough prior knowledge about component needs to eliminate 
subsidiary controls com- pletely. We would 
still insist on 
a large 
degree of control flexibility. Muster clock 
The master clock consists 
of an oscillator, pulse 
shaper and divider 
that generate 1- and 2-megacycle clock pulses, 
and a 
clock pulse 
control that distributes these clock pulses 
to the 
various controls in the machine. It is this unit that determines which 
of the sub- sidiary controls actually 
is controlling the machine. This unit also stops and starts the 
machine and provides for 
push-button opera- 
tion. Operation control The operation control, see Fig. 
4, was designed for maximum 
flexibility and minimum number of operationdigits, and, conse- quently, minimum register 
length. It is of the completely decoding 
type. The operation switch 
is a 32-position crystal matrix switch 
that receives the 5-bit instruction 
from the bus and in 
turn selects one 
of 32 output lines corresponding 
to the 32 built-in operations. 
There are 
120 gate tubes on the output 
of the operation control. 
Pulses on 
the 120 output lines go to the 
gate drivers, pulse drivers, 
and control 
flip-flops all over 
the machine; 120 is a generous number. The suppressors of these gate tubes are connected 
to vertical wires that cross the 32 output lines from 
the operation switch. Crystals 
are inserted at the desired junctions 
to turn on those gate tubes that are to be 
used for 
any operation. I 32-P:TlON SWITCH IIII Fig. 4. Operation control. 

Chapter 6 1 The Whirlwind I computer 141 The time 
pulse distributor consists of an 8-position switch driven from a three binary-digit counter. 
Clock pulses 
at the input 
are distributed in sequence 
on the eight output lines. The control grids of the output gate 
tubes are connected 
to these timing 
lines. The output 
of the operation control 
is thus 120 control lines on 
each of which can appear 
a sequence 
of pulses for 
any combination 
of orders at any combination 
of times. Central control 
The Central 
Control of the machine is shown in Fig. 5. The control switch is in the foreground with the operation matrix 
to the right. Electrostatic storage 
The electrostatic storage 
shown in Fig. 
6 consists of two banks of 16 storage tubes each. 
There is a pair 
of 32-position decoders Fig. 6. View of electrostatic storage. 
set up by address digits 
read in from the bus. There is a storage 
control that generates the sequence of pulses needed to operate the gate generators, et cetera. A radio frequency 
pulser generates a high power 10-megacycle pulse for 
readout. Each digit column contains, 
besides the storage tubes, 
write plus and write minus gate generators 
and a signal plate gate 
generator for each tube. 
Ten-megacycle grid pulses 
are used for 
readout in order 
to get the required discrimination 
between the 
fractional volt readout pulses and the 100-volt signal 
plate gates. For each 
storage tube there 
is a 10-megacycle amplifier, phase- 
sensitive detector and gate 
tube, feeding into the program register. 
The program register 
is used for 
communicating with 
the storage tubes. Information read 
out of the tubes appears in the program register. Information to be written into 
the tubes must be placed Fig. 5. View of central control. LJ h, r \ (.L, \-v 4 in the program register. 
142 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
m MULTIPLICAND w CLOCK I PULSE s Fig. 7. Arithmetic element. Arithmetic element 
The arithmetic element, 
see Fig. 
7, consists of three registers, a 
counter, and a control. The first register is an accumulator (AC) which actually consists of a partial-sum or adding register and a carry register. The accu- mulator holds the product during 
multiplication. The second or A-register 
holds the multiplicand during 
multi- plication. All numbers entering the 
arithmetic element 
do so through AR. The third or B-register holds 
the multiplier during multiplica- tion. The accnmulator and B-register shift 
right or left. A high-speed carry is provided for addition. Subtraction 
is by 9™s complement and end-around-carry. Multiplication 
is by successive additions, division by successive subtractions, and shift orders provide 
for shifting right 
or left by 
an arbitrary 
number of steps, with or without roundoff. The arithmetic element 
is straightforward except 
for a 
few special orders 
and the 
high speed at which it operates. Addition takes 3 microseconds complete with 
carry; multiplication, 
16 microseconds average including 
sign correction. In Fig. 
8 are shown several digits of the arithmetic element. 
The large panels are accumulator digits. Above the accumulator is the B-register, below it the A-register. Test control Test control, shown in Fig. 9, is used at present both for operating and for trouble shooting the computer. The control includes: 
Power supply control and meters. Neon indicators for all flip-flops in the machine. Switches for setting up special conditions. 
Manual intervention switches. Oscilloscopes for 
viewing wave 
forms. A probe and amplifier system allows 
viewing any wave form in the computer on one scope 
at test control. Test equipment to provide synchronizing, 
stop, or delay 
pulses at any step of any order of a program, allowing 
viewing wave forms on 
the fly anywhere in the machine. An important part of the test facilities 
is the test storage, 
a group of 32 toggle-switch registers plus five flip-flop registers that can be inserted in place 
of any five of the toggle-switch registers. 
This storage 
has proved invaluable 
not only for 
testing control and Fig. 8. View of arithmetic element. . id ~\L.-I[ r c( 
Chapter 6 I The Whirlwind I computer 143 Fig. 9. View of test control. - Lr w w\pG arithmetic element 
before electrostatic storage 
was available but also for 
testing electrostatic storage itself. When not in 
use for test purposes test storage earns its 
keep as part of the terminal equipment system. The toggle-switches hold 
a standard read-in program; the flip-flop registers are used as in-out registers for 
special purposes. Checking Logical checking facilities built 
into WWI are rather 
inconsistent. A complete bus transfer checking 
system has 
been provided, dupli- 
cate checking of some terminal equipment is permitted, but little else is thoroughly checked. We felt that it was worthwhile to thoroughly check 
some substantial portion 
of the machine. This portion would then serve as 
a prototype 
for studying the tube 
circuitry used throughout the machine. We did not 
feel it was worthwhile to check all 
the machine, a procedure 
that requires a great deal 
of added equipment 
and logical complexity plus 
a substantial loss in computing speed. Operating experience has shown 
us that it 
is not worthwhile 
to provide 
detailed logical checking of a machine. 
In a new 
machine we 
would leave out the 
transfer checking. The amount of information and security given by 
the detailed 
checking system is not enough 
to warrant the 
expense of building and maintaining 
it. This decision 
is based on the expectation that a computing machine should operate 95 per cent of total time 
or better and that the 
average time between 
random failures should 
be of the order of 5 to 10 hours or 
approximately IO9 operations. In our opinion the way to achieve the extremely high 
reliability needed in 
some real-time control 
problems is to provide three or more identical but distinct machines, thus obtaining error correc- 
tion as well as 
detection, plus such 
features as standby, safety, and damage control. 
Even so the failure probability 
of each machine 
must be kept low by 
proper design, marginal 
checking, and pre- 
ventive maintenance. 
Extremely high 
reliability means a reliability 
far beyond that achieved in 
existing machines 
and not conveniently represented 
as a per 
cent. Consider a system consisting 
of three machines, each 
operable 98 per cent 
of the time and each 
averaging 10 hours between random 
errors. One machine 
will be out of operation y2 hour per 
day. Two machines will 
be out of operation '/4 hour per month. All three machines will 
be out 
of operation 4 minutes per 
year. Furthermore undetected 
random errors 
might occur on the aver- age of once a 
year. Such 
reliability is needed in some systems. 
Our decision to omit detailed checking does not extend to checking devices intended to detect 
programming errors. Devices 
to check for overflow from the arithmetic 
element or for non- 
existent order configurations are necessary. Programmers make 
many mistakes. Techniques for 
dealing with programming errors 
are very important and need 
future development. Terminal equipment At the present time, Whirlwind 
is using the following terminal equipment: A photoelectric paper tape reader Mechanical paper 
tape readers and punches 
Mechanical typewriters 
Oscilloscope displays 
5 to 16 inches in 
diameter with 
phos- phors of various persistencies 
including a computer-con- 
trolled scope camera Inputs from various analogue 
equipments needed for control studies Outputs to analogue equipment To be added during 
the next year: 1 Magnetic Tape (units 
by Raytheon). One 
such unit is now being integrated with machine. 
Magnetic drums 
(units by Engineering Research Associates, 
Inc.). Many more analogue 
inputs and outputs. 2 3 
144 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
This great complexity of terminal equipment requires a flexible switching system. There is a single in-out register 
(IOR) through 
which most 
of the data passes. There is a switch 
which is set up by an order to select the desired piece 
of terminal equipment. Other 
orders put data into 
IOR or remove 
data from IOR. 
The in-out control provides the necessary control pulses to go with each 
type of equipment. In general the computer continues to run during 
terminal equipment wait times; suitable interlocks are provided to prevent trouble. 
This complete equipment 
has not yet been 
fully installed. References whirlwind: EverR51; SerrR62; TaylN51. EdSAC: SamuA57; WilkM56. 
Chapter 6 I The Whirlwind I computer 145 APPENDIX 1 WHIRLWIND I INSTRUCTION CODE' Note: In operations mr, mh, dv, sir, 
srr, srh, sf, the 
C(BR) is assumed 
to be the magnitude 
of the least significant part of AC + BR. For 
the ab and dm oper- ations, the BR is treated just 
as any storage register. 
Whirlwind I Instruction Code came from "Comprehensive System Manual, 
A System of Automatic Coding for 
the Whirlwind Computer," published 
by Massa. chusetts Institute of Technology, Digital Computer Laboratory, Cambridge, Mass. 

Some aspects of the logical design of a control computer: 
a case study1 
R. L. Alonso / H. Blair-Smith / A. L. Hopkins Summary Some logical 
aspects of a digital 
computer for a space vehicle are described, and the evolution of its logical design 
is traced. The intended application and the characteristics of the computer™s ancestry form a frame- work for the design, which 
is filled in by accumulation 
of the many decisions made by its designers. 
This paper deals with the choice of word length, 
number system, instruction set, memory addressing, and problems of multi- ple precision arithmetic. The computer is a parallel, 
single address machine with more 
than 10,000 words of 16 bits. Such a 
short word length 
yields advantages 
of efficient storage and speed, but at 
a cost 
of logical complexity in 
connection with addressing, instriiction selection, 
and multiple-precision arithmetic. 1. Introduction In this paper we attempt to record the reasoning that led us to certain choices in the logical design 
of the Apollo Guidance Com- 
puter (AGC). The AGC is an onboard computer for one of the forthcoming manned space 
projects, a 
fact which 
is relevant pri- 
marily because it puts a high premium on 
economy and modularity of equipment, and results in much specialized input and output 
circuitry. The AGC, however, was designed in the tradition of parallel, single-address general-purpose 
computers, and thus has many properties familiar to computer designers [Richards, 
1955J, [Beckman et al., 19611. We will 
describe some of the problems of designing a short word 
length computer, and 
the way in which the word length 
influenced some of its characteristics. These 
characteristics are number 
system, addressing system, 
order code, 
and multiple precision arithmetic. A secondary purpose 
for this paper is to indicate the role of evolution in the AGC™s design. Several smaller 
computers with 
about the same structure had been designed previously. 
One of these, MOD 
3C, was to have been the Apollo Guidance Computer, but a decision 
to change the means of electrical implementation 
(from core-transistors 
to integrated circuits) afforded the logical designers an unusual second 
chance. It is our belief, as practitioners of logical design, 
that designers, computers and their applications 
evolve in time; that a frequent ‚IEEE Trans., EC-12 (6), 687-697 (December, 1963) reason for 
a given choice is that it is the same as, or 
the logical next step to, a 
choice that was made once 
before. A recent conference on airborne computers 
[Proc. Con.. Space- borne Computer 
Eng., Anaheim, Calif., 
Oct. 30-31, 19621 affords a view 
of how other designers treated two 
specific problems: word 
length and number system. All of these computers have 
word lengths of the order of 22 to 28 bits, and use a two™s complement system. The AGC stands in contrast in these two 
respects, and our reasons for 
choosing as we 
did may therefore be of interest as a minority view. 
2. Description of the AGC The AGC has 
three principal sections. The first is a memory, the fixed (read only) portion of which has 24,576 words, and the 
erasable portion 
of which has 
1024 words. The next section may be called the central section; 
it includes, besides 
an adder and a parity computing register, an instruction decoder 
(So), a memory 
address decoder (S), and a number of addressable registers 
with either special features 
or special use. The third section is the sequence generator which includes a portion for generating various microprograms and a portion 
for processing various 
interrupting requests. The backbone of the AGC is the set of 16 write busses; these are the means for transferring information between the 
various registers shown 
in Fig. 
1. The arrowheads to and 
from the various registers show the possible directions of information flow. In Fig. 1, the data 
paths are 
shown as 
solid lines; the control paths are shown as 
broken lines. M~~oTY: fired and erasable 
The Fixed Memory is made of wired-in ﬁropesﬂ 
[Alonso and Laning, 19601, which are compact and reliable devices. The num- ber of bits so wired is about 4 x lo5. The cycle time is 12 pec. The erasable memory is a coincident current system with the same cycle time as the fixed memory. Instructions can 
address registers in either memory, and can be 
stored in either memory. 146 
Chapter 7 I Some aspects 
of the logical design 
of a control computer: a case study 
147 T1 4 OUT - SEQUENCE GENERATOR -Lysy INSTRUCTION I MICROPROGRAM - -, I ARITHMETIC UNIT - I ADDER USES L --_------____ _J --- Control paths 
- Data paths Fig. 1. AGC block diagram. The only logical 
difference between the two 
memories is the inability to change the contents of the fixed part by program 
steps. Each word in memory is 16 bits long (15 data bits and an 
odd parity bit). Data 
words are stored as signed 
14 bit words using 
a one™s complement convention. Instruction 
words consist 
of 3 order code bits 
and 12 address code bits. 
The contents of the address register S uniquely determine the address of the memory word only 
if the address lies 
between octal 0000 and octal 
5777, inclusive. 
If the address lies between octal 6000 and octal 7777, inclusive, the address in S is modified by the contents of the memory bank register 
MB. The modification con- 
sists in adding 
some integral multiplies of octal 2000 to the address in S before it is interpreted by the decoding circuitry. 
The memory bank register MB is itself addressable; its 
address, however, is not modified by its own contents. Transfers in and out 
of memory are made by way of a memory local register 
6. For certain specific addresses, 
the word being transferred into G is not sent directly, 
but is modified by 
a special 
gating network. 
The transformations on the word sent to G are right shift, left shift, right 
cycle, and left cycle. 
Central section 
The middle part of Fig. 1 shows the central 
section in block form. 
It consists of the address register S and the 
memory bank register 
MB both of which were mentioned above. 
There is also a block of addressable registers called ﬁcentral and 
special registers,ﬂ which will 
be discussed later, an arithmetic unit, and an 
instruc- tion decoder 
register SQ. The arithmetic unit 
has a parity 
generating register and an adder. These two 
registers are not 
explicitly addressable. 
The SQ register bears the same relation to instructions as the S register bears to memory locations; 
neither S nor SQ are ex- plicitly addressable. The central 
and special 
registers are A, Q, 2, LP, and a set 
of input and output 
registers. Their properties are shown in Table 1. Sequence generator The sequence generator 
provides the basic memory 
timing, the sequences of control pulses (microprograms) which 
constitute an 
instruction, the priority interrupt circuitry, and a number 
of scal- ing networks which 
provide various pulse 
frequencies used by 
the computer and 
the rest of the navigation system. Instructions are arranged so as to last an integral number 
of memory cycles. 
The list of 11 instructions is treated in detail in Sec. 6. In addition to 
these there are a number 
of ﬁinvoluntaryﬂ sequences, not under normal 
program control, which may 
break into the normal sequence 
of instructions; these 
are triggered either by external events, 
or by 
certain overflows within the AGC, and 
148 Part 2 1 The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
Table 1 Special and 
central registers Octal Register (s) address Purpose and/or properties A 0000 Central accumulator. 
Most instructions refer to A. 0 0001 If a transfer of control (TC) occurred at 
L, (Q)=L+ 1. Z 0002 Program counter. Contains L + 1, where L is the address of the instruction presently 
being executed. LP 0003 Low product register. 
This register modifies words written into 
it by shifting them in a special way. IN OUT Several registers which are used for sampling either external lines, or internal computer 
conditions such 
as time or alarms. Several output registers 
whose bits 
control switches, networks, 
and displays. may be divided into two categories: counter incrementing 
and program interruption. Counter incrementing may take place between any two 
mem- ory cycles. 
External requests 
for incrementing a counter are 
stored in a counter priority circuit. 
At the end 
of every memory cycle 
a test is made to see if any incrementing 
requests exist. If not, the next normal memory cycle 
is executed directly, with no time 
between cycles. If a request is present, an incrementing 
memory cycle is executed. Each ﬁcounterﬂ 
is a specific location in erasable memory. The incrementing cycle consists of reading out the word stored in 
the counter register, incrementing it 
(positively or nega- 
tively), or shifting 
it, and storing the results back in the register of origin. All outstanding counter incrementing 
requests are proc- essed before proceeding 
to the 
next normal memory cycle. This 
type of interrupt provides for 
asynchronous incremental or serial 
entry of information into the 
working erasable memory. 
The pro- gram steps may 
refer directly to a ﬁcounterﬂ to obtain the desired information and do not have 
to refer to input buffers. Overflows from one counter 
may be used as the input to another. A further property of this system is that the time available for normal pro- gram steps is reduced linearly by the 
amount of counter activity 
present at any given time. Program interruption occurs between normal program steps 
rather than between memory cycles. 
An interruption consists of storing the contents of the program counter and transferring con- trol to a fixed location. Each interrupt line has a 
different location 
associated with it. Interrupting programs may not 
be interrupted, but interrupt 
requests are not lost, and are 
processed as soon as 
the earlier interrupted program is resumed. Calling 
the resume sequence, which restores 
the program counter, is initiated by referencing a special address. 3. Word length In an airborne computer, granted 
the initial choice 
of parallel transfer of words within it, it is highly desirable 
to minimize the word length. This 
is because memory 
sense amplifiers, 
being high- 
gain class A amplifiers, are considerably harder to operate with wide margins (of temperature, voltages, input signal) than, say, the circuits made up of NOR gates. It is best to have 
as few of these as possible. 
Furthermore, the number of ferrite-plane inhibit 
drivers equals 
the number of bits in 
a word in this 
case. Similarly, 
the time required 
for a 
carry to propagate in 
a parallel adder is proportional to the word length, and in the present case, this factor 
could be expected to 
affect the microprogramming of instructions. The initial intent, then, 
was to have as short a word length as possible. Another initial choice 
is that the AGC should be a ﬁcommon storageﬂ machine, which means 
that instructions may 
be executed from erasable memory 
as well as 
from fixed memory, and that data 
(obviously constants, in the case of fixed memory) may 
be stored in either memory. This 
in turn means that the word sizes of both types of memory must be compatible in 
some sense; for 
the AGC, the easiest form 
of compatibility is to have equal 
word lengths. 
So-called ﬁseparate storageﬂ solutions which allow different word 
lengths for instructions and data can be 
made to work [Walend- 
ziewicz, 19621 but they have 
a drawback in 
that three 
memories are then 
required: a data memory (erasable), 
and two fixed memo- ries, one for 
instructions and one for constants. In 
addition, we have found 
that separate storage machines 
are more awkward to 
program, and use memory less efficiently, than common storage 
machines. There are three principal 
factors in the choice of word length. These are: 
1 Precision desired in the representation of navigational vari- ables. Range of the input variables which are entered 
serially and counted. 2 
Chapter 7 I Some aspects 
of the logical design 
of a control computer: a 
case study 
149 3 Instruction word format. Division of instruction words into two fields, one for operation code and one 
for address. 
As a start, the choice of word length (15 bits) for two previous machines in this series was 
kept in mind as a satisfactory word 
length from the point of view of mechanization; i.e., the number of sense amplifiers, 
inhibit drivers, the carry propagation time, 
etc., were all considered satisfactory. 
The act of ﬁchoosingﬂ word 
length really meant whether or not to alter the word length, at the time of change from MOD 
3C to the 
AGC, and in 
particular whether 
to increase it. The 
influence of the three 
principal factors will 
be taken up in turn. Precision of data words The data words used 
in the AGC may be divided 
roughly into two classes: data words used 
in elaborate navigational computa- tions, and data words used 
in the control of various appliances in the system. Initial estimates 
of the precision required by 
the first class ranged from 27 
to 32 bits, 0(108ﬂ). The second class of variables could almost always 
be represented with 15 bits. The fact that navigational variables require about twice the desired 15-bit word 
length means that there is not much advantage to word sizes 
between 15 and 28 bits, 
as far as precision 
of represen- tation of variables is concerned, because 
double-precision numbers must be used in any event. Because of the doubly signed number representation for double-precision words, 
the equivalent word length is 29 bits (including sign), 
rather than 
30, for a basic word 
length of 15 bits. The initial estimates 
for the proportion of 15-bit vs 29-bit quantities to be 
stored in both 
fixed and erasable memories indi- cated the overwhelming preponderance 
of the former. It was also estimated that a significant portion of the computing had to 
do with control, 
telemetry and 
display activities, all of which can 
be handled more economically 
with short words. A short word length allows faster and more efficient use 
of erasable storage 
because it reduces fractional word operations, such 
as packing and editing; 
it also means 
a more efficient 
encoding of small integers. Range of input variables 
As a control computer, the AGC must make 
analog-to-digital conversions, many 
of which are of shaft angles. 
Two principal forms of conversion exist: 
one renders a 
whole number, the other 
produces a train of pulses which must 
be counted to yield the desired number. 
The latter type of conversion is employed by the AGC, using 
the counter 
incrementing feature. 
When the number of bits of precision required is greater than 
the computer™s word length, the effective length of the counter 
must be extended into a 
second register, either by programmed 
scanning of the counter register, or by using 
a second 
counter register to receive the overflows of the first. Whether programmed scanning is feasible depends largely 
on how frequently this scan- 
ning must 
be done. 
The cost of using an extra counter register is directly measured in terms 
of the priority circuit associated with it. In the 
AGC, the equipment saved by reducing 
the word length below 15 bits would probably not match 
the additional expense incurred in 
double-precision extension of many input variables. The question is academic, however, since a 
lower bound on the word length is effectively placed by 
the format of the instruction word. Instruction word format An initial decision was made that instructions would consist 
of an operation code and a 
single address. 
The straightforward choices of packing one 
or two such instructions per 
word were the only ones seriously 
considered, although 
other schemes, such 
as packing one and a 
half instructions per 
word, are possible [England, 19621. The previous computers MOD 3s and MOD 3C had a 3-bit 
field for operation codes and a 12-bit field for addresses, 
to accommodate 
their 8 instruction order 
codes and 4096 words of memory. In 
the initial core-transistor version 
of the AGC (i.e., MOD 3C), the 8 instruction order 
codes were in reality 
augmented by the various special registers provided, such as shift 
right, cycle left, edit, so that a transfer in and out 
of one of these registers 
would accomplish 
actions normally specified 
by the order code 
(see Sec. 
6). These registers 
were considered 
to be 
more economical 
than the corresponding instruction decoding and control 
pulse sequence generation. Hence 
the 3 bits assigned to the order code 
were considered 
adequate, albeit not 
generous. Furthermore, as will be seen, it is possible to use an indexing instruction so as to increase to eleven the number of explicit order codes provided 
for. The address field of 12 bits presented a different problem. 
At the time of the design of MOD 3C we estimated 
that 4000 words 
would satisfy 
the storage requirements. 
By the time 
of redesign it was clear that the requirement was for 
lo5 words, or more, 
and the question then became whether the 
proposed extension of the address field by a bank 
register (see Sec. 7) 
was more economical 
than the addition of 2 bits to 
the word length. For 
reasons of modularity of equipment, adding 2 more bits to the 
word length would result 
in adding 2 
more bits 
to all the central and 
special registers, which 
amounts to increasing the size of the nonmemory portion of the AGC by 10 per 
cent. 
150 Pari 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
In summary, the 15-bit word 
length seemed practical enough so that the 
additional cost of extra bits 
in terms of size, weight, and reliability did not 
seem warranted. A 14-bit word 
length was thought impractical because 
of the problems with certain input 
variables, and it would further restrict the already somewhat 
cramped instruction word format. Word lengths of 17 or 18 bits would result 
in certain conceptual 
simplicities in the decoding of instructions and 
addresses, but would not help in the represen- tation of navigational variables. These 
require 28 bits, and so they must be represented to double precision in any event. 
4. Number representation 
Signed numbers In the 
absence of the need to represent numbers 
of both signs, the discussion of number representation 
would not extend 
beyond the fact that numbers in 
AGC are expressed to base two. But 
the accommodation of both positive and negative numbers 
requires that the 
logical designer choose 
among at least three possible forms 
of binary arithmetic. These three principal alternatives 
are: (1) one™s complement, (2) two™s complement, and (3) sign and magni- tude [Richards, 19551. In one™s complement arithmetic, the 
sign of a number 
is re- versed by complementing 
every digit, and ﬁend around 
carryﬂ is required in addition of two numbers. In two™s 
complement arithmetic, sign reversal is effected by complementing each bit 
and adding a 
low order one, or some equivalent operation. 
Sign and magnitude representation 
is typically used where direct human 
interrogation of memory is desired, as in ﬁpost- 
mortemﬂ memory dumps, for example. 
The addition of numbers of opposite sign requires either one™s or two™s 
complementation or comparison 
of magnitude, and sometimes may 
use both. No advantage is offered in efficiency with the possible exception 
of sign changing, which only 
requires changing 
the sign bit. A disad- vantage is engendered in magnetic core 
logic machines 
by the extra equipment needed for subtraction or conditional recomple- mentation. The one™s complement notation 
has the advantage of having easy sign reversal, which 
is equivalent to Boolean complementa- tion; hence a single machine instruction 
performs both functions. Zero is ambiguously represented by all 
zero™s and by all one™s, 
so that the number of numerical states in 
an n-bit word 
is 2ﬂ - 1. Two™s complement arithmetic is advantageous where 
end around carry 
is difficult to mechanize, 
as is particularly true in serial computers. 
An n-bit word has 
2ﬂ states, which is desirable for input conversions from such devices as 
pattern generators, geared encoders, or 
binary scalers. Sign reversal is awkward, how- ever, since a full addition is required in 
the process. The choice in 
the case of the AGC was to use one™s 
complement arithmetic in general 
processing, and two™s complements for cer- tain input angle conversions. Since 
the only arithmetic done in 
the latter 
case is the addition of plus or minus 
one, the two™s complement facility 
is provided simply by suppressing end around 
carry and 
using the proper representation 
of minus one. 
The latter 
is stored as a fixed constant, so that no sign reversal 
is required. Modified one™s complement system In a standard one™s complement adder, overflow is detected by examining carries into and 
out of the sign position. 
These overflow indications must be ﬁcaught on the flyﬂ and stored separately 
if they are 
to be 
acted upon later. The number system adopted in the AGC has the advantage of being a 
one™s complement system with the additional feature of having a 
static indication of over- flow. The implementation of the method depends 
on the AGC™s not using a parity 
bit in most central registers. Because 
of certain modular advantages, 16, rather than 
15, columns are available in all of the central registers, including the adder. Where the 
parity bit is not required, 
the extra bit position is used as an 
extra column. 
The virtue of the 16-bit adder is that the 
overflow of a 15-bit sum 
is readily detectable upon examination 
of the two high order bits 
of the sum (see Fig. 
2). If both of these bits are the 
same, there is no overflow. 
If they are different, 
overflow has 
occurred with the sign of the highest order bit. The interface between the 
16-bit adder and the 
15-bit memory 
is arranged so that the 
sign bit of a word coming from memory 
enters both 
of the two high order adder columns. These are de- noted S, and SI since they both have the significance of sign bits. When a word is transferred from the accumulator A to memory, only one of these two signs can be stored. Our choice 
was to store the S, bit, which is the standard one™s complement sign except in the event of overflow, in which case it is the sign of the two operands. This preservation of sign on 
overflow is an important 
asset in dealing with carries 
between component words of multi- ple-precision numbers (see Sec. 
5). In a standard 
one™s complement system, a series of additions may result 
in subtotals which overflow, 
yet still produce a 
valid sum so long as 
the total 
does not exceed the capacity of one word. In a modified one™s complement system, however, where 
sign is preserved on overflow, this 
is no longer true; and the total may depend on the order in 
which the numbers are added; 
this is not a serious drawback, but it 
must be accounted for in all 
phases of logical design 
and programming. 
Chapter 7 I Some aspects 
of the logical design 
of a control computer: a case study 
151 - ~- MODIFIED S TANDAR D - SI4321 3 2 1 EXAMPLE 1: Both operands positive; Sum positive, 
no overflow. Identical 
results 0 0 0 
0 1 000001 in both systems. 00011 000011 00100 000100 
EXAMPLE 2: Both operands positive; positive overflow. 
Standard result 
is nega- 0 1 0 0 1 tive; Modified result is positive using 
Sz as sign of the answer. 0 1 0 1 1 
Positive overflow indicated by SI Sz. 10100 EXAMPLE 3: Both operands negative; Sum negative, 
no overflow. End around 1 1 1 1 
0 carry occurs. Identical results in both systems using either SI or Sp 1 1 1 0 0 11010 as the sign of the answer. 1 carry 11011 001001 001011 010100 111110 111100 111010 111011 1 carry EXAMPLE 4: Both operands negative; negative 
overflow. Standard result 
is posi- 1 0 1 1 0 110110 tive; modified result is negative using S2 as the sign of the answer. 1 0 1 0 0 110100 Negative overflow indicated by SI . Sz. 01010 101010 1 carry 1 carry 01011 101011 
EXAMPLE 5: Operands have opposite sign; 
Sum positive. Identical results i.1 both 1 1 
1 1 0 111110 systems. 00011 000011 00001 000001 00010 000010 
1 carry 1 carry EXAMPLE 6: Operands have opposite sign; sum 
negative. Identical results in 1 1 1 
0 0 111100 both systems. 00001 000001 11101 111101 Fig. 2. Illustrative example 
of properties of modified one™s complement system. 
5. Multiple precision arithmetic A short word 
computer can 
be effective only 
if the multiple- precision routines are efficient corresponding to their share of the computer™s word load. 
In the AGC™s application there is enough use for 
multiple-precision arithmetic to warrant consideration in the choice of number system and in the organization of the instruc- tion set. Although the limited number 
of order codes prohibits multiple-precision instructions, special features 
are associated with the conventional instructions 
to expedite 
multiple-precision opera- 
tions. Independent sign representation A variety of formats for multiple-precision 
representation are possible; probably the most common of these is the identical sign representation in which the sign bits of all component words agree. The method used in the AGC allows the signs of the components to be different. Independent signs arise naturally in multiple-precision addition 
and subtraction, and the identical sign representation is costly because sign reconciliation is required after 
every operation. 
For example, ( + 6, + 4) + ( - 4, - 6) = ( + 2, - 2), a mixed sign 
repre- sentation of (+ 1, + 8). Since addition and subtraction are the 
most frequent operations, it is economical to store the result as it occurs and reconcile signs only when necessary. When overflow occurs in the addition of two components, a one with the sign of the overflow is 
carried to 
the addition of the next higher components. 
The sum that overflowed retains the sign of its operands. This 
overflow is 
termed an interflow to distinguish it from an overflow 
152 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with 
one address per instruction that arises when the maximum multiple-precision number is ex- 
ceeded. The independent sign method has a pitfall arising 
from the fact that every number has two representations, either one of which may occur 
as a sum. There are 
some numbers 
for which one of the representations exceeds 
the capacity of the most significant 
component. The overflow is false in the sense that the double- precision capacity is not exceeded, only the single word capacity of the upper 
component. Sign reconciliation can be used in this case to yield an acceptable 
representation. This problem can be avoided if all numbers are scaled so that none are large enough 
to produce false overflows. Such a restriction is not necessary, however, since 
the false overflow condition arises infrequently and can be detected at no expense in time. The net cost of reconcilia- tion is therefore very 
low. Multiplication and division 
For triple and higher orders of precision, multiplication and divi- sion become excessively complex, unlike addition 
and subtraction where the complexity is only linear with 
the order of precision. The algorithm for double-precision multiplication 
is directly applicable to 
numbers in the independent sign notation. False 
overflow does not 
arise, and the treatment 
of interflow is simplified by an automatic counter 
register which 
is incremented when 
overflow occurs during an add instruction. The sign of the counter increment is the same as the sign of the overflow; and the incre- ment takes place while one of the product components of next higher order is stored in that counter. Double-precision division is exceptional in that the independ- ent sign notation may not be used; both operands must be made positive in identical 
sign form, and the 
divisor normalized so that the left-most nonsign 
bit is one. Triple precision A few triple-precision 
quantities are used in the AGC. These are added and 
subtracted using independent sign notation with inter- 
flow and overflow features the same as those used for 
double- precision arithmetic. 6. Instruction set Basic design 
criteria The implicit requirements 
for any von Neumann-type machine demand that facilities exist for: Storing in memory Negating (complementing) 
Combining two operands (e.g., addition) Address modification 
(more generally, executing 
as an in- struction the result of arithmetic processing) Normal sequencing 
(to each location from which an instruc- tion can be executed there corresponds one location whose contents are the next instruction) Conditional sequence changing, or transfer 
of control Input output An instruction can, of course, provide several 
of these facilities. For instance, some 
computers have 
an instruction that subtracts the contents of a memory location 
from an accumulator and leaves the result in that memory location 
and in the accumulator; this instruction fulfills all of requirements 1-4 above. Requirement 
5 is met in 
a somewhat primitive 
manner if instructions can be executed from erasable memory, and is met elegantly by 
the use of index registers. Still 
another scheme, somewhat 
similar to one used in 
the Bendix G-20, is employed in the AGC. Requirement 6 is usually fulfilled by having 
an instruction location 
counter which contains 
the address of the next instruction to 
be executed, and is incremented by one when an instruction 
is fetched. Alter- 
natively, each instruction may include 
the address of the next instruction, as is 
often done in 
machines having 
drum memories. In the AGC, as 
in most short-word computers, 
the former method, with one 
single-address instruction per word, is clearly the simplest and cheapest. Requirement 
7 is generally met by examining 
a condition such as 
the sip of an accumulator and, 
if the condition is satisfied, 
either incrementing 
the instruction location counter (skipping), or using an address included 
in the instruction as that of the next instruction (conditional transfer 
of control). An uncon- ditional transfer 
of control is usual but not necessary, since any 
desired condition 
can be forced. Most machines have special 
input-output instructions to satisfy requirements 8 and 9. In the 
AGC, however, since 
input and output 
is through addressable registers, input is subsumed under fetching 
from memory, 
and output under storing in 
memory. Counter incrementing and pro- gram interruption aid 
these functions 
also. Further criteria The major goals in the AGC were efficient use of memory, reason- able speed of computing, potential 
for elegant programming, effi- 1 Fetching from memory 

Chapter 7 I Some aspects of the logical design 
of a control 
computer: a case study 153 cient multiple 
precision arithmetic, efficient processing 
of input and output, and reasonable simplicity of the sequence generator. 
The constraints affecting 
the order code 
as a whole were the word length, one™s complement notation, parallel 
data transfer, and 
the characteristics of the editing registers. The ground rules governing 
the choice of instructions arose from 
these goals and constraints. a Three bits of an instruction word are devoted to operation code. b Address modification must 
be convenient and efficient. c There should be a multiply instruction 
yielding a double 
length product. d Treatment of overflow on 
addition must be flexible. e A Boolean combinatorial operation 
should be available. f No instruction need 
be devoted to input, output, or 
shifting. This list 
is by no means 
complete, but gives a good indication of what kind of computer the AGC has to be. In the 
following para- graphs the ways in which 
the instructions fulfill the above require- ments are described. Details of the instruction set In the listing that follows, L denotes the location of the instruction; K denotes the data address contained in 
the instruction. Paren- 
theses mean ﬁcontent of,ﬂ and the leftward arrow 
means that the register named at the 
arrowhead is set to the quantity 
named to the right. L: TC K; Transfer Control 
QcL + 1; go to K. This is the primary method 
of transferring control 
to any stated location, and thus meets 
part of requirement 7. The setting 
of the return address register 
Q renders complex subroutines feasible. TC Q may be used to return from a subroutine (with 
no other TC™s) because the binary number 
ﬁL + 1ﬂ is the same as 
the binary word ﬁTC L + 1,ﬂ by virtue of the TC code being 
all zeros. TC A behaves like 
an ﬁexecuteﬂ instruction, executing whatever instruc- 
tion is in A, because Q follows A in the address pattern, see Table 1. L: CCS K; Count, Compare, and 
Skip If (K) > +0, A c (K) - 1, no skip; if (K) = +0, A t +0, skip to L + 2; if (K) < -0, At 1 - (K), skip to L + 3; if (K) = -0, A t +0, skip to L + 4. This instruction 
fulfills the remainder of requirement 7 and provides several features. 
It is clear that in a machine 
with a 3-bit 
operation code 
there should be only one code devoted entirely to 
branching, if at all possible. 
It is inefficient to program a zero test using only 
a sigmtesting code; 
it is even more inefficient 
to pro- 
gram a sign test using only 
a zero-testing code. 
This instruction was therefore designed to test both types of conditions simultane- 
ously. It has to be 
a four-way branch, and since there is only one address per instruction, it follows that CCS must be a skipping- type branch. 
The function of (K) delivered to A is the diminished absolute value (DABS). It serves two primary 
purposes: to do most 
of the work in generating 
an absolute value, 
and to apply a negative 
increment to the 
contents of a loop-counting register, 
so that CCS has some 
of the properties of TIX in the IBM 704. L: INDEX K; Index using K Use (L + 1) + (K) as the next instruction. In a short-word machine where 
there is no room in 
the instruc- tion word to specify indexing 
or indirect addressing, this code 
meets requirement 
5 in a 
way far 
superior to forming an instruction and placing it 
in A or in 
erasable memory for execution. 
INDEX operates on whole words, 
so that the 
operation code as well 
as the address may 
be modified. It may be used recursively 
(consider the implications of several INDEX™S 
in succession, assuming 
that no operation codes are modified). Finally, it permits more than 8 operation codes to be 
specified in 3 bits, since overflow 
of the indexing addition is detectable. L: XCH K; Exchange This instruction meets requirements 1, 2, and 8. When K is in fixed memory, it is simply 
a data-fetching (clear 
and add) 
code. Its use with erasable memory aids efficiency 
by reducing 
the need for temporary storage. 
XCH is also an important input 
instruction in a machine where addressable counters, 
incremented in 
response to external 
events, are an input medium, because a 
counter can 
be read out 
and reset (to zero or any desired value) 
by XCH 
with no chance of missing a count. 
(A)*(K). L: CS K; Clear and Subtract CS is the primary means of sign-changing and logical negation, and so fulfills requirements 1 and 3. Since there is no clear and add instruction, 
it is the usual operation for nondestructive readout 
of erasable memory in simple data transfers, that is, when no addition or other arithmetic 
is required. Usually the programming can be arranged so that complementing during transfer is accept- able; otherwise 
the CS can be 
followed by CS A before storing. L: TS K; Transfer to Storage K +(A); if (A) includes ? overflow, A c 51, skip to L + 2. A c -(K). 
154 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with 
one address per instruction 
This instruction is the primary means of transfers to memory and output, satisfying requirements 2 and 9. It is also the most convenient method 
of testing for overflow. Since 
A and 
the other central registers have 
two sign positions, 
overflow indication is retained in a 
central register. TS always stores 
(A) and tests whether overflow is present. If K is in erasable 
memory and is not a 
central register, the lower-order sign bit SI is not transmitted; 
this is the process or overflow correction. If positive overflow indication is present in 
A, TS skips over 
the next instruction and sets A 
t +1 (+1 denotes octal 
000001); if negative overflow is present, TS skips over 
the next instruction and sets 
A t - 1 (- 1 denotes octal 177776); otherwise (A) are unchanged. The sequence TS K XCH ZERO (ZERO in fixed memory) suffices to store in 
K an overflow-corrected word of a multiple- 
precision sum 
and leave in A 
the interflow to the 
next higher-order part. TS A skips if either type 
of overflow is present, but leaves all 16 bits 
of (A) unchanged. Finally, a 
computed transfer of control may be achieved by 
TS Z because Z is the program counter; only the low-order 12 bits of (A) are significant, being the address of the instruction to 
which control is transferred. Overflow in (A) in this case does 
not affect the transfer but sets A t 51. L: AD K; Add A +(A) + (K); if the final (A) includes 
2 overflow, OVCTR t (OVCTR) tl. Addition is the most frequently used combinatorial operation 
(requirement 4). The property of OVCTR is used chiefly 
in devel- oping double-precision products and quotients, partly because 
the additions in 
these processes 
are less susceptible to false overflow 
than are multiple-precision additions. 
L: MASK K; Mask This is the only combinatorial Boolean instruction, and may A t (A) n (K). be used with CS to generate any Boolean function. Ex tracodes The AGC instruction set 
was carried over in large part from its ancestor, MOD 3C [Alonso 
et al., 19611. All instructions of MOD 3C were retained in the AGC, modifications 
and additions being 
adopted where a substantial increase in computing power could 
be obtained at small cost. 
The MOD 3C instruction set 
was like the one described above 
for the AGC with two major exceptions: 
first, instead of a mask instruction, MOD 3C 
had a multiply 
in- struction. Second, the transfer to storage instruction did not in- 
clude the property 
of skipping on overflow, although it 
did have properties which aided masking. After the design of MOD 3C was completed, it was discovered that the INDEX instruction could 
be used to expand the instruc- tion set beyond eight instructions by producing 
overflow in the instruction word following 
the INDEX. For example, the addition of octal 47777 to the instruction word ﬁCS 
Kﬂ in the course of an INDEX instruction 
will cause 
negative overflow, producing MP K, a multiply instruction with operand address 
K. In order to implement the extracodes in the AGC, it was necessary to provide a path from the high-order 4 bits 
of the adder to the unaddressable sequence selection register 
SQ. Part of this path is the unaddressable buffer register 
B; these requirements 
helped to suggest the benefits of retaining two sign bit positions in all the central 
registers. In principle, eight additional instruction 
codes can be obtained by causing overflow, but we did not feel 
obliged to use them all. 
Because every extracode 
must be indexed, the instructions chosen for this 
class had two properties 
to some degree: they 
are normally indexed, or they take 
long enough 
so that the 
cost of indexing without address modification 
is small. All the extracodes are com- binatorial, and therefore 
relate to requirement 4. L: MP K; Multiply A t upper part, 
LP t lower part, of (A) - (K); the two words of the product agree in 
sign, which 
is determined strictly by 
the sign bits of the operands. Experience with MOD 3C showed that it 
was worthwhile making a completely algebraic, 
self-contained multiply instruction, 
especially in doing double-precision 
multiplication whose oper- ands have 
independent signs. The AGC multiply is much faster than that of MOD 3C, being limited by 
adder carry propagation 
time rather than 
core-switching time. 
L: DV K; Divide A t quotient, Q t - 1 remainder 1, of (A)/(K); LP t nonzero number with 
the sign of the quotient. 
Many facets of AGC design originally 
adopted for other reasons combined to make a divide instruction 
inexpensive. The foremost of these is the nature 
of the editing registers, which 
are in the standard erasable 
memory and have no special wiring. The special properties of these registers are supplied by a shift or cycle of the word being written into the 
memory local register 
G, when the address of an editing register is selected. The central 
loop of DV selects such an address and inhibits 
memory operations, so that all the left shifts required in 
division are accomplished in 
the G register while 
the editing register itself remains unchanged. The microprogrammed nature of order construction 
makes a restoring 
Chapter 7 I Some aspects 
of the logical design 
of a control computer: a case study 
155 algorithm more efficient 
than a nonrestoring 
one. The quotient delivered to A has a sign determined according to normal algebraic 
rules by 
the signs of (A) and (K); the same sign is available in 
LP to aid in 
determining the correct sign of the remainder from those 
of the divisor and quotient 
in case the quotient 
has been absorbed by subsequent processing. DV is not usually indexed, 
but it pays such large benefits in space and 
time, especially in double-pre- cision division, 
that the cost of extracode indexing is negligible. If the divisor is less 
in magnitude than the 
dividend, or is 
zero, the quotient 
has correct sign and, in general, 
maximum magnitude. No infinite loop results 
in any 
case. L: SU K; Subtract A c (A) - (K); if the final (A) includes 2 overflow, OVCTR t (OVCTR) 21. The primary justification for this 
instruction is that it allows multiple-precision addition subroutines 
to be 
changed into multi- ple-precision subtract subroutines merely by changing 
the indexing quantity. There 
are occasions in the middle of involved calcula- tions where it 
is clumsy to construct a 
subtraction out of comple- mentations and additions, especially 
when the sign of an overflow is of interest. Since SU differs from 
AD only in that the 
operand from memory 
is read out 
of the complement side of the buffer register B rather than 
the direct side, its cost is virtually zero. This last 
is not necessarily true when using core-transistor logic, or two™s complement notation. 
7. Expansion of memory addressing The AGC™s 12-bit address field is insufficient for specifying 
directly all the registers in its memory. This 
predicament seems increas- 
ingly to afflict most computers, either because indirect addressing is assumed as a necessary evil from the start 
or, as was our case, because our earliest estimates 
of memory requirements were 
wrong by a factor 
of two or three. The 
method of indirect addressing we arrived at uses a bank register 
MB, but with an important 
modification: the 5-bit number stored in 
MB has no effect unless 
the address is in the range (octal) 6000 to 7777. The MB register contents are 
not interpreted as higher-order bits of the address; they are interpreted 
as integers which 
specify which bank 
of 1024 words is meant in 
the event of the address part of the instruction being in 
the ambiguous range. The over-all map of memory is shown in Table 2. 
The unambiguous, fixed memory addresses 
domain has come 
to be 
known as 
ﬁfixed-fixed.ﬂ It is interesting that this method 
of extending the addressing capability was not the result of trying to improve upon more conventional methods, 
but was almost a consequence 
of the phys- Table 2 Address part of an instruction word 
(Decimal) 0-3071 3072-4095 Fixed and erasable memory: unambiguous addresses. 
Fixed memory, ambiguous address. Contents 
of MB used to resolve 
the ambiguity. Up to 32 such banks are possible. ical difference between fixed and erasable memory. Since all 
data other than constants are concentrated in the erasable memory, these had to be 
exempt from modification by 
the MB register. An alternative arrangement, whereby 
only the addresses of instruc- tions (as opposed 
to the addresses within an instruction word) are 
modified, would 
be deficient in that it 
would allow only 
instruc- tions to be 
stored in 
banks; there would be no way 
to refer to constants stored in 
banks, or to use bank addresses 
to store argu- 
ments of arithmetic operations. The possibility of using two bank registers is worthy of serious consideration [Casale, 
19621, but it did not occur 
to us. In addition to the 
addresses in erasable, 
it is necessary to exempt the addresses of interrupting programs (ie., the addresses to which a 
program interrupt transfers control) from the influence of the MB register. It was clear that it would be valuable to have 
a large body 
of unambiguous addresses for use 
in executive 
and dispatcher programs. The most frequent and critical applications 
of bank changing are in the AGC™s interpretive mode. Most of the programs relevant to navigation are written 
in a 
parenthesis-free pseudocode notation 
for economy 
of storage. An interpretive program executes these 
pseudocode programs by 
performing the indicated 
data accesses and subroutine linkages. The format of the notation permits 
two macrooperators (e.g., ﬁdouble-precision vector dot 
productﬂ) or one data address to be stored in one 
AGC word. Thus 
data addresses appear as full 15-bit words, potentially capable 
of addressing up to 32,768 registers. 
Each such address 
is examined in the interpreter and the contents of the bank register 
are changed 
if necessary; preparation is also made for 
subsequent return if a subroutine call 
is being made. 
The structure 
of the interpretive 
program, and its relationship to the computer 
characteristics discussed in this paper will not be taken up here except 
to point out 
that parenthesis-free notation is particularly valuable in a 
short-word computer such as 
the AGC. It permits a 
very substantial expansion of the address and pseudo- operation fields without sacrificing efficiency 
in program storage 
[Muntz, 19621. 
156 Part 2 
I The instruction-set processor: rnain-line computers 
Section 1 I Processors with 
one address per instruction 
The conversion of a 15-bit 
address into a bank number 
and an ambiguous 12-bit address 
is as follows: the top 5 bits correspond 
directly to the 
desired bank number. 
The remaining lower-order 
10 bits, logically added to octal 6000, form the proper ambiguous address. If the 15-bit address 
is less 
than octal 6000, however, the address is in erasable or fixed-fixed memory. In this case the logical addition of octal 6000 is 
suppressed. It is possible to have 
a program in one bank call 
a closed 
subroutine in another bank, 
and then 
have control returned to the 
proper place 
in the bank of origin. This is done by means 
of a short bank switching routine which 
is in fixed-fixed memory. One potential 
awkwardness about this method of extending memory addresses is the possible requirement for a 
routine in 
one bank to have access to large amounts 
of data stored in another. 
There are 
many programming 
solutions to this problem, obviously at a cost in 
operating speed; a better solution would 
be to have two bank registers. No problems of this nature have yet 
material- ized, however. References AlonR63; AlonR6O; AlonR61; AlonR62; 
ReckF61; CasaC62; EnglW62; 
HopkA63; MuntC62; RichR55; WaleW62; Proc. Conf. Spaceborne Cm- puter Eng.; Anaheim, Calif., Oct. 30-31, 1962. 
APPENDIX 1 Name, Memory size 
a™ute (F = $xed Number Number of Purpose Features incorporated 
completed E = erasable) of hits instructions of design at this stage MOD 1, F:448 11 and parity 4 plus involuntary Feasibllity Prototype Counter increments, 
1960 E: 64 Interrupts, BACKGROUND FOR AGC DESIGN Core-Transistor Logic, Pulse rate 
outputs, Editing registers, Wired-in fixed memory, Interpretive programs. 
23 and parity 16 plus indirect Unmanned 
Space Probe ﬁExtended 
Operationﬂ subroutine 
linkages (only instance). MOD 2, about 4000 total not built 
MOD 3S, F: 3584 1962 E: 512 15 and parity 8 Earth Satellite MOD 3C, F: greater than 104 15 and parity 8 and involuntary 
Apollo Guidance 1962 E: greater than 103 AGC, F: greater than 
104 15 and parity 11 and involuntary Apollo Guidance 1963 E: greater than 103 Modified one™s complement, Parallel adder, Addressable central registers. 
CCS, INDEX, MULTIPLY in- structions, Overflow counter, Bank switching. DV, SU, MSK instructions, Editing memory 
buffer, All transistor NOR logic instead of core-transistor logic, Extracodes, Parenthesis-free interpreter. 

The UNIVAC system1 J. Presper Eckert, Jr. / Jumes R. Weiner H. Frazer Welsh / Herbert F. Mitchell Organization of the UNIVAC system In March 1951, 
the first UNIVAC2 system formally passed 
its acceptance tests and was put promptly 
into operation by 
the Bureau of the Census. Since 
the UNIVAC is the first computer which can handle both alphabetic and 
numerical data to reach 
full-scale operation so far, its operating record 
and a review of the types of problems to which it has been applied provide an interesting milestone in the ever-widening field of electronic digi- tal computers. The organization of the UNIVAC is such that those functions which do 
not directly require 
the main computer are 
performed by separate auxiliary units each having its 
own power supply. Thus 
the keyboard to magnetic tape, punched card 
to magnetic tape and tape to typewritten copy operations 
are delegated to 
auxiliary components. The main computer assembly includes all of those units which are directly concerned with 
the main or 
central computer 
opera- tions. A block diagram of this arrangement is shown in Fig. 1. All of the elements shown are contained within 
the central computer 
casework except the supervisory control desk (SC) and the Uni- servos,2 to which the lines in the upper right section 
of the diagram connect. The supervisory control, in addition 
to all the necessary control switches and indicator 
lights, contains an input keyboard. Also cabled to 
the supervisory control is a typewriter which is operable by the main computer. By means of these two units, limited 
amounts of information can be inserted 
or removed either at the 
will of the operator or by the programmed instructions. 
The input-output 
circuits operate on all data entering 
or leav- ing the computer. The input 
and output synchronizers properly time the 
incoming or outgoing data for either the 
Uniservos (tape devices) or 
the supervisory control devices. The input and output registers (I and 0) are each 
60 word (720 characters) temporary 
storage registers which 
are intermediate between 
the main com- 
puter and the input-output 
devices. The high-speed bus amplifier 
is a switching 
central through 'AZEE-IRE Conf., 6-16, December, 1951. 2Registered trade mark. 
which all 
data must pass during transfer between any arithmetic register and the main memory or 
between the 
memory and the 
input-output registers. The arithmetic registers are shown along the bottom of diagram each 
connected to the high speed bus 
system. The L-, F-, X-, and A-registers are each of one word or 
12- character capacity and are 
directly concerned with 
the arithmetic operations. The V- and Y-registers are of 2- and 10-word capacity, respectively. They are used solely for 
multiple word transfers 
within the main memory. Associated 
with the arithmetic 
registers are the 
algebraic adder (AA), the comparator (CP), and the multi- plier-quotient counter (h4QC). Addition-subtraction instructions The addition-subtraction operations 
are performed in conjunction 
with the comparator since all niimerical 
quantities are absolute magnitudes with 
an algebraic sign attached. Before either an 
addition or subtraction is performed, the two quantities, 
one already in the A-register and the other either 
from the memory or from 
the X-register, depending upon 
the particular instruction, are compared for magnitude and 
sign. The adder inputs 
can then 
be switched so as always to produce a noncomplemented result 
for any operation. 
The choice of adder input 
arrangement is there- fore under the 
control of the comparator. The comparator also determines the proper sign for 
the result according to the usual algebraic rules. One additional function 
performed by the comparator for addi- tion and subtraction 
is to control the complementer. This deter- mination is based upon which operation (+, or -) is indicated, and, whether 
the signs are like or 
unlike. For a 
subtract instruction, the sign of the subtrahend is reversed before 
entering the com- parator. The comparator then compares the signs of the quantities 
in order 
to determine whether 
the two quantities 
are subtracted 
or added. Multiplication instruction The multiplication process requires the services of the adder, the 
comparator, the multiplier-quotient counter and the four arith- metic registers. During the first step of multiplication the X-reg- 157 
158 Part 2 1 The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
r -- I I I I I I I I I I 1 I I I STANDARD PULSES TO ALL UNITS 
(1AAAiA -\ i~iiiii TO AND FROM UNISERVOS CYCLlWG UNIT INPUT- OUTPUT FROM TO CONTROL ALL UNITS 
CIRCUITS CONTROL SIGNALS TO GATES L--*- r--- I I CHECK CIRCUITS - CONTROL CIRCUITS 1 TIME OUT 
(TO) (1000 WORDS) I t---, I I I I I I __- I I I INSTRUCTION MEM LOCATION OlGlTS OlGlTS CONTROL INPUT- OUTPUT 
TO -+--- /T}-+SIGS STATIC REGISTER DISTRIBUTOR LINE 
- - 
-. - - - - 
- - INPUT FROM REGS. I I AND OTHER UNITS I I----*- ;SIGNAL ;SIGNAL I -3 LEGEND: - INFORMATION SIGNALS CONTROL SIGNALS 8 PULSES ---- Fig. 1. Block diagram of UNIVAC. 
Chapter 8 I The UNIVAC system 159 ister receives the multiplier from the memory and the comparator determines the sign of the final product by comparing 
the signs of the multiplier and multiplicand. During 
the next three steps the multiplicand, which has 
been stored in 
the L-register by some 
previous instruction, is transferred three times to the A-register through the algebraic adder. The result, three times the multi- plicand, is then stored in 
the F-register. During 
the next 11 steps of multiplication, the successive multiplier digits, beginning with 
the least significant, are transferred from the X-register to the multiplier-quotient counter. The multiplier-quotient counter then 
determines whether each particular multiplier digit 
is less 
than three, or greater than or equal to three. If the former, the L-register releases 
the multiplicand to the 
A-register via 
the adder, 
and the multiplier-quotient counter is stepped downward one 
unit. If the multiplier digit 
is equal to or greater than three, the 
multiplier-quotient counter sends a signal to the F-register which releases 
three times the multiplicand to the A-register and the 
multiplier-quotient counter is stepped three 
times. Thus a multiplier digit 
of seven would 
be processed as 
two transfers from 
the F-register to the A-register and one transfer 
from the L-register to the A-register, or 
a total of three transfers. When the multiplier-quotient counter 
reaches zero, 
the next multiplier digit 
is brought in 
from the X-register, while the A-reg- ister, containing 
the first partial product, 
is shifted one 
position to the right. During the final step of multiplication, the sign is attached to the product which has 
been built 
up in the A-register. One of the several available multiplication instructions 
causes the least sig- nificant digits, as 
they are 
shifted beyond 
the limits of the A-reg- ister, to be 
transferred to the X-register where they replace 
the multiplier digits as 
they are moved to the multiplier-quotient counter. Thus 
22 place products 
can be obtained as well as 11 place. Division instruction The division operation is performed by a nonrestoring method. 
The divisor is stored in 
the L-register by some previous 
instruction and 
the dividend is brought from the memory and put in the A-register during the 
first step of the division instruction. As in multiplica- 
tion, the signs of the two operands are compared in 
the comparator at this time and the sign of the quotient is then stored in 
the comparator pending completion 
of the division operation. The principal stages of division consist 
of transferring the divisor from 
the L-register to the A-register through the complementer and 
adder as many times 
as required to produce a 
quantity less than zero in the A-register, the dividend having 
been first shifted one 
position to the 
left. The multiplier-quotient counter counts each 
transfer, thereby building 
up the first quotient digit. As soon as the quantity 
in the A-register, (neglecting its 
original sign) 
goes negative, the digit in 
the multiplier-quotient counter, 
not counting the transfer which causes 
the remainder to go negative, is trans- ferred to the X-register and the remainder in the A-register is shifted one place 
to the left. The divisor is then added 
to the 
A-register until the quantity becomes positive. This 
time the 
multiplier-quotient counter must give 
the complement of the number of transfers for the real quotient digit. Special comple- menting read-out 
gates provide this 
method of interpreting the multiplier-quotient counter. The X-register therefore collects 
the quotient, 
digit by digit, from the multiplier-quotient counter until the full 11 digits have been obtained. 
The quotient is then transferred to the A-register and the 
sign from 
the comparator (CP) is affixed 
during the final stage of the divide instruction. The other internal 
operations of the UNIVAC include many transfer instructions by 
which words may 
be moved among 
the registers and memory with and without 
clearing, the extraction instruction by which 
certain digits of a word may 
be extracted into another 
word according to the 
parity of the corresponding digits of an extractor word; 
shift instructions; and special control 
instructions such as breakpoint, transfer of control, (explained in 
subsequent paragraphs) and stop. 
Basic operating cycle 
The basic operating cycle of the UNIVAC is founded upon single address instructions which specify 
the memory location of one word. In 
the case of the arithmetic instructions which require two operands, one of the operands must be moved into the proper register by some previous 
instruction. In order to control the sequence of instructions, a special counter, called 
the control counter (CC), retains the memory location from which 
the succeed- ing instruction word is to be 
obtained. Each 
time a new instruction 
word is received from the memory, the quantity 
in the control counter is passed through the adder 
where a unit 
is added to it. 
Therefore the normal sequence is to refer to successive memory 
locations for successive 
instruction words. Initially the control counter is cleared to 
zero and the first group of instructions must, 
therefore, be placed in 
memory locations from zero 
upward. A transfer of control instruction enables 
the programmer to change 
the control counter reading whenever 
desired and thus shift from 
one sequence to another. 
After a transfer of control takes place, 
the new number in 
the control counter is increased by 
unity each time a 
new instruction word is obtained from the memory. 
160 Part 2 I The instruction-set 
processor: main-line computers 
Section 1 1 Processors with one address per instruction 
Transfer of control instructions The transfer of control instructions 
are of three types, the uncon- ditional transfer 
which changes 
the control counter reading with- 
out question, and two conditional instructions 
which require that either equality or a specific inequality exists between the 
words in the A-register and the L-register. In the former case 
the quan- tities must be identical for transfer 
of control to occur 
and in the latter the quantity 
in the A-register must 
be greater than the 
quantity in the L-register for 
the control counter reading to be changed. Since the UNIVAC can handle alphabetic 
as well as numerical data, these conditional transfer instructions are as useful for 
alpha- betizing as they are 
to determine if a certain iterative arithmetic 
process has been performed often enough 
to come within specified numerical tolerances. Control register Since six characters (intermixed 
alphabetic and 
numerical) are sufficient to specify an instruction and 
there are 12 characters per 
word, each instruction 
word can represent two 
independent in- structions. A 1-word register, called 
the control register (CR), has been provided 
which stores 
each instruction 
word as 
it comes from 
the memory. Thus one 
memory referral is sufficient for 
a pair 
of instructions and 
the control register stores 
both halves so that the 
second instruction is available as 
soon as the first has been com- pleted. The general term control circuits includes 
all those 
elements which work 
together to process the instruction routine. 
As each instruction word reaches the control register, 
the first half 
of it is passed immediately into the static register (SR). The static 
register drives 
the main function table and memory switch. The instruction digits are translated by 
the function table into the 
appropriate control signals for 
the instruction called 
for. The memory switch selects 
the location called 
for by 
the memory location digits 
and opens the proper memory channel to the 
high- speed bus system 
at the 
proper time. 
Since the memory is con- structed of 100 channels, each holding 
ten words, the memory switch is a combination 
of spatial and temporal selection. 
Cycle counter Implicit within each instruction, 
as translated by 
the function table, is an ending signal which causes 
the computer to move on to the next instruction. The key to this sequence is the cycle counter (CY), which is advanced by the ending pulse. The cycle counter is a 2-stage 4-position 
counter, which is connected into 
the function table. By virtue of this relation, 
CY develops signals in addition 
to those developed by 
the instruction, which, 
for ex- ample, can 
cause the control register to transfer the second half of the instruction word into the static register when 
the first half has been completed. 
Similarly, after the second half instruction is finished the cycle counter causes the reading of the control counter to pass into the memory location section 
of the static 
register and thus cause the next instruction word to be transferred 
from the memory to the control register. 
When the 
word reaches 
the control register, 
the cycle counter also causes 
the control counter reading to be increased by unity. 
The four cycles 
are designated by 
the first four Greek 
letters a (transfer CC to SR), ,8 (transfer memory to CR), y (perform first instruction), and 
S (perform second instruction). Program counter The multistage instructions, 
such as 
multiplication, are guided through their various steps by 
the program counter (PC). The program counter has four stages or 
16 positions. All multistage instructions can be performed within this number of steps. Checking circuits The checking circuits 
of the UNIVAC are of two main types, 
odd-even checkers and 
duplicated equipment 
with comparison circuits. The odd-even checker depends upon 
the design of the pulse code used within the computer. This code provides seven 
pulse positions for 
every character. Six of the seven positions 
are significant as 
the actual 
code while 
the seventh is the odd-even channel. If the number of pulses or ones 
within the first six chan- nels of any character is even, a 
one is placed in the seventh channel 
to make the total odd. Thus, the total 
number of ones across 
the seven channels is always odd. 
By means of a binary 
counter and 
a few gates, an odd-even checker has been constructed 
which examines every seven pulse 
group which passes through the high speed bus amplifier. 
In this connection, mention 
must be made of the periodic memory check which 
interrupts operation every five seconds to pass the entire contents 
of the memory over 
the high speed bus system 
and, consequently, through 
the odd-even checker. Any discrepancy is immediately signalled to the super- visory control and further 
operation ceases. The duplicated equipment type 
of checking consists of dupli- cating the most essential part of the arithmetic circuits and 
their controls and producing 
simultaneously independent results, which 
can then be compared for equality. For this type of checking, the A-, F-, X-, and L-registers, algebraic adder, comparator, multi- 

Chapter 8 I The UNIVAC system 161 plier-quotient counter, 
and the high speed bus 
amplifier are dupli- cated. The memory is not duplicated, but is checked by the periodic memory check 
mentioned previously. Various sections of the con- trol circuits 
are duplicated such as 
the program counter and cycle counter. Timing pulse generator and 
cycling unit 
The timing pulse generator and cycling 
unit (CU) are the 
source of the basic timing signals throughout the computer. The timing pulses occur at 2.25 megacycles 
per second. The cycling unit subdivides this 
rate into 
the character rate and 
word rate. The 
character rate 
is one seventh 
of the basic pulse 
rate since there are seven pulses for 
each character. There are 
12 characters per word but space for a 13th 
character is included in a 
word time and is called the space between words. This 
time is used for 
switching purposes. The cycling unit, therefore, develops the word signals 
at y7 x yl3 or yS1 of the basic pulse 
rate. Within the 
cycling unit (CU) are numerous duplications and 
comparisons to ensure com- plete reliability. Input-output circuits The operation of the input-output system is dovetailed as effi- ciently as possible 
with the operation of the arithmetic 
circuits. Whenever possible, parallel operations 
are allowed to proceed so as to minimize the time lost on internal operation while 
the slower input-output operations are taking place. 
The principal input-output instructions are handled in a 
man- ner identical to 
that for the internal operations, except 
that now the function table develops signals which 
bring the input-output control circuits 
into operation. The information supplied to 
the input-output control circuits 
by the function table includes the following: 1 2 Which of the ten possible Uniservos 
is being called 
on Whether it is a read 
or write, that is, an input 
or output operation If it is ﬁread,ﬂ the direction in which the tape is to move 3 The input-output 
control circuits, therefore, begin by testing 
whether or not the Uniservo indicated now is in use or not. If it is already in 
use, everything else waits 
until that Uniservo is free. Next, the input-output 
control circuits test 
to determine whether the Uniservo selected last moved 
backward or forward. If the previous direction does not 
agree with 
the new direction 
called for, the input-output 
control circuits 
generate the proper signals to prepare the Uniservo to move in the opposite direction. 
If the instruction is to rewind a 
Uniservo, the input-output control circuits then direct 
the center drive of the selected Uniservo to rewind the tape to the 
beginning and stop. 
As soon as the instruction has proceeded to the point where 
the input-output 
control circuits need 
no further information from 
the function table, the instruction ending 
signal is generated and the internal circuits proceed 
to the next instruction, even while the reading, writing 
or rewinding continues. 
The UNIVAC can process an input, an output 
and several rewind operations 
while simultaneously carrying on internal computation. So far the method by which 
the words are transferred 
from the I-register to the memory has not 
been mentioned. 
This opera- tion is combined with 
certain read instructions in a manner not 
immediately obvious. There are two instructions which read from the tape to the 
I-register, one causing 
the tape to 
move forward, the other 
causing it to move backward. There are two other input 
instructions similar to those just 
mentioned, but they have 
the additional operation 
of first reading from the I-register to the memory and then 
reading a new group 
of 60 words from 
tape into the I-register. Thus the first type of input instruction reads from 
tape to 
the I-register only. 
It must be followed by 
the second type of instruction in order 
first to clear 
the I-register and then read in the second block of 60 words. The output 
instructions do not operate in this way 
but instead read directly 
from memory 
to the 0-register and then 
to the tape as one instruction. 
A third type 
of checking circuit 
occurs in the input-output 
control circuits 
which counts the number of characters transferred 
from the tape 
in each block. Since 
there must always 
be 720 characters per 
block, the 720 checker signals any discrepancy 
to the supervisory control. One other phase of the input-output operation concerns 
the two supervisory control input-output instructions. One 
of them permits a 
single word 
to be typed in from the input 
keyboard and the other causes a single word 
to be 
typed out 
automatically. Auxiliary equipment 
The two principal 
auxiliary devices 
mentioned earlier were 
the Unityper,l which converts keyboard operations to tape 
recording, and the Uniprinter,l which converts magnetic recording 
to type- written copy. lRegistered trade 
mark. 
162 Part 2 I The instruction-set 
processor: main-line computers Section 1 I Processors with 
one address per instruction 
Unityper. A simple block 
diagram of the Unityper is shown in Fig. 2. Each keyboard operation pulses the input 
to an encoding func- 
tion table which, in turn, drives the appropriate 
heads for 
record- ing the particular 
combination on the tape. 
Simultaneously, the same pulse triggers 
a motor delay flop which operates the tape 
motor for 
an interval sufficient to move the tape 
across the head 
for the distance required to 
record one character. However, there is a punched paper 
loop system associated 
with the Unityper for the purpose of providing the typist with 
various guideposts 
individ- ually set 
up for each problem. 
The loop control system serves 
three distinct control 
functions. First, it allows the programmer to set 
up various numbers 
of characters for the individual items being 
entered for a given problem. If the typist ever enters other than 
the specified number of characters, the loop control signals an error. Although the basic word 
length is 12 characters, the pro- grammer may subdivide or 
group the words to suit any length 
of item. The loop can then be punched with what are called ﬁforce checkﬂ punches. Whenever 
the typist completes a correctly en- 
tered item, 
she must 
operate a release key before 
entering the 
next item. If the forced check is released too 
early an error is created, or if an additional character is typed after 
the forced check should have been released, 
an error is similarly indicated. The second function of the loop is to control the erase opera- tion. The erase operation is the only way 
in which an 
error can 
be recalled. 
When the 
erase key 
is operated, the loop and tape KEYBOARD El- l i ENCODING FUNCTION TABLE RECORDING HEAD I I I RELAYS Fig. 2. Simplified block diagram of Unityper. are both stepped backward until a 
stop punch (usually associated 
with each 
forced check) is encountered. Thus 
the entire erroneous item is erased, and 
at a much higher 
rate than 
that at which the backspace key can be operated. The 
backspace, incidentally, can- 
not cancel 
an error indication, 
but it 
can be used to correct a wrongly typed character 
if the typist recognizes it. The third function 
of the loop system 
is to enter, automatically, various fill-in characters. Under one such system 
of operation, the loop control records the characters only at the behest of the oper- ator. This function is useful where individual entries, 
such as personal names, 
do not fill out all of the space allotted. 
The other 
operation is fully automatic in which the loop assumes full 
control to record, for example, 
a group 
of fill-in characters later 
to be 
replaced by 
computed data 
within the central computer. 
The block diagram therefore 
shows the loop motor 
connected to the same delay flop that steps the tape 
motor. The same signal 
which moves the two motors also sets 
a second delay 
flop (DF2) which produces 
a delayed probing 
pulse. The probing pulse exam- 
ines the paper 
loop photoelectrically for the new combination. A third delay 
flop (DF3) produces another probing pulse 
after the relays associated 
with the loop photocells have 
had time to set 
up. If any automatic function is indicated by 
the photocells, the probing pulse passes through the interpreting 
relays, enters the encoding function 
table to generate the fill-in characters, and thus 
starts the cycle over again. All automatic functions take place 
at about 22 characters per second. 
Numerous odd-even checks 
are introduced in the Unityper to provide checks 
on tape and loop motion 
and on the recorded code combination. Uniprinter. The Uniprinter is shown in simplified block diagram 
in Fig. 3. Its operation is a simple cycle which 
is initiated by a 
start button. 
The start button triggers the motor flip-flop (MFF). The motor pulls 
the tape across the reading head until a combina- 
tion is detected. The presence of pulses on any of the seven lines 
between the 
reading head and the relay decoding function 
table is sufficient to restore the motor flip-flop (MFF) and stop 
the tape motion. Simultaneously 
a print delay 
flop (DF1) is triggered. During the delay flop interval, the decoding relays are given time to set up. 
When the 
delay flop recovers, a pulse is sent through 
the relay table which reappears at one of the typewriter magnetic actuators. As the typebar reaches the platen, a printer action switch (PAS) is operated which pulses 
the motor flip-flop and starts a new 
search for 
the next character on the tape. The 
odd-even properties of the UNIVAC pulse code are utilized for checking purposes. 
Chapter 8 I The UNIVAC 
system 163 I I Fig. 3. Simplified block diagram of Uniprinter. Engineering aspects The entire 
UNIVAC system is constructed of circuits which are as conservative as is 
consistent with the desired reliability and speeds of operation. The circuits have been 
designed as building blocks and the entire computer 
is constructed around these 
blocks. One of the most important of these blocks is the pulse reshap- 
ing circuit which consists 
of a timing 
pulse gate and a 
fast acting flip-flop which generates the pulse envelope equivalent 
of the gated timing pulses. Two 
polarities of timing pulse are used, the one being capable 
of tripping the flip-flop into one state, the other polarity of tripping it to the 
other state. 
As a deteriorated pulse envelope is applied to the timing pulse gate input, either 
one or the other polarity 
of pulse is always gated. The 
flip-flop therefore produces a sharpened 
and correctly timed 
output waveform. The gating and switching circuits in 
the central computer are 
constructed of germanium crystal 
diodes, which 
include the main and subordinate function 
tables. The registers are all circulating delay type using a mercury 
tank of one, two, 
or ten word-times of delay, except 
the static register. The latter is composed of 27 flip-flops which are required to maintain the static signals applied to 
the function tables, for 
at least an entire 
word-time. The switching time allowed 
by the seven pulse-times 
of the space between words is, in general, 
not sufficient for 
a new 
func- tion table excitation to stabilize. Therefore 
the time-out system used successfully 
in the BINAC, also is employed in 
the UNIVAC. Whenever an ending pulse is generated, or any other pulse which 
indicates that a new set of control signals are required 
from the function table, an interval of one word-time 
is introduced to allow the function table signals to reach equilibrium. 
The time-out in- terval is controlled by a single fast-acting flip-flop. All gates attached to the function table signals which 
are critical as to opening and 
closing can be inhibited by the time-out flip-flop during time out. Regardless of the presence of the function table signals, the gate 
does not 
operate until 
the time-out flip-flop re- leases it. Thus, the burden of speed imposed by the short space 
between words has 
been shifted to a single flip-flop which can accommodate the needs of the entire computer. 
The UNIVAC uses 
the excess-three pulse 
code system which 
requires a second binary 
adder after the 
main binary adder in order 
to provide the excess-three correction after each addition. 
On the other side of the ledger, the complementing operation 
for sub- 
traction and 
division is very much simplified, since 
the substitution of ones for zeros 
and vice versa 
is sufficient to form a complement. 
The excess-three part of the pulse code occupies 
the four least significant digit positions. The next two positions beyond the excess-three digits 
are used as 
zone indicators. 
When these digits 
are both 
zero, the last four positions are interpreted 
as a numerical 
quantity; when nonzero, 
an alphabetic 
or punctuation symbol is indicated. The seventh channel 
is the check pulse channel. The adder 
is provided with 
an alphabetic 
bypass circuit which allows an alphabetic 
letter to enter one input and 
emerge un- 
scathed provided a numeral 
enters the other input. 
Thus additive 
numerical constants 
can be combined with instruction 
words to adjust the memory location part 
of an instruction without affecting 
the alphabetic 
instruction symbols. The power supply 
for the computer 
is separately housed. It can be placed any 
reasonable distance from the central computer. 
Almost all rectification 
is done by dry disc rectifiers. 
The power supply provides all a-c 
and d-c potentials to 
the central computer, 
supervisory control, directly-connected 
printer, and the Uniservos. A complete fusing system has 
been included 
which serves 
both as protection and as a short-circuit 
isolating means. 
Each section, of which there are 39, is locally fused, 
enabling the engineer to locate a short within 
only 12 chassis, rather than 
the total of 468. An automatic voltage monitoring system may 
be used to test every d-c voltage at the rate of one per 
second. A meter movement relay signals any discrepancy 
from standard. Similarly, overheat thermostats detect any unfavorable temperature condition in 
the bays or 
mercury tanks. Cooling for 
the power supply 
and central computer 
is provided by three blowers. Local cooling 
in the Uniservos is provided by 
small fans 
in each unit. 
The operating statistics 
of the UNIVAC are as follows: 

164 Part 2 1 The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
Tape reading 
and recording: Pulse density: 120 
per inch Tape speed: 
108 inches per second Input block size: 60 
words: 720 characters Tape width: 
‚/z inch: 8 channels Internal operations: Memory capacity: 1,000 words; 
12,000 characters Memory construction: 100 mercury channels; 10 
words/ channel Access time: Average: 202 
microseconds Maximum: 404 
microseconds Word length: 12 characters 9 pulses (include space 
between words = 7 pulses) Basic pulse rate: 2.25 megacycles 
Addition: 525 microseconds Subtraction: 525 microseconds Multiplication: 2,150 microseconds Division: 3,890 microseconds 
(All times shown 
include time 
for obtaining instructions 
and operands from memory) 
Applications of UNIVAC Types of problems for which UNIVAC is applicable True to its name, Universal Automatic Computer, the 
UNIVAC system is capable of handling data processing or calculation in virtually all 
fields of human endeavor. It is particularly well suited to applications requiring large 
volumes of input or output data, 
or both. For convenience 
and classification, applications of the UNIVAC will be treated under four headings: scientific, 
statistical, logistical, and commercial. The scientific problem usually, though not al- ways, has 
relatively small 
amounts of input and output 
data, with emphasis on computation. The statistical problem has relatively 
large volumes of input data with a small volume 
of output data and simple processing procedures. The commercial and logistical problems both have relatively large amounts 
of input and output 
data with processing requirements varying from slight 
to relatively great. A number of problems in each 
of these four 
fields have been studied and found suited 
for solution on the UNIVAC system. Several in each field have actually been processed on the com- puter. Scientific problems 
A general-purpose matrix 
algebra routine 
designed to add, sub- tract, multiply, and reciprocate matrices 
of orders up to 300 has 
been prepared 
and applied to a number of matrices. Inverses have been calculated 
for three different matrices of orders 40, 50, and 44. The error matrices 
for the first two of these inverses 
also were calculated. In 
both, the largest error term 
was of the order of 1W8. A triple product 
matrix was formed 
from component matrices 
ranging from 5 by 40 to 40 by 40. A check product 
was obtained by reversing 
the sequence of multiplications, verifying the original product to within 2 units in 
the 11th place. 
The computer time 
required for these calculations was 1 hour and 15 
minutes to calculate the inverse of order 50,45 minutes to determine 
its error 
matrix. The other calculations were proportionately shorter. In 
all of this work, 
magnetic tapes 
were used as 
temporary storage 
for the bulk of the matrix elements involved. The high speed of the tape reading units 
more than kept 
up with the computer™s need for data. No mathematical checks, other than the over-all check mentioned, were included in the computation, the self-checking features of the system making these 
completely unnecessary. A second computation-that of obtaining six different specific solutions to a system 
of 385 simultaneous equations-was com- pleted in 27 minutes 
on the computer. The system of equations arose from 
a second 
order nonlinear 
differential equation of gas flow through a turbine. The error terms resulting 
from the sub- stitution of the computed 
unknowns into the basic equation were 
of the order of The third example is that of a 2-dimensional 
Poisson equation, using a 
22 by 22 mesh. Each iteration required 13 seconds and produced a maximum separation of successive surfaces 
of the order of 10-* after approximately 
300 iterations. Statistical problems In the second major field 
of statistical computation, 
the Census problem has 
been a prime example. The Census problem produces a part of the Second Series Population on Tables for the 1950 Decennial Census. The Second Series 
contains 30 types of tables covering 
the statistics of our population-age, sex, race, country 
of birth, edu- cation, occupation, employment, 
and income. These 
tables are to be compiled for every county, and for every city, rural farm, 
and rural nonfarm area within 
a county. The preparation of these tables 
by the UNIVAC system requires 
three major steps: 1 Tabulation of each individual™s characteristics by groups of about 7,000 
Chapter 8 1 The UNIVAC system 
165 2 3 Arranging these groups by cities, 
counties Assembling from the tabulations the data required for each table The raw data were prepared in the form of a punched card 
for each individual in the United States. The data from these enumeration cards are then 
transcribed onto magnetic tape. From these tapes, 
the computer 
processes the data 
sequentially through 
the three steps, producing output tapes from which the tables are 
printed on Uniprinters. The only manual operations 
encountered in this entire procedure are the handling of the original punched cards, mounting 
and demounting tape reel (the equivalent of 9,700 cards), and the removal of the printed 
tables from the Uniprinters. The most important feature 
of the present procedure 
is the elim- ination of handling and sorting tremendous quantities 
of punched cards. Each handling 
of the card 
stacks is a source 
of potential error and delay. The UNIVAC memory permits the simultaneous accumulation of the 580 tallies which 
describe our population 
for each local area being studied 
by the UNIVAC system. Commercial problems In the commercial field, the UNIVAC system has handled premium billing for a life insurance company. 
This program 
produces pre- 
mium notices, dividends, 
and commissions. In a particular example worked out, approximately 1,000,000 bills, 340,000 
dividends, and 100,000 commissions 
have to be 
produced monthly. The necessary information for processing a 
particular policy is contained in 240 
digits, or, 
in special 
cases, 480. 
This compactness 
is made possible by a logical 
system of 40 symbols, comprising both alphabetic and 
numeric characters, which 
denote over 90 definitions. The UNI- VAC processes the policies as directed by 
the symbols, policy dates, and policy numbers. 
The problem includes inserting over 250,000 changes each 
month before further handling 
is done. After this 
step, the policies to be 
processed are selected from a file of 1,500,000 items. Next, 
a list is produced of the cases which have symbols indicating that special notices 
must he sent to the policyholders. Following 
the calculation of dividends and commissions, additional lists are pro- duced: one group contains 
information pertaining to commissions and agents; another contains 
information regarding dividends; and finally, there is a listing 
of option changes for later insertion into the policy files. Policies requiring premium notices are 
then edited 
and the notices are automatically printed from the data contained on magnetic tapes. 
The UNIVAC time needed 
for a program of this proportion is about 135 hours a 
month. The average computer time per policy processed is less than 0.5 second. The average time for all change insertions, printing, calculations, 
and unityping is 9 seconds per item. Logistical problems In the field of logistics, five major studies have 
been conducted, four of these resulting in actual problems executed on the com- puter. The first is the type 
of computation in which 
the basic purpose 
is to determine quantitively whether a given operational or mobi- lization plan can 
be logistically supported. The ultimate desired is to find, by calculation, the optimum program 
for carrying out such plans. 
At the time of writing, only a small model has been actually run 
on UNIVAC, 
but full size 
models will 
be run within 
the next few weeks. Two 
computations have 
been executed, one 
a set of three tables of thousands of lines each, giving a 
detailed breakdown of machine deployment, fuel requirements, 
and over- haul requirements. 
The other problem was a computation of the amounts of critical raw materials required 
to construct a given 
number of each type of equipment, these requirements being 
phased by 
quarters over a 2-year 
period. The fourth problem, 
which was actually computed, 
was a sample of a similar 
calcu- lation in which 
every pound of critical raw material required each 
month for the ultimate construction of a complete building pro- gram was computed. The UNIVAC program which was 
prepared is capable of accommodating every type of equipment, individually tailored construction schedules, detailed hills of materials running 
into the 
millions of items and of determining the actual amounts 
of alloy elements based 
on thousands of tables of percentages for the many alloys employed. The demonstration showed that this computation for 400 pieces 
of equipment of a given 
type could be executed in three hours of computer time. The last problem 
in this field has not 
yet been run, 
but the study has shown 
that the 
entire gamut of stock control for a large 
supply office can be covered by the computer in approximately 
3 weeks time. This program involves the maintenance of stock balances of hundreds of thousands of stock items 
for many service points 
and provides for the preparation of stock transfer orders, 
purchase requisitions, critical lists and summary reports. Performance record of 
the UNIVAC Acceptance tests 
The Acceptance Tests, prepared jointly by 
the Bureau of Standards and Bureau of Census, are fully discussed in the following paper by Dr. Alexander and Mr. McPhers0n.l However, a few 
comments lPaper not included in this book. See McPherson and Alexander [1951]. 
166 Part 2 I The instruction-set processor: main-line computers 
Section 1 I Processors with one address per instruction 
concerning them 
from the engineering point 
of view are appro- priate. The Census computer was given two tests; the first, a test 
of its computational ability; 
the second, a 
test of its input-output system which particularly 
stressed the tape 
reading and recording abilities. The Central Computer 
Acceptance Test A consisted 
of two parts. During Part 
1, every available internal operation, 
except input-output operations, was performed. Among these operations 
were addition, subtraction, 
comparisons, division, 
and three 
different types of multiplication operations. Each of the arith- 
metic operations handled a pair of 11-decimal digit 
quantities. Altogether there were about 
2,500 operations in 
the routine, yet 
the entire routine required 
only 1.26 seconds 
to do. The routine was performed 808 times 
in 17 minutes making a 
total of about 2,000,000 operations in 
all. The second part of Test A included the solution of a heat distribution equation, 
a short routine 
involving the input-output 
device and a sorting routine. 
The sorting routine arranged 
ten numerical quantities each containing 
12 decimal digits in correct 
numerical order 
in about 0.2 second. 
All three routines took a total of 1% minutes to perform. They were performed twice for each test and 
when added to Part 1 made a total of 20 minutes for unit test A. The Acceptance Test B examined 
the input-output tape devices (Uniservos). During the first part of Test B, 2,000 blocks or 
about 1.4 million digits, 
which included 
every available character (numeric and alphabetic) were 
recorded on 
a tape and 
then read back into the computer with the tape 
moving backward. The information read back 
was then compared with 
the original data read out. 
The recording operation required about 
4 minutes while reading back 
and comparison required about 
8 minutes. The sec- ond part of Test B consisted 
of recording and reading over one spot of tape for 700 passes in 
order to determine the readability of tape as it wears. This test required 
13 minutes and when com- bined with Part 1, made a total of approximately 25 minutes for Test B. This test was repeated 19 
times. The first test run passed in 6.6 hours (minimum 
theoretical time: 6.0 hours) and the 
second test 
was passed 
in 9.47 hours 
(minimum theoretical time: 
7.45 hours). Of the 2.02 hours down time, 1.45 hours 
were accumulated at one time with the remaining 0.58 hours 
spread over the rest of the test. The Uniprinter test required 
that a block of information (60 
words) be printed 200 times in tabular form. The minimum time 
for printing was five hours. The test was passed in 6.16 hours. 
The card-to-tape test required 
that ten good reels of tape be produced in 12 hours. 
There were certain restrictions as to reading accuracy and other 
criteria of reproducing ability which 
defined ﬁgoodﬂ reels. 
In 10 
hours, the converter had prepared 
over 15 reels, 14 reels 
had been tested, 
11 of the 14 were found satisfactory 
and the converter was accepted for payment. Although the test was run on only 
one of two converters, the Bureau of Census put both card-to-tape machines into operation and after 
six months of use, the acceptance test was run on the second card-to-tape converter. This test 
differed to some extent from the first test in 
that the 
Census Bureau 
was satisfied 
with the reading ability 
of the machines and did 
not require 
a digit-by- 
digit verification of the information. However, a new stipulation was added that, after the engineers had checked the converter out preparatory 
to running the test, the converter was to be used in actual operation for eight hours before 
doing the remainder of the test with no engineering intervention 
between the two portions of the test. 
The first part was run on Friday, October 
5, 1951; the device remained idle Saturday 
and Sunday and was turned on Monday morning to complete 
the test. It passed with flying colors, 
preparing ten acceptable 
reels (out of ten reels) plus 
two decks of check cards in slightly less than 7 hours. Both card-to-tape converters now are in Washington and the remainder of the system is in operation by the Bureau of the Census on the Eckert-Mauchly premises in Philadelphia. Reliability and factors affecting performance 
The first UNIVAC 
system now has 
been operating for approxi- mately 8 months. In 
that time, much 
has been learned about how 
UNIVACs should be operated and maintained. The situation has been somewhat complicated 
by having 
to shake down the equip- ment while in the customer™s possession; that is, there were certain 
faults in the system from both engineering and production stand- 
points which could 
only become apparent in the course of time and under actual 
operation conditions. For example, weak 
tubes or faulty solder joints 
did not reveal 
their presence at the 
time of installation. Another type of difficulty only 
became apparent under certain duty 
cycle conditions imposed by 
various types of problems. Because only 
certain problems present this particular duty cycle, these 
troubles remained in 
the machine causing inter- mittent stoppages until they could 
be tracked down. 
Patient isolation and elimination of such problems, 
most of which have occurred 
only with conditions of operation infre- quently encountered, 
is a powerful, 
though sometimes painful 
proving ground for the engineering group charged with 
such re- 
sponsibility. The experience and depth of judgment acquired 
by such a 
group in 
the course of performing such work have become 
unmistakably apparent in the already noted improved performance 
of following UNIVACs and generally advanced ability 
to predict 
Chapter 8 1 The UNIVAC system 
167 and realize performance 
in any large scale 
and complex apparatus of the same character. Some of the troubles encountered are interesting to study 
in detail. On a rather complicated routine 
requiring the use of a number of Uniservos, all ran smoothly for 15 minutes. At that time, one of the Uniservos executing a backward read 
somewhere in the middle of the reel, did not stop 
at the end 
of the block but con- tinued to run until it ran 
off the end of the tape. After much work, it was shown that a cycling unit signal was 
being overloaded 
because it was being used both by a multiplication instruction 
and the backward read 
which were occurring 
simultaneously. The input precessor loop was cleared as a 
result and the count of the pulses coming off 
the tape 
was thereby lost. Once the trouble was found, it was simple to remedy. Another rather interesting case 
occurred intermittently 
over an extended 
period. Normally when reading out of the memory, the contents should not be cleared. Occasionally, however, reading 
from the memory also caused the contents to 
be cleared. As the trouble only remained for a 
period of seconds or, at most, a few 
minutes, it was somewhat difficult to localize. Of course, parasitic 
oscillations of some sort were suspected and, in fact, the trouble was traced to 
the actual 
source on a logical basis; 
but the source, a high power 
cathode follower, showed no evidence of oscillation. Before the problem was remedied, various combinations of para- sitic suppressors were tried; 
the trouble would vanish for 
perhaps a week 
and then 
return. The oscillation finally cropped up during a maintenance shift, was found 
to be in the suspect tube at 100 megacycles and was eliminated rather 
easily. Other types of troubles that have occurred 
include intermittent parasitic oscillations in other 
circuits, bounce in Uniservo 
relay circuits, various mechanical problems 
in Uniservos, time constants not consistent with the longest duty cycle signals, and various types of noise in the input 
circuits. The tubes, which initially 
were bothersome, have now stabilized 
to the point where two tubes 
per week (on the average) stop the computer during computation. 
All of the above troubles 
and others not 
discussed here have contributed to 
lost computing time 
on the UNIVAC. However, they cannot 
influence future operation because 
the reasons for 
them have been 
found and eliminated. The fact that these troubles 
will not occur in future UNIVACs cannot be emphasized too strongly. Under a contract with the Bureau of Census, Eckert-Mauchly Computer Corporation 
maintains the Census installation. This 
system is operated 24 hours a day, seven 
days a 
week, except 
for four 8-hour preventive 
maintenance shifts each week. This 
allows approximately 32 hours 
for regular maintenance and 136 hours 
for operation or 21 and 79 per cent 
respectively. Table 1 shows the engineering time spent 
on the computer system during typical weeks of operation. The figures are given both in hours 
and per- centages. Both nonscheduled engineering 
time as well as preven- tive maintenance time are 
shown. The sum of the two gives the total engineering time spent 
on the computer per 
week. It should be noted that this is actual engineering time and does not include 
time that the computer may have been shut down while waiting 
for an 
engineer to report. According to our maintenance contract, this must be within a half hour during 
regular working 
hours and within two hours at all other times. Attention should 
be given to the fact that the preventive maintenance time 
does not total 
exactly 32 hours each week. This 
is due in part to a half-hour 
period each morning devoted to checking and cleaning the mechanical portions 
of Uniservos. It is expected that this work will be taken over by the 
UNIVAC operators since 
the procedures and the 
techniques involved 
are quite simple. In addition, one extra shift was required the week ending June 3 and three extra shifts the week ending 
October 7, 1951. These shifts were required 
to incorporate engineering changes 
which had 
been developed over a 
period of time and could not be incor- porated in the equipment during 
the normal preventive main- 
Table 1 ?btd Week Nomcheduled Precentiue engineering 
Rrcentuge of nonscheduled ending enginvering muintenunce 
tinw 19.51 Z;lours Per Cent Hours Per Cent Hours Per Cent engineering 
- June 3 18.9 26 20.5 July 14 14.7 21 19.4 28 39.2 Aug. 4 26.2 Sept. 2 28.8 9 16.1 16 22.6 23 42.3 30 21.8 Oct. 7 15.9 14 14.0 21 10.4 28 20.8 Nov. 4 40.4 11 10.1 18 30.5 25 13.7 Dec. 2 14.8 9 19.6 11.3 12.2 8.8 11.6 23.3 15.6 
17.1 9.6 13.5 25.2 13.0 9.5 8.3 6.2 12.4 24.0 6.0 18.2 8.2 8.7 11.7 40 23.8 34 20.2 33 19.6 34.5 20.5 34.5 20.5 33 19.6 34.5 20.5 34.5 20.5 33 19.6 34.5 20.5 34.5 20.5 56 33.3 34.5 20.5 34.5 20.5 33 19.6 34.5 20.5 34.5 20.5 34.5 20.5 34.5 20.5 34.5 20.5 34.5 20.5 58.9 54.5 47.7 53.9 73.7 59.2 63.3 50.6 55.6 76.8 56.3 71.9 48.5 44.9 53.8 74.9 44.6 65 48 49.3 54.1 35.1 14.8 32 15.3 28 10.9 32 14.5 43.8 29.4 35.2 19.4 37.7 21.6 30 12.1 33 16.7 45.7 31.7 33.5 16.3 42.8 14.2 28.9 10.5 26.7 7.8 32 15.4 44.6 30.3 26.5 7.6 38.7 22 28.6 10 29.3 12.6 32.2 14.7 
168 Part 2 I The instruction-set processor: main-line computers 
Section 1 1 Processors with one address per instruction 
tenance time. The nonscheduled engineering 
time has varied from as little as 10.1 hours or 6 per cent to 42.3 hours or 25 per cent. The last column 
in the Table shows the amount of nonscheduled engineering time as compared to the 
allowable operating time 
(total time 
less preventive maintenance time). 
Here there 
is a variation of from 7.6 
to 31.7 per cent and 
an average for the weeks shown of 16.9 per cent. It 
is believed that these figures, while good for the first months of operation of a new piece of equipment, will show definite 
improvement over the next year. Although the opportunity to prove or disprove the following theory of operation has not 
presented itself, it is believed logical that optimum use of the UNIVAC equipment might be obtained by means of scheduling preventive maintenance 
only at such times 
as it is indicated in the judgment of competent operators. In other words, there are 
many occasions preceding a scheduled main- 
tenance shift when the system is performing very well. At such times, it is extremely inefficient to shut 
down the operation in order to provide maintenance. For many reasons, however, it has been impossible to operate and maintain the first system in this way. It is hoped that such operation will be possible in following installations. It should be realized that the 
UNIVAC system requires a super- visor of the same caliber as the one required 
for a large punched card installation. However, 
the large group of operating personnel would be replaced by a small 
group of well-trained extremely 
competent people 
thoroughly familiar 
with the details of the computer and associated equipment. The time spent 
in providing a high degree of training for these people is more than repaid in 
increased operating efficiency and consequently higher 
work out- put. For 
example, situations arise 
in the course of running a prob- lem where 
a correct operational decision can save hours of elapsed computation. Also, a competent operator 
will recognize malfunc- 
tions sufficiently early to prevent serious delays. 
He is capable of deciding whether to continue 
with machine operation or 
to stop to diagnose. The second UNIVAC system which is ready for installation in Washington, will 
be operated by a group of engi- neers who have been trained 
in operation and maintenance. This procedure, it is believed, will result in the UNIVAC system being of maximum benefit 
to the Air Comptroller™s Office. Evaluation of UNIVAC design Checking features Maintenance of the UNIVAC has been vastly simplified by use of duplicate arithmetic 
and control equipment and other checking methods. Many factors which would have led 
to undetected errors have, by 
virtue of duplication, immediately stopped 
the computer. Although checking by means 
of inverse operations can provide operational checks 
on the arithmetic circuits, there is some ques- tion as to whether it 
provides as good a 
check as duplication. However, in connection with 
odd-even codes, it may conceivably 
be comparable. It should be remembered, however, that this is from an operational standpoint and not a maintenance standpoint. 
When the 
control equipment is considered it is difficult to visualize a check that is as good 
as duplicated equipment. 
Other checks ed in UNIVAC 
include the periodic memory check, 
intermediate line function 
table checker, function 
table output checker, memory switch 
checker, and 720 checker. As explained earlier in the paper, the periodic memory check is accomplished by reading 
out of all memory channels sequen- 
tially and performing an odd-even check on each digit as it passes through the high speed bus amplifier. The period at which the check is repeated may be varied over a 
large interval. 
At present, it is set at 5 seconds, the check taking 
52 milliseconds or about 1 per cent 
of the computing time. 
The function table has a 
check at the 
very input by bringing 
in the check pulse in each character so that if an odd-even error 
occurs between the control register 
and the static register, no order 
will be set up and the 
computer will grind to a halt! If the input sets up properly but an error 
occurs farther on in 
the table, but not ahead 
of the intermediate lines (the linear set 
into which the input combinations are decoded), the error is caught at this point. 
The intermediate lines are broken into groups in such a way 
that an error is indicated when more 
than one line is set up in one group or the entire set. There is an exception to this in some groups where no error 
is indicated by this 
checker if more than one 
line is set 
up within the group. This has been allowed only in 
those cases where it 
has been shown that setting up two or more lines 
will cause some 
other checker or checkers to indicate 
the trouble. If the error occurs beyond 
the intermediate lines, the output 
checker then comes into play. This 
checker makes an odd-even count on the number of gates used on 
each instruction: dummy 
lines having been added 
so that the 
count is normally always odd. The memory switch or tank selector checker 
ensures that one and only one memory channel is selected on any instruction. It checks each of the two digit positions separately indicating 
which if either, is in error. The 720 checker counts 
the digits coming 
off the tape and 
if there are either 
more or 
less than 720 in one 
block, the computer stops; by examining the indicators on the supervisory control console, the operator can determine 
the number of digits actually 
Chapter 8 1 The UNIVAC 
system 169 read. By means of some rather simple manipulations, 
the operator can then reread the block without losing his 
place in 
the routine; and if the information is then read correctly, he may again 
start the computer on the routine. The same procedure may be followed if an odd-even error 
is made in reading from 
the tape. Many checks 
other than 
those mentioned before have been 
built into the UNIVAC. On the basis of operating experience, the engineers cannot 
recommend too strongly 
the use of built-in checking facilities. 
All in all, 
the faith that can be put into results obtained from an unchecked computer comparable 
in size 
to UNIVAC is in the writers™ opinion 
exceedingly low. 
More than this, however, the methods by which 
the UNIVAC is checked have been 
of extreme usefulness in 
trouble shooting. The duplication of circuits has 
amply repaid 
the increase of space and the number of components required by this checking system. General comments 
After evaluating UNIVAC performance over a 
period of eight months, the over-all picture of the UNIVAC design, in 
the minds of its designers, is extremely good. Certain phases of its design exceeded expectations, while 
of course, other phases were some- what disappointing. The first eight months 
of actual operation have taught 
more than years of experimentation with laboratory models. Many 
improvements have already been 
conceived of this experience and are 
continuing daily to increase reliability. 
The other major factor influencing computer design, cost, 
has been duly considered 
in the UNIVAC design; and it is being met with plans for a continuing full-scale production of UNIVAC sys- tems. As the production techniques 
are developed concurrently with the engineering design details, the UNIVAC becomes the realization of a hope which has long been in the minds of its designers: An economical, completely reliable commercial com- 
puter for performing the routine mental 
work of the world much as automatic machinery has taken over the routine mechanical work of the manufacturer. References McPhJ51. 
Section 2 Processors with a general register 
state The processors described 
in this 
section all have 
a processor state consisting 
of registers which are 
used for multiple 
(i.e., general) purposes. Perhaps 
a better 
name might be processors 
with a state consisting 
of a register 
array(s). The following machines are fairly similar in their ISP structure: Pegasus (Chap. 9), the DEC PDP-6,10, the SDS Sigma 5 and 7, and 
the UNIVAC 1107 and 1108. However, other computers 
includ- ing an 8-bit character computer (Chap. 10) and the CDC 6600 (Chap. 39) also use arrays of registers. The general register 
organization appears as a compromise 
between the 1 and 2 address organizations. It avoids some of 
the extra instructions for 
shuffling data, inherent 
in a 1 address system, but avoids taking the space for a full additional address. The index register 
organization is also such a compromise, 
but one that is specialized to address calculations. The general 
register organization moves further toward a 
full 2 address organization without much additional 
cost. This 
assumes a small relative cost for a small amount of memory that is sig- nificantly faster than the 
larger Mp. The design philosophy 
of Pegasus, a quantity-production computer 
Chapter 9 describes Pegasus™s logical organization and 
the technology from which it was implemented. The technology includes vacuum 
tubes, a cyclic memory, and 
dynamic logic based on delay lines. Pegasus has the nicest ISP processor structure discussed in this section-perhaps in the book. It is included because it is probably 
the first 
machine to use an array 
of general registers as accumulators, multiplier-quotient regis- ters, index registers, 
etc. This 
ISP organization should be 
com- pared with the IBM System/360 
(Chap. 43). Note that the multiple-register organization 
is independent of Mp.cyclic. This 
organization improves performance 
by generality. The structure of System/360 Part I-outline 
of the logical structure The IBM System/360 
is described in Part 6, 
Sec. 3, and is included mainly because of the very large number of such systems that have been built. An 8-bit-character computer 
This computer (Chap. 10) has been invented by the authors to show the composite features 
of a small character/word-oriented 
computer. In reality, 8-bit machines turn out to 
look either like 16-bit machines, because the Mp size accessed is 
usually >28 words, or like 
character-string processors. Because of the primitive nature 
of this machine, it is a 
possible alternative to the larger more complex microprogrammed processors for defining more 
complex ISP™s. Parallel operation in 
the Control Data 
6600 The CDC 6600, described in Chap. 39, has three arrays 
of eight registers each. Two of the arrays are used rather generally, and the third 
array is 
used to access words in Mp. The design 
of the CDC 6600 is a classic because of the computing 
power it provides. It is also worth studying 
as an example of a Pc assigned exclusively to data operation, 
with all 
concern with the 
larger PMS structure located in Pio™s. A discussion 
of it is given in Part 5, Sec. 4, page 470. 170 
Chapter 9 The design philosophy 
of Pegasus, a quantity-production computer1 
W. S. Elliott / C. E. Owen / C. H. Devonald B. G. Maudsley Summary The paper gives an 
historical account 
of the development of the packaged method 
of construction of computers, and 
the advantages of this method 
are discussed. The packages 
used in the computer Pegasus are described from both an 
electronic and 
a mechanical point 
of view. The 
specification of the machine is given and the arguments which led 
to this specification are discussed. The 
detailed logical design procedure leading 
from the specification to the wiring lists is described. The method 
of maintenance and 
some reliability fipres are given. Introduction The development of standard plug-in unit circuits (‚packages™) for 
digital computers began in this country [England] in 1947, and some of the advantages of the method have been discussed in earlier papers [Elliott, 
1951; Johnston, 1952; Elliott et al., 1952; Elliott et al., 19531. The advantages start in the design stage of a new computer 
project and follow through production 
and com- missioning to maintenance. In the design stage, what is known as ‚logical™ design 
is sepa- rated from engineering 
design. Once the packages have 
been designed by 
electronic engineers and the rules for 
their inter- connection have 
been laid down, 
the ‚logical designers™ (usually, 
but not necessarily, mathematicians) can 
begin organizing 
the packages into various computers to carry out 
different functional 
requirements. The electronic and mechanical design work 
invested in the packages is thus drawn on for 
more than one computer 
design, and each computer can 
be assembled from stock parts 
without further 
engineering effort. Design 
time and cost are there- fore much reduced. In production, 
whether we consider 
one design of computer or several designs using 
the same packages, costs 
and time are also much reduced. Quantity production 
lines for the relatively few 
types of standard package are set up, and are common to different computer designs, thus reducing inspection 
and planning costs. Standard cabinet work has been designed for Pegasus, and this ‚PRJC. IEE, pt. •3, vol. 103, supp. 2, pp. 188-196, 1956. too can be taken from stock 
or established production 
lines to make other computers. 
In commissioning a 
computer, because all 
the packages have been pretested, when 
power is first applied to 
the complete machine it is known that a large part is already fault-free. It remains to detect a few errors which may 
have been made 
in the interconnections. Perhaps an even more 
important consideration is ease and speed of maintenance. Test programmes will usually 
indicate the part of the machine in 
which a fault is occurring. Several monitor sockets are located on the front of each package, and by inspection the faulty package is speedily found 
and replaced. The package method has been criticized on the grounds of the cost and questionable reliability 
of plugs and sockets, and some redundancy of components. The authors believe 
that the 
many advantages far outweigh 
the cost of plugs and sockets. The present trend is to use copper- etched printed circuits, and these fall naturally into the plug-in unit idea, the plug contacts being part of the printed wiring; there has been no trouble 
in Pegasus from plugs 
and sockets. Component redundancy in Pegasus 
is about 10% of the diodes and a few 
resistors, the cost of redundant components being 
about 2 150. Electrical design of the packages Circuits used for arithmetic and switching operations Historical. A previous data-processing machine [Elliott et al., 1952; Elliott et al., 1956bl used 
330 kc/s serial-digital circuits; 
they had originally been designed for 1 Mc/s operation, 
but 330 kc/s 
waschosen to suit an anticipation-pulse cathode-ray-tube store. This 
frequency has been retained to 
the present time because it suits the magnetostriction delay-line store [Fairclough, 
19561 and the magnetic-drum store [Merry 
and Maudsley, 19561. Experience with the data 
processor led to 
work (commenced in 1951) on a 
new set of circuits [Elliott 
et al., 19521, particular emphasis being 171 
172 Part 2 1 The instruction-set processor: main-line computers 
Section 2 I Processors with 
a general register state 
laid on flexibility 
of use and ability to work without error in 
high electrical interference 
fields. These circuits 
form the basis of those in Pegasus. Operations to be carried out. The following well-known 
opera- tions are used to build up the logical structure of the computer: ‚And.™ This operation, which 
may be carried out between 
two or more input serial trains 
of pulses, produces 
an output train in which pulses occur only when pulses are present at the same time on all inputs. ‚Or.™ This operation produces an output train 
in which 
pulses occur at all times 
when a pulse 
is present on any of a number of inputs. ‚Not.™ 1™s are changed 
into 0™s and 0™s into 1™s; this is achieved by inverting the pulse train. Digit Delay. The passing of a pulse 
train through 
a digit 
delay produces a pulse 
train similar to the input, but each pulse is one pulse position 
later in timing 
and restandard- ized in shape. 
(I b c d All operations in the computer, including addition, subtraction, 
and staticizing, are carried out by combinations 
of these elements. There is no circuit specifically for 
addition, and there are, 
in general, no 
flip-flops such as are often used 
for staticizing or storing a single 
digit. A similar philosophy 
was arrived at independently by the designers of SEAC and DYSEAC [Elbourne and Witt, 19531, bnt the 
detailed working out 
is considerably different. 
Digit wavefoms. The timing of digit pulses throughout the ma- chine is controlled by a common 
‚clock™ waveform-a 
3 micro- sec square wave (Fig. 
la) in which 
the positive-going portions define digit positions. The digit pulses, which are routed about 
the machine and ap- plied to logical circuits, are generally of the form shown 
in Fig. lh; as generated, they have 
their leading edges well 
in advance of the clock pulse 
and are of a greater amplitude. 
This means 
that considerable distortion 
of the pulse is tolerable, since only the portion which 
coincides with positive clock pulse 
is of conse- quence. Digit pulse 
trains are 
‚clocked™ (‚and™ operation with 
clock) only at their entry into a storage system or into a digit-delay 
circuit. Inverted pulses are also employed: as an illustration, 
consider the operation ‚A and not B™. Pulses A and B (Fig. 1) are on two lines and are of the same nominal 
timing, and we wish to form A . B (symbolic representation of ‚A and not B™). To do this pulse B is inverted (forming B, or ‚not 
B™) and is used to gate pulse A and prevent its 
passage. The inverted pulse will 
be a little late 
on B, which also may 
have been later 
than A, as shown in Fig. IC; thus when 
A and B are ‚anded™ together 
a spike may 
be pro- duced, as shown 
in Fig. le. This spike, however, lies between clock pulses and so will be rejected on clocking. The pulse system used 
allows several logical 
operations to be 
performed in cascade without 
any loss in nominal timing, so easing the problem of logical design 
(particularly by permitting after- thoughts). The maximum number of logical operations performed 
m +2 to + 3 volts ™I j I 1.5~ sec , I -10 to -11 volts Fig. 1. Basic waveforms. 
Chapter 9 I The design philosophy 
of Pegasus, a quantity-production computer 
173 (b +zoo volts t200 volts + 200 VOI tS 470 kfi &--...., Input clock +zoo Volts +zoo Volt* 330kSZ ,, Clock -150 Volts (ai ﬁ2 -output 1 ,output 2 (bl Reset -150 -150--150 ﬁDits YOltS YOltS Fig. 2. Digit-delay circuit. 
in cascade 
in Pegasus is 
five, though up to 12 could be performed in special circumstances. 
The logicul circuits. Each of the logical packages has more 
than one circuit 
unit. A circuit unit 
is defined as 
that part of a package which has 
input and output pins, and no connections to other parts of the package other than 
supplies. We may make 
the following generalizations: a h Each unit 
has an ‚and™ gate at its input. Each unit has a cathode-follower output (half a 12AT7 valve). Each unit 
has an 
additional output via a germanium diode 
for making 
‚or™ gate connections. c [Note: There are 
exceptions to (a) and (c) on one package type.] 
There are three possibilities for 
the part of the circuit unit 
between the input 
‚and™ gate and the output 
cathode-follower, namely a digit 
delay (half 
a 12AT7 valve), an inverter 
(half a 12AT7 valve), and a direct connection. Space does 
not permit a 
description of all the circuits, so it is proposed to deal 
only with the digit delay. The circuit is shown in Fig. 2, and some typical waveforms are shown in Fig. 3. The input 
circuit can be 
of two forms, namely a 3-input 
‚and™ gate and two 
such gates with 
their outputs 
‚or-ed™ together. In both cases there is a further gating with a 
clock pulse. 
The clocked digits from the gate input circuit are applied to the grid of VI, the anode voltage of which falls, 
so building up a current in L. When VI is cut off at the 
end of the digit, this current flows through diodes D, and charges up a storage condenser, 
C, which is discharged at the end of the next clock pulse by 
a ‚reset™ pulse applied through D,. The reset pulse supply is a common computer supply whose amplitude and phasing relative to 
the clock pulse 
is shown in Fig. 3. It will be noted that the 
reset pulse is also present at a time, just after V, is cut off, when the current 
in the inductor is about to charge the storage condenser. 
This merely has 
the effect of deferring the charging of C until the 
end of the reset pulse, 
the -10 volts (C) approximate Fig. 3. Digit-delay waveforms. 

174 Part 2 I The instruction-set processor: main-line computers 
Section 2 I Processors with 
a general register state 
current in the meantime continuing to 
flow through the diodes with little loss in the stored energy of L, since the voltage across L is low at this time. The output 
cathode-follower V, is caught at - 10 volts in the negative direction 
by a diode; this safeguards 
the crystal-diode circuits driven 
by it in 
the event of failure of the h.t. supply or V,, and it removes residual ripple on the bottom of the input waveform, and thus reduces 
the back voltage and hence leakage in diodes 
of gates driven by 
the output. The second output through a diode can be 
used in 
conjunction with similar outputs from other circuits and a resistor 
(pins 3 and 4) to make an ‚or™ (up to about 
16-way). In general, 
each output circuit has two available load 
resistors, disposed between direct 
and ‚or™ outputs according 
to a set 
of rules which are applied 
for each case. The number of units which 
can be driven by an output can vary between three 
and 16 according to circumstances; where more have to 
be driven than the rules allow, use is 
made of ‚booster™ cathode-followers available 
on one of the packages. Some examples of the use 
of the logical circuits 
Two examples will 
be given, the first being a simple arrange- 
ment-the staticixor-which 
is used frequently, and the second being a complicated arrangement-the adder/subtracter-which 
is used infrequently. The symbols used to indicate the circuit units are shown in 
Figs. 2c and 5h. The staficizor. The function of a staticizor is to remember the fact that a digit occurred at a particular time, 
for an indefinite period, the method generally used in Pegasus 
being shown in Fig. 
4. A digit delay 
with a twin ‚and™ gate input 
has its output con- nected to one of its inputs. It is turned on by gate 1, which causes a digit 
to circulate 
as long as 
the inputs 
to gate 2 remain positive. Staticizor is turned /off if either of these leads is negative Staticiror is set if these leads are positive r Fig. 4. The staticizor. X+Y or X-Y (Delayed one Carry Add Subtroct (O) suppression Cathode Inverter Digit AND Gate follower delay (b) Fig. 5. The adder/subtracter. It is normally turned off by an inverted pulse (a ‚0™ following a 
series of 1™s) on one of the gate 2 inputs. The adder/subtracter. Figure 5 shows an adder/subtracter unit 
with inputs X and Y and an output X + Y for the sum or 
X - Y for the difference. There are 
two further input 
control leads marked ‚add™ and ‚subtract™. If the ‚add™ lead is held positive while the ‚subtract™ lead is held negative, the unit acts as an adder. If the ‚subtract™ lead is held positive and the ‚add™ lead negative, the unit acts as a subtracter. Carry 
suppression is controlled by the lead marked ‚carry 
suppression™. Carries are 
allowed to propa- gate when this lead 
is held positive, so that a negative signal on 
this lead will snppress carry. 
Table 1 gives the digits appearing at the outputs of logical elements in 
the adder/subtracter unit 
for all combinations 
of input and carry digits when the unit is operating as an adder. 
Arrangement of circuits bused on 
packages It was required to 
base the logical circuits OII a standard size of package which could 
also be used for 
other circuits, e.g. a nickel- 
line 1-word store [Fairclough, 
19561. A unit which could accom- 
modate three 
valves and had a 32-way 
plug was decided on; 
the 
Chapter 9 I The design 
philosophy of Pegasus, a quantity-production computer 
175 Table 1 when set 
to add, for all combinations 
of the input 
and carry digits 
Digits at various internal points 
of the adder/subtracter unit 
Present Digits at 
internal points digit A B c DEF Inputs digits carry (Sum) (Next XY Z carry) 00 00 01 01 10 10 11 11 0 1 0 100 1 1 
0 110 1 1 
0 110 0 1 1 010 1 1 0 101 0 0 
1 111 0 0 1 111 1 1 1 011 Note.-A and 
Care at the grids 
of the digit delay units. 
problem then was to arrange 
the various circuits in such a way as to enable 
a computer to be designed using a 
minimum total number of packages without too 
many types. Five types 
were arrived at and these are shown in 
Fig. 6. As an example of the factors involved, consider package types 
WW NOTE Clock connections are not shown, they are implied whenever a delay 
symbol is used. (U) 1 and 2. The circuit units 
based on package type 1 can perform all the functions of those on 
type 2. However, there are many uses for a 
digit-delay circuit with 
a single ‚and™ gate input (package type 2), and since three units of this kind (instead of two for a 
2- ‚and™-gate input delay) can be based on one package, 
a saving 
can be 
effected. In Pegasus this saving amounts to 32 packages, which is considered to be well worth an 
extra package 
type. In addition to the 
five logical packages, a 
further 16 types (three of which are peculiar to each computer) 
are required. The numbers used for the various functions are given below: Number Type 1 113 Type 2 64 Logical types Type 3 55 Type 4 45 Type 8 37 61 38 17 14 Total 444 i Nickel line 1 word store Drum-store packages 
(8 types) Input/output packages 
(3 types) Clock and reset waveforms (3 types) ~ Fig. 6. Contents of logical packages. 
The arrowhead 
on an output lead denotes 
the presence of an OR crystal connection. 

176 Part 2 I The instruction-set processor: main-line computers 
Section 2 I Processors with 
a general register state 
The magnetic-drum store 
and the circuit packages used 
with it are 
described in another paper 
[Merry and Maudsley, 19561, as is the nickel-line store [Fairclough, 
19561. The mechanical design 
of the packages General form Each standard package 
consists of three main parts, namely the valve panel, the component panel and 
the plug. The valve panel is an aluminium 
pressing, there being three types-a 3-valve type, a 2-valve 
type and 
a blank. 
The package type number is marked on the panel by 
two dots according 
to the standard resistor colour 
code. The component panel 
houses up to 100 components, including 
small transformers, 
chokes and coils, the panel and the handle 
being made in one piece 
from sheet insulating material. 
This design provides a minimum resistance 
to airflow over the valves and gives ample protection to the valves against accidental dam- 
age. The plugs and sockets are used in multiples 
of eight connec- 
tions. Most of the packages have four plugs providing 32 
connec- tions, but up to 
64 are possible in 
each package. 
The plug contacts 
are made 
of brass and are 
heavily silver-plated. The socket uses a proprietary valve-holder contact, which can readily be replaced if damaged. SOCKETS / PLUGS Fig. 7. Standard package. This combination 
of plug and socket has a consistently 
low contact resistance (0.003 ohm at 1 amp); the 
insertion and with- drawal force is about 4 oz per contact. The wiring of the packages 
At present packages are wired 
and soldered by 
hand. The wiring is point-to-point, and within the limitations of layout for efficient 
performance, wire lengths are standardized 
for mass 
production on automatic wire-cutting and stripping machines. The symmetry of the eyelet positions makes 
it possible to use components which 
are preformed to a standard pitch 
and would allow 
for automatic preforming and insertion of components. Experimental packages have been produced by photo-etched wiring and dip soldering. Specification of the computer Pegasus Summary specijication A detailed specification would cover 
the ground of the program- ming manual 
[Pegasus Programming 
Manual, Ferranti Ltd., 
London] and would be out of place here. Pegasus is a binary serial-digital computer. The word length 
is 42 binary digits, of which 39 digits are used for a number and its sign (negative numbers 
are represented by their complements 
with respect to two), 
one digit 
is used for a parity check 
and the other two 
are gap 
digits. The length of an order 
is 19 binary digits, so that one word may 
consist of two orders, 
the remaining digit 
being a ‚stop-go™ digit. If the ‚stop-go™ digit is a ‚V, the computer will stop before obeying 
the orders in the word, but will proceed unhindered if the digit is a ‚1™. There is a 2-level store, a magnetic drum holding 5120 words 
and an immediate-access or 
computing store 
of 55 single-word magnetostriction delay lines. 
An order is made up of seven N-digits, three X-digits, six F-digits and three M-digits, the N-digits being the most significant 
and the 
M-digits the least significant. 
The N-digits allow 128 addresses in the immediate-access store (of which only 63 are used). The reg- isters in this store are shown in Fig. 8. The X-digits refer to one of the accumulators, the registers corresponding 
to N-addresses 0-7. Thus the order code is a 2-address code with one 
address referring to only a limited part of the store. The F-digits indicate the function of the order. A list of functions and their correspond- ing F values are given in the appendix of this chapter. The M-digits indicate a modifier for 
the order: they select one 
of the accumula- tors, and the 
modification process 
is to add certain parts of the contents of the selected accumulator 
to the 
order before it is 
Chapter 9 I The design philosophy 
of Pegasus, a quantity-production computer 
177 In E- + 8 a z 8 9 NAME OF ADDRESS NOTES REGISTER OF REGISTER BLOCK TRWYERS TO AND FROM MAIN STORE 
1 0 ----- ALWAYS ZERO SINGLE- WORD TRANSFE ACCUMULATORS 2 - (ORXREGISTER 3 - THESE ARE 4 - THE REGISTERS 5 - USED FOR 6 - MODIFICATION 7 - ]DOUBLE LENGTH HAND SWITCHES 
(20 DIGITS) i1 - - INPUT/OUTPUT CHECKED (5 DIGITS) SPECIAL I:ﬁ l7 -  UNCHECKED (5 DIGITS) BCOCK 0 BLOCK 1 BLOCK2 BLOCK3 BLOCK4 BLOCK 5 ALWAYS- 1.0 ALWAYS ‚Tl0 REGISTERS 32 - 33 - ALWAYS % I:: = ALWAYS 2-13 0™7 10 ™ROGRAMMERS NOTATION Fig. 8. Allocation of 
addresses in store. obeyed, the part chosen depending on the function of the order to be modified. Figure 9 gives a schematic representation of the modification process. The effect of modifying an order depends on the function of the order and can be to make the effective order length 22 digits. This extension is necessary when specifying an address in the main store. Transfers of information can take place between 
the computing store and the 
main store, and vice versa, either in 
single words or in blocks 
of eight words. For single-word transfers, 
only the register with address 1 in the computing store is involved. For block transfers 
the address on 
the drum of the first word of the block must 
be divisible by eight, 
and the registers in the computing store that are 
involved will 
be one of the discrete blocks indicated in Fig. 8. Input and output 
is by means 
of punched paper tape. 
An ‚exter- nal conditioning™ order is included in the code to 
enable a choice of input and output 
equipment to 
be made. In the standard machine, two tape 
readers are used. All stored information is checked (when read) 
by means of a parity digit, which 
is such that the 
total number 
of 1™s in any 
correctly stored word is odd. The input and output 
of decimal characters on tape can 
be checked by a similar process. 
The considerations 
which led to the specification and the 
logical design The main features of the design are a The use of a computing store from which all orders and numbers are taken while 
computing The provision of multiple accumulators The provision of special orders 
and facilities for dealing easily with ‚red tape™l b c The computing store. The use of a fast-access 
store from which all numbers 
and orders are taken increases the speed of the machine and eliminates the need for optimum programming. It is this computing store which 
makes it possible to use an inexpen- sive magnetic drum (with 
a relatively long access 
time) as the main store, and yet have 
a machine which is fast 
and relatively simple 
to programme. On 
the other hand, 
programmes have more ‚red 
tape™ and are 
not as simple as with single-level storage. Transfer between levels is in blocks of eight words; this 
is a simplification and saves time. One 
block holds a 
reasonable amount of programme and other blocks hold data. Four 
blocks in 
all (32 words) would be just sufficient, 
and Pegasus was originally 
de- signed with this number. The design was 
subsequently modified to six blocks, which is quite adequate, in conjunction with 
the seven accumulators. 
Any further increase in the size of the com- puting store would 
be achieved by increasing 
the size, not the number, of blocks. As it is there is an economic balance 
between the usefulness and the cost of the computing store. ﬁRed tape™ is an expression for 
the non-arithmetic orders in a 
programme. SHADED PORTION IS ADDED TO THE ORDER. THE FULL 13 DlGlTS ALWAYS APPEAR IN X REGISTERS IN SIGNIFICANCE SUCH THAT THE MOST SIONIFIUNT DIGIT CORRESPONDS 
TO 2-I (AND LEAST SIGNIFICANT TO 2-13.) FUNCTIONS 0037 . . .... . ... FUNCTIONS 40.61- FUNCllONS lO,lI,74.13 fUNCTlONS 7.?,13,16,71~ 0. Fig. 9. Order-modification process. 
178 Part 2 1 The instruction-set processor: main-line computers 
Section 2 I Processors with a general register 
state The procision of several accumulators. This is the most novel feature of the logical design 
of Pegasus. It is generally agreed 
that the simplest order code from the user™s aspect is the 3-address code with orders of the form, A + B+ C. An examination of this form of code, however, 
shows that in many cases two of the ad- dresses are the same, so that the order takes 
the 2-address form, 
A + H 4 A. A further examination shows that in a 
large propor- 
tion of cases the address A is confined to a very 
few addresses. This leads 
to the suggestion of a code of the form N + X--t X, where X covers only a small 
part of the store while N covers the whole store. This 
will have the advantage of yielding a reasonably 
short order. In Pegasus two such orders 
are incorporated 
in one word, leaving 
sufficient digits to specify a modification 
register (a Mancunian B-line) in each order. The extreme case 
of this code is, of course, the single-address code, where 
X is confined to one address, the accumulator. How- ever, experience 
had convinced the programmers collaborating 
in the design of Pegasus that, with 
single-address codes, a large number of orders are concerned 50kly with 
transfers of numbers from one register 
to another; the single accumulator is a restriction through which 
all numbers must 
pass and in which all operations have to 
be performed. In the Manchester University computer the B-lines serve two very valuable but distinct purposes: they allow order modification and rudimentary arithmetic (such as counting) to 
be done without 
disturbing the accumulator. It was felt that fuller arithmetic and logical facilities on 
these B-lines would have 
been extremely valu- able. The seven accumulators 
in Pegasus, used for modification 
and arithmetic, are a development of the B-line concept. Special facilities for dealing with ‚red tape™. The difficulties asso- 
ciated with 
the 2-level storage system have been 
greatly reduced by having an order-modification procedure which depends on the function of the order (Fig. 9). This method of modifying orders, 
used in conjunction with order 
66 of the code (the unit-modify order), enables the counting through 
blocks of information to be done with 
relative ease. 
The use of the group-4 orders 
of the code enables counters 
to be set conveniently 
and a constant (up to 127) to be placed in 
an accumulator, the constant being 
the value of the N-digits of the order. Order 67 (the unit-count order) 
enables the counting of cycles of operations to be dealt with in 
a simple way. A jump to another part 
of the programme can be 
programmed to take place automatically when the required number 
of cycles has been performed. Having a large number of jump instructions greatly helps 
in organizing a programme. In particular, 
one order 
enables a jump to be made depending 
on the condition of an accumulator (being 
zero, for example), and another order on the complementary con- dition (being not 
zero). When only one of these orders 
is available it is necessary to think ahead to see whether or not the correct condition will be satisfied. Although the eight jump instructions included in 
the code were felt initially 
to be enough, it is now suggested by programmers 
that even more such orders would 
be helpful. The logical shift 
orders, 52 
and 53, are also included to simplify ‚red tape™. In 
particular, they 
are used for 
packing and unpacking words holding several items 
of information. As a result 
of including these 
various orders, the order code 
of Pegasus is quite large. It is worth remarking, however, 
that by a sensible 
grouping of the orders in the code the remembering of the code is a very 
simple task. A sensible 
arrangement of the code tends to reduce 
the amount of equipment needed 
to engineer it. For 
example, when the equipment 
for dealing with 
group 0 of the code has been allocated, groups 1 and 4 require the addition of only three gates. Facilities for checking programmes. 
The features mentioned above make the computer easier to programme, and there are 
other facilities in Pegasus 
that make it easier to check out and develop new programmes. These include causing 
the machine to stop 
obeying orders, 
either under 
programme control 
or when the programme is in error. In 
particular, the machine stops 
if an order for writing in 
the main store is reached and an overflow indicator is set. A further aid when testing new 
programmes is the automatic punching out 
of all main-store addresses appearing in block- 
transfer orders. 
When this information 
is examined an indication of the course of a programme is readily obtained. The punching can be inhibited by a switch when 
a return to 
full-speed running is needed. Machine rhythm The logical design 
of Pegasus is built around a nucleus that deals with the simple arithmetic orders, groups 
0, 1 and 4, of the code. This nucleus 
contains the 
control section, 
i.e. the order register and order decoding 
equipment, and the 
mill in 
which these orders 
are executed. The design of this nucleus could 
not begin until a basic rhythm for dealing with the extraction from the computing store and the 
execution of such a pair was determined. When the 
outline of this nucleus 
was clear, the equipment for dealing with the remaining orders 
in the code was designed to 
fit it. 
Chapter 9 I The design philosophy 
of Pegasus, a quantity-production computer 
179 The following arguments led to the basic rhythm. Since the orders of groups 0, 1 and 4 are similar in many respects, 
for definiteness, it will be sufficient to consider a 
particular order, 
11 of the code, say. This is an order which takes two numbers 
from the computing store 
and replaces one of them by their sum. It would take a prohibitive amount 
of equipment to extract these 
numbers, add them 
together and have the least significant digit of the sum available for replacing in the store in the same digit 
time as the least significant digits of the two components taken 
out of the store. In practice, some four digit times 
at least would 
be needed for this sequence 
of operations. Thus, 
it would be im- possible to return 
the sum to the 
store in the same word as the operands are extracted without having an entry point 
to each register which is in a different timing 
from the normal circulation entry. To produce two 
such entry points 
to each register would 
mean more equipment associated with each 
register, which was considered an 
uneconomical use of extra equipment. Instead, 
it was decided to delay the sum so that it could enter the 
register in the computing store 
in the next word time in standard timing. 
This involves one common delaying 
circuit instead of one for every register. Such 
an order therefore 
takes two word times 
to execute. It may be argued that this second word 
time could 
be made 
to overlap with 
the first word time for the next order. Two reasons 
oppose this: 
the new 
contents of the register being changed might 
be required by 
the next order; and two 
different sets of equipment for selecting a storage register would 
be needed if numbers were 
to be 
extracted from one and replaced in 
another register in the same word time. Thus, the execution of a pair 
of orders taken 
from the comput- ing store requires 
four word times. The reasons for 
opposing the overlapping of the execution of two orders also oppose the extrac- tion of an order pair while 
the previous pair is being dealt with. 
Five word times 
are therefore needed for the process of extracting and obeying a 
pair of simple arithmetic orders. More 
time may be needed for some of the other 
orders in the code. The basic 3-beat rhythm 
is thus established: a h c Obey the second order. Extract the order pair 
from the computing store. 
Obey the first order of the pair. The duration of beat (a) is one word time; 
beats (b) and (c) are each two 
word times long 
for orders in groups 0, 1, 4 and 6 of the code, 
but may be longer for other orders. Times for typical operations The times for the various arithmetic operations are: 
millisec Addition and subtraction . . . 
. . . . 
0.3 Multiplication . . . 
. . . . . . 
. . . . . 
2.0 Division . . . 
. . . . 
. . . . 
. . 
. . 
. . . 
5.4 These times include an allowance 
for the time to extract the orders. Some times for standard subroutines are: millisec Exponential function 
. . 
. . . . . 
. . 
. 29 Sine function . . . . . . 
. . 
. . . 
. . . 
. 24 Logarithmic function 
. . . . . . 
. . 
. . 34 Finally, to give some 
indication of the time for a typical prob- 
lem, a set of 50 simultaneous 
equations (with a single 
right-hand side) takes 
about 10y4 min. Of this time, 3 min 8 sec is for input, 7 min 17 sec is for calculation and 18 sec is for output. Realizing the specification The detailed logical design 
It would take too long to describe fully the detailed logical design. 
One aspect is worth mentioning, however, 
namely the avoidance of all ‚exceptions™ 
in the results of orders. As an example of an exception consider the overflow indicators, which should 
be set whenever the final result of an order 
is outside the permissible range of numbers. In multiplication this can occur only when both the multiplier and the multiplicand are - 1, and this is likely to occur very infrequently. Rather 
than provide equipment to sense this infrequent case, it is easier to put 
a footnote in 
the program- ming manual, where 
the overflow indicator is described, pointing 
out the exception. It was felt, however, 
that such exceptions should 
be avoided even at the expense of extra equipment or extra com- plication. For this and other 
reasons concerned with facilitating 
machine use, the logic of Pegasus is quite complicated. The end-product of the detailed logical design is a series of diagrams with 
symbols corresponding to the circuit units of the packages, as shown, for example, in Fig. 5. The inputs and outputs 
of the units on these diagrams correspond 
to the pins of the sockets into which the packages plug. 
Thus, the wiring lists of connections of these pins can be produced from these logical diagrams. 
The first step in the production of these lists is to allocate a position 

180 Part 2 1 The instruction-set processor: main-line computers 
in the cabinets to each 
logical circuit in 
such a way as to reduce the amount of wire needed. When the layout has 
been completed, 
the last stage 
of producing the wire lists 
can proceed. 
General construction 
of machine The main units are shown in Fig. 10. 
The package 
frame. This unit is a simple light-alloy 
frame sup- porting diecast light-alloy frame racks to which the back socket 
panels are fixed. The packages slide 
into grooves in the rack and plug into 
sockets at the 
back, a polarizing 
feature preventing the insertion of a package upside 
down. If electrical or magnetic Section 2 I Processors with 
a general register state 
screening is necessary between any packages, 
a special 
metal plate is inserted in 
slots in the cast rack 
and is fixed by a single screw 
in the back panel. Coded 
aluminium strips containing 
coloured plastic studs which identify 
the position of each package are fixed to the 
front of each casting. Arrangement of the packages. There are 
200 packages per cabinet, arranged in ten horizontal rows of 20 units per 
row. The metal valve panels are placed so that the 
edges almost 
touch. The com- ponent panel 
of each unit is in register with the unit in the corre- sponding position in each 
of the other 
rows, thereby providing 
vertical chimneys for cooling the components secured 
to these BAY I LOGlC PACKAGES \ I 8AV 2 .OGlC PACKAGES BAY 3 INPUT EQUIPMENT Fig. 10. Main units. 

Chapter 9 I The design philosophy of Pegasus, a quantity-production computer 
181 panels. Warm 
air from the main source of heat, the valves, is prevented by the 
valve panels from reaching the more tempera- ture-sensitive components, such 
as diodes, secured to the 
com- ponent panel. 
The back panel 
wiring. For locating long 
signal wires 
between sockets a system 
of plastic strips 
is used, which hold 
the wires at definite positions given 
by the instructions on the wiring lists. The exact route of every wire 
is predetermined, thus making 
wiring and inspection more reliable 
and fault finding and mainte- nance easier. Final assembly. The completely wired frame is assembled in its 
cabinet, which has already been fitted with the control and auxili- ary supply circuit unit, 
heater transformers, fuses, cooling assembly 
and cablefornis. The work of connecting the cableforms, heaters and earths can 
be done by relatively unskilled labour working to clearly written instructions and diagrams. The cooling system. Each cabinet has its own 
cooling system 
as an integral part 
of the construction; there is therefore no difficulty in cooling 
cabinets added to existing computers. Two axial-flow 
turbo blowers are mounted in 
the base beneath an 
airtight pressure chamber, each 
providing 300 ft3/min of air at a total pressure head of 1 in (water 
gauge). The maximum temperature rise is 10ﬂ C. The power supply. A separate cubicle 
houses metal rectifiers, shunt stabilizing valves and control circuits. 
The power is obtained from the mains through a motor-alternator set, 
the output 
of which is stabilized to 2%, the main purpose of this set being to act 
as a buffer against switching 
surges and other mains voltage variations. 
The valve heaters in 
the computer are energized 
from the stabi- lized alternator output, which is expected to extend the valve life. 
Maintenance General All digital computers so far have a fault rate which cannot be ignored. When the best has been done in 
the choice of components, circuits and mechanical construction, 
attention must be paid to 
the following points to get the best out of a machine: a Rapid fault location 
b Getting the machine working again 
as soon as 
possible after locating a fault c Preventive maintenance Fault location There are 
parity-checking circuits 
on both the main and the high- speed stores. Errors of a single 
digit in the stores stop the machine. The fault can then 
be quickly located by examination of the monitors. For other 
faults the general method is to run a test programme 
(assuming the fault is not in 
the main control) which will indicate the area of the fault. Detailed 
examination can then be carried out with the monitors. All outputs of circuit units are readily accessible at monitoring sockets on 
the front of each package, and in addition about 80 points can be directly selected by switches from 
the monitoring position: these include 
all store lines 
and a number of key wave- forms. Fault-finding 
is normally a matter of tracing 0™s and 1™s through the machine with reference to logical diagrams rather than electronic circuit 
diagrams. A variety of triggers can be selected for the monitor time-bases, these including 
a Trigger at any word 
position within a drum revolution (128 different times selectable by switches) 
Trigger at any word 
time of any selected 
order h These triggers and some other monitoring facilities 
are pro- duced by 19 standard packages and are found to be well worth the extra equipment. Fault repair Once a faulty package 
has been located, the machine can be got working again immediately by replacement of the package with a spare; repair 
of the faulty package 
can be done at leisure with the aid of a package tester. With this equipment 
a package can quickly be given a series 
of standard tests; each is selected by 
switches, and the 
performance is measured either by observation of meters or a built-in oscillograph. During commissioning not one 
case was found of the first machine doing 
other than what one 
would expect 
from the logical diagram (except 
for a very few 
cases of incorrect wiring). Preuentiue maintenance The machine h.t. supplies are reduced while the test programmes 
are being run. This marginal 
testing shows up incipient faults such 
as deterioration in valves, crystal diodes or resistors. The machine is at present kept in good 
running order down 
to 10% margins 
182 Part 2 I The instruction-set processor: main-line computers 
30 31 32 33 34 35 
36 37 Section 2 I Processors with a general register 
state ,Not allocated (the supplies are normally controlled to about 1% of nominal), although correct running at about 20% reduction has been ob- served. for 55ﬂ/, hours™ running. The majority of package replacements are 
done during routine maintenance. 
The packaged method of construction of computers has proved 
to have great advantages 
in design, 
construction and operation. Conclusions The first machine has 
been computing 
regularly for only a few months and has been on regular preventive maintenance (about 
1 hour per day) 
for a 
few weeks. Error-free runs of over 30 hours are common, and at the time of writing there has been no error References ElliW56a; ~lbo~53; ElliW51, 52, 53, 56b; FairJ56; JohnD52; MerrI56; 
Pegasus Programming Manual, 
Ferranti Ltd., London: 
Pegasus Mainte- nance Manuals, 
Ferranti Ltd., London. 
40 x‚ = c 41 X™=X+C 42 x™ = -e 44 x™=c--x 45 x™=x&c 46 X™=X~C 43 = - APPENDIX .c = ~2-38 The Pegasus Order Code 00 x™ = n 01 x™=x+n 02 x‚ = -n 03 x‚=x-n 04 x™=n-x 05 x™ = x & n 06 x™ = x$n 07 Not allocated 10 n™ = x 11 n™=n+x 12 n™ = -x 13 n™=n-x 14 n™=~-n 15 n™ = n & x 17 Not allocated 16 n™=nfx this order 
assumes that any overflow is 
due to opera- tions in 7. Clears overflow I unless n‚ overflows 23 (nq)™ = n + 2-3xy 0 2 p™/n < 1 (unrounded division) 25 -y2 5 p™/n < ‚/z (rounded division) 26 q‚ + 2-38(:) = x; -y2 5 p™/n < Y2 (rounded single- 27 Not allocated n length division Note: x™ = x single-length arith- metical shifts 50 x‚ = ZNx 51 x™ = 2-lVx (rounded) 53 Shift x down N places ] shifts 52 Shift x up N places single-length logical Note: p™ = p and q™ = q I ifN=O double-length arith- metical shifts I 54 (pq)™ = 2N(pq) 55 (py)™ 2-N(pq) (un- rounded) 
Chapter 9 1 The design philosophy 
of Pegasus, a quantity-production computer 
183 56 (Normalize) (pq)™ = 2p(pq); either (1) y4 5 (pq)™ < ‚/z and I -1IpsN-1 57 Not allocated 60 Jump to N if x = 0 61 Jump to N if x#O 62 Jump to N if x 2 0 63 Jump to N if x < 0 64 Jump to N if overflow staticizor clear; clear 
overflow staticizor. 65 Jump 
to N if overflow staticizor set; clear 
overflow staticizor. 66 (Unit-modify) x& = xm + 1. Jump to N if x& $0 (mod. 8) 67 (Unit-count) x: = x, - 1. Jump to 
N if x: # 0 70 Single 
word read to accumulator 
1. 71 Single word write from accumulator 1. 72 Block 
read from main store 
1™ = s s‚ = 1 u‚ = b 73 Block write into main store 74 External conditioning 
ﬂ]Not 76 allocated 77 stop h‚ = u The notation used here is as follows: N is the first address (the register address) 
in an 
order. X is the accumulator specified in 
an order. n is the word in N before obeying 
the order. x is the word in X before obeying 
the order. p and q are the 
words in 6 
and 7 before obeying 
the order. (pq) = p + 2-38q, with 2 0. This is a double-length number. x‚, n™, p™ and 9™ are the corresponding values after obeying the B is a block in 
the main store (the drum). 
U is a block in the computing store. P is the position number of a word within 
a block. OVR is the overflow indicator. xm is the modifier in X, i.e. an integer represented 
by the digits xc is the counter in 
X, i.e. an integer represented 
by the digits order. 1 to 13 of x. 14 to 38 of x. 
Chapter 10 000 001 010 011 loo 101 An 8-bit-character computer 
10 101 50 SO1 A-M[Ra A-M[Ra:R-R+L' M[RD]-A M[Ra-A; R+R+L' Ir, ori srd aid (2-51 (2-51 (3) (31 odl SUI br bld (11 111 (2) (31 cbr cbd C"r cnd (0' (11 (11 (11 Q-im Q-Rtim M [d]- R Q-M[d] R-RfL' R- R - I.' P- R P-d,R-P f(r.i.N,Z,C+(P- f(s.d)l - (2) (31 (2) (3) (11 (1) (11 (11 ad odc sb sbc D!-A+R A'-A+ R t C A'-A - R A'tA-R-C mu, muf dii dif 
A'-AXR(i) A8-AxR(ffr) A'-A/R(I) A'-A / R (fr} 111 (1) (1) (11 Introduction We present 
in this chapter the result of an exercise to design an %bit computer. Although a 
rather trivial machine, 
it is not without 
interest, either 
as manipulator of variable-length character strings or as 
an interpreter 
of more complex computers in a 
role similar 
to a microprogrammed Pc. In the latter 
role a read-only memory 
could be used as 
Mp to speed up the Pc. This computer is typical of %bit character-oriented computers. 
Among the similar machines are the Interdata 
Model 3, the RCA 1600, the IBM System/360 Model 25, and the Data Machines Inc. 
DMI 520/I. A processor 
of this type rarely stands alone 
but is used with a fixed program in the following ways: 
as a control in a 
larger C, as a control to a laboratory or 
other complex instrument, and as a 
microprogrammed processor to interpret an 1SP.l The processor must perform 
fixed-length operations on both %bit characters and 16-bit addresses. The address (double length) operations are necessary for 
performance reasons, because almost all programs 
operate on address integers. 
(For example, see 
the program on page 185.) Thus, extending (generalizing) 
the operation length to three 
and four characters is comparatively inexpensive. It should be noted that a processor 
might allow the operation length to be specified between 1 and perhaps 28 (256) characters for a much more general capability. We limit 
the directly addressa- ble Mp 
to 216 (or 65,384) characters. An alternative design might allow the maximum addressable Mp to 
be zz4 words, or, 
alter- natively, it could be variable. Although 24-bit 
operations are defined, their implementation 
might be expensive. Aligning 
the 24-bit words on 
32-bit-word boundaries would 
simplify the address calculation hardware. 110 111 The ISP The basic information unit is the 8-bit character. Instructions are, 
in general, one 
character in length. However, both instructions and data 
formats are of variable length, instructions being 
1, 2, 3,4, and 5 characters long, and data 
being 1,2,3, and 4 characters long. The Pc state 
contains -35 characters, which are organized to be dealt with as eight 8-, 16, 24-, or 32-bit 
registers (shown and or XO, cmpr (1) (11 (1) (1) A-A AR A-A" R A-A@ R N.2-A-R Id St shift SI. L- r A'-A X 2' B'cR R- A (0 ~ (1) (1) (11 'The structure 
should be compared with 
the elaborate microprogrammed 
IBM System 3BO/Model 30 (Chap. 32). in the ISP description in Appendix 1 of this chapter). Of these registers, the first (register 0) is taken to be a special accumu- lator, A. The Pc 
state contains both operands and addresses to operands. The instructions to load or store register 
A, from or into Mp, with 
or without incrementing 
a general register, all use the general registers as a 
two-character address pointer. Any general register 
may be loaded or stored direct from or to Mp. The binary arith- metic and logical operations are with a register and the 
accumu- lator, and leave the result in 
the accumulator; i.e., they are 
of the form A t A b R[r] 
Instruction execution .= (oP=xxxyyzl Instructions Formots FOrmat Chorocler length __ Name ~ Behavior' 047 1 No parameters 0 2 address Integer or relative lop IrT 5 I b 0 7 15 IOP I r 1 d J c 3 Direct address 07 23 0 7- 15 23L--31Lp-z 2-5 Immediate doto d m- - - ---I 1 r - - - 7---7 '( 1 encloses instruction length in characters shown In formats toble 'See Stote diogrom, 
Fig. 2 Fig. 1. Instruction coding 
for an 8-bit-character computer. 
184 
Chapter 10 I An 8-bit-character computer 
185 00100,011 1 character r 1010,1001 0000,0111 
Instruction lengths 2 chorocters 3 Characters Instruction lengths 2 chorocters 3 Characters o The operotton 
specified by the instruction q 0.q a q 0.v Operation to determine variobles specified 
by Instruction q a.v Access to obtain variables or return result variables Operation to determine location 
of instruction q Access to obtain instruction 
q Fig. 2. An 8-bit-character-computer instruction-interpretation state dia- 
gram. (a) 
No parameters; (b) integer or relative address; (c) direct ad- dress; (d) immediate data. 
The general registers discussed above 
are similar to those of the general register processors. Since 
it is assumed that this type of processor might 
be used to interpret another 
ISP, the + 1 and - 1 instructions provide 
for both string and 
stack memory opera- tions. The instructions for a microprogrammed P and 
the 1/0 devices are not defined. For example, a 16-way branch instruction which branched to one 
of 16 locations 
based on 
4 bits of the accumulator might facilitate writing 
an interpreter. 
The ISP is given in Appendix 1 of this chapter. The 
Pc state is organized about a 
small scratch-pad memory, although Mp could be used instead. The instruction formats and the operation code 
assignments are shown in Fig. 1. The instructions behave 
as illustrated in 
the state 
diagram (Fig. 2). For example, the instruction ﬁhi 3, A907,,ﬂ is coded The instruction, xor 3, with L = 2, is coded and the effect is R[0](0:23) t R[0](0:23) @ R[3](0:23) In these examples, 
the behavior of Iri and xor is 
specified in the state diagrams of Fig. Id and la, respectively. An open subprogram to perform the n-component vector 
(16-bit) addition™ 
+% +% is start sl 2 - 1 lri 4, A Iri 5, B Iri 6, C lri 7, 2 x n loop la1 5 st 3 la1 6 ad 3 stl 4 sul 7 cnr 4, loop set register 
length = 2 set up vector pointers to locutions A, B, C in Mp set up count ut 
2n fetch B storc B temporarily fetch C add store in A decrement n count brunch if 
negative n The above program 
loop is nine characters long. A program loop for the IBM Systern/360 is about 16 characters long. The setup is 13 characters, as opposed to 6 - 16 characters for the 360. Conclusions We have violated our principle 
of showing ﬁrealﬂ computers by designing this 
computer. We think it 
is typical of a small processor, 
but slightly more 
interesting. ‚The length is specified by register L 
186 Part 2 1 The instruction-set processor: 
main-line computers Section 2 I Processors with a general register state APPENDIX 1 AN 8-BIT-CHARACTER COMPUTER ISP DESCRIPTION Appendix 1 An 8 Bit Character Computer ISP Description Pc State The following array of 8 general registers, R, are mapped into the first 8 x &:6 x iL+ll) - I>. (IhI) cells. The register length is The first register of each array, R[Olis an accumulator, md has special properties. 
R[O:71<0: (8 x L') -I> := M[O:7][0:L1<0:7> A4:(8 x L') -I> := R[O]63:(8 X L') -1> := M[O:71 [0:31<0:7> RQ[O : 7 14: 31> AQ<0:31> := RQ[0]<0:31> RT[O: 71<0:23> := M[O:71 10:21<0:7> AT<O:23> := RT[01<0:23> RO[ 0: 7 14: 15> := M[O:71 [0:11<0:7> AD<0:15> := RD[Ol<O:15> RSEO: 71G: 7> := M[0:71 [o:oIUl:7> AS<O:7> := RS[Ol<O:P General Registers of length IL+lI x 8 bits AccumuLator IgeneralZyJ Quadruple Registers Quadruple Accumulator Triple Registers Triple Accumulator Double Registers Double Accumulator Single Registers Single Accumulator The following flags are set by the result of all arithmetic and logical instructions 
on the Accumulator, A. to A to form A'. These are connected N Negative resu2.t flag 2 Zero flag, set if the register contains a zero C A'<N,Z,C,O:(8 X L') -1> := NoZOCOA<O:(~ x L') -1> L<o: 1> Carry flag, set if there is a carry or borrow from bit 0 of the addition 2 bit register to indicate the 
character length of operations; 1,2,3,4 for S,D,T,Q L'<1>4 := L+l P<O : 1 5> Program counter Mp State M[O:17777781<0:h primary memory Instruction Format i [ 0 :41<0 : n op<0:4> := i [ 014 :4> r<O:2> := i[O]<5:h s<O:p := i[1] &O :IS>:= i[1:2] i60:(8 x L') -I> := i[l:L']d):P 1 to 5 character instruction Op Code register address signed integer for shifts 
address integer variable length 
innnediate data Instruction Interpretation 
Process ((instruction[O:4]*J:D cM[P:P+k]; P t P + 1); next fetch ((op = Oil*) v (op = 1@11) v (op = 1001)) 4 (p t P t 2)) ((op = 1MO) v (op = 1010)) + cp t p + I); (op = 010$) 4 (P t P + L+]): next Instruct ion-execut ion) execute 
Chapter 10 I An 8-bit-character computer 187 Instruction Set and Instruction Execution Process Instructionaxecution :* ( la (:= op = 0) + (A +MCRD[rll); toad A la1 (:= op = I) i (A +M[RD[r]]; next RO[r] t RO[rl + L'); load A, Zncrement sa (:= op = 2) + (M[RDCrlI +A); sal (:= op = 3) i (M[RD[r]] +A; next RD[r] +RO[r] + L'); lri (:= op = 4) i (R[r] e im); ari (:= op = 5) 3 (R[r] t im + R[r]); srd (:= op = 6) i (M[d] +R[r]); Ird (:= op = 7) i (R[r] tM[d]); ad1 (:= op = OIOOO) + (R[rl cR[r] + L'); SUI (:= op = OlOOl) 3 (R[rl +R[r] - L'); br (:= op = 
01010) i (P +R[r]); bld (:= op = OlOll) i (P cd; R[r] +P); cbr (:= op = 01100) + ((cond # 0) iP CP + s); cbd (:= op = OllOl) i ((cond # 0) -tP cd); cnr (:= op = OlllO) i ((cond = 0) iP tP + 5); cnd (:= op = Ollll) + ((cond = 0) +P cd); cond := (r h NoZoC) ad (:= op = 10000) i (A' +A + R[r]); adc (:= op = 10001) +(A' +A + R[r]+ C); sb (:= op = 10010) i (A' +A - R[r]); sbc (:= op = lOOl1) 3 (A' '-A - R[r] - C); mui (:= op = 10100) 3 (A' <-A x R[r] (i}); muf (:= op = 10101) 3 (A' cA x R[r] dii (:= op = IOIIO) + (A' +A / R[r] dif (:= op = IOllI) i (A' <-A / R[r] and (:= op = 11000) i (A +A A R[r]); or (:= op = IlOOl) (A +A v R[r]); xor (:= op = 11010) + (A eA @ R[r]); cmpr(:= op = IiOll) i (No2 +A - R[r] Id st (:= op = 
IllOl) + (fdrl +A); shift(:= op = 11110) i (A' +Ax 2'); s1 (:= op = 11100) +(A' tR[rl); (:= op = 11111) --f (L + r) 1 store A store A, increment load register innnediate add register innnediate store register load register add I to register subtract I from register branch return branch and link direct 
conditional branch relative conditional branch direct conditional not 
branch relative conditional not 
branch direct add add with carry suhtract subtract with 
carry integer multiply fraction multiply integer divide fraction divide logical and logical or 
exclusive or compare used to N and Z load store shift right or left set operation length 
end Instruction,execution 

The instruction-set processor level: variations in the processor In this part 
we discuss computers whose ISP™s 
are variations from the main-line 
computers in Part 2. These variations represent historical computers 
that have not remained viable in the 
judgment of 
the computer engineering 
community, responses to particular technology, and explorations that were either too advanced for their time or still exist as open options. 
Section 1, Processors with greater than 1 address per instruction, is mostly of 
historical and comparative interest. The general 
register organization with 
large Mp™s (hence large 
addresses) almost surely dominate them. 
Section 2, Processors constrained by a cyclic, primary memory, describes a response to a historical feature 
of Mp technology. The use of a drum, delay line, or disk was a matter of necessity rather than choice. When better random access core memories 
were available, the drum 
ceased to be a primary memory component. 
Section 3 presents processors for variable string data. These processors are 
no longer built in their original form. However, they were very 
successful for a while (IBM 1401). Furthermore, string data-types have been 
incorporated in later proc- 
essors. Section 4 presents two desk calculator computers. Although 
we too often dismiss these devices as 
mere desk calculators, they 
have facilities that qualify them as general purpose stored 
program computers. Unlike most computers, because of the production cost constraint, these calculator computers 
are all very cleverly designed. 
Section 5, Processors with stack memories, 
describes an organization that has never reached the main line state. Nevertheless, the idea of a stack memory is 
gradually being assimilated. For example, the DEC PDP-6 and PDP-10 computers 
use their general registers 
for stack pointer control, 
as suggested in Chap. 3, page 62. In Sec. 6 the ideas of multiprogramming are presented. These ideas are recent and have not yet been adequately incorporated in main 
line designs. They undoubt- edly will be standard features in the next generation, 
although the exact form can- not yet be 
known. 189 

Section 1 Processors with greater than 1 address per 
instruction Multiple-address instruction formats exist for several reasons. The addition of 
an explicit address to determine the next in- struction occurs with cyclic Mp's to make them efficient. Section 2 is devoted to this 
case, and it will not be considered 
further here. These 
processors are 
known as n + 1 address. A second reason is that many 
operations have more than one operand 
(as in A + B or A 
V B), and it seems to be efficient encoding 
to put them 
all into an instruction. A third reason is that many operations need to be followed 
by writing the 
result in memory, to permit the Pc to be used 
for operations on other data. 
Thus, coupling each operation with the 
address where the result is to be stored seems to be advantageous. 
However, in evalu- ating complex arithmetic expressions, more instruction bits and 
memory references are 
required than in 
a single-address 
com- puter. Also, for unary operators one 
address field is unused. It seems fair to say that ISP organizations with two 
or three 
addresses have not proved themselves 
in competition with 
the main line of 1, (1 + index), or (1 + general register) 
organiza- tions. However, no definitive demonstration 
of their inefficiency under all technological 
conditions exists, and they are worth studying. For microprogrammed processors, multiple-address instruc- tions allow a high 
degree of parallelism to be obtained in a single instruction. Multiple-address formats survive in this form. 
The Pilot ACE The National Physics Laboratory's Pilot 
ACE is the first 
of several cyclic memory computers 
which have 
been designed to provide optimum coding of instructions. Subsequent machines which it influenced include 
the nearly identical English 
Electric Deuce, the Bendix G-15, 
and the 
Packard Bell PB-250.' The 
PMS structure does not strictly follow our lattice model 
(page 65). The Deuce PMS structure is given in Fig. 1. A 32-word block in Mp.delay-line can be transferred to Ms.drum in one instruction (transfer 
time of 1,024 ps). Another capability 
of 'H. D. Huskey was involved in the 
design of ACE, G-15, and PB.250; he was 
undoubtedly the idea carrier. ACE allows it to perform operations on vectors of 
up to 32 elements in 1 instruction. The ACE structure (Chap. 11) has a common M which con- tains much of 
the processor state and 
Mp. Many of 
the locations used for processor state can store programs 
for direct execu- tion. The diagram on page 198 in Chap. 11 describes the in- struction execution process and implementation. 
Alan M. Turing is credited with the 
basic design of 
ACE (see introduction, page 193, and Turing's biography [Turing, 
19591). ZEBRA, a simple binary computer ZEBRA illustrates the organizational details of another serial arithmetic computer with Mp.cyclic. ZEBRA, like ACE, allows the user to construct instructions 
for the hardware which are 
almost directly interpreted. 
In both ACE and ZEBRA very little decoding is built into the machine; a 
large instruction set is available since the instructions are microcoded. 
In these computers the programming problem can 
be as 
complex as the user wishes, 
because a large 
number of different instructions 
can be micro- S-T.console - K-Ms moving head drum; 8192 
w; 32 b/w; 
16 tracks/posi- tion; 32 w/track; 16 posi- [t ions ] 'Mp(delay line; cyclic; 
32 - 1024 ps/w; 32 w; 32 b/w) 'Pc(techno1ogy: vacuum tubes; 1955 - 1961; (2+1) address/ I instruction; ancestors: NPL ACE) ~~ ~ ~~ 
Fig. 1. English Electric 
Deuce PMS diagram. 191 
192 Part 3 I The instruction-set processor level: variations in the processor 
coded. The LGP-30 
(Chap. 16), by contrast, has only a 
basic instruction set. Hence 
a problem can 
be coded 
only one 
or two 
ways. ZEBRA'S performance of 
60 percent memory-cycle 
utiliza- tion is rather outstanding and raises the possibility that ran- 
dom-access primary memories may not be necessary. UNIVAC scientific (1103A) instruction logic 
The UNIVAC 1103A (Chap. 13) is a two-address computer. 
The computer was designed initially by Engineering Research Asso- 
ciates (ERA) of St. Paul.' UNIVAC 
acquired ERA in 1952 as a scientific-computer division. The evolution of the 1103A later yielded the 1107 and 
1108 general register processors. The 
reader should compare 
the 1103A with the IBM 
704 series (Chap. 41). At the time both 
were used, 
it was not clear which computer was better. 'As the third 
in a series that started with the 
ERA 1101 and 1102 Section 1 I Processors with greater 
than 1 address per instruction 
The RW-400: a new polymorphic data system 
The RW-400 in Chap. 38 is a two-address, binary computer. 
It is discussed in Part 5, Sec. 4, page 470. Instruction logic of the MIDAC The University 
of Michigan's MIDAC (Michigan Digital Auto- 
matic Computer) is 
based on the National Bureau of Standards' SEAC (Standards' Electronic Automatic Computer). MIDAC, a three-address, binary computer, 
is presented in Chap. 14. Instruction logic of the Soviet Strela (Arrow) 
The Russian Strela 
is presented 
in Chap. 15. Since it is used only to illustrate a three-address organization, 
the chapter con- sists of only the instruction set. 
Chapter 11 The Pilot ACE1 J. H. Wilkinson Introduction General 
description A machine which was almost identical with the 
Pilot ACE was first designed by 
the staff of the Mathematics Division at the 
suggestion of Dr. H. D. Huskey during his stay 
at the National Physical Laboratory in 1947. 
It was based on an earlier 
design by Dr. A. M. Turing and its principal 
object was to provide experi- ence in the construction of equipment of this type. 
It was not intended that it 
would be used on an extensive programme of computation, but it was hoped that it would give practical experi- ence in the production of subroutines which 
would serve 
as a useful guide to the design of a full scale 
machine. An attempt to build the Pilot Model, 
during Dr. Huskey™s stay, was unsuccessful, but a year 
later after the formation of an Electronics 
Section at the NPL a combined team consisting of this section and four members of the Mathematics Division started on the construction of a Pilot Model, 
the design of which was taken over almost 
unchanged from the earlier version. The machine first worked, in the sense that it 
carried out automatically 
a simple sequence 
of operations, in 
May 1950 
and by the end 
of that year it had reached 
the stage at which a 
successful Press 
Demonstration was held. The successful application of the machine to the solution of a number of problems made it 
apparent that, 
in spite 
of its obvious short- comings, it was 
capable of being converted 
into a powerful com- 
puter comparable with 
any then in existence and much faster 
than most. Accordingly a small 
programme of modifications was 
em- barked upon early 
in 1951, 
but the machine was not functioning satisfactorily again 
until November of that year. After a month of continuous operation it 
was transferred from the Electronics Section to Mathematics Division where it has since been in use on a 13-hour day. During its first year of full scale 
operation it 
achieved a 65% serviceability figure based on a very 
strict criterion. 
Its performance 
during its second year 
has so far been considerably better than this. The Pilot ACE is a serial machine 
using mercury delay line 
storage and working at a pulse 
repetition rate of 1 megacycle/sec. Its high speed store 
consists of 11 long delay lines 
each of which stores 
32 words 
of 32 binary digits each, with 
a corresponding 
circulation period of 1024 microseconds, 5 short lines 
storing one word each 
with a circulation period 
of 32 microseconds 
and two delay 
lines storing two 
words each. It was inevitable that in the design of a machine originally intended for experimental purposes, over- 
riding consideration should 
be given to the 
minimization of equip- ment rather than to making the machine logically satisfying 
as a whole. 
This is reflected to a certain extent in 
the code adopted for the machine and in its arithmetic facilities, which 
are in gen- eral fairly rudimentary. The design of the machine was also de- cisively influenced 
by the attempt 
to overcome the loss of speed due to the high access 
time of the long storage 
units. The machine in fact 
uses what is usually known as a system 
of ﬁoptimum coding.ﬂ Code of Pilot ACE The Pilot ACE 
may be said to have a ﬁthree-address codeﬂ though 
this form of classification is not particularly appropriate. Each instruction calls for the transfer of information from one of 32 ﬁsourcesﬂ to one of 32 ﬁdestinationsﬂ and selects which 
of eight long delay lines 
will provide the next instruction. This 
third address is necessary because consecutive instructions do not occupy 
consecutive positions but are placed in such relative positions that, in so far as is possible, each instruction 
emerges during the 
minor cycle in which 
the current instruction is completed. An unusual feature of the instructions is that the 
transfers they describe 
may last for any number of consecutive minor cycles 
from one to thirty- two. The instruction word contains three other 
main elements which are 
known as 
the wait number, the 
timing number and 
the iAutmatic ~i~i~~l cmputat~on, ~~ti~~~l physical Laboratory, Teading. ton, England, pp. 5-14, March, 1953. characteristic which together determine when the transfer starts, 
when it stops and which instruction 
in the selected instruction 
193 
194 Part 3 I The instruction-set processor level: variations in 
the processor Section 1 I Processors with greater 
than 1 address per instruction 
source is the next to be 
obeyed. The structure of the instruction word is as follows: 
Next instruction source Digits 2-4 Source Digits 5-9 Destination Digits 10-14 Characteristic Digits 15-16 Wait number Digits 17-21 Timing number 
Digits 25-29 Go digit Digit 32 
The remaining digits are spare. 
Coding of a problem takes place 
in two parts, in 
the first of which only the source, the destination and the period of transfer are specified, the last being a function of the characteristic, wait 
number and timing number. 
In the 
second part, the detailed cod- ing, the other elements are 
added. The sources and destinations Simplest among 
the sources and destinations are 
those associated 
with the short delay lines. 
The six one-word delay lines are 
each given numbers and these for reasons 
associated with the history of the machine are 11, 15, 16, 
20, 26 and 27. They are 
usually referred to as Temporary Stores 
or TS™s because they 
are used to store temporarily 
those numbers which 
are being operated upon 
most frequently at each stage 
of a computation. In general 
TSn has associated with it a source, source n, and 
a destination, des- tination n. 
An instruction of the type 15-16 in the preliminary stage 
of the coding represents 
the transfer of a copy 
of the contents of TS15 via 
source 15 to 
TSl6 via the destination 16. After it has taken place both 
stores contain the number originally in TS15. 
The period of the transfer is not mentioned in 
the coding because a transfer 
of more than one minor cycle is irrelevant. Most transfers are for one minor cycle 
and hence the period of transfer is not specified unless 
it is greater than one minor cycle. 
Associated with the TS™s are a number of functional sources and destinations. TSl6 for instance has two other destinations 17 and 18 associated with it, in addition 
to destination 16. Any number transferred 
to destination 17 is added to the contents of TSl6 while any number transferred to destina- tion 18 is subtracted from the contents of TS16. TS16 
may be said to have 
some of the functions associated 
with the accumulator on an orthodox machine. The period of transfer to destinations 17 and 18 is very important. Thus 15-17 (n minor cycles) 
has the effect of adding the contents of TS15, n times 
to the 
contents of TS16. This prolonged transfer 
is used in this way 
to give small 
multiples (up to 
32) of numbers. Similarly, we may 
have 15-18 (n mc) 
The instruction 16-17 (n mc) is of special significance because it 
has the effect of adding the content of TSl6 to 
itself for 
each minor cycle 
of the transfer, that is it gives multiplication by 271 or a left 
shift of n binary places. TS26 has 
associated with it 
a number of functional sources. Source 17 
gives the ones complement of the number in TS26, 
Source 18, 
the contents divided by 2, 
and Source 19, 
the contents multiplied by 2. 
The instruction 18-26 (n mc) thus has the effect of dividing the contents of TS26 by 2n, that is a right shift of n places. Similarly 
19-26 (n mc) gives a left shift of n places. 
There are two functional 
sources which 
give composite func- 
tions of the numbers in 
TS26 and TS27. These are Source 21 which gives the number TS26 & TS27 and Source 22 which gives the number TS26 f TS27 There are a number of sources which give constant numbers which 
are of frequent use in 
computation. These are Source 23 which gives the number 
which has a zero 
everywhere except in 
the 17th position, usually known as 
P17, Source 24 which gives P32, 
Source 25 which gives P1, Source 28 
which gives zero and Source 29 which gives a number consisting of 32 consecutive ones. These sources are valuable because they 
provide numbers with an access time of one minor cycle 
and are thus 
almost as useful as 
several extra TS™s. The use of a number of TS™s with the arithmetic facilities distributed among them 
makes it possible to take advantage 
of the placing of instructions in appropriate positions in the long 
Chapter 11 I The Pilot 
ACE 195 storage units 
so that they emerge 
as required. The coding of a trivial example 
will illustrate the uses of the TS™s and their asso- ciated sources. It is required to build up the successive natural numbers, their squares and their cubes simultaneously. It is natural to store the values in 
TS™s and we 
may suppose TS15 contains n, TS20, n2 and TS26, n3. Instruction Description 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 28- 15 
zero to 
TS15 i.e. 0 28-20 zero to 
TS20 i.e. 02 initial values 28-26 zero to 
TS26 i.e. 03 26-16 TS16 contains n3 20-17 (3rnc) TS16 contains n3 + 3n2 15-17 (3rnc) TS16 contains n3 + 3n2 + 3n 25- 17 TS16 contains n3 + 3n2 + 3n + 1 16-26 TS26 contains (n + 1)s 20-16 TS16 contains n* 15-17 (2rnc) TS16 contains n* + 2n 25-17 TS16 contains n* + 2n + 1 16-20 TS20 contains (n + 1)2 15-16 TS16 contains n 25-17 TS16 contains (n + 1) 16-15 TS15 contains (n + 1) Next instruction (4) These 3 instructions set the 
- The instructions (1) to (3) set the initial conditions. 
The instruction (4) - (15) have the effect of changing the contents of 15, 20, 26 from n, n2, n3 
to (n + l), (n + 1)2, (n + l)3. As remarked earlier, 
each instruction selects 
the next instruction and here 
instruction (15) selects instruction 
(4) as the next instruction. In the 
prelimi- nary coding this 
is usually denoted by using an arrow; 
it must be catered for in the detailed coding by 
the correct choice of the timing number, as will 
be shown below. The branching of a programme is achieved by the use of two destinations, destination 
24 and destination 25. If a transfer is made from any source to destination 24 then the next instruction is one or other of two according as the number transferred is positive or negative. 
Similarly if a transfer is made to destination 25 then the next instruction is one or other of two according 
as the number transferred is zero or non-zero. In the 
preliminary coding 
the bifurcation is denoted by the use of arrows, thus: In the detailed coding the effect is that if the number transferred to destination 24 is negative then the 
timing number is increased by 1. Similarly for destination 25; the two possible next 
instructions are consecutive in the store. The two double word 
stores are numbered 
DS12 and DS14. DS12 has only source 
12 and destination 12 associated with it, but DS14 has, in 
addition to source 14 and destination 14, a number of functional sources and destinations. Source 
13 gives the contents of DS14 divided by 
2, while transfers 
to destination 13 have the effect 
of adding the numbers transferred 
to DS14. In specifying transfers from, 
and to, the double length stores, the time of the transfer must be specified, i.e. whether it takes place in an 
even or an odd minor cycle 
or both. Thus 
the transfer 12-14 (odd minor cycle) 
usually written 12-14 (0) represents the transfer of the word in the odd positions of DS12 to the odd position in DS14 while 12-14 (2 minor cycles) represents the transfer of both words in 
12 to the corresponding positions in 14. The operation 13-14 (2n) gives us a 
method of shifting the contents of TS14 n places to the right while 
14-13 (2n) produces a shift 
of n places to the 
left. The machine is not equipped with a fully 
automatic multiplier. To multiply two numbers, a and b, 
together, a must be sent to TS20, b to DS14 odd, zero to DS14 even and a transfer (source 
irrelevant) made to destination 19. The product is then produced in DS14 in 2 milliseconds, but a and b are treated 
as positive numbers. Corrections must 
be made to 
the answer if a and b are signed numbers. To make multiplication fast, it has been made 
possible to perform other operations while multiplication 
is pro- ceeding. Thus the corrections necessary if a and b are signed numbers may 
be built up in TS16 during multiplication, and signed multiplication takes only 
a little over two millisecs. It is, of course, therefore, a subroutine but a very fast one. The amount of equip- ment associated with 
the multiplier is very small. The main part of the store consists of the long storage units known 
as DL1, DL2, . . . 
, DL11. Each of these has a 
source and a destination with 
the same number as the DL number. The words in each 
DL are numbered 0 to 31 and the nth 
word in DLM is usually denoted by DLM,. Transfers to and from long lines 
in the preliminary coding are denoted 
thus: 
196 Part 3 I The instruction-set 
processor level: variations in the processor 
8,- 16 (transfer nth 
word of DL8 to TS16) 8,-,-17 (add all the words from 8, to 8, i.e. n - m + 1 con- secutive words of DL8" to TS16) Detailed coding In the 
second stage 
of the coding the true instruction words are derived from the preliminary coding. This is a fairly 
automatic process and recent experience 
has shown 
that it can 
be carried out satisfactorily by 
quite junior staff. The timing of each instruc- 
tion is given relative to the position of that instruction in the store. This is an incidental feature of the code which arose from 
the attempts to minimize equipment. It would be dropped in any 
future machine in favour of an absolute timing 
system. If an in- struction occupies 
position m 
in a DL and has a wait number W and timing number 
T then the 
transfer always begins in minor cycle (m + W + 2) and the next instruction is always in minor cycle (m + T + 2) of the selected next instruction source. The period of transfer depends on the value of the characteristic. If the characteristic is zero then the transfer lasts 
for the whole period from (m + W + 2) to (m + T + 2), that is (T - W + 1) minor cycles. If the characteristic is one, then 
the transfer is for one minor cycle, that is minor cycle (m 
+ W + 2). If the charac- teristic is three then 
the transfer is for two minor 
cycles (m + W + 2) and (m 
+ W + 3). The characteristic value, two, 
is not used. The characteristic value zero 
gives a prolonged transfer 
which is peculiar to the 
Pilot ACE. The characteristics 1 and 3 are analogous to the 
facility on EDSAC 
whereby full length or l/-length words may 
be transferred. On the Pilot ACE 
we transfer single or 
double length 
words. This facility 
is invaluable for double length, floating and complex arithmetic. In the 
above definitions the numbers (m + W + 2) etc. are to be interpreted 
modulo 32. In general, timing 
and wait numbers 
are simpler than they appear from the definitions because they 
are very frequently both 
zero, corresponding to a transfer for one minor cycle. 
The detailed coding of the problem given earlier will illustrate the procedure. All the instructions are in DLl so that the 
next instruction source 
is always one. 
The key to the headings in the following table is: m.c. N.I.S. Next 
instruction source S Source D Destination C Characteristic W Wait 
number T Timing number 
Minor cycle position of instructions in 
DLI Section 1 I Processors with greater 
than 1 address per instruction 
The last column gives the position of the next instruction in DL1; it is given by (m 
+ T + 2). The first 4 instructions occupy minor 
cycles, 0, 2 and 4, 6 and each takes two minor 
cycles, and gives a transfer 
for one minor cycle 
only. The next instruction occupies minor cycle number 8 and it requires a transfer lasting 
3 minor cycles. The simplest and fastest way 
of getting this is to have W = 0 and T = 2 giving a transfer 
of (2 - 0 + 1) minor cycles. 
The next instruction is in position (8 + 2 + 2), that is minor cycle 
12, and so on. When we reach the instruction in minor 
cycle 31, viz. 25-17, a transfer 
for one minor 
cycle is required. The simplest way is to have W = 0 T = 0 and this makes the next instruction occupy position (31 + 0 + 2) i.e. position 
33 which is position 1. If position 1 had been already occupied, 
a value 
of T could have been chosen in order 
to land in an unoccupied position. In order 
to ensure that a transfer of one minor cycle only took place, the characteristic could have been 
made 1. It should be appreciated 
that the choice of C, W and T is far from unique. Whenever 
possible T = 0 and W = 0 are chosen because this gives the highest speed of operation besides being simplest. 
The instruction occupying position 1 is of special interest because 
this is the last instruction of the cycle needed to build up a square and cube and it must select as its next instruction the first of the cycle, which 
is, in position number 6. This is achieved by making T 
= 3 (giving the next instruction in m.c. 1 + 3 + 2 = 6). This incidentally gives a transfer lasting 
four minor cycles 
but since it is a transfer 
from one TS to another and 
no functional 
source or 
destination is in use, 
the prolonged transfer produces 
no harmful 
effect. If a prolonged transfer 
had to be 
avoided then the characteristic could 
be taken as 1. It is seldom necessary 
to use any characteristic other 
than zero for transfers to and from TS's 
but when transfers are made to and 
from DL's, 
characteristic values of 1 or 3 are almost universal. All 12 instructions which comprise 
the repeated 
cycle of the computation take 
a total time of one major cycle exactly 
(32 minor cycles) 
the last instruction of the cycle having 
been specially designed 
to get back 
to the beginning of the cycle. This is in contrast 
to the position in a machine not 
using optimum coding, where 
12 major cycles would 
be necessary quite apart 
from the fact that the multiplications by factors 
of 3 and 2, each of which uses one instruction, would normally 
need more than one 
instruction if a prolonged 
transfer were not available. Figure 
1 gives a simplified diagram of the machine. The sequence of events in obeying the instruction NS D CWT 2 16 - 2C 0 8 10 occupying DL1, for example is as follows. Starting from the time when the 
last instruction was completed, the instruction from 
Chapter 11 I The Pilot ACE 197 Minor cycle Minor cycle position of Next position of instructions instruction 
Charac- Wait Timing next 
in DLl source Source Destination teristic no. no. instruction 
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 
25 26 27 28 29 30 31 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 28 16 28 28 26 20 15 25 16 20 15 25 16 15 25 15 15 20 16 16 17 17 17 26 16 17 17 20 16 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 2 2 0 0 0 1 0 0 0 0 DL1, will have passed 
into the special TS marked TS COUNT during minor cycle 
number 2. By the end of minor cycle 
number 3, S switch number 16 will 
be over and also N switch number 
2. The contents 
of TSl6 will be passing into HIGHWAY and those of DL2 into INSTRUCTION HIGHWAY. 
At the beginning of minor cycle number 
12 (i.e. 2 + 8 + 2), D switch number 
20 will 
go over, and TS20 will stop recirculating and 
the number on the HIGHWAY will 
pass into TS20. The transfer will 
continue until 
minor cycle 
14 (i.e. 2 + 10 + 2) when the D switch number 
20 will switch back. 
At the beginning of minor cycle 14, the switch X on COUNT will go over and the number on INSTRUCTION HIGHWAY during this minor 
cycle, DL2,,, will pass into COUNT. At the end of minor cycle 
14, the X switch will close again 
and DL2,, will be trapped in COUNT. 
The cycle of events is now complete. COUNT 
is associated with a counter and it 
is this counter which determines 
from the wait, timing, and characteristic 
numbers of the trapped 
instruction, when 
the D and X switches go over and back. Input and output The only part of the instruction word not described 
is the GO digit. If the GO digit is a one, the instruction is carried out at high speed, but if it is a zero the machine stops and does not proceed until a manual switch is operated. The GO digit is omitted in strategic instructions when a programme 
is being tested. 
It also 
198 Part 3 I The instruction-set processor level: variations in the processor 
1s IS TS. 27 DS 14 os ta etc. c Fig. 1. Simplified diagram showing 
some sources, destinations, and next-instruction sources. serves a 
further purpose in synchronising the input and output 
facilities with the high speed computer. 
Input on the machine is by means of Hollerith punched cards. When cards are passed through the reader the numbers on the card may be read row by 
row as each passes under a set of 32 reading brushes. When a row 
of a card is under the reading brushes, the number punched 
on that row, regarded as a 
number of 32 binary digits, is available on source 0. In 
order to make certain that reading takes 
place when a row 
is in position 
and not between rows, transfers from source 0, have 
the GO digit omitted and it is arranged that the 
Hollerith reader has the same effect as 
operating the manual switch each time 
a row 
comes into position. The passage of a card through the reader is called for by a transfer from any source to destination 31. No transfer of information from 
the card takes place unless the appropriate instruction using source 0 is obeyed during the passage of the card. Output on the machine is also provided Section 1 I Processors with greater 
than 1 address per instruction 
by a Hollerith punch. The passage of a card through the punch is called for by a transfer from any source to destination 30. While a card is passing through the punch a 32 digit 
number may be punched on each row by a transfer to destination 28. Again syn- 
chronisation is ensured by omitting the GO digit in instructions calling for a 
transfer to destination 28, and arranging that the 
Hollerith punch effectively operates the manual switch as each row comes 
into position. The reader feeds cards 
at the rate of 200 cards per minute 
and the 
punch, at the 
rate of 100 cards per minute. The speed of input for binary digits 
is 200 
x 32 x 12 per minute or 1280 per second. The output 
speed is 640 digits per second. Data may be fed in and out in decimal, but it then 
requires conversion subroutines. The computation involved in the conver- sion is done between the rows of the card and up to 30 
decimal digits per card 
may be translated. This speed 
of conversion is only possible because of the use of optimum coding. The facility for carrying out computation between 
rows of cards is used extensively 
particularly in linear algebra when 
matrices exceeding 
the storage capacity of the machine are involved. The matrices are stored on cards in binary form with one number on each of the 12 rows of each card, 
all the computation being done either between 
rows when reading or 
when punching. Times 
comparable with 
those possible with the matrices stored 
in the memory are often achieved in this way, when the computation uses a high percentage of the available time between 
rows. Up to 80% of this time may be safely used. Initial input 
The initial input of instructions is achieved by choosing destination 0 in a 
special manner. When a transfer is made to 
destination 0, then the instruction transferred becomes 
the next to be obeyed and the next instruction source 
is ignored. Source 0 has already been chosen specially since 
it is provided from a row 
of a card. The instruction consisting 
of zeros has the effect of injecting the instruction punched on a 
row of a card into the 
machine as the next to be obeyed. The machine is started by clearing the store and starting the Hollerith reader which contains cards 
punched with appropriate instructions. Destination 0 is also 
used when an 
instruction is built up in an arithmetic unit 
ready to be obeyed. Miscellaneous sources and 
destinations Destination 29 controls a buzzer. If a non-zero number is trans- ferred to destination 29 the buzzer sounds. Source 30 
is used to indicate when the last row of a card is in position in 
the reader or punch. This source 
gives a non-zero number only when a last row 
is in position. 
The operation of the arithmetic facilities on DS14 may be modified by 
a transfer to 
Chapter 11 1 The Pilot ACE 199 destination 23. If a transfer with an odd characteristic is made from any source to destination 23 then, from then on, DS14 be- haves as 
though it were two 
single length accumulators 
in series. This means 
that carries are suppressed at the 
end of each of the single words. 
This condition 
persists until a transfer is made to 
destination 23 using an even characteristic, 
when DS14 behaves as an accumulator 
for double length numbers 
with their 
least significant parts in 
even minor 
cycles and more significant 
parts in odd minor 
cycles. The operation TS20 is modified by transfers to destination 21. If a transfer with an 
odd characteristic is made to destination 21 then TS20 ceases to have an independent existence and from then on is fed continuously from DL10. Source 20 then gives the con- tents of DLl0 one minor cycle 
later than from source 10. TS20 reverts to its former condition 
when a transfer with an even char- acteristic is made to destination 21. The facility is used to move the 32 words in 
DLlO round one 
position so that the word in minor cycle n is available in minor cycle 
(n + 1). Assessment of optimum coding A detailed assessment of the value of optimum coding is by no means simple. 
Roughly speaking, 
subroutines are on an average about 4 or 5 times as fast as 
on an orthodox machine 
using the same pulse repetition rate. 
In main tables 
a somewhat lower 
factor is usually achieved. The factor of 4 or 5 would be exceeded if less of the advantage given by optimum coding were used to overcome disadvantages due to the rudimentary nature of the arithmetic facilities on Pilot ACE. Even so, the bald statement 
of the average ratio of speeds does not do full justice to the value of optimum coding on the Pilot ACE. Its value springs 
as much from the fact that it has made possible the programmes in which computing is done between the rows of cards and also the high output speed of decimal numbers. 
The binary decimal 
conversion routines for punching out 
several decimal numbers simultaneously 
on a 
card and also decimal-binary conversion routines for 
reading several numbers, achieve 
a ratio of something like 14 to 1, and on a 
machine which 
is being used extensively for scientific 
computation on a 
commercial basis this is of immense importance. Future programme Engineered versions of the Pilot Model are now under construction by the 
English Electric Company. These machines 
will be similar to the Pilot Model but will have a little more high-speed store, 
an automatic divider, two quadruple 
length stores 
and a subtrac- tive input on the double length 
accumulator besides several minor 
modifications including a rationalization of the numbering of the stores! In addition a magnetic drum intermediate 
store with the equivalent of 32DL™s storage capacity will be added. A full scale machine will probably soon be under development employing 
a 4 address code. Typical instructions 
will be of the form AkB C and will select the next source of instruction. This code 
is more economical in instruction storage 
space and since all 
single word stores will then become complete 
accumulators with all facilities except multiplication 
on them, it 
will be possible to take much 
fuller advantage of optimum coding. Sources, destination and next instruction 
sources Sources Des tinations Next instr. 
sources 0. Input 1. DL1 2. DL2 3. DL3 4. DL4 5. DL5 6. DL6 7. DL7 8. DL8 9. DL9 10. DLlO 11. DLll 12. DS12 13. DS14 + 2 14. DS14 
15. TS15 16. TS16 17. TS26 18. TS26 i 2 19. TS26 x 2 20. TS20 
21. TS26 
& TS27 22. TS26 $ TS27 23. P17 24. P32 25. P1 26. TS26 27. TS27 28. Zero 29. Ones 0. INSTRUCTION 0. DLll 1. DL1 1. DL1 2. DL2 2. DL2 3. DL3 3. DL3 4. DL4 4. DL4 
5. DL5 5. DL5 6. DL6 6. DL6 7. DL7 7. DL7 8. DL8 9. DL9 10. DLlO 11. DLll 12. DS12 13. DS14add 14. DS14 15. TS15 16. TS16 17. TS16add 18. TS16 subtract 19.t MULTIPLY 20. TS20 21. Modifies Source 20 22. - 23. Modifies Source 13, Destination 13 24. DISCRIMINATE on sign 25. DISCRIMINATE on zero 26. TS26 
27. TS27 28. Output 29. BUZZER 30. Last row of card 30.t PUNCH 31. - 31.t READ t Independent of source used. References WilkJ53; TuriS59 
Chapter 12 ZEBRA, a simple binary computer1 
W. L. van der Poel Summary The computer ZEBRA is a computer 
based on the following 15 bits AKQLRIBCDE VX~XZXI W test bits 
I/ operation part ideas: 1. 2. 3. 4. 5. 6. 5 bits 00000 fast store address The logical structure of the arithmetic 
and control units 
of the machine have been 
simplified as much as possible; there is not even 
a built-in multiplier 
nor a divider. The separate bits in an instruction word are 
used functionally and 
can be put together in any combination. 
Conventional two stage operation (set-up, execution) has been aban- 
doned. Each unit time interval can be 
used for 
arithmetical opera- tions. A small number 
of fast access 
registers is used as temporary storage; 
at the same time these registers serve 
as modifier registers (B-lines). 
Optimum programming 
is almost automatically done 
to a 
very great 
extent. The percentage of word times effectively 
used is usually greater than 
60%. An instruction can be 
repeated and 
modified while repeated 
by using an accumulator 
as next instruction source 
and the 
address counter as counter. This can be done without any special hardware. 
This has resulted 
in a machine which has 
a very simple structure and hence contains only 
a very moderate number 
of components, giving high 
relia- bility and easy maintenance. Because 
of the functional bit coding, the programming is extremely flexible. In fact the machine code 
is a sort of micro-programming. Full-length inultiplication or half-length mnltiplica- 
tion in 
half the time are just 
as easy, only require a different micro- 
programme. The minimum latency programming together with 
the effec- tive use of word times 
lost in 
other systems results in a very high speed 
of operation compared 
to the basic clock 
pulse frequency. 
Introduction In the Dr. 
Neher Laboratory of the Dutch Postal & Telecom- munications Services the logical design 
of a computer called ZE- BRA has been developed, and this computer has 
been engineered and constructed by Standard Telephones & Cables Ltd, England. 
The logical system 
is so different from most computers, that it is worth while to devote a special lecture to it. As time is limited, 'Proc. ICIP, UNESCO, pp. 361-365, June, 1959. no technical details nor 
questions about dimensions or capacity will be discussed. They can all be found in the literature [van der Poel, 1956; 
van der Poel, 19521. The main idea of the machine is to economise as far as 
possible on the number of components by simplifying the logical structure. For example, multiplication 
and division are not built 
in but must be programmed. Of course this 
system can only work 
with an appropriate internal code which has 
enough properties 
to execute basic arithmetic and logical routines effectively. In fact, the inter- nal machine 
code is more or 
less a system of microprogramming [Wilkes and Stringer, 19531. 
Chapter 12 1 ZEBRA, a simple binary computer 
201 Arithmetic Control unit store store Arithmetic I,] El store store Fig. 1. The main 
units of the computer. arithmetic unit 
or the control. In 
the same way 
the K-bit controls the interconnection of the fast store with the arithmetic unit 
or the control unit. These interconnections can 
be seen from Fig. 
1. It will be seen that A and K can have 4 possible combinations: Case 1. A = 0, K = 0. This is called the adding jump 
(Fig. 2a). While a new instruction is coming into the control from the drum, the arithmetic unit can 
at the 
same time do 
an operation with 
the operand coming from 
the fast store. This 
is the fastest type of operation. When the following instruction is placed in the next location on the drum there 
is no waiting time, and 32 instructions of this type can be executed per revolution. (One revolution = 10 ms, one word 
time = 312 ps.) Case 2. A = 0, K = 1. This is called the double jump (Fig. 2b). Both stores 
are now used for giving 
information to the control, i.e., making a jump. Since the fast store is used for the control, the instruction coming 
in from 
the drum is modified 
by the con- tents of a fast register. In 
this way the B-line facility, as it is often called, is realised. Case 3. A = 1, K = 0. This is called the double addition (Fig. 2c). Both stores 
are now connected to the arithmetic unit. 
The control must take care of itself using the address counter which is stepped up by 2 at a time, thus enabling this 
type of instruction to reach the number lying between the two successive instructions without any waiting time. Constants in particular 
will always 
be taken from optimum places on the drum. Case 4. A = 1, K = 1. This is called the jumping addition (Fig. 
24. While the drum is used for 
the arithmetic unit 
the address counter is modified by a fast 
register. Control may thus be passed to any instruction, and not only to the next instruction. D- and E-bits The functional bits 
D and E control the direction of flow of infor- mation. D = 0 means: read from the drum. E = 0 means: read from the fast store. D = 1 means: write to the drum. E = 1 means: write to the 
fast store. A few possible instructions will be given below. In 
the written code a drum address will always 
be written with 3 or more digits 
and the absence of the A-bit will 
be indicated by the letter X. (This is necessary for 
the input programme to recognize the be- ginning of a new instruction.) A200.5 Add (200) (the contents of address 200) and (5) to the 
accumulator. Step the 
address counter by 2. Take next 
instruction from 200 (= jump to 
200) and store contents 
of accumulator in 5. Jump to 200 and store previous contents of ad- dress counter in 
5. This amounts 
to placing a link 
instruction for return from a 
sub-routine. Take next 
instruction from 200 but modify it with 
(5) thus making a variable instruction. 
X200E5 X200KE5 X200K5 Arithmetic bits 
The remainder of the function bits have arithmetic 
meanings. We 
shall only 
briefly indicate their 
different actions. 
B: Do not use the A accumulator (most significant 
accumulator) but the 
B accumulator. IC) Id1 Fig. 2. The possible combinations 
of the A- and K-bits. 

202 Part 3 1 The instruction-set processor level: variations in the processor 
C: Clear the accumulator specified by B after storing, or before addition. (In a serial 
machine like ZEBRA this is auto- matically the case, cf. Fig. 3.) I: Subtract instead 
of add. Q: Add one (unit 
in the least significant place) to the B-accu- mulator. L: Shift both accumulators one place 
to the 
left. R: Shift both accumulators one place 
to the 
right. The accu- mulators are always coupled together in 
shifting except when C is present. A few more examples 
will be given. A200BCE25 Store (B) in 5, clear B and add (200) to B. Jump to 200. Store (B) in 6, put - 1 in B (because of QIBC) and shift the A accumu- lator one place to the left. Shifting from B into A is prevented by the presence of C. Jump to 200. Shift A to the right. Copy (3) into B. As register 3 is just an address for the B accumulator itself, this means 
that A is shifted while B is static. Take the instruction from 200 and modify it with the contents of the B accumulator (= register 3). Put -1 in B afterwards. X2WQLIBCE6 X200RBC3 X200KSQIBC Drum store Fast store To store Fig. 3. Accumulator. Section 1 1 Processors with greater 
than 1 address per instruction 
As can be seen, many complicated operations can 
be composed by the elementary possibilities of the separate 
bits. The accumulator A simplified block diagram of one of the accumulators is shown in Fig. 3. Shifting is effected by looping the accumulator over one place 
less or one place 
more. In a double addition 
the contents of the drum store and the 
fast store are first added together in 
the pre- adder (possibly augmented by unity in the B accumulator, if Q is present) and this result is added into 
the accumulator (or 
sub- tracted in case of I). A clearing gate controlled by C 
interrupts the recirculation of the previous contents. The control unit 
The control unit has two shifting registers, 
the C-register which 
receives the next instruction to be executed and the 
D-register or counter. The block diagram is shown in Fig. 4. After a new instruction has come into C, 
it is taken over in parallel 
form into E in the interword time. 
It remains in E while the next instruction is coming into C. Let us explain the action of this control with 
a short programme. 
Examples of programmes 100 X101E5 
101 AC102 102 constant 103 etc. The actions in 
the several registers 
are now: X1007 X101E5 X102 .L const. X103E5 Suppose Xl00 is in C at the 
start. This will take ( 100) into C. (C) + 2 -+ D. Another jump comes into 
C taking in (101) and storing (A) + 5. (C) + 2 -+ D gives X103E5. Note that the operational part is kept in the counter. The necessary constant from 102 is just becoming available. 
The next instruction is taken from 103 which is immediately following. The constant in A is stored to 5 by E5, and is still active after coming 
back from D. 
Chapter 12 I ZEBRA, a simple binary computer 
203 D If To store Fig. 4. Control unit. 
This is the most important aspect of the machine. An instruction in the address counter comes back after 
an A-instruction and can do something useful. To our surprise 
we found that in many more 
cases than we first suspected, the second action could be used effectively. In most other computers 
the time of access to the next instruction is lost 
because nothing 
can be done concurrently in 
the arithmetic unit. 
Another example of the action of the control is the jump to a sub-routine. Suppose that we 
have the following piece of pro- gramme: 100 X200KE.5 Jump to sub-routine starting 
in 200. Place return jump 
in 5. 102 etc. Sub-routine returns 
here. The action is as 
follows: (C) (D) The instruction is taken from 
100. x1007 X200KE5 X102 X200KE54 C and Xl00 + 2+ D. Now KE5 stores D in 5. Thus (5) = X102. The subroutine at 200 is executed and ends with XK5: jump to 5. (200) XK5 Take instruction from 
5. XI02 (102) etc. Now the main programme proceeds to 102 By ending the 
sub-routine: 220 X221K5 221 - 1 we can return not two but 
one location 
further on, i.e., X221K5 takes as next instruction (5) - 1 = X101. Here 5 contains the instruction and the 
drum modifier. The test bits 
The digits V x4 x2 x1 will not be dealt with 
extensively but the different combinations 
of these 4 digits represent different types 
of test. When for example V1 is attached to an instruction, this 
instruction will be executed when (A) is negative, but will be skipped altogether when (A) 
is positive or zero. The harmless A-instruction will 
then be executed instead. 
The test can be at- tached to a jump, giving a 
conditional jump, 
as well 
as to an 
A-instruction, giving a 
conditional addition. The W-bit So far the digit W has 
not been mentioned. 
When W is present in an instruction the drum address is not used. The instruction is not kept 
waiting but is immediately executed 
and the 
drum is completely disregarded. 
With the help 
of this digit W, jumps can be made to instructions in the fast store, e.g., XK5W takes the instruction from 5 only, and the 
drum does not deliver any 
number. The use of this type of instruction has 
very peculiar consequences. 
Let us take the following example: 100 XlOlKE6 (5) = ARW 101 X8186KSRW 102 etc. (6) = filled with return 
instruction The action is as follows: 
Take instruction from 100. XlOlKE6 X102 Jump to 101 and store return instruction X102 in 6. X8 186K5WR Do 1 right shift. 
\ 4/2 a ARW X8188KSRW Do another right shift by ARW. The drum address in D is counted up but is not active. The register address remains 
the same. Hence the instruc- tion in 5 is repeated. I 
204 Pari 3 1 The instruction-set processor level: variations in 
the processor 2-2 - a X8188K5RW The repeating instruction as well as the repeated instruction are both shifted one 
place to 1 F3 * a ARW 
X8190K5RW the right. 2-5 * a ARW 
XOOOK6RW As the drum address overflows into the fast store address the repeating instruction becomes 
X8192K5RW = XOOOK6RW taking the next instruction from 6. 2-4 a X8190K5RWb d 1 2-6 a XOOOK6RW 2Y.a X102 As (6) = X102 the repetition returns to the main programme and the 
A accumulator is shifted over 7 places. The instruction ARW has thus 
been repeated 
p times when the drum address of the repeating instruction is 8192-213. This way 
of repeating an 
instruction has 
made it 
possible to do multipli- cation, division, block transfers, table look up and many other small basic repetitive processes in a very 
simple way. 
There is no special hardware present in the machine to do the counting neces- sary for the repetition, as this counting is done by the normal address counter. As a last 
example we shall give a 
programme for the summa- Section 1 1 Processors with greater 
than 1 address per instruction 
tion of a block 
of locations from 200 to 300 in the store. This 
involves 101 locations. The programme reads: 100 AlOlBC 101 A200Q 102 X103KE4C 
Put A200Q in B (B has address 
3). Put return jump 
X104 in 4. Clear A in advance. Repeat A200Q 101 times. Because 
A200Q is standing in B 
the Q augments the in- struction itself at every repetition. Hence 
successively (200), (201) etc. 
are added to A. At the end 
the sum is left in 
A and the programme proceeds 
at 104. 103 X7990K3W 104 etc. It is left to the reader to 
work out the action diagram. This example 
is not programmed for minimum waiting, 
but by supplying the repeating instruction X7990K3W with a Q it will step up the repeated instruction A200Q by 2 every time. Now, once the first instruction has been located, all even locations 
follow- ing are emerging from 
the drum just at the right time. 
The odd numbered locations must 
be summed in a 
second, similar 
repeti- tion. References VandW59; VandW52, 56; WilkhlS3a. 
Chapter 13 oc 6 bits 
UNIVAC Scientific (1103A) U V 15 bits 15 bits instruction logic1 
John W. Cam Ill The UNIVAC Scientific computer is a (35, 0, 0)2 binary machine, 
with option of (27, 8, 0). The arithmetic unit 
contains two 36-bit X (exchange) and Q (quotient) registers and one 72-bit A register (accumulator). Negative numbers 
are represented in 
one™s com- plement notation. Input-output is via high-speed 
paper tape 
reader and punch, direct card reader 
and punch, and Uniservo magnetic tape units, which may be connected to peripheral punched card 
readers and punches and a high-speed printer. In addition, information 
may be recorded on magnetic tape directly from keyboards 
by the use of Unitypers. Communication with external 
equipment is via an %bit (IOA) register 
and a S6-bit 
(IOB) register. Information sent 
to these registers controls magnetic tapes 
as well as other input- output equipment. The program address counter (PAK) contains the present instruction 
address. Storage is in up to 12,288 locations of magnetic core storage, along 
with a directly addressable 
drum of 16,384 locations. Instructions 
are of the two-address form, 
with six bits for the operation code and two fifteen-bit addresses 
(11 and v). The following information is taken from 
a Univac Scientific Manual [Univac Scientific 
Electronic Computing 
System Model 1103A, Form EL3381. Definitions and conventions lnstruction word ‚In E. M. Grabbe, S. Ramo, and D. E. Wooldridge (eds.), ﬁHandbook of Automation, Computation, and 
Control,ﬂ vol. 2, chap. 2, pp. 77-83, John Wiley & Sons, Inc., New York, 1959. 2Carr™s triplet notation for: fractional 
significant digits, digits in exponent, and digits to left of radix point. oc Operation code u First execution address 
v Second 
execution address 
For some of the instructions, the form jn 
or jk replaces the u ad- dress; for 
others the form k replaces the v address. 
j n k One-digit octal number 
modifying the instruction Four-digit octal 
number designating number of times in- 
struction is to be performed Seven-digit binary number 
designating the number of places the word is to be shifted to the 
left Address allocations 
(octal) 00000-07777 4096 00000-17777 8192 or 00000-27777 12,288 36-bit words Q 31000-31777 1 36-bit word 
A 32000-37777 172-bit word MD 40000-77777 16,384 36-bit 
words Fixed addresses F, 00000 or 40001 F, 00001 F, 00002 F, 00003 Arithmetic section registers 
A A, A, Q X 36-bit exchange register 
72-bit accumulator with 
shifting properties 
Right-hand 36 bits 
of A Left-hand 36 bits of A 36-bit register with shifting properties Note: Parentheses denote contents of. For example, 
(A) means contents of A (72-bit word in A); (Q) means contents of Q (36-bit word in Q). 205 
206 Part 3 I The instruction-set processor level: variations 
in the 
processor Section 1 I Processors with greater 
than 1 address per instruction 
Input-output registers IOA 8-bit in-out register 
IOB 36-bit in-out register TWR 6-bit typewriter 
register HPR 7-bit high-speed 
punch register Word extension 72-bit word whose 
right-hand 36 bits are 
the word at address u, and whose left-hand 36 bits are 
the same as the leftmost bit of the word at u. 72-bit word whose 
right-hand 36 bits 
are the word at address u, and whose left-hand 36 bits are zero. 72-bit word-right-hand 
36 bits are 
in register Q, left- hand 36 bits are 
same as leftmost 
bit in register Q. same as 
D(Q) except left 36 
bits are zero. D(AR), S(AR) are similarly defined. 
L(Q)(u) 72-bit word-left-hand 36 
bits are zero, right-hand 36 bits are the bit-by-bit product 
of corresponding bits of (Q) and word at address u. 72-bit word-left-hand 36 
bits are zero, right-hand 36 bits are 
the bit-by-bit product 
of corresponding bits of the complement of (Q) and word at ad- dress v. L(Q™)(v) Transmit instructions 
11™ 13 12 15 16 35 36 22 Transmit Positive TPuv2: 
Replace (v) with (u). Transmit Negative TNuv: Replace (v) with the comple- ment of (u). Transmit Magnitude TMuv: 
Replace (v) with the absolute magnitude of (u). Transmit U-address TUuv: Replace the 15 bits of (v) desig- nated by vl:, through vZ9, with 
the corresponding bits 
of (u), leaving the remaining 21 bits of (v) undisturbed. 
Transmit V-address TVuv: Replace the right-hand 15 bits of (v) designated by 
vo through vI4, with the corresponding bits of (u), leaving the remaining 21 bits of (v) undisturbed. Add and Transmit ATuv: 
Add D(u) to 
(A). Then replace 
(v) with (AR). Subtract and Transmit STuv: 
Subtract D(LI) from (A). Then replace (v) with (AR). Left Transmit LTjkv: Left circular 
shift (A) by k 
places. If j = 0 replace (v) with (AL); if j = 1 replace (v) with (AR). 
‚Octal notation. Mnemonic notation Q-controlled instructions 
51 Q-controlled Transmit QTuv: Form in A the number L(Q)(u). Then 
replace (v) by 
(AR). 52 Q-controlled Add QAuv: Add to (A) the number L(Q)(u). Then replace (v) 
by (AR). Q-controlled Substitute 
QSuv: Form in A the quantity 
L(Q)(u) plus L(Q™)(v). Then replace 
(v) with (AR). The effect is to replace selected 
bits of (v) with the corre- sponding bits of (u) in those places corresponding 
to 1™s in Q. The final (v) is the same as 
the final (AR). 53 Replace instructions 
21 Replace Add RAuv: 
Form in A the sum of D(u) and D(v). Then replace (11) with (AR). 
Replace Subtract RSuv: Form in A the difference D(u) minus D(v). Then replace (11) with (AR). Controlled Complement CCuv: 
Replace (AR) with (u) leaving (AL) undisturbed. Then complement 
those bits of (AR) that correspond to ones in (v). Then replace 
(u) with Left Shift in A LAuk: Replace (A) with D(u). Then 
left circular shift (A) by k places. Then replace 
(u) with (AR). If u = A, the first step is omitted, so that the initial content of A is shifted. Left Shift in Q LQuk: Replace (Q) with (u). Then left circular shift (Q) by k places. Then replace (u) with (0). 23 27 (AR). 54 55 Split instructions 
31 Split Positive 
Entry SPuk: Form S(u) in A. Then left circu- lar shift (A) by k places. Split Negative Entry SNuk: 
Form in A the complement of S(ii). Then left circular shift (A) by k places. Split Add SAuk: 
Add S(u) to (A). Then left circular shift (A) by k places. Split Subtract SSuk: Subtract S(u) from (A). Then left circular shift (A) by k places. 33 32 34 Two-way conditional jump instructions 
46 47 Sign Jump SJuv: 
If A,, = 1, take (u) as NI. 
If A,, = 0, take (v) as NI. (NI means next 
instruction.) Zero Jump ZJuv: 
If (™4) is not zero, take 
(u) as NI. If (A) is zero, take (v) as NI. 
Chapter 13 I UNIVAC Scientific (1103A) instruction logic 207 44 Q-Jump QJuv: 
If Q35 = 1, take (u) as NI. If Q35 = 0, take (v) as NI. Then, in either case, left 
circular shift (Q) by one place. 
One-way conditional jump instructions 
41 Index Jump 
IJuv: Form in A the difference D(u) minus 1. Then if A,, = 1, continue 
the present sequence 
of in- structions; if A,, = 0, replace (u) with (AR) and take (v) 
as NI. Threshold Jump TJuv: 
If D(u) is greater than (A), take (v) as NI; if not, continue 
the present sequence. 
In either 
case, leave (A) in its initial state. Equality Jump EJuv: 
If D(u) equals (A), take (v) 
as NI, if not, continue the present sequence. 
In either case leave (A) in its initial state. 42 43 One-way unconditional jump instructions 
45 Manually Selective Jump MJjv: If the number j is zero, take (v) as 
NI. If j is 1, 2, or 3, and the correspondingly numbered MJ selecting switch 
is set to ﬁjump,ﬂ take (v) 
as NI; if this switch is not set 
to ﬁjump,ﬂ continue the present sequence. 
Return Jump RJuv: 
Let y represent 
the address from 
which CI was obtained, Replace the right-hand 15 bits of (u) with the quantity 
y plus 1. Then take (v) as NI. Interpret IP: 
Let y represent 
the address from which 
CI was obtained. Replace 
the right-hand 15 bits of (F,) with the quantity y + 1. Then take (F,) as NI. 37 14 Stop instructions 
56 Manually Selective Stop MSjv: If j = 0, stop computer 
operation and 
provide suitable indication. 
If j = 1, 2, or 3 and the correspondingly numbered MS selecting switch 
is set to ﬁstop,ﬂ stop computer 
operation and provide suitable indication. 
Whether or not 
a stop 
occurs, (v) is NI. Program Stop PS-Stop computer operations and 
provide suitable indication. 
57 External equipment instructions 
17 External Function EF-V: Select a unit 
of external equip- 
ment and 
perform the function designated by 
(v). 76 External Read ERjv: 
If j = 0, replace the right-hand 8 bits of (v) with (IOA); 
if j = 1, replace (v) 
with (IOB). 
External Write EWjv: 
If j = 0, replace (IOA) with 
the right-hand 8 bits of (v); if j = 1, replace (IOB) with (v). 
Cause the previously selected unit to 
respond to the 
infor- mation in 
IOA or 
IOB. PRint PR-V: 
Replace (TWR) with the right-hand 6 bits of (v). Cause the typewriter 
to print the character 
corre- sponding to the 6-bit code. 
Punch PUjv: Replace (HPR) with the right-hand 6 bits of (v). Cause the punch to respond to (HPR). If j = 0, omit seventh level hole; if j = 1, include seventh 
level hole. 77 61 63 Arithmetic instructions 
71 72 73 74 Multiply MPuv: 
Form in A the 72-bit product of (u) and (v), leaving in 
Q the multiplier (u). Multiply Add MAuv: Add to (A) the 72-bit product of (u) and (v), leaving in Q the multiplier (u). Divide DVuv: 
Divide the 72-bit number (A) by (u), putting 
the quotient in Q, and leaving in A a non-negative re- 
mainder R. Then replace (v) 
by (Q). The quotient and remainder are 
defined by: 
(A), = (u) - (Q) + R, where 0 5 R < I(u)I. Here (A)i denotes the initial contents of A. Scale Factor SFuv: 
Replace (A) with D(u). 
Then left cir- 
cular shift (A) by 36 places. Then continue to shift (A) until A,, # A,5. Then replace the right-hand 15 bits 
of (v) with the number of left circular 
shifts, k, which would 
be neces- sary to return (A) to its original position. 
If (A) is all ones 
or zeros, 
k = 37. If u is A, (A) is left unchanged in the first step, instead of being replaced 
by D(A,). Sequenced instructions 75 Repeat RPjnw: This instruction calls for 
the next instruc- tion, which will 
be called NIuv, to be 
executed n times, 
its u and v 
addresses being modified or not according to the value of j. Afterwards the program is continued by the execution of the instruction stored 
at a fixed address F,. The exact steps carried out are: 
a Replace the right-hand 15 bits of (F,) with the address w. 
Execute NIuv, the next instruction in the program, n times. b 
208 Part 3 1 The instruction-set processor 
level: variations in 
the processor c If j = 0, do not change 
u and v. If j = 1, add one to v after each 
execution. If j = 2, add one 
to u after each execution. If j = 3, add one to u and v after each execution. The modification of the u address and v address is done in program control 
registers. The original form 
of the instruction in storage is unaltered. d On completing n executions, take (FJ, as the next instruction. F, normally contains 
a manually selec- 
tive jump whereby 
the computer is sent to w for 
the next instruction after 
the repeat. If the repeated instruction is a 
jump instruction, the occurrence of a jump terminates 
the repetition. If the instruction is a 
Threshold Jump or an Equality Jump, and the jump to 
address v occurs, 
(Q) is replaced by the quantity j, (n - r), where r is the number of executions that have taken place. e Floating point instructions 
64 65 66 Add FAuv: 
Form in Q the normalized rounded 
packed floating point sum (u) + (v). Subtract FSuv: Form 
in Q the normalized rounded 
packed floating point difference (u) - (v). Multiply FMuv: Form in Q the normalized rounded packed floating point product (u) - (v). 67 01 02 03 04 05 Section 1 I Processors with greater 
than 1 address per instruction 
Divide FDuv: Form in Q the normalized rounded packed 
floating point quotient 
(u) + (v). Polynomial Multiply FPuv: Floating 
add (v) to the floating product (Q)i (u), leaving the packed normalized rounded 
result in Q. Inner Product FIuv: Floating 
add to (Q)i the floating product (u) * (v) and store 
the rounded normalized packed 
result in Q. This instruction 
uses MC location F4 = 00003 for temporary storage, where (FJf = (Q)i. The subscripts i and f represent ﬁinitialﬂ and ﬁfinal.ﬂ Unpack UPuv: Unpack 
(u), replacing (u) with (u)~ and replacing (v)~ with (u)~ or its complement if (u) is negative. The characteristic portion of (u)~ contains sign bits. The sign portion and mantissa portion of (v)~ are set to zero. Note. The subscripts M and C denote the mantissa and characteristic portions. Normalize Pack NPuv: Replace 
(u) with the normalized rounded packed floating point number obtained 
from the possibly unnormalized mantissa in (u)~ and the 
biased characteristic in (v)~. Note. It is assumed that (u)~ has the binary point between uZ7 and uZ6; that is, that (u)~ is scaled by 2-27. Normalize Exit NEj-: If j = 1 normalize without rounding until a master clear or until the instruction is again exe- cuted with i = 0. References Univac Scientific 
Electronic Computing 
System Model 1103A, Form EL 338 
Chapter 14 Instruction logic of the MIDAC1 John W. Cam III The MIDAC, Michigan 
Digital Automatic Computer [Carr, 
19561, was constructed on the basis of the design of the SEAC at the National Bureau 
of Standards. Its instruction code is particularly of interest because 
it incorporates the index register concept into a three-address binary instruction. Numbers 
in this machine 
are (44, 0, 0)2 fixed points. The word length 
is 45 binary digits with serial operation. 
Word structure The data 
or address 
positions of an instruction are labeled 
the a, j3, and y positions. Each contains twelve 
binary digits represented externally as three hexadecimal digits. Four binary digits, or one hexadecimal digit, 
are used to convey the instruction modification or relative 
addressing information. The next four binary digits or 
single hexadecimal digit represents 
the operation portion 
of the instruction. The final binary digit is the halt or breakpoint indi- cator for use with the instruction. For example, the 45-binary-digit word 00000110010000001100100000010010l100000001011 considered as an instruction would 
be interpreted as a P Y abcd Op halt 000001100100 000011001000 o00100101100 
0OOo 0101 1 In external hexadecimal 
form this would 
be written 064 0c8 12c 0 5 - The above binary word 
is the equivalent machine representation 
of the following instruction: ﬁTake the contents of hexadecimal address 064, add to it the 
contents of hexadecimal address 0c8, 
and store the result in hexadecimal address 12c. There is no modification of the 12-binary-digit address locations 
given by the 
‚In E. M. Grabbe, S. Ramo, and D. E. Wooldridge (eds.), ﬁHandbook of Automation, Computation, and Control,ﬂ vol. 2, chap. 2, pp. 115-121, John Wiley & Sons, Inc., New York, 1959. 2Carr™s triplet 
notation for: fractional significant digits, digits 
in exponent, and digits to left of radix point. instruction. Upon completion of the operation, stop 
the machine if the proper external switches 
are energized.ﬂ The binary com- 
bination represented 
by 5 is the operation code for addition. 
Data or addresses The addresses given 
by the 
twelve binary digits 
in each of the three locations designate 
in the machine the individual acoustic 
storage cells and blocks of eight magnetic drum 
storage cells. The addresses from 
0 to 1023 (decimal) or 
000 to 3FF (hexadecimal) correspond to acoustic storage 
cells. The addresses from 1024 
to 4095 (decimal) or 
400 to FFF (hexadecimal) correspond 
to mag- netic drum 
storage blocks. In certain operations, however, 
the addresses 0 to 15 (decimal) or 
0 to F (hexadecimal) represent 
input-output stations rather than storage locations. 
These twelve-binary-digit groups 
will in some cases be modified by the machine in order to yield a 
final twelve-binary-digit address. The method of processing will 
depend on the values of the instruc- tion modification digits. After modification, 
the final result will then be 
interpreted by the control unit as a machine address. In some instructions, namely those 
that perform change of control operations, which 
involve cycling and counting rather than 
simple arithmetic operations on numbers, the a and /3 positions in an 
instruction are not considered as addresses. In those cases, they are 
used instead as counters or 
tallies. In other instructions, which do not require three 
addresses, but only one or two, 
the p position is not considered 
as an address. In these cases, the oddness or evenness 
of the /3 address is used to differentiate be- 
tween two 
operations having 
the same operation 
code digits. That is, the parity of binary digit 
P22 is used as an extra function 
designator. Instruction modification digits 
The four binary 
digits P9-P6 are used as 
instruction modification or relative addressing digits. 
Their normal function 
is relatively simple; nevertheless, the possible exceptions to the general rule 
can make their behavior complicated. These 
four digits are labeled 209 
210 Part 3 I The instruction-set processor level: variations in 
the processor the a, b, c, and d digits. Ordinarily 
the a digit is associated with the a position, the b digit with the position, and the 
c digit with the y position in 
an instruction. When binary digit 
P22 (or the p position) is used in 
an instnic- 
tion to represent extra operation information, 
the instruction modification digit b 
is ignored. In 
the case of input and output 
instructions, when 
the various address positions represent machine 
address locations 
on the drum, input-output 
stations, or block lengths, and modification of these addresses is not desired 
in any case, the corresponding relative 
addressing digits 
are ignored. The purpose of the instruction modification digits 
is to tell the machine whether or not 
to modify the twelve binary digits making 
up the corresponding address 
position in 
an instruction by addition 
of the contents of one or the other of two counters. In 
the normal case, if the a, b, or c digit is a zero, the twelve binary digits 
in the corresponding position are interpreted, 
unchanged, as the binary representation 
of the machine address of the number word to be processed by the instruction. If one or more 
of the a, b, or c digits 
is a one, the contents of one of two auxiliary address counters is added to the 
corre- sponding twelve binary 
digits to yield a 
final address usually differ- 
ent from that given by the original twelve-digit portion 
of the instruction word. 
The addresses are then 
said to be relative to the 
counter. The two counters involved in the address modification feature of the MIDAC are known as the instruction counter and the 
base counter. In the normal case, if the fourth instruction modification or d digit is a zero, 
the contents of the instruction counter will be added to the 
contents of the various twelve-digit addresses (dependent on the values of the a, b, 
and c digits) 
before further processing of the instruction. If the a digit is one and the d digit 
zero, the contents of the instruction counter will be added to 
the a address; similarly for b 
and d digits 
and P address, etc. If the d digit is a one, the contents of the base counter will be normally added to the contents of the twelve digits in 
the a, b, and y positions (again dependent on the values of the a, b, and 
c digits), 
before further processing of the results. If the a digit is one and the d digit one, 
the contents of the base counter will be added to the a address, etc. marized as follows: The effect of the instruction modification digits 
may be sum- The contents of the two 
counters will he designated by 
C, (d = 0, 1). C, = contents of the instruction counter C, = contents of the base counter Section 1 1 Processors with greater 
than 1 address per instruction 
Then the modified addresses a™, b™, and y™ are related to the 
a, /I, and y addresses appearing in the instruction by 
the following: a™ = a + aC, p™ = + 1Xd y™ = y + cC, (a, b, c, 
d = 0, 1) In certain instructions addresses relative to one of the two counters may be prohibited. Thus, 
if in a particular instruction N may be relative only to the instruction counter, then for that instruction a™ = a + aC, no matter whether the d digit is a 0 or a 1. in the location whose address 
is a™, b™, or y™. The notation (a™), (b™), or (y™) is used to indicate the word stored Instruction counter The instruction counter is a twelve-binary digit (modulo 
4096) counter which contains 
the binary representation 
of the address of the instruction which 
the control unit is processing or is about to process. In normal 
operation when no change 
of control opera- 
tion is being processed, the contents of the instruction counter is increased by 
one at the completion of each instruction. Thus, 
normally the next instruction to be processed is stored in the acoustic storage cell immediately 
following the cell which contains 
the present instruction. 
A change of control operation 
is one which selects 
a next 
in- struction not stored in sequence in the acoustic storage. 
That is, at the completion of such instructions 
the contents of the instruc- tion counter is not increased by one, 
but instead is changed en- 
tirely. Base counter 
The base counter is a second twelve-binary-digit 
counter (modulo 4096), physically identical to 
the instruction counter, which con- tains the binary representation 
of a base 
number or tally. 
Unlike the instruction counter, however, the base counter does not se- quence automatically, but remains unchanged until a change of base instruction is processed. This counter serves two primary purposes, dependent on the usage to which it is put: 1 It may contain the address of the initial word 
in a group, thus serving as a base 
address to which integers representing 
the relative position of a given 
word in the group 
of words may be added by using the address modification digits. 

Chapter 14 I Instruction logic of the MIDAC 211 2 It may contain a counter or tally 
which can be 
increased by a base 
instruction. This instruction 
makes use 
of the address modification digits to change the counter so as to count the number 
of traversals of a particular cycle of instructions. Instruction types 
Instructions used in MIDAC can be divided into three 
categories: change of information, change of control, and transfer of informa- tion. The first category can 
be further subdivided into arithmetic 
and logical instructions. 
In the arithmetic 
instructions are included 
addition, subtraction, 
division, various 
forms of multiplication; power extraction, 
number shifting; and number conversion instruc- tions. The sole logical instruction is extract, which 
modifies infor- 
mation in a nonarithmetic fashion. The transfer of information or 
data transfer instructions include 
transfers of individual words 
or blocks of words into and 
out of the acoustic storage 
and drum and magnetic tape control. The possible change of control instructions includes 
two com- parisons that provide different future sequences dependent on the differences of two numbers. In 
the compare numbers 
or algebraic comparison, the difference is an algebraic, signed one. 
In the compare magnitudes 
or absolute comparison, the difference is one between absolute values. Two other instructions, file and base, perform other tasks beside transferring control. 
The file instruction transfers control unconditionally. 
The file instruction files or stores 
the contents of the base or 
instruction counter in a specific address position of a particular word 
in the storage. The base or tally instruction provides a 
method for referring addresses automatically relative to the address given by 
the base counter, irrespective 
of its contents. 
The base instruction also gives a conditional transfer 
of control. The nineteen MIDAC instructions can 
be described function- ally as follows: 
Change of information Add. (a™) + (p™) is placed in y™. Result must 
be less than 1 in absolute 
value. Subtract. (a™) - (p™) is placed in 
y™. Result must be less than 1 in absolute value. Multiply, Low Order. The least significant 44 binary digits of (a™) x (p™) are placed in 
y™. Multiply, High Order. The most significant 
44 binary digits of (a™) x (p™) are placed 
in y™. 5 6 7 8 9 10 11 Multiply, Rounded. 
The most significant 
44 binary digits of (a™) x (p™) k 1 2-45 are placed in 
y™. The 1 * 2-45 is added if (a™) x (p™) is positive, and subtracted 
if (a™) x (p™) is negative. Divide. The most significant 44 binary digits of (D™)/(a™) are placed in 
y™. (Note the 
inversion of order of a and p.) Result must be less than 1 in absolute 
value. Power Extract. 
The number n * 2-44 is placed in 
y™ where n is the number of binary 0™s to the left of the most signifi- 
cant binary 1 in (a™). The b digit 
is ignored; p may be any even number. 
If (a™) is all zeros, zero 
is placed in y™. Shift Number. The 44 binary digits immediately to the right of the radix point in 
(a™) * 2(P™)™2™‚ are placed 
in y™. The result, in 
y™, is the equivalent of shifting (a™) n places, 
where n - 2-44 = (p™) and 11 positive indicates a shift 
left, n negative a shift right. If In1 2 44, zero is placed in y™. Extract or 
Logical Transfer. Those 
binary digits in (y™), including the sign digit, whose positions correspond 
to 1™s in (p™) are replaced by the digits in the corresponding positions of (a™). Decimal to Binary Conversion. This operation 
may be interpreted in two ways: (a) (a™) is considered as a binary- 
coded-decimal integer 
times 2-44. It is converted to the equivalent binary integer 
times 2-37 and the 
result is placed in y™, or (b) (a™) is considered as a binary-coded- 
decimal fraction, 
D. It is converted into an 
intermediate binary fraction, 
Ri, such that Bi = D x loll x 2-37 and the result placed in y™. To obtain B, the true 
binary equiv- 
alent of D, Bi must be multiplied by x 237). How- ever, since 
this factor is greater than l and therefore can- 
not be represented in 
the machine, two operations 
must be performed. For example, B~ x (10-11 x 237 - 1) = B~ B = Bi + Bj Here the 
b digit is 
ignored, and p may be any eoen number. Binary-to-Decimal Conversion. 
(a™), considered as a 
binary fraction, is converted into the equivalent eleven-digit bi- 
nary-coded-decimal fraction. The result is placed in 
y™. The b digit 
is ignored, and /3 may be any odd number. 
Change of control 12 Compare Numbers. y can be relative only to the instruc- tion counter. If (a™) 2 (p™), the contents of the instruction counter are 
increased by one as is normally done at the 
end of each instruction. 
If (a™) < (B™), the contents of the instruction counter are 
set to y™. 
212 Part 3 I The instruction-set processor level: variations in 
the processor 13 14 15 Compare Magnitudes. y can be 
relative only to the instruc- tion counter. 
If I (a™) 1 2 I (p™) 1, the contents of the instruc- tion counter are increased 
by one as is normally done at the end of each instruction. 
If I (a™) I < 1 (p™) 1, the contents of the instruction counter is set to y™. Base or Tally. The d digit is ignored. a and p may be relative only to the base counter, y only to the instruction counter. If a™ 2 p™, the contents of the base counter are 
set to zero and the 
contents of the instruction counter increased by one as usual. If a™ < /3‚, the contents of the base counter are 
set to a™ and the 
contents of the instruc- tion counter to y™. (Note. The comparisons made here are 
of addresses themselves, 
not their contents.) File. p may be any odd number. 
a and y may be relative only to the instruction counter. 
If d = 0, the contents of the instruction counter in- creased by one is placed in 
the y position of (a™), and the instruction counter is set to y™. If d = 1, the contents of the base counter is placed in 
the a position of (a™), and the instruction counter is set to y™. In addition, 
if b = 1, the contents of the base counter is set to zero; if b = 0, the contents of the base counter is not changed. 
Transfer of information 
16 17 Section 1 I Processors with greater 
than 1 address per instruction 
16 Alphanumeric Read 
In. The a digit must 
be 1; the b digit 
is ignored. If p is in the range 0 to 7 (decimal) or 000 to 007 (hexadecimal) a characters are read 
into the acoustic storage from input-output station /3. The first character read in is placed in 
y™, the second in 
y™ + 1, etc. Each character occupies the six most significant 
digit positions 
of the register into which it 
is read; the other 
positions are set to zero. This operation may not 
be used to read words from the drum into the 
acoustic storage. Alphanumeric Read Out. 
The a digit must 
be 1; the c digit is ignored. Starting with 
(p™), read out 
a consecutive char- 
acters from the acoustic storage 
to input-output station y; y must be in the range 0 to 7 (decimal) or 000 to 007 (hexadecimal). This operation may not be used to read words from the acoustic storage 
onto the drum. Move Tape Forward. 
(a, b, c and d digits 
are ignored.) /3 may be any even number; y must be in the range 0 to 15 decimal (000 to OOF hexadecimal). The magnetic tape at 
input-output station y is moved forward n blocks where 17 18 a-1 n=[T] +1 that is, one plus the integral part of a - yx, or the number of blocks that include a words. 19 Move Tape Backward. (a, b, 
c, and d digits 
are ignored.) /3 may be any odd number; y must be in the range 0 to 15 decimal (000 to OOF hexadecimal). The magnetic tape at input-output station y is moved backward n blocks where Read In. The 
a digit must 
be 0; the b digit is ignored. If p is in the range 0 to 7 (decimal) or 000 to 007 (hexadeci- 
mal) a words are read into the acoustic storage from in- put-output station p. The first word read 
in is placed in y™, the second in y™ + 1, etc. If p is in the range 1024 to 1791 decimal (400 to 6FF hexadecimal), a words are read into the acoustic storage from the drum starting with the 
first word in the drum block whose address 
is p. The first word is placed in 
y™, the second in y™ + 1, etc. Read Out. The a digit must 
be 0, the c digit 
is ignored. Starting with (p™), read out 
a consecutive words 
from the acoustic storage 
to input-output 
station y, if y is in the range 0 to 7 decimal (000 to 007 hexadecimal), 
or to the drum starting 
at the beginning of the drum block whose 
address is y, if y is in the range 1024 to 1791 
decimal (400 to 6FF hexadecimal). references: LeinA54. 
References CarrJ56. SEAC computer references: AinsE52; AlexS51; 
ElboR53; GreeS52, 
53; HaueR52; PikeJ52; SerrR62; ShupP53; SlutR51. DYSEAC computer a-1 .=[TI +1 that is, one plus the integral part of a - yx, or the number of blocks that include a words. 
Chapter 15 Instruction logic 
of the Soviet Strela 
(Arrow)l John W. Caw III A typical general purpose digital 
computer using three-address 
instruction logic is the Strela (Arrow) 
constructed in quantity 
under the leadership of Iu. la. Basilewskii of the Soviet Academy of Sciences, 
and described in detail 
by Kitov [1956]. This com- puter uses a (35, 6, 0)2 binary floating point number system. Its instruction word, 
of 43 digits, contains a six-digit operation code, and three 
12-digit addresses, 
with one breakpoint bit. 
In octal notation, 
two digits represent the operation, four 
each the addresses, and one bit the 
breakpoint. This machine operates 
with up to 2048 words of high-speed cathode ray tube storage. Input-output is ordinarily via punched cards and punched paper tape. 
A ﬁstandard program libraryﬂ 
is attached to the 
com- puter as well as magnetic tape units (termed ﬁexternal accumula- 
torsﬂ below). 
Note. This computer is different from both the BESM described by 
Lebedev [ 19561 and the Ural reported by Basilewskii [ 19571. Apparently, it is somewhat lower 
in performance than BESM. Since all arithmetic is ordinarily in floating point, ﬁspecial 
instructionsﬂ perform fixed point computations 
for instruction modifications. Ordinarily instructions 
are written 
in an octal notation, 
but external to the 
machine operation 
symbols are written 
in a 
mnemonic code. 
The two-digit numerals 
are the octal instruction equivalent. Arithmetic and 
logical instructions 01. + cy /3 y. Algebraic addition of (a) to (p) with result in y. 02. + a /3 y. Special addition, 
used for 
increasing ad- 
dresses of instructions. The command (a) or (/?) is added to the 
number (/3) or (a) and the result sent to the 
cell with address y. ‚In E. M. Grahhe, S. Ramo, and D. E. Wooldridge (eds.), ﬁHandbook of Automation, Computation, and Control,ﬂ vol. 2, chap. 2, pp. 111-115, John Wiley 
& Sons, Inc., New York, 
1959. ™Carr™s triplet notation for: fractional significant digits, digits 
in exponent, and digits to 
left of radix point. As a rule, the address of the instruction being 
changed corresponds to the 
address y. 03. - a /3 y. Subtraction with 
signed numbers. From 
the number (a) is subtracted the number (p) and the 
result sent to y. 04. - ‚cy /3 y. Difference of the absolute value 
of two numbers I(a)I - I(P)I = (VI. 05. X a /3 y. Multiplication of two numbers (a) and (/?) with result sent to y. 06. A a /3 y. 1,ogical multiplication of two numbers in cells a and P. This instruction 
is used for 
extraction from a given 
number or instruction 
a part defined by the special number (p). 07. V cy /3 y. Logical addition of two numbers (a) and (p) and sending the result to cell y. This instruction 
is used for forming numbers 
and commands from parts. 10. Sh a /3 y. Shift of the contents of cell a by the number of steps equal to the 
exponent of the (p). If the exponent of the (p) is positive then the shift proceeds to the left, in the direction of increasing value; 
if negative, then the shift is right. In addition, 
the sign of the number, which is shifted out of the cell, is lost. 11. - cy /3 y. Special subtraction, used for 
decreasing the addresses of instructions. In 
the cell a is found the instruction to be transformed, and in cell p the specially selected 
number. Ordinarily addresses a and y are identical. 
12. # a /3 y. Comparison of two numbers (a) and (p) by means of digital additions 
of the numbers being 
compared modulo two. In the cell y is placed a number possessing ones in those digits in 
which inequivalence 
results in 
the numbers being compared. Control instructions 
13. C 
cy /3 0000. Conditional transfer 
of control either to instruction (a) or to instruction (p), depending on the results of the preceding operation. 
With the operations of addition, sub- 
traction, and subtraction of absolute values, it appraises the sign 213 
214 Part 3 1 The instruction-set processor level: variations in 
the processor of the result: for a positive 
or zero result it transfers control to the command (a), for negative results to the command (p). The result of the operation of multiplication is dependent on the relationship to unity. Transfer is made to the 
command (a) in the case where the result is greater than or equal to one, and to command (p), if it is smaller than one. For conditional transfer 
after the operation of comparison, transfer to the instruction (a) is made in the case of equality of binary digits, and to (p) when there 
is any inequivalence. After the operation A (logical sequential multiplication) the conditional transfer 
command jumps to the 
instruction (a) when the result is different from 
zero, and to instruction (p) when it 
is equal to 
zero. A forced comparison 
is given by c a a 
0000 The third address in this command is not used 
and in its place is put zero. 14. 1-0 a 0000 0000. This instruction 
is executed paral- lel with the code of the other operations, and guarantees bringing 
into working position in good time the zone of the external ac- 
cumulator (magnetic 
tape unit) with 
the address a. 15. H 0000 0000 0000. This instruction executes 
an ab- 
solute halt. Group transfer instructions 
Special instructions 
for group transfer serve 
for the accomplish- ment of a transfer of numbers to and from the accumulators. In the second address 
in these instructions stands 
an integer, 
desig- nating the quantity of numbers in the group which must 
be trans- ferred. Group transfers always are produced in 
increasing sequence of addresses of cells in the storage. 16. T, 0000 n y. The instruction T, guarantees transfer 
from a given input unit (with 
punched cards, perforated tape, etc.) 
into the storage. In the third address y of the instruction is indi- cated the 
initial address 
of the group of cells in the storage where numbers are to be written. With punched paper tape 
or punched cards the variables are written 
in sequence, 
beginning with the first line. 17. T, 0000 n y. The instruction T, guarantees transfer 
of a group of n numbers from 
an input unit into 
the external accumulator in 
zone y. 20. T, a n y. This instruction guarantees a line-by-line 
sequence of transfers of n numbers from zone 
a of the external accumulator into the 
cells of the storage beginning with 
the cell with address y. Section 1 1 Processors with greater 
than 1 address per instruction 
21. T, a n 0000. This instruction 
guarantees the trans- fer to the input-output unit 
(to punched paper tape 
or punched cards) of a group of n numbers from the storage, beginning 
with address a. The record on punched paper tape or punched cards as a rule will begin with the 
first line and therefore a positive indication of the addresses of the record is not required. 
22. T, a n y. Instruction T, guarantees transfer 
of a group of n numbers from one place in the storage with initial address a into another place 
in the storage with initial address 
y. 23. T, a n y. Instruction T, guarantees transfer 
of a group of n numbers from the storage with initial address 
a into the external accumulator with 
address y. 24. T, a n 0000. Instruction T, serves for 
transfer of n numbers from the zone of the external accumulator with 
address a into the input-output unit. 
Instructions T, and T, cannot be performed concurrently with other machine 
operations. Standard subroutine instructions 
Certain instructions in the Strela, although 
written as ordinary instructions, are actually ﬁsyntheticﬂ instructions which call 
on a subroutine for computation of the function involved. 
The amount of machine time (number 
of basic instruction cycles) for 
an itera- 
tive process depends on the required precision of the computed function. The figures given below are based on approximately ten-digit decimal numbers 
with desired precision one in the tenth place. 25. D a /3 y. This standard subroutine 
serves for 
exe- cution of the operation of division: The number (a) is divided into the number (p) and the 
quotient is sent to cell 7. The actual operation of division is executed in two 
steps: the initial obtaining of the value of the inverse of the divisor, by which the dividend is then multiplied. The computation 
of the inverse is given by the usual Newton formula, 
originally used 
with the EDSAC [Wilkes et al., 19521. Yn+1 = YnP - Yn4 For x = d * 2P, where ‚/z < d < 1, the first approximation is taken as 2-P. The standard subroutine takes 
8 to 10 instructions and can be executed in 18-20 machine cycles (execution time for one typical command). 26. < a 0000 y. This instruction guarantees obtaining 
the value & from the value x = (a) and sending the result to cell y. Initially l/& is computed by the 
iteration formula 

Chapter 15 I Instruction logic of the Soviet Strela (Arrow) 
215 where the first approximation is taken as - Z‚P/Z™ 0- the bracket indicating 
ﬁintegral part of.ﬂ After this the result is multiplied by x to obtain 
6. This standard subroutine contains 
14 instructions and is executed in 40 cycles. 27. ex a 0000 y. This instruction 
guarantees formation of L for the value x = (a) and sending the result to cell y. The computation is produced by means 
of expansion of ex in a power series. The standard subroutine contains 
20 instructions and is executed in 40 cycles. 30. lnx a 0000 y. This instruction guarantees 
forma- tion of the function In x for the value x = (a) and sending the re- sult to location y. computation is produced by expansion of In x in series. The subprogram contains 
15 instructions and is executed in 60 cycles. 31. sinx a 0000 y. This instruction guarantees 
execu- tion of the function sin x and sending the result to location y. The computation is produced in 
two steps: initially 
the value of the argument is translated into the first quadrant, then 
the value of the function is obtained by a series expansion. 
The subroutine contains 18 instructions and is executed in 
25 cycles. 32. DB a n y. This instruction 
performs conversion 
of a group of n numbers, stored 
in locations a, a + 1, . . . 
from bi- nary-coded decimal 
into binary and sending of the result to loca- tions y. y + l,. . . . 
The subroutine contains 14 instructions and is executed in 50 cycles (for 
each number). 
33. BD a n y. This instruction 
performs the conversion of a group of n numbers stored 
in locations a, a + 1,. . . 
from the binary system into binary-coded decimal and sends them to loca- tions y, y + l,. . . . The subroutine contains only 30 instructions and is executed with 100 cycles (for 
each number). 
34. MS a n y. This is an instruction for storage sum- ming. This instruction produces 
the formal addition 
of numbers, stored in 
locations beginning 
with address a, and the 
result is sent to location y. Numbers and instructions are added in fixed point. This sum may be compared with 
a previous 
sum for control of storage accuracy. 
References BasiI57; KitoA56; LebeS6; WilkM52. 
Section 2 Processors constrained by a cyclic, primary memory These processors use one extra (the + 1) address to specify the address of the next instruction. Obviously this address is used to allow complete freedom 
in the location of both operands and next instructions 
in an optimum manner. The IBM 650, a 1 + 1 address computer, is the most 
straightforward to un- derstand. ACE and ZEBRA have subtle microcoded 
instructions to achieve powerful instruction sets. The LGP-30 
and LGP-21 have a 
simple 1 address instruction format; 
they interlace sev- eral logical 
addresses between the physical addresses to help with the 
optimum location of operands. The Olivetti Underwood Programma 
101 desk calculator The Programma 101 is a desk 
calculator computer implemented with a cyclic Mp. The cyclic 
memory is 
not apparent from the user™s viewpoint 
because the response is adequate (less than 0.1 sec for simple arithmetic operations). The Programma 101 is discussed in Part 3, Sec. 4, page 235. ZEBRA, a simple binary computer 
The ZEBRA is presented in Chap. 12 and is discussed in Part 3, Sec. 1, page 190. The LGPSO and LGP-21 The LGP-30 
(Chap. 16) is a first-generation, 31-bit computer with an Mp.cyclic and a very simple ISP. The computer appears to be characteristic of small-scale drum computers in the first 
generation. We think of this class of 
computer as having very little power when 
compared, for example, with the IBM 701. However, the power is mostly related to the drum-based tech- 
nology, with 0.26 
- 16.66 millisecond 
access times. The Pilot ACE The NPL 
Pilot ACE is presented 
in Chap. 11. Its relationship in the computer space is discussed in Part 3, Sec. 1, page 190. The UNIVAC system The UNIVAC I is described in Chap. 8. A discussion is given in Part 2, Sec. 1, page 91. The design philosophy 
of Pegasus, a quantityproduction 
computer The Pegasus cyclic memory, general register computer 
(Chap. 9) is discussed in Part 2, Sec. 2, 
page 170. IBM 650 instruction logic The IBM 650 has a 1 + 1 address format and a very complete instruction set. Because of 
the long word length (10 decimal digits) we would consider it to have general utility. The 650™s high performance is achieved 
by using a fast drum (6 
millisec- onds/revolution). The characteristics given in Chap. 17 present the machine as it was first introduced in 1954. Later versions 
provided options for floating 
point arithmetic and 
index regis- 
ters. A 96-word core buffer was also added 
for disk and mag- 
netic-tape buffering. 
The machine 
structure is a simple 1 Pc without concurrent processing 
and input/output transfer abil- ity. Although the 650 has a large word, it initially processed only fixed point integers. NOVA: a list-oriented computer 
The NOVA (Chap. 26) is a specialized 
computer for processing array data. 
It is discussed in Part 4, Sec. 2, page 315. 216 
Chapter 16 The LGP-30 and LGP-21 The LGP-30 is a small computer with 
an Mp.drum. It is distinct from the first (and succeeding) generation computers 
using Mp.random,access and can be described by using 
the PMS dia- gram in Fig. l. The LGP-21, a direct descendant of the LGP-30, having the same ISP, 
is also described by Fig. 
1. Since there is only one address/instruction, a method 
is needed for the optimal allocation 
of operands. Otherwise, each instruction 
might have 
to wait a complete 
drum (or disk) revolution each 
time a data reference is made. The LGP-30 provides for 
operand- location optimization 
by interlacing the logical addresses on 
the drum so that two adjacent addresses (e.g., 
00 and 01) are separated 
by nine physical locations.' These spaces allow for 
operands to be located next to the 
instructions which use them. There 
are 64 tracks, each with 
64 words (sectors). Each 
word is accessed by a track 
address of 6 bits and a 
word address 
of 6 bits. The sequence of words (sectors) 
within a track 
is 00, 57, 50, 43, 36, 
29, 22, 15, 08, 01, 58, 51, 
44, 37, 
. . . 
, 06, 63, 56, 49, 42, 35, 
28, 21, 
14, 07,00. The time between 
two adjacent 
physical words 
is approxi- mately 0.260 millisecond, and the time between two 
adjacent addresses is 9 x 0.260 or 2.340 milliseconds. The actual 
maximum t.access is 16.66 rns2 Half of the instruction (15 bits) is unused. It could be used for 
extra instructions, 
indexing, indirect addressing, or a second (+ 1) address to locate the 
next instruction, all of which increase the preformance. lThe LGP-21 has a space of 18 words. 2The later LGP-21 appears to have a lower performance 
than the LGP-30 by about 
a factor 
of 3. 'LGP-30; technology: (113 vacuum tubes), (I350 diodes); power: 1500 watts: weight: ROO pounds; number produced: 320- 490; t.delivery: September 1956; descendant: 'LGP-21: Pc(l address; 1 instruction/w; data: w,bv,i,fr; Mps(- 2 w); operations: (+.-.x ,/,A,x 2)) Mp(drum; t.cycle: 260 us/w; t.access: (.260 - 16.6) ms; i .rate: 2.34 ms/w contiguous addresses: 4096 w; (31 ,I space) b/w) T(Flexowriter, paper tape) LGP-21 ; technology: (460 transistors), (375 diodes); power: 300 watts; weight: 
90 pounds; number produced:- 150; t.delivery: December 1962; Mp(fixed head disk; cyclic; t.cycle: 400 us/w; t.access: (0 - 52) ms: i.rate: 7.26 ms/w contiguous addresses: 
4096 w: (31.1 space) b/w) T(#1:32; Flexowriter, paper tape, analog, 
CRT, card) , Fig. 1. LGP-30 and LGP-21 PMS diagrams. The ISP, given in Appendix 1 of this chapter, is about the 
most straightforward in 
the book. There are only 16 instructions, and 
the program state is less than two 
words. Although 
the perform- ance is limited because 
of an Mp.cyclic,access, an Mp.ran- dom-access would serve 
to make the ISP fairly similar 
to other faster computers, e.g., an IBM 701. 217 
218 Part 3 1 The instruction-set processor 
level: variations in 
the processor Section 2 
Processors constrained 
by a cyclic, primary memory 
APPENDIX 1 LGP-30 AND LGP-21 ISP DESCRIPTION Appendix 1 LGP-30 and LGP-21 ISP Description pc State Ad): 302 C-48: 23,24 :29; OV Run pc Console State 0P4,8,16,32> TC I Accmtator Propram Counter register Overflow, LCP-21 only on LCP-30 machine stops if an overflow Break Point switches 
Transfer Control switch I 8% State M[O:778~~O:778~<O:30~~ primar,u memory; 212 w; track and sector (word) I( State The following Input Output devices do not have synchronization r'escription 
variables. LCP-21 only. LCP-30 has a Flexowriter. Input-devi ce LO: 31 1<1 : 6> stop code condition signifying input dev.ce has read a special code OutputJevice[O:31 ]<1:6> Instruction Format i<0:30> opaJ:j> := i42:15> td:5> := i<18:23> t'al:4>:= t<l:5> sd:5> := iQ4:29> skip condition := t(t4:3> A Instruction Interpretation 
Process BP) # 0) Run -(i tM[C]; C tC + I; next lnstructiongxecution) Instruction Set and Instruction Execution Process Instruct iongxecut ion 
:= ( 2 (:= op = 0) - ( (t = OOOOOe) - (Run to): skip condition 
-(C tC + 1); ia> +(OV +(OV to; c t~ + 1))); B (:= op = 1) +(A +-M[tl[~l): Y (:= op = 2) + (M[tl[s]<18:29>~-A<18:29>); R (:= op = 3) +(M[t][s1<18:29> CC + 1): I (:= op = 4) + ( 7 iaJ> A (t=62) +(A tA x Z6 [logical)); ia> A (t=62) -(A +A x Z4 [logical]): 7 iaJ> A (tf62) --f (input,b,bit) : id> A (t#62) +(inputhJit)): instruction operation code track select bit on Mp innut-output select, LCP-21 only sector setect hit of W fetch execute stop sense BP and transfer sense overflow and transfer bring from memory store address set return address shifts, and input 
Chapter 16 I The LGP.30 and LGP-21 219 APPENDIX 1 LGP-30 AND LGP-21 ISP 
DESCRIPTION (Continued) 6 inputdubit := (A cA x 2 (logical): next k25:3CD t Input,device[t'l; next hA<o>V stop code) + input,&bit) input,4,bit := (A +Ax Z4 {logical); next A<27:3CD t Input device[t'l<l:b; next hA<O>V stop code) + inputY4,bit) D (:= op = 5) + (0v.A t round(A / MCtl[sl)); N (:= op = 6) + (A +AX M[tl[sl {s.inteqer\): M (:= op = 7) + (A tA x M[tl[sl {s.fraction)); P (:= op = lo8) + ( i4> + (Output,device[t'l<l:6> tA<D:5>): i a> + (Ou tpu t,dev i ce[ t ' ]<I : 6> c A<D : i>OlOO) ) : E (:= op = 118) + (A +A A M[tl[s]); u (:= op = 12) + (C t tos); T (:= op = 13) + (i<D + ((A<CU V TC) + (C c t0s)); Ti<@ + (A<@ -f (C t tOs))); H (:= op = 14) + (M[tl[Sl +A); C (:= op = 15) + (MCtICsl +A; next A to); A (:= op = 16) + (OvoA +A + M[t][sl); 5 (:= op = 17) + (OvoA +A - M[tl[51) ) input processes wait divide multiplly, save right multipl,u, save left print 6 bit print 4 bit extract unconditional transfer trans.fer control 
conditional transfer hold and store clear add subtract end Innstruction,execution 
Chapter 17 10 9 
8 7 6 5 
4 2 3 1 Data Next Instruction OP. Code Address Address 
IBM 650 instruction logic1 0 Sign John W. Cam III The basic IBM 650 is a magnetic drum 
(10,0, 0)2 decimal computer Input-output instructions with one-plus-one address 
instruction logic. It has a storage of 1000 or 2000 10-digit words 
(plus sign) with addresses 0000-0999 or 0000-1999. More extended versions of the equipment have built-in 
floating point arithmetic and 
index accumulators, but the 
basic machine will be described here. 
There are three arithmetic 
regis- ters in addition to the 
standard program register 
and program counter. All information from 
the drum to the 
arithmetic unit passes through a 
signed 10-digit 
distributor. A twenty-digit ac- cumulator is divided into a 
lower and upper part, 
each of 10 digits with sign. Each of these is addressable (distributor 
8001, lower 
accumulator 8002, and upper 
accumulator 8003). Each accumula- 
tor may be cleared to zero separately (in 
IBM 650 terminology, ﬁresetﬂ). The entire 20-digit register 
can be considered as a unit, or each part separately (but affecting the other in 
case of carries). The 10-digit instruction is broken down 
into the 
following form: 
One particular instruction, 
Table Look-Up, allows 
automatic table 
search for one 
particular element in a table, 
which can be stored with a corresponding functional 
value. Input-output is via 80-digit 
numerical punched cards. An ﬁalphabetic deviceﬂ allows limited alphabetical entry on cards. Only 
certain 10-word groups 
on the magnetic drum 
are available for input and output. The 
following information is taken from an IBM 650 manual [Type 650, 
Magnetic Drum Data-Processing 
Machine Manual of Operations]. Much of the input-output is handled via board wiring, which 
is not de- 
scribed in detail below. The two-digit pair represents 
the machine code. The BRD (Branch on Digit) operation 
is used with special 
board wiring to tell 
when certain specific card punches exist. iIn E. M. Grabhe, S. Ramo, and D. E. Wooldridge (eds.), ﬁHandbook of Automation, Computation, and Control,ﬂ vol. 2, chap. 2, pp. 93-98, John Wiley & Sons, Inc., New York, 1959. Carr™s triplet notation for: 
fractional significant digits, digits in exponent, and digits to left of radix point. 70 RD (Read). This operation code 
causes the machine to read cards by a two-step 
process. First, the contents 
of the 10 words of read buffer storage are automatically transferred to one 
of the 20 (or 40) possible 10-word groups 
of read general 
storage. The group selected 
is determined by the D address of the Read instruction. Secondly, a card is moved under the reading brushes, 
and the information read is entered into buffer storage for the next Read instruction. 71 PCH (Punch). This operation code 
causes card punch- 
ing in two steps. First 
the contents 
of one of the 20 (or 40) possible 10-word groups 
of punch storage are transferred to punch buffer storage. The group selected 
is specified by 
the D address of the Punch instruction. Secondly, the card 
is punched with 
the infor- mation from buffer 
storage. 69 LD (Load Distributor). 
This operation code causes 
the contents of the D address location 
of the instruction to be 
placed in the distributor. 24 STD (Store Distributor). 
This operation code 
causes the contents of the distributor with 
the distributor sign to be stored in the location specified by the D address 
of the instruction. The contents of the distributor remain undisturbed. Addition and subtraction instructions 
IO AU (Add to Upper). This operation code 
causes the contents of the D address location to 
be added to 
the contents 
of the upper half of the accumulator. The lower half 
of the ac- cumulator will remain 
unaffected unless the addition causes the sign of the accumulator to change, in which case the contents of the lower half 
of the accumulator will be complemented. Also, the units position of the upper 
half of the accumulator will be reduced by 
one. 15 AL (Add to Lower). This operation code causes 
the contents of the D address location 
to be 
added to the contents 
of the lower half of the accumulator. The contents of the upper half of the accumulator could be 
affected by 
carries. 11 SU (Subtract from Upper). This operation code 
causes the contents of the D address location to 
be subtracted from the 220 
Chapter 17 1 IBM 650 instruction logic 221 contents of the upper half of the accumulator. The contents 
of the lower half of the accumulator will remain unaffected 
unless the subtraction causes a change 
of sign in the accumulator, in which case 
the contents 
of the lower half of the accumulator will be complemented. Also, the units position of the upper half of the accumulator will be reduced by one. 16 SL (Subtract from Lower). This operation code 
causes the contents of the D address location to be 
subtracted from the contents of the lower half 
of the accumulator. The contents of the upper half of the accumulator could 
be affected by carries. 
60 RAU (Reset and Add into Upper). 
This operation code 
resets the entire 
accumulator to plus zero and adds the contents 
of the D address location into the upper half of the accumulator. 65 RAL (Reset and Add into Lower). 
This operation code 
resets the entire 
accumulator to plus zero and adds 
the contents of the D address location into 
the lower half 
of the accumulator. 61 RSU (Reset and Subtract 
into Upper). This operation code resets the entire accumulator to plus zero and subtracts 
the contents of the D address location into the upper half of the accumulator. 66 RSL (Reset and Subtract 
into Lower). 
This operation code resets the entire accumulator to plus zero 
and subtracts 
the contents of the D address location into 
the lower half of the accumulator. Accumulator store instructions 
20 STL (Store Lower in Memory). 
This operation code 
causes the contents of the lower half of the accumulator with 
the accumulator sign to be stored in the location specified by 
the D ad- dress of the instruction. The contents of the lower half of the accumulator remain undisturbed. 
It is important to remember that the 
D address for all 
store instructions must be 0000-1999. An 8000 series 
D address will not be accepted as valid by 
the machine on any of the store instruc- 
tions. 21 STU (Store Upper in Memory). 
This operation code causes the contents of the upper half of the accumulator with the accumulator sign to be stored in 
the location specified by the D address of the instruction. If STU is performed after a 
division operation, and 
before another division, multiplication, or reset operation takes place, the contents 
of the upper accumulator will be stored with 
the sign of the remainder from the divide operation 
(Op-Code 14). The contents of the upper 
half of the accumulator remain undisturbed. 
22 STDA 
(Store Lower Data Address). This operation code causes positions 
8-5 of the distributor to be replaced by the con- tents of the corresponding positions of the lower half 
of the ac- cumulator. The modified word in the distributor with 
the sign of the distributor is then stored in 
the location specified by the D address of the instruction. 23 STIA (Store Lower Instruction Address). This operation code causes positions 
4-1 of the distributor to be 
replaced by the contents of the corresponding positions of the lower half of the accumulator. The modified word in the distributor with the sign of the distributor is then stored in 
the location specified by 
the D address of the instruction. The contents 
of the lower half of the accumulator remain unchanged, and 
the sign of the accumu- lator is not transferred to 
the distributor. The modified word re- 
mains in the distributor upon completion 
of the operation. Absolute value instructions 17 AABL 
(Add Absolute 
to Lower). This operation code causes the contents 
of the D address location to be added 
to the contents of the lower half 
of the accumulator as a positive factor regardless of the actual 
sign. When the 
operation is completed, the distributor will contain the D address factor with its 
actual sign. 67 RAABL (Reset and Add Absolute into Lower). This operation code 
resets the entire 
accumulator to 
zeros and adds 
the contents of the D address location into 
the lower half of the accumulator as a positive factor regardless of its actual sign. When the operation is completed, the distributor will contain the D ad- dress factor with its 
actual sign. 18 SABL (Subtract Absolute 
from Lower). This operation code causes the contents of the D address location to be subtracted from the contents of the lower half 
of the accumulator as a positive factor regardless of the actual sign. Wnen the operation is com- pleted, the 
distributor will contain the D address factor with 
its actual sign. 68 RSABL (Reset and Subtract Absolute into Lower). This operation code resets the entire accumulator to plus zero and 
subtracts the 
contents of the D address location into the 
lower half of the accumulator as a positive factor, regardless of the actual 
sign. When the 
operation is completed, the distributor will contain the D address factor with its 
actual sign. Multiplication and 
division 19 MULT (Multiply). This operation code 
causes the ma- chine to multiply. A 10-digit multiplicand may be multiplied 
by 
222 Part 3 1 The instruction-set processor level: variations in 
the processor a 10-digit multiplier to develop a 
20-digit product. The multiplier must be placed 
in the upper accumulator prior to multiplication. The location of the multiplicand is specified by 
the D address of the instruction. The product is developed in the accumulator beginning in the low-order position of the lower half of the ac- cumulator and extending 
to the 
left into the upper half of the accumulator as required. 14 DIV (Divide). This operation code 
causes the machine to divide without resetting 
the remainder. A 20-digit dividend may be divided by a 10-digit divisor 
to produce a 
10-digit quotient. In order to remain within these limits, 
the absolute value 
of the divisor must 
be greuter than the absolute value of that portion of the dividend that is in the upper half of the accumulator. The entire dividend is placed in 
the 20-position accumulator. The location of the divisor is specified by 
the D address of the divide instruction. 64 DIV RU (Divide and Reset Upper). 
This operation code causes the machine to divide as explained 
under operation code 14 (DIV). However, the upper 
half of the accumulator con- taining the remainder with its 
sign is reset to zeros. Branching instructions (decision operations) 
44 BRNZU (Branch on Non-Zero in Upper). This opera- 
tion code causes 
the contents of the upper half of the accumulator to be 
examined for zero. 
If the contents 
of the upper half of the accumulator is nonzero, the location of the next instruction to 
be executed is specified by the D address. If the contents of the upper 
half of the accumulator is zero, the location of the next instruction to be 
executed is specified by 
the I address. The sign of the ac- cumulator is ignored. 45 BRNZ (Branch on Non-Zero). This operation code 
causes the contents of the entire accumulator to be examined for 
zero. If the contents of the accumulator is nonzero, the location of the next instruction to 
be executed is specified by the D address. If the contents of the accumulator is zero, the location of the next instruction to be 
executed is specified 
by the I address. The sign of the accumulator is ignored. 46 BRMIN (Branch 
on Minus). This operation code 
causes the sign of the accumulator to 
be examined for minus. 
If the sign of the accumulator is minus, the location of the next instruction to be 
executed is specified by 
the D address. If the sign of the accumulator is positive, the location of the next instruction to be executed is specified by the I address. The contents 
of the accu- mulator are ignored. 47 BROV (Branch on Overflow). This operation code 
Section 2 1 Processors constrained 
by a cyclic, primary memory 
causes the overflow circuit to be examined to see whether it has been set. If the overflow circuit is set, the location of the next instruction to be 
executed is specified by the D address. If the overflow circuit is not set, the 
location of the next instruction to 
be executed is specified by the I address. 90-99 BRD 1-10 (Branch on 8 in Distributor Position 
1-10). This operation code 
examines a particular digit position in the distributor for the presence of an 8 or 9. Codes 91-99 test positions 1-9, respectively, of the test word; 
code 90 tests position 
10. If an 8 is present, the location of the next instruction to 
be executed is specified 
by the D address, If a 9 is present, the location of the next instruction to be executed 
is specified by 
the I address. The presence of other than an 8 or 9 will stop the machine. Shift instructions 30 SRT (Shift Right). This operation code 
causes the con- tents of the entire 
accumulator to be shifted right 
the number of places specified by 
the units digit 
of the D address of the shift instruction. A maximum shift of nine positions is possible. A data address with 
units digit 
of zero will result 
in no shift. 
All numbers shifted off the right end of the accumulator are lost. 31 SRD (Shift Round). 
This operation causes the contents of the entire accumulator to 
be shifted right the number of places specified by 
the units digit 
of the D address of the instruction. A 5 is added ( -5 if the accumulator is negative) in 
the twenty-first (blind) position of the amount in 
the accumulator. A data address units digit 
of zero will shift 10 places right 
with rounding. 35 SLT (Shift Left). This operation code 
causes the con- tents of the entire accumulator to be 
shifted left 
the number of places specified 
by the units digit 
of the D address of the instruc- tion. A maximum shift 
of nine positions is possible. A data address with a units digit 
of zero will result 
in no shift. 
All numbers shifted off the left end of the accumulator are lost. However, the overflow circuit will not be 
turned on. 36 SCT (Shift Left 
and Count). This operation code 
causes (1) the contents of the entire accumulator to 
be shifted to 
the left until a nonzero digit 
is in the most significant 
place, (2) a count 
of the number of places shifted 
to be 
inserted in 
the two 
low-order positions of the accumulator. This instruction is to aid fixed-point scaling. Table look-up instructions 
84 TLU (Table Look-up). This operation code 
performs an 
automatic table 
look-up using 
the D address as the location of 
Chapter 17 I IBM 650 instruction logic 
223 the first table argument 
and the I address as the address of the next instruction to be executed. The argument for which a search is to be made must be in the distributor. The address of the table argument equal 
to, or higher than (if no equal 
exists) the argument given is placed in positions 
8-5 of the lower accumulator. The search argument remains, unaltered, in 
the distributor. Miscellaneous instructions 00 No-Op (No Operation). This code 
performs no opera- tion. The data address is bypassed, and the machine automatically 
refers to the location specified by the instruction address 
of the No-Op instruction. 01 Stop. This operation code 
causes the program to stop 
provided the programmed switch on the control console is in the stop position. When the 
programmed switch is in the run position the 01 code will be ignored and treated 
in the same manner as 00 (NO-Op). References Type 650 Magnetic Drum Data-Processing Machine 
Manual of Operations: HughE54; SerrR62. 
Section 3 Processors for variable-length-string data 
Although only 
two computers are 
described in this section, the reader might refer to other computers in the book which handle variable-length strings. The IBM System/360 
processes a string whose length is specified in the instruction. The Burroughs B 5000 has a 
very nice string data ISP (both simple and power- ful). Variable-length strings imply 
some method to specify at in- struction execution time the 
actual length of 
the character strings being processed. Which method is used 
has a 
substan- tial effect on the 
ISP of the resulting machine, and it is note- worthy that a wide 
variety of devices has been tried without 
any apparent consensus yet 
on the 
appropriate mechanism: 
1 An extra bit in each character to mark the string bound- ary (IBM 1401) 2 A special 
terminal character to mark the string 
boundary (IBM 702) 3 A 
field variable in the instruction to specify the string 
length (IBM System/360) 
4 A register 
variable in the processor to specify the string 
length (an 8-bit-character computer-Chap. 10) 5 A fixed number of 
characters at the head of the string 
to specify the length (and 
data type) of the string 
(used extensively for variable-length records on tape and 
disk, though we know of 
no ISP that uses it) The IBM 1401 The 1401 was IBM's most popular computer, measured 
by quantity produced, prior to the 1130/1800 and 
System/360. However, the authors of this book were unable to find any technical papers on its 
design or design philosophy. The 
1401 is based on earlier business-oriented computers 
(Fig. 1, page 225). It evolved a great deal, as can be seen from the number of "features" which 
can be appended to improve it. Successors, the 1440 and 1460, are also 
improvements. It is assumed that early computers mainly influence successor computers within the same organization. An 8-bit-character computer 
An 8-bit-character computer 
(Chap. 10) has been suggested by the authors. It is a very restricted computer 
for processing string data and illustrates another 
approach to string 
defini- tions; the string 
length is specified by a variable 
in the proc. essor. 224 
Chapter 18 The IBM 1401 The second-generation transistor-technology 
IBM 1401 has 
been included both because a 
large number' have 
been produced 
and because it 
differs from common 
fixed word length binary and deci- 
mal computers. IBM 1401s are used in business data-processing 
applications requiring variable-length 
character strings or fields and rather 
limited calculating ability. 
Two specific applications are as a card processor in making a transition 
from plugboard programmed calculators to 
full-scale automatic computations 
and for converting data from one medium 
to another, for example, from 
card to tape. The 1401 was little used by 
the scientific, engineer- ing, and 
scientific business 
data-processing communities, probably 
because of the limited Mp size, 
the low overall processing 
speed, and the 
lack of concurrent 1/0 operation in 
the smaller configura- 
tions. However, it 
did achieve considerable 
use as a stand-alone 
Cio in 
C('7090) installations, perhaps because 
of the speed and quality of the T('1403; line; printer). Although undoubtedly influenced by 
machines outside the IBM organization, the IBM 1401 is derived primarily 
from the IBM 702 and 705, which are variable word 
length decimal 
machines. The relationship of the various IBM decimal computers 
to one another is shown in Fig. 1. (RCA's early computers2 
also use 
a combination 
of fixed-length and variable-length 7-bit character strings and may have influenced the 1401.) The IBM 1401's ISP was the first to be 
adopted by another company. Honeywell defined 
its H-200 ISP to be 
a superset 
of the IBM 1401 ISP. The ISP of the H-200 is more complex 
and increases performance by organizing Mp 
by both characters and words. The IBM 1401, 1440, 
and 1460 are the 
only IBM computers to be completely character-string oriented. 
That is, both instruc- tions and data are 
stored in variable-length character strings; these 
strings are addressed by a pointer register to the string. The ad- dress integer is fixed 
at three characters. The encoding process for addresses 
is given in Appendix 1 of this chapter. The 3-char- acter address (3 x 6 bits) 
is assigned as 
3 x 4 bcd characters for encoding addresses 0:999; 
2 x 2 bits for selecting 16 
x 1,000 addresses; and 2 bits for selecting one 
of the three index registers. 
The IBM 1620 processes 
variable-length data strings, although 'Up to 1966, more 1401s were produced than any other model. An esti- mated 7,500 1401s, 1,500 1401 
G's (card-only system), 
3,600 144Os, and 1,500 1460s were produced. 
About 1,800 
1620s were produced. 
2RCA 301, 501, and 601. the instruction length 
is a fixed 12-digit string corresponding 
to a word in Mp. The 1620, though not identical 
to the 
1401, is almost a member 
of the same family. 
The 1401 evolved. 
Figure 1 shows the evolution of "features" which have 
created new computers. 
The 1401's optional features 
are mainly design 
afterthoughts; they 
sometimes increase perform- ance, sometimes make 
certain operations possible, and sometimes provide substantive change. 
There are 
approximately 19 
features in the 1401: memory expansion 
beyond the anticipated 4,000 characters and 
index registers 
required encoding 
the field bits of the A and B addresses; store A-Address and store 
B-Address register f I i Fixed - lengih instructon, variable - character string doto 1 (Honeywell H-200) I i I 7070 7074 7072 , +-tt I , +-+'+ 1 1620 1710 1620 Ill I 702 705 705 111 , 7080 Core, ++ vocwk tubes 1 -+ vt,drum, XR,dish,mognetiC tope Fixed ~ length instruction, 
flied  length doto 603 608,6101 ti+ {L Technalogy'vocuum felectromeCh~n#C(Il, tube/st, t tt Plugboard ond Punched- 
cpc 607 604 C(I1CYlatOrS card progrommed MICZOO digit) tlrr! ge?ero?Io" . , , 
, , . 1 , C('Honeyse1i H-ZOO, data W,Chor Hrlng, 2 ps/chor,1401 COmDotible) Cl'1410;10zl80 hChar,4,5pr/chor, 
MPO 115 x5ehorl;1401 ComDotlble) Cl'7OlO; 4O*UlOO Ichor, 1 2 pilchor, doto W,Choi.rtring, 1401,1410 compohblc) C('l403, 4-16 kchor;ll.Spis/chor, 8 blehor.2 addrerr,Mpsb8 char); Cl'7070; 6ps/r,5%10 hw,ilO,1signldlw,5bld; I (Iddre4S1,nstruction;Mpr 199'XRlI C1'7074; 6ps/w;5% 30 hw), Cl7072, 4pi/w, 5% 30 hw) CC1620; 20-60 kohor, 6 blchor, 20pi/chor;(2 chor/inStruCtlOn; storogem rtoroge mrfrocrms), cI't460,6p0/ch~rl; CI'1440; l1.tp$/chm) Z Oddie.i/lnltrYCfion); C1'1620 
111; inferrupf copob~l~tyl 1 addrerslinstruction; MpsI512 
char11 Cl'702, 20%60 kchor. 23ps//char, 6 blchar; 5 Char/instruCtion, Ci'650; drum , 12.4 hn, (I t1 ) address /#nrtruction ; (10 t1 sign) d/w ; 5 b/d) Fig. 1. IBM decimal and character-string computer relationships. 
225 
226 Part 3 I The instruction-set processor level: variations in 
the processor instructions are necessary for subroutines-the Store Address 
Regis- ter Feature; 
Indexing Feature; Multiply-Divide Feature; High- Low-Equal Compare Feature; 
Read Release 
and Punch 
Release Feature; the 
Column Binary Feature; Early-Card-Read Feature; Processing Overlap Feature, etc. 
PMS structure The 1401 PMS structure (Fig. 2) is an early 1 Pc structure. The 
diagram does not show 
the S(fixed) Pc interconnection 
structure with the Ms and T. 
The Pc-(MslT) 
interconnection restricts 
the concurrency of T and 
Ms. The optional processing overlap feature provides a link to Mp to 
allow the T(card; read, punch) 
to be run concurrently with Pc processing. When any of the peripheral devices are operating without the processing overlap feature, 
the Pc is dedicated to be a 
data transmission link 
or K (as in earlier computers). The device K is connected directly to Pc. For example, Ms(disk, magnetic tape) data 
transfers use the main registers 
of the Pc and can tie it up 
full time during 
data transmission. By careful programming, several devices 
can be synchronized and thus run concurrently 
for communicating with Pc 
from a K. The Pc does not have an 
interrupt system. Thus the peripherals have 
no way of communicating with Pc. 
Subsequent models, 
the 1440 and 1460, added interrupt 
capability and made it 
easier to control 
multiple simultaneous data transfers among 
the peripheral K's and Pc. T.consol* I Ms('l405; disk) 
Mp2- P!'T('1402; card; reader,punch)- T('1403 1'1404; line; printer)+ T('1407 Console Inquiry Station; typewriter)- 
T (paner tape; reader)+ 
Ms(#l :6; magnetic tape)- 'Pc(string; 1 - 8 char/instruction; M.processor state (7- 16 char); technology; vacuum tubes; 1960-1965; descendants:1440, 1460) 'Mp(core; 11.5 ps/char; 4000 - 16000 char; (7,l parity) b/char) Fig. 2. IBM 1401 PMS diagram. Section 3 I Processors for variable-length-string data 
ISP structure The IBM 1401 ISP is given in Appendix 1 of this chapter. Instruc- tion strings and data strings are delimited by the special F bit in a 
character. A character in Mp 
is of the form1 C(check,F,B',A', 8, 4, 2, 1) An n-character string is C[O], C[1], . . . 
C[n - 11 and would be stored in 
Mp[j:j + n - 11 The first character (or head) of an instruction must contain the word-mark flag 
or F bit. The head 
.of the instruction, which is to be interpreted next, is held at Mp[IJ, and. succeeding characters 
of the instruction are 
at Mp[I + 11, Mp[I + 21, etc. Correctly defined instructions are 1, 2, 4, 
5, 7, and 8 characters long. Un- defined instruction lengths 
of up to 
8 characters are 
also inter- preted without an error condition. 
The interpretation 
algorithm presented in 
the ISP description does not explain the action of instructions which have 
an incorrect length. 
Actually, the 1401 Reference Manual 
does not go into details of general instruction 
interpretation but 
dwells on "correct" operation. Table 1 presents the correct instruction lengths and 
formats. If we take 
the instruc- tions in 
the table, the set is not variable in length 
but is fixed 
at these six sizes. The instruction set 
(not including the input/output instructions) is presented in 
Table 2. This 
table also provides 
a hint of the implementation, since 
the execution times 
are given in terms 
of memory cycles. 
The ISP state, unlike that of more conventional processors, has 
no temporary operand storage 
(e.g., accumulators). The ISP state has registers which 
point to operands. 
The state 
of the machine (see Appendix 1) is basically: Mp, 
the Instruction Location 
Counter, Indicators or miscellaneous 
bits, three 3-character blocks of Mp reserved for Index registers, 
and the two registers A-address and B-address which point to 
data operands. Instruction interpretation 
There are three 
principal state types in processing an instruction: o.q., when the instruction is being formed; o.v., when the operands are being accessed or the results are being stored in Mp; 
and 0, when the operation specified by the instruction is being carried out. Each state transition corresponds essentially 
to a memory access. The three 
instruction types of Fig. 3 each have 
their own particular states. 
Only types 
1 and 2 process the variable-length 'See Appendix 
1 of this chapter for the meaning of the bits in a character. We have renamed the A arid B bits A' and B' to avoid confusion 
with the registers. 
Chapter 18 I The IBM 1401 227 Table 1 IBM 1401 instruction formats 
1 C[OI no-op, halt, 
or single character to specify a chained instruction 
2 Wl (311 the d-character is used to specify addi- tional instruction information (e.g., 
select, card stacker) 4 C[OI C[1, 2, 31 unconditional branch instruction 
or sin- gle address arithmetic; M[A] t f(M[A]) 5 C[OI C[1, 2, 31 C[41 conditional branch 
instruction; C[4] se- lects a specific test 7 C[Ol C[1, 2, 31 C[4. 5, 61 two address instruction; M[B] t M[B] b M[A]; (e.g., add, 
sub- tract) 8 C[Ol C[1, 2, 31 C[4, 5, 61 WI conditional branch based 
on Mp[B] char- acter; d-character is test character; 
(e.g., branch if character equal) 
Function of instruction characters: 
C[O] op code: 
always contains a word-mark flag 
or F bit. C[1, 2, 31 = branch address for 1-Address register or first operand 
address for the A-Address register. C[1] or 
C[4] or C[7] d-character; used as a single character for additional operation code information 
or a character 
for comparison, or to select a test. C[4, 5, 61 primary operand (B-Address register specification). 
character strings, { charstring}, and the state 
diagram accounts for strings on a 
character-at-a-time basis. For an add instruction Fig. 3 oversimplifies the execution because 
it implies that each character of the A and B operand is accessed, the addition is per- formed, and the result is restored according 
to the B-address register. A more complex description must account for A 
and B strings of unequal length, 
and the 
case of getting a number which must be recomplemented because it is the wrong sign. The re- complementation process requires a reverse scan to find the end 
of the B string and then a forward scan 
to recomplement each character of B. Figure 4 is a detailed state 
diagram of the add execution process. The states in 
the ISP description (Appendix 
1) within the in- struction-interpretation process correspond to the three state types just described: the single-instruction character-fetch 
operation, the fetch-operand-addresses for the remainder of the instruction, and Instruction-execution. Instruction-execution 
is not given in 
any detail. For 
example, the execution of add is defined as ﬁAﬂ(:= op = 110001) + OvOM[B] c M[B] + M[A] {charstring};. The state diagram (Fig. 
4) presents this execution 
in detail. 
Note that in the ISP description we 
omit telling the reader that the A and B address registers point to the next lowest variable-length 
string in M after an operation is performed. We 
allow the definition of a variable-string operation, 
for example, + { charstring}, to imply the action on the processor state. Some instructions can be defined with a single 
character, and these are called chained instructions. Chained instructions take 
the previous values 
of the pointer registers, the A and B address registers, as the operand addresses. The add 
instruction, for exam- ple, can be either 1 (chained), 4, or 7 characters; the forms of all instructions appear in Table 1. 
The 4-character add instruction places the A address field in both 
the A and B address registers; thus the effect is an instruction to double a string (add it to 
itself). Data An n-decimal-digit numeric data string is represented as C[n - 11, C[n - 21,. . . , C[l], C[0], C[M] - The underlined characters, C[n 
- 11 and C[M], have the flag bit present, that is, (C[n - 1](F) = 1) and (C[M](F) = 1). The n characters are stored in locations Mp[ jl, Mp[ j + 11, . . . 
, Mp[j + - 
228 Part 3 I The instruction-set processor level: variations 
in the processor Table 2 IBM 1401 instruction set (excluding input, 
output) Section 3 I Processors for variable-length-string 
data Instruction OP Execution time Codet in memory cyclest Length Du tu (char.) tY Pe Add (no recornplernent) A LI + 3 + LA + LB 1, 4, 7 Add (recomplement) A LI + 3 + LA + 4LB 1, 4, 7 Branch B LI + 1 4 Branch if 
Bit Equals W LI + 2 8 Branch if Character Equal 
B LI + 2 8 
Branch if Indicator On B LI + 1 5 Branch if Word Mark and/or 
Zone V LI + 2 8 Clear Word 
Mark M LI + 3 1, 4, 7 Compare C LI + 1 + LA + LB 1, 7 Divide (aver.)§ % LI f 2 + 7LRLQ + ~LQ 7 Halt LI + 1 1 Load Characters to A Word Mark L L, + 1 + 2LA 4, 7 Modify Address5 j# L, + 9 4, 7 Move Characters 
and Edit 
E LI + 1 + LA + LB + L, 7 Move Characters 
to Record or Word Marks P LI + 1 + 2LA 7 Move Characters 
and Suppress Zeros Z LI f 1 + 3LA 7 Move and Insert Zeros§ X LI + 1 + 2zLA + ZLz 7 Move Numeric D LI + 3 1, 7 Move Zone Y LI + 3 1, 7 Mu It i ply (aver.)§ No operation N 
Lr + 1 1 
Set Word Mark LI + 3 4, 7 Store A-Address Registers 
Q LI + 5 4 
Store B-Address Registers 
H LI + 4 4 
Subtract (no recomplement) S LI + 3 + LA + LB 1, 4, 7 Subtract (recornplernent) S LI + 3 + LA + 4LB 1, 4, 7 Zero and 
Add ? LI + 1 + LA + LB 1, 4, 7 Zero and 
Subtract ! LI + 1 + LA + LB 1,4, 7 Clear Storage 
/ LI + 1 + Lx 1,4, 7 Move Characters 
to A or 
B Word Mark M LI + 1 + 2Lw 4, 7 @ LI + 3 + 2Lc + 5LCLM + 7LM 7 ?Alphanumeric code used to specify instruction. $M(t.cycle: 11.5 ps/char) §Optional-feature instructions. 
Abbreviations for symbols used 
in timing: La = length of the A-field (in 
characters) LB = length of the 
6-field Lc = length of multiplicand field 
L, = length of instruction LM = length of multiplier field 
L, = length of quotient field 
LR = length of divisor field Ls = number of significant digits 
in divisor (excludes highorder 
Os and blanks) 
Lw = length of A- or B-field. whichever is shorter Lx = number of characters 
to be cleaned Ly = number of characters back 
to rightmost 0 in control field 
Lz Z = number of fields included 
in an operation 
number of Os inserted in a field 
char. string char. string 3 char 1, 3 char 1, 3 char 1, 3 char 1, 3 char char. string 1 char char. string char. string char. string 3 char char. string char. string char. string char. string char. string 1 char 1 char char. string 1 char 3 char 3 char char. string char. string char. string char. string 
Chapter 18 I The IBM 1401 229 character for q Operotion complete Type 1. Type 2: MCBl+f (MCAl,MCBl.~chor string}) MCBl~f(MCA1,Cchor. string}) NOTE' The 
time in each state is roughly 1 memory cycle 
q The instruction q 0.q Operation and memory access 
to determine instruction q, a correct length instruction = 1. 2,4.5,7, and 8 characters OY Operation and memory access 
fetches to determine an operand 
0 Operation specified 
in the instruction q, requires no time 0.v' Operand and memory access stores to restore result operand Fig. 3. IBM 1401 instruction-interpretation state diagram. 
n - 11. The values of the string are based 
on the bcd 
value of the 8, 4, 2, 1 bits of each digit. 
The magnitude of the integer is C[n - 11 x 10n-1 + C[n - 21 x IOn-' + . . 
. + C[O] x 10" and the 
sign is Sign := ((lC[O](A') A C[O](B'))+ -; l(lC[OI(A') A C[~I(B')) + +) A string is addressed (or accessed) via 
the A-address or B-ad- dress pointer registers. These 
point to the tail (or least significant digit), that is, C[0], of the string. The instruction-execution state diagram of a variable-string 
add is shown in Fig. 4. The state 
diagram assumes that A and B address 
registers are set up accord- ing to Fig. 3. Thus Fig. 4 is a more detailed description of states o.v, o.v, 0, and 0.v'. Each horizontal pair 
of states (Fig. 
4) corre- sponds to a single scan 
of the states of type 1 instruction o.v, o.v, 
0, 0.v' in 
Fig. 3. Transition: among 
states 2 and 3 correspond to the 
character-by-character scan with string A and B being 
added together; the result string is placed in 
B. States 4 and 5 define the string addition, 
when string A is terminated; i.e., it is con- sidered to be zero. States 
7, 8, 9, and 10 define the recomple- mentation process in which the B string 
has to be 
recomplemented. This condition occurs when 
the operand signs differ, 
and the 
A-field result is greater than 
the B field; the results are in ten's 
complement form. States 7 and 8 define the B-field scan (to return to find the least digit 
of B), and states 
9 and 10 define the recom- plementation of each character. Thus an add operation may re- quire up to three scans of the B string. 
The 1401 ISP (Appendix 1 of this chapter) has four parts: 
State Declaration, Instruction-interpretation 
process, Instruction-exe- 
cution process, and Operand 
address-register calculation proc- 
ess. The Operand 
address-register calculation process is analogous to the Effective-address calculation in 
more conventional Pc's and is the most elaborate part 
of the instruction interpretation. The operand address registers 
A-address and B-address are part of the Pc state and must be retained between 
instructions. At the end of an instruction, these registers 
point to the 
character of the next lowest data string in 
Mp, that is, the character 
at C[n]. Implementation The 1401 has a small Pc state, and there are 
only a few registers 
in the implementations. Figure 
5 shows the registers, interregister transfer paths, and 
data operations that make up the 
register- Initiol stote; operand oddressee in AuAddress and BuAddress registers 
pointing to A and B str8ngs COrry,M[E]-M[B] +M[A] t mry, chor string addition 
A string has terminated Not recomp-, M [E+l]<F>-. B string hor terminated 8 Go to head Result string. 
B. and must be '1 -7 !of E stmg 1 M[EI<Ft-r(B+B-I) has wrong slgn Fig. 4. IBM 1401 add-instruction-execution state diagram. 

230 Part 3 I The instruction-set processor level: variations in 
the processor L Section 3 I Processors for variable-length-string data 
11 INHIBIT MIlVE ADDER STORAGE LOGIC I. I I 4 4 
4- I f t 1 f * 4- - I A A - AUX B B - AUX 0 ADDRESS ADDRESS ADDRESS ADMIESS ADDRESS ADDRESS 1 1 ADDRESS ~~~~~ OP MODIFIER REG DECODE f- 1 f f f t I A A 
- AUX ADDRESS ADDRESS ADDRESS 
OP MODIFIER DECODE 
1401 PROCESS OVERLAP 
Fig. 5. IBM 1401 system data 
flow (registers structure). (Courtesy 
of International Business Machines Corporation.) c transfer level primitives 
of the complete computer 
together with several options. 
The options, of course, increase 
the complexity (and concurrency). Without the overlap feature, for example, 
all data are 
accessed in Mp via Pc's address registers. 
There are 
register pairs consisting of a 3-character 
memory address (access) 
register, and 
a 1-character data register. The memory-address, memory-data register pairs 
are A-address, A-data; B-address, B-data; 
1-address, Operation/Op; Overlap- ,address, Overlap,data/O. 
The implementation is straightforward, and the instruction times (Table 2) show the implementation at the 
register-transfer level. For example, as an instruction is being read by Pc, prior 
to instruction 
execution, each new 
character is taken in and 
ex- amined for the instruction-terminating flag bit. When the 
flag bit is present, the instruction is complete and ready to be 
executed. The character of the next instruction is not saved but is picked up again after the previous instruction has been executed. 

Chapter 18 I The IBM 1401 231 APPENDIX 1 IBM 1401 ISP DESCRIPTION Appendix 1 tBM 1401 ISP Description The following description is a highly simplified description of the IBM 1401. line corresponds to a three page description in the Reference Manual for the 1401. tions which transfer character strings to fixed blocks of primary memory. ch.string/ch.s. B strings at the end of the operations; this aspect of the 
operation is not described--but implied 
in the string operations. Pc, Pc Console, and IO Device Control States For example, the edit instruction given below in onr It does not include the input-output inStrUc- The character strings are denoted as character.string/ For the character.string 
operations the 
A,address/A and 
B,rrddress/B registers contain a pointer to the next 
A and I [1:3]4',A',8.4.2,1> Laddress register, 
the instruction location pointer 
Laddress register 
Laddress register 
String Data pointer registers A and B point to the least significant 
digit end of a variable length 
string in memory (see Mp State definition below). Ech.6) 0perations.B is normally the result string, and the length is defined by a word mark, F, the last character of the 
B string. a pointer to the most significant digit of the 
instruction. has two additional bits check, and field. The bits of Mp are: Normally A and B are decreased by one and move 
to the more significant end for varinble length 
string If A string has a word mark, and is shorter than the B string, then the remaining A string is taken to be a zero. I is Although Pc register characters have the B',A',8,4,2,1 bits, the M Check/Parity,bit. W~/Word~ark/F/Fiel~bit. digit (the last digit) of a variable length numeric integer string. 
B',A',8,4,2,1 bits. as a bed digit. (A' = 0) A (B' = I). The sum (modulo 2) +I, of the 
F,B',A',8,4,2,1, bits. This bit defines the beginning of each instruction. The F bit also defines the most significant If numeric data is represented, the 8,4,2,1 bits are used A 6 bit character is encoded in these bits. The sign is encoded with the least significant 
digit. For numeric data, a minus sign, -, is encoded b,u All other combinations of A',B' represent a plus sign, f. XR [1 :31 [I :3la1,A;8,4.2,l>i= M[87;89,92;94,97~99]6',At,8.4.2,1> 3 three character optional inder registers stored in Mp I nd i ca tors (0 : 631 There are a set of 31 status bits of the possible 
64. are used by external ?c status or I/O status. The ?e indicators assignment to Pc State is: logical bit array encoding F% State (not including I,A, and Bl They can be cleare? or set under instruction control. Some Indicators The indicators can be selected for testing bu the d character of an instruction. Unconditional := 1 alqus a 1 Sensegwi tchd, B,C ,D, E, F,G> Unequa I-compare B#A a set of 7 covrsole keus Equa1,compare R-A Low-compare Hi ghdompare R< A R> A Overflow set b,g arithmetic overflow, cleared 
by a 
branch instruction if it is set The indicator array is partially encoded below. Indicator [OOOOOO] := Unconditional Indicator [I IOOOI] := Sense-swi tch<A> 
I nd i ca tor (0 IO00 I] := Unequa1,compare Indicator [011001] := Overflow Mp State M[D:15999]<Check,F,B',A',8,4,2,1> address[X[l :31<B' ,A' ,8,4,2,1>1<1:5>,, := ( primaru memoru Address encoding 
for 1 0.f 16000 from a 3 char value of repis- ter X. Ivdexing described 
below. 
232 Part 3 I The instruction-set processor level: variations 
in the processor APPENDIX 1 IBM 1401 ISP DESCRIPTION (Continued) 
Section 3 1 Processors for 
variable-length-string data X[?]<B' ,A'> x 4000,, t X[l]<B1 ,A'> x IOOO,, + x [I :?1<R ,4,7, I>(bcd. s tri nq) ) Instruction Format op6,Bt,A',8,4,2.1> instruction register speci,fyivq the operation dLhar4, B',A',8,4,2,1> d-char-present additional character usi~d tn eome instructions indicates a djhar is used in the current 
instruction active A-address-present Badd res sup re sent indicates an instructiori strinp is still being .fetched indicates there is an A address nart of an instruction indicates there is a R (Iddress part of an instruction Vove, load, and store instruction 
types control the 
initialization of A and E. mdve or load or store 
A or B/mls := ((move characters and edit = opl v (load characters 
to A word mark = opl v (move characters to A or B idords mark 
= op) v (move characters and suppress zeros = op) V (move numerical = OD) V (move zone = opl V (store A address register = opl v (store R address register = opll lnstruction Internretation 
Process Run + (op c M [I] : I I I + I ; next Fe t ch-ope rand,a dd res ses ; next Instruct i OnLexecut ion) ,fetch operation fetch addresses ,for 
A and R execute Address Calculation Process The 1401 calculates explicit effective 
addresses by first setting up the A, and R address riigisters. in Instruction~xerution. (respectively): no char, d char, the I or A address, the I or A address and d char, the A and B address, and the I GP A address and E address and d char. Operands are not 
fetched There are 1,2,4,5,7 and 8 character instructions which have the op and the ,following operands The folloiuinp process defines the operation ,for 
correct lenpth irstmctions. Fetch-operand-addresses := ( d~har-present + 0: M[I]<D + (active 0); 1 char instruction --Y[I]<D + (active t I; rnls + B +o); next proceed to pet an T or il adr7ress active + (d,char get-char; next A[l] d-char; I or A address set un or d-char d,char,,present t I ; next ~m1s + (@,[I] +A[I])): next active + (A[2] get-char; next mls + B[2] A[2] ): next active + (A[?] + get-char; next A[?] ): next active + (Adddressupresent '-1); record alhether I or A address is present mls -f E[?] ~ active -> (A-address-present -0); next A-address-present + (d,char,present t 0: add index register to I or A (A[2]<B',A'> # 0) + (A <-A + XR[A[2]<B',A1? I?.chl)); 7 M[I)<R + (B 0): next F ad?ress set up or d-char active + (d-char t get-char: next B[l] t d-char: d-char-present t 1) ; active + (B[2] +get-char); next 
active + (B[?] +get-char): next active + (B,address,present t I); active + (Baddressupresent t 0); next B,address,present --f ( record 1,ihether R addres:: is oresent add index wgister to B d-char-present t 0; (El[2]<B1,A'> # 0) + (B cB + XR[B[Z]<B',A'>]( 3.ch.J)); 
Chapter 18 I The IBM 1401 233 APPENDIX 1 IBM 1401 ISP DESCRIPTION (Continued) 
(7 M [1]6> A active) 4 (dLhar t get,char; (7 M[l]<h A active) + Run to; f{nal hchar d,char,present t 1) ; next halt if more than 8 char instruction ) end Fetch,overanhaddresses get character: A sub-process used 
to fetch each new character in the instruction. 
T,f F is ,found in a charazter, ?he process terminates. qet,char<B1,A',8,4,2,1> := ( 7 M[l]<F> A active --t (M[I]; I +I + I): M[I]<R +active eo); value is present character no value, terminate 
rnstruction Set and Instruction Ezecution Process lnstruction~xec~tion := ( character string/ch.s movement and 
clear memory: move characters to P or B biord ma?k - character string ich.si moue characters and sutmress zero.$ "M" (:= op = IOOIOO) + (M[B] cM[A] "Z" (:= op = OllOOl) + (M[B] cM[A] [ch.s]); (ch.z.1; next M[B] cf(M[B]) (ch.s)); "L" (:= op IOOOll) + (M[B] cM[A] (ch.51); load characters to A word mnrk 
"E" (:= op = IlOlOl) --t (M[B] cf(M[A],M[B], {ch.s\)); moue characters and edit This instruction moues the A field string to the B ,field string under control of an edit character stm'ng in the original R field. "/" (:= op = OlOOOl) --f (M[B] c0 (ch.s.mod.1001; -. Bdddress,present --f : B,address,present + I -A): character string, {ch.sI, 
arithmetic: "A" (:= op = 110001) + (Ov,M[B] +M[B] + M[A] (ch.sl) ".j" (:= op = 010010) + (Ov,M[B] cM[Bl - MIA] {ch.s)) ,1111 . (:= op = 101010) + (M[B] e0 - M [A] (ch.s)) "?" (:= op = ll1010) +(M[B] e0 + M[A] (ch.5)) I, I, (:= op = 001100) + (0v,M[E] -M[B] x M[A] {ch.~]); "%" (:= op = OlIlOO) + (Ov,M[E] tM[B] / M[A] Ich.5)); "#" (:= op = OOlOll) + (M[B] eM[B] + M[A] (3.chl; B cB - 3: A +A - 3); branches, halt, no-operation: "N" (:= op = 100101)+ ; *'." (:= op = lllOll)+ (Run eo; Laddressupresent + ; A,address,present + I *A); ,IB" (:= op = 110010) + ( (l B,address,present (1 B,address,present A d,char,present) + I c A; d,char,present) + ( Indicator [f(d,char)] -(I +A); I nd i cator [f (d,char) 1 t- 0) ; (Bgddresepresent A d-char-present) + ( B cB - 1; (M [E] = d,char) + I t A)!: clear storage, ipnores 
the 100 address clear storage clear storage ant'bnanch 
mark and 
moves to next modulo add subtract zero and subtract zero and add multiply; full length p,roduct in U [B], special harduare ODtion divide; auotient and remainder both end UD in M[Bl. vac'ifi, address 
no ooeration halt halt and branch branch branch i.f inc'icator on branch if char eaual 

234 Part 3 1 The instruction-set processor level: variations in 
the processor APPENDIX 1 IBM 1401 ISP DESCRIPTION 
(Continued) Section 3 I Processors for variable-length-string data 
I,"" (:= op = 010101) + fB CB - 1; M[B]d(d,char)> + (I +A)); 'IC" (:= op = IlOOll) + ( Indicators tM[A] 
= M[B] [ch.s)): subroutine calling: "Q" (:= op = 101000) -+ ( M[A - Z:A] ,A[1:3]; A 
+A - 3); "H" (:= op = 11 1000) 4 ( M[A - 2:A] +B[1:3]; A ,A - 3); single character operations 
"," (:= op = OllOll) --f (M[A]<Dcl; M[B]<Pcl; A +A - 1; B tB - I); Ig,, (:= op = ll1100) -+ (M[A]<F> to: M[R]<F> to; A cA - 1; B tB - I); "0" (:= op = 110100) + (M[B]G3,4,2,1> tM[A]d,4,2,1>; A +A - 1: B tB - 1); "Y" (:= op = 011000) + (M[B]<B',A'> tM[A]<Bl,A'>; A tA - 1; B tB - I); ) branch if uod mark and/or zona compare store A address register store B address register set word mark clear word mark move nwnerical move zone end Instruction,-execution 
Section 4 Desk calculator computers: 
keyboard programmable processors with small memories These stored program computers 
have interesting features. For example, the keyboard is utilized 
several ways: 
1 T.console mode; 
a conventional 
console for entering 
data in response to a stored 
program Program entry mode; a device for creating stored pro- grams Desk calculator mode; a part of the arithmetic 
(data) element by issuing direct instructions and thus 
obtaining results directly independent 
of a program 2 3 Uses 2 and 3 are both internally and externally programmed. 
The data types 
are decimal (both fixed and floating) 
because of the intimate 
interface they require 
to the user. Some 
calcu- lators interpret 
nested (parenthesized) algebraic expressions. 
These calculators easily meet the definition for a stored- 
program computer. It is apparent 
their designers know a great 
deal about general 
purpose stored-program computers. The machines are cleverly designed and make efficient use of the hardware they possess. Eventually there 
may be more 
of these 
computers than conventional stored 
program computers. The reader should 
note that not 
all ﬁelectronic desk calculatorsﬂ are computers; 
most are electronic versions of 
their mechanical 
or electromechanical ancestors. 
The OLlVETTl UNDERWOOD PROGRAMMA 
101 desk calculator The Programma 101 (Chap. 19) is at the limit 
of what we call a stored program computer. It has a sufficient 
instruction set to be classified as a computer, but the 
storage for temporary data, constants, 
and programs 
is limited. The machine™s in- struction set is interesting because memory is not addressed explicitly. A jump, for example, is executed by scanning the program for 
a particular marker 
which was named in the jump 
instruction. The Programma 101 uses an Mp.cyclic. The program library for the Programma 101 is extensive and provides an indication of 
its capability. The Hewlett-Packard Model 
9100A computing calculator The HP 9100A (Chap. 20), like the Programma 101 (Chap. 19), 
is a desk calculator. They are both stored program computers. Programma is 
designed for simpler accounting and statistical- 
tabulation tasks and has fixed-point decimal data. (Programma 
101 costs somewhat 
less.) The 
HP9100A operates on both fixed- and floating-point 
decimal data 
with scalar, rectangular, and polar coordinate 
vectors and is designed for engineering and scientific calculations. Thus, according 
to a measure 
based on data types and operators, the HP 9100A is about 
the most complete computer 
in the book. Its operations are given 
in the 
PMS diagram of 
Fig. 1. Mp(read,write; core; 368 w; 6 b/w) T . consol e (keyboard) c T.console(CRT; display; numeric; decimal; 
mixed, floating)+ ; data:(scalar, rectangular co-ordinate 
vector, polar co- -1 ordinate vector); fixed, floating; 
decimal; operations:(+, 
-, x, /, cos, sin, tan, sin-‚, COS tanh, sinh-l, cosh-I, tanh-I, In, log,,, abs, e, sqrt, integer part,{rectangular 
co-ordinate vector) c {polar co- ordinate vector), {polar 
co-ordinate vector] c {rectangular co-ordinate vector)) t , tan-‚, sinh, cosh, c 6 b/program&tep 3 -T.numer I i cgri nter+ -T. p 
I ot ter-, -L.external device 
- LT-M magnetic card; 2 programs; 196 program&teps/prograrn; - i. !mi croprogramrned (H. processor state (40 b)) 1 ™Pc := Mp(read only; 512 w, 64 b/w) ‚P.microprogrammed := P.rnicroprogrammed I Mp(contro1; read only; 800 ns/w; 64 w; 29 b/w) Fig. 1. Hewlett-Packard Model 
9100A Computing Calculator PMS diagram. 235 
236 Part 3 I The instruction-set processor level: variations in 
the processor The implementation has approximately 36.2 
kb of memory, including the read-only and read-write 
parts. The design 
is physically outstanding, and its 
use of microprogramming 
is superb. The reader 
should note there are two levels of M(read only). We could draw 
the PMS structure of Pc as a P.micro- 
programmed within a P.microprogrammed. 
HP rightfully re- gards the two 
ISP's (29-bit and 64-bit word) a.s proprietary and 
carefully avoids discussing these points in the 
article (Chap. 20). It might be noted that an IBM System/360 
Model 30 requires about 2.9 milliseconds for a floating-point square root, whereas the HP 9100A 
requires 19 milliseconds. By way of evidence 
of its outstanding packaging, its cost is about five-eighths 
that of a PDP-8/1 
for about the same amount of 
physical hardware. The cost difference, 
though trulydifficult to compare, is partially 
the result of a design from an instrument maker (Hewlett- 
Packard) versus a design from a computer 
manufacturer (DEC). The TV-like construction 
of the HP 9100A is an important les- son that computer manufacturers 
have not learned. In other Section 4 I Desk calculator computers: keyboard processors with small memories 
words, a Henry 
Ford has yet to emerge from the computer field. (Our guess is that he may come from Japan.) Whereas many computers 
in this book are included because they are 
typical of points 
in the computer space, the HP 9100A is included because it is innovative. It is worthy of note that 
only one of the engineers had some computer design experi- 
ence; Cochran, who did the programming, had 
prior experience with circuitry and instrumentation. 
Had he been a programmer by training, a larger Mp might have been required. By way of comparative evidence, the IBM 1800 floating-point arithmetic functions +, -, X, /, sin, cos, tan-', fl, log, exponential, tanh, binary to decimal, and decimal to binary require approxi- 
mately 1,425 16-bit words, or 23 
kb. On the other hand, 
the FOCAL1 interactive calculator 
program for 
a 4,096-word PDP-8 
(49 kb) provides the user with all 
but polar-rectangular coordi- 
nates and hyperbolic functions, but it does have 
a complete 
program editing 
capability, text handling, control 
structure, and 1,600-character Mp. 'Similar in 
scope to Dartmouth's BASIC. 
Chapter 19 The OLlVETTl Programma 
101 desk calcu latorl The Programma 101 
is manufactured by the Olivetti Underwood Corporation. The cost of Programma 101 is about $3,500 (in 1968). Several thousand are currently in use. Unlike conventional 
stored program 
computers it 
has instructions which 
can be exe- cuted directly as commands from a 
keyboard or instructions which 
can be stored in a 
program and interpreted by 
the processor. The processor uses the decimal representation 
for mixed 
numbers. The decimal point location is controlled manually. 
Although informa- tion is stored in character strings, the maximum length is 22 digits 
or 24 instructions for a 
register. A program can be up 
to 120 characters long and is stored as a 
continuous string. 
The internal encoding of a character is 8 bits. There are no 
absolute addresses for instructions, and jump instructions are programmed by placing 
labels or references in the string to transfer to. 
The Programma 101 is composed of the following elements. Memory. The memory stores 
nnmeric data and 
program instruc- 
tions. Keyboard. The keyboard has four functions: 
It is used for operator control of the calculator (power 
on, off, etc); in manual 
mode the instructions are executed immediately 
as in a 
conventional desk calculator (e.g., add); the keys write a program™s 
instructions in the memory, and the 
instructions are executed when the program is run; and numeric data may be entered to 
a running program. Printing unit. Serial printing is from right to left, at 30 characters 
per second; this 
unit prints all 
keyboard entries, programmed 
output, and instructions. Magnetic-card reader/recorder. This device 
permits instructions and constants for a 
program to be stored and retrieved from magnetic cards. Control and 
arithmetic units. The control unit is the administrative section of the computer. It receives the incoming information, 
determines the computation to be performed, and directs the lThe description is partially taken from the Programma 101 Programming Manual. arithmetic unit where to 
find the information and what 
operation to perform. The PMS diagram shown below is, of course, very simple. It conforms closely 
to the classic diagram of what a digital computer looks like: Mp-Pc T-M.magnetic-card- TT I LT.printer+ LT.keyboard + Primary memory 
and processor memory The memory has 10 registers; eight are for general storage 
and two are used exclusively for 
instructions. A character can have several meanings, 
depending on the register and its use. The two instruction registers, 1 and 2, each store 
24 instruc- tions. An instruction is one character 
long. The eight storage registers, M, A, 
R, B, C, D, E, and F, have a capacity of 22 decimal digits, plus decimal point 
and sign. The sign and decimal point do not require character space. 
Alterna- tively, D, E, and F hold 24 instructions. M, A, and R are operating registers and take part in all arithmetic operations. They are 
considered to be the arithmetic unit. 
The M register is the Median (or distributive) register. 
All keyboard figure entries are held 
in the M register and distributed to the other registers as 
instructed. The A register functions 
with the arithmetic unit 
to form the Accumulator. Arithmetic results are developed and retained in the A register. A result 
of up to 23 
digits can be produced in 
the A register. The R register retains the complete results in 
addition and subtraction, the complete product in 
multiplication, the remainder in division, 
and a remainder in square root. B, C, D, E, and F are storage registers. Each can 
be split into two 
registers, each with a capacity of 11 digits, plus decimal point 
and sign. When storage registers 
are split, the right portion 
of the split register 
retains its 
original designation, and the left side is identified with the corresponding lowercase 
letter. Thus 
these registers become 237 
238 Part 3 1 The instruction-set processor level: variations in 
the processor Section 4 1 Desk calculator computers: keyboard processors with small memories 
b, B, c, C, d, D, e, F, f and F. The lowercase designation 
is obtained by first entering the corresponding uppercase letter and 
then depressing the "/" key, for 
example, c G C/. The registers D, E, and F or their splits have the additional capability of storing either instructions or constants to be used within programs. Thus they 
can store 1 signed 22-digit 
number, 2 signed 11-digit numbers, 1 signed 11-digit 
number, and 11 instructions, or 24 
instructions. Programs 
of up to 120 instructions can be stored internally (Fig. 
1). When registers D, E, and F and their splits are not used for instructions, they 
are free to store constants or intermediate results. The relationship of memory, keyboard, 
printer, and magnetic card is shown in Fig. 1. Registers are referenced explicitly. Pro- 
grams do not 
use explicit addresses in instruction. Thus, special 
marker characters are placed in the instructions to serve as jump reference addresses (program labels). 
Fig. 2. Programma 101. (Courtesy of Olivetti Underwood Corporation.) 
Structure The calculator parts are described briefly below. The parts corre- 
spond to both the numbers (Fig. 
2) and the 
lettered keyboard (Fig. 
3). The following parts are, in effect, 
the console. Some 
of the keys are used for 
control of the calculator, and some can be used either as programmed instructions 
or as 
commands which 
are executed directly. The following section discusses their instruction function. 
The on-off key (1). This is a dual-purpose switch 
for both the on and off positions. (Note: The OFF position automatically clears all stored 
data and instructions.) The error (red) light (2). This lights 
when the computer is turned on and whenever the computer detects an 
operational error, 
e.g., exceeding capacity, division by zero. The general reset key (3). This key erases all data and 
instruc- tions from 
the computer and turns off the error light. 
The correct-performance 
(green) light (4). This light indicates 
the computer is functioning properly. 
A steady light indicates 
that the computer is ready for an operator 
decision; a flickering light 
indicates that the 
computer is executing programmed instructions and that 
the keyboard is locked. The decimal wheel (5). This determines the number of decimal places (0, 1,. , . , 15) to which computations will be carried out in the A register and the 
decimal places 
in the printed output, ~i~. 1. programma 101 functional block diagram. (Courtesy 
,,f oli- vetti Underwood Corporation.) 
except for results from 
the R register. up to 22 decimal digits may 
be developed in, 
and printed from, the R register. 
Chapter 19 I The OLlVETTl Programma 101 desk calculator 
239 Fig. 3. Programma 101 keyboard. (Courtesy 
of Olivetti Underwood 
, Corporation.) ' The record program switch (6). When this switch 
is off, the commands pressed on 
the keyboard are executed directly. 
When this switch 
is on, it directs the computer to store 
instructions either in the memory from the keyboard or onto a magnetic program card from the memory. The record program switch 
must be off to load instructions 
from a magnetic program card into 
the memory. The print program switch (7). When this switch is on (in), it directs the computer to print out 
the instructions stored in 
memory from its present location 
in the program to the next Stop instruc- tion (S), whenever the print key (20) 
is depressed. The magnetic program card (8). This is a plastic card with 
a ferrous oxide backing, used 
to record programs 
for external storage. 
The card is inserted into a magnetic reader/writer 
(9) to record instructions and/or constants into or from 
the computer memory. Once inserted, the card may be removed from the computer (10) without disturbing the stored instructions. (Note: The magnetic-card reader/writer uses only half the magnetic card 
at a time; consequently, 
two sets of 120 instructions and/or constants may 
be stored on a single 
card.) The keyboard release key (11). This key reactivates a locked keyboard. If two or more 
keys are depressed simultaneously, 
the keyboard will lock 
to indicate a misoperation. Because the opera- tor does not know what entry was accepted by the computer, after touching the keyboard release 
key, the clear entry key (16) must be depressed and the 
complete figure reentered. Tape advance (12). This advances 
the printing paper tape. Tape release lever 
(13). This enables adjustment when 
changing tape rolls. The routine selection (keys V, W, Y, and 2). These keys direct the computer to the proper program or 
subroutine. The numeric keyboard (keys 0, 1,. . . 
,9,. , -). This keyboard 
allows entry of a signed, 
mixed decimal number. 
Keyboard entries are automatically stored in the M register. The clear entry key. This key clears the entire keyboard entry. When keying in 
the program, a depression of the clear key will 
erase the last instruction 
that has been entered into the memory. The printing tape will be spaced. The start key (S). This key restarts the computer in programmed 
operation; it is used to code a stop instruction when keying in programs. The register address (keys A, B, C, D, E, F, and R). These keys identify the corresponding registers. The operating register M has no keyboard identification since the computer automatically re- 
lates all instructions 
to the 
M register unless otherwise instructed. The split key (/). This key combined with a register (for 
exam- ple, C/) divides that register into two equal 
parts. When storage registers are split, the right portion 
of the split register retains 
the original designation, 
and the 
left side 
is identified on the tape with the corresponding lowercase 
letter (for example, C/ G c). The print key ( 0 ). This key prints the contents of an addressed register. The clear key ("). This key clears the contents of an addressed register. When the 
computer is operated manually, a depression 
of this key will 
print the number in 
the register and clear it. The transfer keys (i, T, $). These keys perform transfer opera- 
tions between the storage registers and the 
operating registers. The arithmetic keys (-, + , x , t , 6). These keys perform their indicated arithmetic 
function. Keyboard and stored-program operations 
All the following keys can be used as direct 
instructions (Le., manually) if the record program switch 
is off. Alternatively, if the 
240 Part 3 1 The instruction-set processor level: variations in 
the processor Section 4 1 Desk calculator computers: keyboard processors with small memories 
record program switch 
is on, the keys specify the instruction to be recorded in the program memory. 
Finally, the descriptions specify the instruction's behavior 
as it is executed within a pro- gram. Start S. The instruction S (used in creating 
a program) directs 
the computer to stop 
and release the keyboard for the entry of figures or the selection of a subroutine. After figure entry, the 
program is restarted by touching the start key (S). The program can also be restarted by touching a routine selec- tion key. When the 
S instruction stops 
the program, the computer may also be operated in the manual mode 
without disturbing 
the program instructions 
in the memory. Any figures 
entered on the keyboard before depression 
of start or an operation key will be printed automatically. Clear *. The clear operation 
' directs the computer to clear 
the selected register. 
The M and R registers cannot he cleared with this instruction. 
When the 
computer is operated manually this 
key will cause 
it to print 
the contents of the selected register, r. (r to) Data-transfer operations To A J. An instruction containing 
the operation J, directs the computer to transfer contents of the addressed register, 
r, to A while retaining them in 
the original register. The contents of M and R are not affected. The previous contents of A are destroyed. From M t. An instruction containing the operation t directs the computer to 
transfer the contents of M to the addressed regis- ter while retaining them 
in M. The contents of registers A and R are unaffected by this instruction. 
The original contents of the addressed register 
are destroyed. (r t M) Exchange $. An instruction containing 
the operation $ directs the computer to exchange the contents of the A register with the contents of the addressed register. 
The contents of M are not affected except 
by the exchange between A and M. The contents of the R register are not affected. (A tr; r +A) D-R exchange RS. The instruction RS directs the computer 
to exchange the contents of D (both D and d registers) 
with the 
contents of the R register. (D t R; R t D) This instruction has 
a special use in multicard 
programs to store temporarily the contents of the D (d,D) register in R, when a new card has to be read to continue the program. During this tem- porary storage 
no instruction 
affecting the R register should 
be executed. Decimal part 
to M /$. The instruction /t directs the computer to transfer the decimal portion 
of the contents of A to the 
M (A +r) register while 
retaining the entire contents 
in A. The original contents of the M register are destroyed. The R register is not affected by this instruction. 
(M t fraction,part(A)) Arithmetic operations All arithmetic operations are performed in the operating registers M, A, and R. An arithmetic operation 
is performed in two phases: The contents of the selected register are automatically transferred to the M register. The M register is selected automatically if no other 
register is indicated. The operation is carried out in the M, A, and R registers. 
1 2 Programma 101 can perform these 
arithmetic operations: +, -, X, i, fl, and absolute value. Figures 
are accepted and 
computed algebraically. A negative value 
is entered by depressing 
the negative key at any time during 
the entry of a figure. 
If there is no negative indication, 
the computer 
will accept the 
figure as 
positive. The subtract operation 
key is separate from the numeric key- board and is used exclusively for 
subtraction (not 
negation). Addition + . An instruction containing the operation + directs the computer to add the contents of the selected register (addend) to the contents of the A register (augend). Addition is executed in two phases: 1 2 Transfer the contents of the selected register (addend) to M. Add the contents of M to the 
contents of A (augend) ob- taining in 
A the sum truncated according to the 
setting of the decimal wheel. 
The complete sum is in R. M contains the addend. (M t r; next R 
t A + M; next A t f(R,deci- mal-wheel)) Multiplication x . An instruction containing the operation x directs the computer to multiply the contents of the selected register (multiplicand) by the contents of the A register (multi- plier). 1 2 Transfer the contents of the addressed register 
to M. Multiply the contents of M by the contents of A, obtaining in A the product truncated according to 
the setting of the decimal wheel. 
The complete product 
is in R. M contains the multiplicand. (M t r; next R 
t A x M; next A t f(R, decimal-wheel)) 
Chapter 19 I The OLlVETTl Programma 101 desk calculator 241 Subtraction - . An instruction containing the operation - directs the computer to subtract the contents of the selected register (subtrahend) from the contents of the A register (minuend). 
1 Transfer the contents of the selected register 
(subtrahend) to M. Subtract the contents of M from the contents of A (minu- end), obtaining in 
A the difference truncated according to the setting of the decimal wheel. The complete difference is in R. M contains the subtrahend. (M t r; next R 
t A - M; next A 
t f(R,decimal,wheel)) 2 Division i . An instruction containing 
the operation i directs the computer to divide the contents of the selected register 
(divisor) into the contents of the A register (dividend). 
1 2 Transfer the contents of the addressed register 
to M. Divide the contents of M into the contents of A, obtaining in A the quotient truncated according to the setting of the decimal wheel. 
The decimally correct fractional remainder is in R. M contains the divisor. (M c r; next A t A - M; RcA mod M) Syuare Root <. An instruction containing 
the operation \r directs the computer to: 1 2 Transfer the contents of the selected register to M Extract the square root of the contents of M, as an absolute value, obtaining in A the result truncated according to the setting of the decimal wheel. 
The R register contains 
a nonfunctional remainder. 
At the end 
of the operation, M contains double 
the square root. (M cr; next M,R t sqrt(abs(M)) x 2; next A c f(M/2, decimal-wheel)) Absolute Value 
AI. The absolute-value instruction 
At changes the contents of the A register, if negative, to positive. (A t abs(A) Jump operations The jump operation directs 
the computer to depart from the normal sequence 
of step-by-step instructions 
and jump 
to a pre- selected point 
in the program. These instructions provide 
both internal 
and external (manual) decision capability and are 
useful to create ﬁloopsﬂ that allow repetitive sequences in a program to be executed; routines 
or subroutines to be performed at the discretion of the operator; and automatically to ﬁbranchﬂ to alternate routines or subroutines 
according to the 
value in the A register. The jump process consists 
of two related 
instructions or 
char- acters: 1 The reference point or label, 1, is where the program begins 
or where the jump is to start. The sequence is restarted at this point. This label 
has no effect 
when interpreted. 
The jump 
instruction specifies the label for the instruction sequence. 2 There are two 
types of jump instructions: unconditional jumps 
and conditional jumps. Unconditional jumps. These jumps are executed whenever the instruction is read. The labels or reference points for unconditional 
jumps, L, and the 
corresponding jump instructions, j, are given as (L,j). The permissible jump labels and jump 
constructions are: (AV,V), (AW,W), (AY,Y), ( AZ,Z), (BV,CV), 
. . . 
, (BZ,CZ), (EV,DV), . . 
. , (EZ,DZ), (FV,RV), . . 
. , (FZ,RZ) All programs must begin with reference parts of an uncondi- tional jump instruction. Reference points 
AV, AW, AY, AZ are used so that these program sequences 
can be started by touching 
the routine selection keys V, W, Y, or Z. Conditional Jumps. If the contents of the A register are: Greater than zero: the program jumps 
to the corresponding reference point 
(label). Zero or less: 
the program continues 
with the 
next in- struction in sequence. The labels or reference points 
for conditional jumps, L, 
and the corresponding conditional 
jump instruction, cj, are given as (L,cj). The permissible jump labels and jump 
instructions are (aV,/V), . . 
. , (az,/z), (bV,cV), . . . 
, (bZ,cZ), (eV,dV), . . 
. , (eZ,dZ), (f V,rV), . . . 
, (fZ,rZ) Constants as instructions A/?. A one-digit constant can be gener- ated by a special instruction. 
The results of the instruction place the digit in M. The digit value 
of the constant must follow A/T. Instructions and data 
in the same 
register. An instruction can be considered to be data and, therefore, used as both a constant and an instruction. Another 
technique allows the computer to interpret 

242 Part 3 1 The instruction-set processor level: variations in 
the processor data as null instructions so that both data (for reading and writing) and instructions can be stored in the same register. 
ExawLpZes. A program to take 
values for the numbers A, B, C, and D from the keyboard and then 
print the value of the expression [(A + B) x C]/D would be written as follows: 
instruction +AV S J or JM1 S +M S XM S tM A0 -V comments label to allow the program to be started by key, V wait; enter A from keyboard into 
M A value 
goes to A register wait, enter B from keyboard 
a register 
contains A + B wait, enter C from 
keyboard a register 
XC or (A + B) x C wait, enter D from keyboard 
a register has 
expression print A register 
jump back to beginning label to recalculate ex- pression for 
new variables 1 M is implied if left blank. The following program computes and prints n!. n is entered from the keyboard, where n 2 1, and an 
integer. The program is started by 
pressing key 
Z. Section 4 1 Desk calculator computers: keyboard processors with small memories 
comments program start, label stop, enter 
n from keyboard into M D t n; D holds 
n! or n x (n - 1) x Atn;Aholdsn,n-1,n-1, ..., 1 label generate 1 in M AcA - 1; (ntn - 1) test if n 2 0 print result get next n from 
keyboard begin to update n!, label A holds n!; D holds n - 1 after execution A holds n 
x (n - 1) x D holds n!; 
A holds n - 1 after execution return to 
compute n - 2 Conclusion Many algorithms have 
been written 
for Programma 101, being coded in 
impressively small 
space. The techniques have sometimes been borrowed from conventional 
computer programming. For 
example, multiple card 
programs operate by using chains in the same way 
as large FORTRAN programs. The significant fact to the reader is that the Programma 101 calculator is a nicely 
de- signed stored program 
computer. 
Chapter 20 The HP Model 9100A computing calculator1 Richard E. Monnier / Thomas E. Osborne / David S. Cochran A new electronic calculator with computerlike capabilities 
operations on two numbers, one in 
X and one in Y, appear in the Many of the day-to-day computing problems faced 
by scientists and engineers require 
complex calculations but involve only 
a moderate amount 
of data. Therefore, a machine 
that is more than a calculator in capability 
but less than a computer in cost has 
a great deal 
to offer. At the same time it must be easy to operate and program so that a minimum amount of effort is required in 
the solution of typical problems. Reasonable 
speed is necessary so that the response to individual operations 
seems nearly instan- 
taneous. The HP Model 9100A Calculator, Fig. 1, was developed to fill this gap between 
desk calculators and computers. 
Easy interaction between the machine and 
user was one of the most important design considerations during its development and 
was the prime guide in 
making many design decisions. 
CRT display One of the first and most basic problems 
to be 
resolved concerned the type of output to be 
used. Most people want a 
printed record, but printers are generally 
slow and noisy. Whatever method is used, if only one register is displayed, it 
is difficult to follow what is happening during 
a sequence 
of calculations where numbers 
are moved from 
one register to another. It was therefore decided that a cathode-ray 
tube displaying the contents of three registers would 
provide the greatest flexibility and would allow 
the user to follow problem solutions easily. 
The ideal situation 
is to have 
both a CRT 
showing more 
than one register, and a 
printer which can be at- tached as an accessory. 
Figure 2 is a typical display showing three numbers. The X register displays 
numbers as they are entered 
from the keyboard one digit 
at a time and 
is called the keyboard register. 
The Y register is called the accumulator since 
the results of arithmetic __ Y register. The Z register is a particularly convenient 
register to use for 
temporary storage. 
Numbers One of the most important features of the Model 9100A is the tremendous range 
of numbers it can handle 
without special 
atten- tion by the operator. It is not necessary to worry about where to place the decimal point to obtain 
the desired accuracy 
or to avoid register overflow. This flexibility 
is obtained because all 
numbers are stored in 
‚floating point™ and all operations performed 
using ‚floating 
point arithmetic.™ 
A floating point number 
is ex- pressed with the decimal point 
following the first digit and 
an exponent representing 
the number of places the decimal point 
should be moved-to the right if the exponent is positive, or 
to the left if the exponent is negative. ‚This chapter is a compilation of three articles [Monnier, 1968; Osborne, 1968; Cochran, 19681, reprinted from Hewlett-Puckurd Journul, vol. 20, no. 1, pp. 3-9, 10-13, 14-16, September, 1968. Fig. 1. This new 
HP Model 9100A calculator is self-contained and 
is capable of performing functions previously possible only 
with larger computers. 243 
244 Part 3 I The instruction-set processor 
level: variations in the processor Section 4 1 Desk calculator computers: keyboard processors with small memories 
explained and key codes 
are listed. Some simple examples are provided to assist those using 
the machine 
for the first time or to refresh the memory of an infrequent user. Most questions re- 
garding the operation of the Model 9100A are answered on the card. Data entry The calculator keyboard 
is shown in Fig. 
4. Numbers can be entered into the X register using 
the digit keys, the v key or the ENTER EXP 
key. The ENTER EXP 
key allows powers of 10 to be entered 
directly which 
is useful for 
very large 
or very 
small numbers. 6.02 x loz3 is entered @ @ 
@ @ 0. If the ENTER EXP key is the first key of a number entry, a 1 is auto- Fig. 2. Display in fixed point 
with the decimal wheel 
set at 5. The Y register has reverted 
to floating point because the number is too large 
to be properly displayed unless 
the digits called for 
by the DECIMAL- DIGITS setting are 
reduced. 4.398 364 
291 x = .004 398 364 291 The operator may choose 
to display numbers in FLOATING 
POINT or in FIXED POINT. The FLOATING POINT mode allows numbers, either positive or negative, from 1 x lopgg to 9.999 999 
999 x 10gg to be 
displayed just as they are stored in 
the machine. The FIXED POINT mode displays 
numbers in 
the way they are most commonly written. 
The DECIMAL DIGITS 
wheel allows setting the number of digits displayed 
to the right of the decimal point anywhere 
from 0 to 9. Figure 2 shows a display 
of three numbers with the DECIMAL 
DIGITS wheel set 
at 5. The number in the Y register, 5.336 845 815 
x 105 = 533 684.5815, is too big to be displayed in FIXED POINT without reducing 
the DECI- MAL DIGITS setting to 4 or less. If the number is too big for the DECIMAL DIGITS setting, 
the register involved 
reverts automatically to floating point to avoid an apparent overflow. In FIXED POINT 
display, the number displayed is rounded, but full significance is retained in storage for calculations. To improve readability, 0™s before the displayed number and un-entered 0™s following the number 
are blanked. 
In FLOATING POINT, digits to the right of the decimal are grouped 
in threes. Pull-out instruction card 
A pull-out instruction card, Fig. 3, is located at the front 
of the calculator under 
the keyboard. The operation of each key is briefly Fig. 3. Pull-out instruction card 
is permanently attached 
to the 
calcula- tor and contains key codes 
and operating instructions. 

Chapter 20 1 The HP Model 91WA computing calculator 
245 Functions available from the keyboard The group of keys at the 
far left 
of the keyboard, Fig. 4, gives 
a good indication of the power of the Model 9100A. Most 
of the common mathematical functions 
are available directly 
from the keyboard. Except for @ the function keys operate on the number in X replacing it with the function of that argument. The numbers in Y and Z are left unchanged. 
@ is located with another group 
of keys for 
convenience but operates the same way. The circular functions 
operate with angles expressed in RADI- ANS or DEGREES as set by the switch above the keyboard. The sine, cosine, 
or tangent of an angle 
is taken with 
a single keystroke. 
There are no restrictions on direction, quadrant or number of revolutions of the angle. The inverse functions are obtained 
by using the 0 key as 
a prefix. For instance, two 
key depressions are necessary to obtain the arc 
sin x: @ @ . The angle obtained will be the standard principal 
value. In radians: Fig. 4. Keys are in four 
groups on 
the keyboard, according 
to their function. matically entered into the mantissa. Thus only two keystrokes @ @ suffice to enter 1,000,000. The CHG SIGN key 
changes the sign of either the mantissa or 
the exponent depending upon 
which one 
is presently being 
addressed. Numbers 
are entered in the same way, regardless 
of whether the machine is in FIXED POINT or FLOATING POINT. 
Any key, other than 
a digit key, decimal point, 
CHG SIGN or ENTER EXP, terminates an 
entry; it is not necessary 
to clear before entering a new number. CLEAR X sets the X register to 0 and can be used when a mistake has 
been made 
in a number entry. 
Control and 
arithmetic keys 
ADD, SUBTRACT, MULTIPLY, DIVIDE involve two numbers, so the first number must be moved from X to Y before the second is entered into X. After the two numbers 
have been entered, the 
appropriate operation can be performed. In the case of a DIVIDE, the dividend is entered into 
Y and the 
divisor into X. Then the 
0 key is pressed causing 
the quotient to appear in Y, leaving the divisor in X. One way to transfer a number from the X register to the 
Y register is to use the double sized 
key, 0, at the 
left of the digit keys. This repeats the number 
in X into Y, leaving X unchanged; the number in Y goes to Z, and the number in Z is lost. Thus, when squaring 
or cubing a number, it 
is only necessary to follow 0 with @ or Q 0. The 0 key repreats a number in Z to Y leaving Z unchanged, the number in 
Y goes to X, and the 
number in X is lost. The @ key rotates the number in 
the X and Y registers up and the number 
in Z down into X. @ rotates the numbers in Z and Y down and the 
number in X up into Z. @ interchanges the numbers in 
X and Y. Using the two ROLL keys and @, numbers can be placed in any order in 
the three registers. -_ -n < Sin-' x 5 2- 2 0 5 Cos-' x 5 $7 < Tan-' x < ?T 2 2 
71 __ The hyperbolic sine, cosine, 
or tangent is obtained using the @ key as 
a prefix. The inverse hyberbolic functions 
are obtained with three key depressions. Tanh-' x is obtained by @ @ @ 
. The arc and 
hyper keys prefix keys below them in their 
column. Log x and In x obtain the log to the base 10 and the log to the base e respectively. The inverse of the natural log is obtained with the 
e' key. These keys are useful when raising numbers to odd powers as shown in one of the examples on the pull-out card, Fig. 3. Two keys in 
this group are very useful in programs. 0 takes the integer part of the number in the X register which 
deletes the part of the number to the right of the decimal point. For 
example int(-3.1416) = -3. @ forces the number in 
the Y register positive. 
Storage registers 
Sixteen registers, in addition 
to X, Y, and Z, are available 
for storage. Fourteen of them, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d, 
can be used to store either one constant 
or 14 program steps per register. The last registers, e 
and f, are normally used 
only for 
constant storage since 
the program counter will not cycle into 
246 Part 3 1 The instruction-set processor level: variations in 
the processor Section 4 I Desk calculator computers: keyboard processors with small memories 
them. Special keys located in 
a block 
to the left of the digit keys are used to identify the lettered registers. To store a number from the X register the key @ is used. The parenthesis indicates 
that another key depression, 
representing the storage register, 
is necessary to complete the transfer. For example, storing a number from the X register into register 8 requires two key depressions: @ @ . The X register remains unchanged. 
To store a number from Y register the key @ is used. The contents of the alpha registers are recalled to X simply by pressing the keys a, b, c, d, e, and f. Recalling a 
number from a numbered register requires 
the use of the @ key to distinguish the recall procedure from digit entry. This key interchanges the number in 
the Y register with the number in the register indicated by the following keystroke, 
alpha or numeric, and is also useful 
in programs 
since neither number 
involved in 
the transfer is lost. The CLEAR key sets the X, Y, and Z display registers 
and the 
f and e registers to zero. The remaining registers are not affected. The f and e registers are set to zero to initialize them for use 
with the 0 and @ keys as 
will be explained. In addition 
the CLEAR key clears the FLAG and the ARC and HYPER conditions, which 
often makes it a very useful first step in a program. Coordinate transformation 
and complex numbers 
Vectors and complex numbers 
are easily handled using the keys in the column on the far 
left of the keyboard. Figure 5 defines the variables involved. Angles can be either in degrees or 
radians. To convert from rectangular to 
polar coordinates, 
with y in Y and x in X, press @. Then the display shows 0 in Y and R in X. In Y y = R sin 0 Fig. 5. Variables involved 
in conversions between rectangular and polar 
coordinates. converting from polar to rectangular 
coordinates, 6' is placed in Y, and R in X, @ is pressed and the display shows y in Y and x in X. ACC+ and ACC- allow addition or subtraction of vector components in 
the f and e storage registers. ACC+ adds the contents of the X and Y register to the 
numbers already 
stored in f and e respectively; ACC- subtracts them. 
The RCL key recalls the numbers in the f and e registers to X and Y. Illegal operations 
A light to 
the left of the CRT indicates that an illegal operation has been performed. This 
can happen either 
from the keyboard or when running 
a program. Pressing any key on 
the keyboard will reset 
the light. When running a program, execution 
will continue but the light will remain on as the program is completed. The illegal operations are: 
Division by zero fi where x < 0 In x where x 5 0; log n 
where x 5 0 six1 x where 1x1 > 1; c0s-I x where (.XI > 1 cosh-' x where x < 1; tanh-' x 
where 1x1 > 1 Accuracy The Model 9100A does all calculations using 
floating point arith- 
metic with 
a twelve digit 
mantissa and a two digit exponent. The two least significant digits are not displayed and are called guard digits. The algorithms used 
to perform the operations and generate the functions were chosen to minimize error and to provide an extended range 
of the argument. Usually any inaccuracy 
will be contained within the two 
guard digits. In certain cases some in- 
accuracy will appear in the displayed number. One example is where the functions change rapidly 
for small 
changes in the argu- ment, as in 
tan x where x is near 90". A glaring but insignificant inaccuracy occurs when an 
answer is known to be a whole number, but the least significant 
guard digit is one count low: 2.000 000 000 N 1.999 999 999. Accuracy is discussed 
fnrther in 
the 'Internal Programming' section in this chapter. But a 
simple summary 
is: the answer result- ing from any operation or function will lie within 
the range of true values produced by a variation of il count in 
the tenth digit of the argument. Programming Problems that require many keyboard operations 
are more easily solved with a program. This 
is particularly true when the same 
Chapter 20 I The HP Model 9100A computing calculator 
247 operations must be performed 
repeatedly or an iterative 
technique must be used. A program library supplied with 
the Model 9100A provides a set 
of representative programs from many different 
fields. If a program cannot be found in 
the library to 
solve a particular problem, a 
new program 
can easily be written since no special experience 
or prior knowledge of a programming lan- 
guage is necessary. Any key on the keyboard can be remembered 
by the calculator as a program step except STEP 
PRGM. This key 
is used to ‚debug™ a program rather than 
as an operation in a program. Many indi- 
vidual program steps, such as 
‚sin x™ or ‚to polar™ 
are comparatively powerful, and avoid the need of sub-routines for these functions 
and the programming space 
such sub-routines require. 
Registers 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, b, c, d can store 
14 program steps each. Steps within the registers are numbered 0 through d 
just as the registers themselves 
are numbered. Programs can start at any of the 196 possible addresses. 
However 0-0 is usually used for 
the first step. Address d-d is then the last available, after 
which the program counter cycles back to 0-0. Registers f and e 
are normally used 
for storage of constants only, one constant 
in each register. As more constant storage 
is required, it is recommended that registers d, then 
c, then b, etc., are used starting from the bottom of the list. Lettered registers are used first, for 
the frequently recalled constants, because constants stored 
in them are more easily 
recalled. A register can be used to store 
one constant 
or 14 program steps, but not both. 
Branching The bank on the far right of the keyboard, Fig. 
4, contains program oriented keys. @ is used 
to set 
the program counter. The two 
sets of parentheses indicate that 
this key should 
be followed by 
two more key depressions 
indicating the address of the program step desired. As a program step, ‚GO TO™ is an unconditional branch instruction, which causes 
the program to branch to the address given 
by the next two program steps. 
The ‚IF™ keys in this group are conditional branch instructions. With @ @ , and@ the numbers contained in 
the X and Y registers are compared. The indicated 
condition is tested and, if met, the next two program steps are executed. 
If the first is alphameric, the second must 
be also, and the two steps are interpreted 
as a branching 
address. When the condition is not met, 
the next two steps are skipped 
and the 
program continues. @ is also a very useful 
conditional branching instruction which 
tests a ‚yes™ or ‚no™ condition inter- nally stored in 
the calculator. This condition is set to ‚yes™ with the SET FLAG from 
the keyboard when 
the calculator is in the display mode 
or from a program as a program step. The 
flag is set to a 
‚no™ condition by either asking IF FLAG in a 
program or by a CLEAR instruction from the keyboard or from a program. 
Data input and output Data can 
be entered for use in 
a program when the machine is in the display mode. 
(The screen is blank while 
a program is running.) A program can be stopped in 
several ways. The @ key will halt the machine at any time. 
The operation being performed 
will be completed before returning to the 
display mode. 
As a program step, STOP stops 
the program so that answers can be 
displayed or new data entered. END 
must be the last step in a program listing to 
signal the magnetic card reader; when encoun- tered as a program step it stops the machine and 
also sets 
the program counter to 0-0. As a program step, PAUSE causes a brief display 
during pro- 
gram execution. Nine cycles 
of the power line frequency are 
counted-the duration of the pause will 
be about 150 ms for a 60 Hz power line 
or 180 ms for a 50 Hz power line. More pauses 
can be used in sequence 
if a longer display 
is desired. While a 
program is running the PAUSE key can be held down to stop the 
machine when it comes to the next PAUSE in the program. PAUSE provides a particularly 
useful way for 
the user and the machine to interact. It 
might, for instance, be 
used in a 
program so that the convergence to a 
desired result 
can be observed. Other means of input and output 
involve peripheral devices such as an 
X-Y Plotter or a Printer. The 
PRINT key activates the printer, causing it to print information from 
the display register. As a program step, PRINT will interrupt the program long enough 
for the data 
to be 
accepted by the printer and then 
the program will continue. If no printer is attached, PRINT as a program step will act as a STOP. The FMT key, followed 
by any 
other keystroke, provides up to 62 unique commands to peripheral equipment. 
This flexibility allows 
the Model 9100A to be 
used as a controller in 
small systems. 
Sample program-N! 
A simple program 
to calculate N! demonstrates how the Model 9100A is programmed. Figure 
6 (top) shows a flow chart to com- pute N! and Fig. 6 (bottom) shows the program steps. With this program, 60! takes less than ‚/z second to compute. Program entry and execution After a program is written it can be 
entered into 
the Model 9100A from the keyboard. The program counter is set to 
the address of 
248 Part 3 1 The instruction-set processor level: 
variations in the processor Section 4 1 Desk calculator computers: keyboard processors 
with small memories 
Store N I np2 i Fig. 6. Flow chart of 
a program to compute N! (top). Each 
step is shown (bottom) and the display for each register. A new value for N can be entered at the end of the program, since 
END automatically sets the program counter 
back to 0-0. the first program step by using 
the GO TO ( ) 
( ) key. The RUN- PROGRAM switch is then switched from RUN to PROGRAM and the program steps 
entered in sequence by pushing 
the proper keys. As each step is entered the 
X register displays 
the address and key code, as shown in 
Fig. 7. The keys and their codes are listed at the bottom of the pull-out card, Fig. 3. Once a program has 
been entered, the steps can 
be checked using the STEP PRGM key in the PROGRAM mode as explained in Fig. 7. If an error 
Fig. 7. Program step address and code are displayed 
in the X register as steps are entered. 
After a program has been entered, each 
step can 
be checked using the STEP PRGM 
key. In this display, step 
2-d is 
36, the code for multiply. is made in a step, it can be corrected 
by using 
the key without having to re-enter the rest of the program. To run a program, the program counter must be set to the 
address of the first step. If the program starts 
at 0-0 the keys @ @ @ are depressed, or simply just @ since this 
key auto- matically sets the program counter to 
0-0. CONTINUE will start program execution. Magnetic card reader-recorder 
One of the most convenient features 
of the Model YlOOA is the magnetic card reader-recorder, Fig. 8. A program stored in the Model YlOOA can be recorded on a magnetic card, Fig. 9, about Fig. 8. Programs can be entered 
into the 
calculator by 
means of the magnetic program 
card. The 
card is inserted into the 
slot and the ENTER button pressed. 
Chapter 20 I The HP Model 9100A computing calculator 
249 Fig. 9. Magnetic programming 
card can 
record two 196-step programs. 
To prevent accidental recording 
of a new program over one to be saved, 
the corner of the card is cut as shown. 
the size of a credit card. Later when 
the program 
is needed again, it can be quickly re-entered using the previously recorded 
card. Cards are easily duplicated so that programs of common 
interest can be distributed. As mentioned earlier, the END statement is a signal to the reader to stop reading recorded 
information from 
the card 
into the calculator. For this reason END should not be used in the middle of a program. Since most programs start 
at location 0-0 the reader automatically initializes the program 
counter to 0-0 after a card is read. The magnetic card reader makes it possible to handle most programs too long to be held 
in memory at one time. The first entry of 
steps can calculate 
intermediate results which are stored in preparation for 
the next 
part of the program. Since the reader stops reading at the END statement these stored intermediate results are 
not disturbed when the 
next set of program 
steps is entered. The stored results are then retrieved 
and the program 
continued. Linking of programs is made more convenient if each part can execute an END when it finishes to set the program counter to 0-0. It is then only 
necessary to press CONTINUE after each entry of program 
steps. Hardware design of the Model 9100A calculator All keyboard functions 
in the Model 9100A are implemented by the arithmetic 
processing unit, Figs. 10 and 11. The arithmetic 
unit operates in discrete time periods called clock cycles. All Specifications of HP Model 9100A* The HP Model 9100A is a programmable, electronic calculator which 
performs opera- tions commonly encountered 
in scientific and engineering problems. 
Its log, trig and mathematical functionsareeach 
performed with a 
single key stroke, providing fast, convenient solutions 
to intricate equa- tions. Computer-like memory enables the calculator to store instructions and con- 
stants for repetitive or 
iterative solutions. The easily-readable cathode ray 
tube in- stantly displays entries, 
answers and inter- mediate results. 
Operations Direct keyboard operations include: 
Arithmetic: addition, subtraction, 
mul- tiplication, division and square-root. 
Logarithmic: log x, In x and eX. Trigonometric: sin 
x, cos x, tan x, sin-lx, cos-™x and tan-lx (x in de- grees or radians). 
Hyperbolic : sinh x, cosh x, tanh x, sinh-lx, cosh-lx, 
and tanh-lx. Coordinate transformation: polar-to- rectangular, rectangular-to-polar, cumulative addition and subtraction of vectors. Miscellaneous: other single-key opera- 
tions include-taking the absolute 
value of a number, extracting the integer part of a number, and enter- 
ing the value 
of ?r. Keys are also available for positioning 
and storage 
operations. Programming The program mode allows 
entry of program instructions, via the keyboard, into program memory. 
Programming consists of pressing keys in the proper 
sequence, and any key on the keyboard 
is available as a program step. Program capacity is 196 steps. 
No language or code-conversions are required. A self- contained magnetic card reader/re- 
corder records programs 
from program memory onto wallet-size magnetic 
cards for storage. It also reads 
programs from cards into program memory 
for repetitive use. Two programs of 196 steps each may be recorded on 
each reusable card. Cards 
may be cascaded for longer programs. 
Average times for total performance of typical operations, 
including decimal- point placement: Speed add, subtract: 2 milliseconds multiply: 12 milliseconds divide: 18 milliseconds 
square-root: 19 milliseconds 
sin, cos, tan: 280 
milliseconds In x: 50 milliseconds 
eX: 110 milliseconds These times include core 
access of 1.6 microseconds. General Weight: Net 40 Ibs, (18,l kg.); shipping Power: 115or230V k 10%,50to60Hz, Dimensions: 8%ﬁ high, 16ﬂ 
wide, 19ﬂ 65 Ibs. (29,5 kg.). 400 Hz, 70 watts. deep. *Courtesy of Loveland Division. 
250 Part 3 1 The instruction-set processor level: 
variations in the processor Section 4 I Desk calculator computers: keyboard processors 
with small 
memories I J No Memory Activate (Read only) .e I 825 ns CLOCK mm cc 00 mm cc .e 22 CY CY -- Activate (Read - Write) caF:iity PROGRAM I Activate (Read only) 512 WORD Description 64 BITIW ADDRESS FLIP FLOPS CONTROL WORD coRNoTioL I I 
64 WORD 800 ns 29 Biriw \ CONTROL LOGIC ADDRESS FLIP FLOP . COINCIDENT CURRENT MEMORY 1 CORE 368 WORDS 6 BIT/W ‚f 1 I High Order Memory 1161 FLIP DAT~~ FLOPS 1-1 ADDRESS 1 LowOrder Memory FLIP FLOPS Fig. 10. Arithmetic processing 
unit block diagram. 
This system is a marriage 
of conventional, reliable diode-resistor 
logic to a 32,000-bit read-only memory and a 
coincident current core 
memory. operations are synchronized 
by the clock shown 
at the top center 
of Fig. 10. The clock is connected to the control read 
only memory (ROM) 
which coordinates 
the operation of the program read only memory 
and the 
coincident current core read/write memory. The former Fig. 11. Arithmetic unit assembly removed from the 
calculator. contains information for 
implementing all 
of the keyboard opera- tions while the latter stores user 
data and user programs. 
All internal operations are performed in a digit 
by digit serial basis using 
binary coded decimal 
digits. An addition, for example, 
requires that the least significant digits 
of the addend and 
augend be extracted 
from core, then added and their 
sum replaced in core. 
This process is repeated one BCD digit at a time until 
the most significant digits have been processed, There is also a substantial 
amount of ‚housekeeping™ to be 
performed such 
as aligning decimal 
points, assigning the proper algebraic 
sign, and floating point normalization. Although the implementation of a keyboard func- tion may involve thousands 
of clock cycles, 
the total 
elapsed time is in the millisecond region 
because each 
clock cycle 
is only 825 ns long. The program ROM contains 512 64-bit words. When the 
pro- gram ROM is activated, signals (micro-instructions) corresponding 
to the bit pattern 
in the word are sent to 
the hard wired logic 
gates shown 
at the bottom of Fig. 10. The logic gates define 
the changes to occur in 
the flip flops at the 
end of a clock cycle. 
Some of the micro-instructions act upon the data flip flops 
while others change the address registers associated 
with the program ROM, 
Chapter 20 1 The HP Model 9100A computing calculator 
251 control ROM and coincident 
current core memory. During the next clock cycle 
the control ROM may ask for a new 
set of micro- instructions from the program ROM or ask to be read 
from or 
written into 
the coincident current core memory. The control ROM also has 
the ability to modify its own address register 
and to issue micro-instructions to 
the hard wired logic gates. This 
flexibility allows 
the control logic ROM to execute special 
pro- grams such as the subroutine for unpacking the stored constants 
required by 
the keyboard transcendental functions. Control logic 
The control 
logic uses a wire 
braid toroidal core 
read only memory 
containing64 29-bit words. 
Magnetic logic of this type is extremely reliable and pleasingly compact. The crystal controlled 
clock source 
initiates a 
current pulse having a trapezoidal 
waveform which 
is directed through one 
of 64 word lines. 
Bit patterns are generated 
by passing or 
threading selected toroids with the word lines. 
Each toroid that is threaded acts as a transformer to turn on a transistor connected to the output winding of the toroid. The signals from 
these transistors operate the program ROM, 
coincident current core, and selected 
micro-instructions. Coincident current core 
read/write memory The 2208 (6 x 16 x 23) bit coincident current memory uses wide temperature range lithium 
cores. In addition, the X, Y, and inhibit 
drivers have 
temperature compensated current drive sources to make the core memory insensitive 
to temperature and 
power supply variations. The arithmetic 
processing unit includes special circuitry 
to guarantee that information is not lost from 
the core memory when power is turned off and on. Power supplies The arithmetic 
processing unit operates 
from a single -15 volt supply. Even though the power supply 
is highly regulated, all circuits are designed to operate over a voltage range of -13.5 to -16.5. Display The display is generated on an HP electrostatic cathode ray tube only 11 inches long. The flat rectangular face 
plate measures 3y4 x 4l3/,, inches. The tube 
was specifically designed 
to gener- ate a bright 
image. High 
contrast is obtained by using 
a low transmissivity filter 
in front of the CRT. Ambient light 
that usually tends to 
'wash out' an image 
is attenuated twice by 
the filter, while 
the screen image is only attenuated once. All the displayed characters are 
'pieces of eight.' Sixteen differ- 
ent symbols are obtained by intensity modulating a 
figure 8 pattern as shown in Fig. 12. Floating point numbers are 
partitioned into 
groups of three digits and 
the numeral 1 is shifted to 
improve readability. Zeros to the left of the most significant 
digit and insignificant zeros 
to the 
right of the decimal point 
are blanked to avoid a confusing display. Fixed 
point numbers 
are automati- cally rounded up according to the 
decimal wheel setting. 
A fixed point display will 
automatically revert 
to floating point notation 
if the number is too large to be 
displayed on 
the CRT in fixed point. Multilayer instruction logic board All of the hard 
wired logic gates 
are synthesized on the instruction logic board using time-proven diode-resistor logic. 
The diodes and resistors are located in 
separate rows, Fig. 13. All diodes are oriented in the same direction and all resistors 
are the same value. The maze of interconnections normally associated 
with the back plane wiring of a computer are located on the six internal layers of the multilayer instruction 
logic board. Solder bridges 
and acci- dental shorts caused 
by test 
probes shorting to leads beneath components are all but eliminated by not having interconnections 
on the two outside surfaces of this multilayer board. 
The instruc- tion logic board also serves as 
a motherboard 
for the control logic board, the two coincident core 
boards and the two flip flop 
boards, the magnetic card reader, and the 
keyboard. It also contains a 
connector, available 
at the 
rear of the calculator, for connecting peripherals. Flip flops The Model 9100A contains 40 identical J-K flip flops, each having a threshold noise 
immunity of 2.5 volts. Worst case design 
tech- niques guarantee that the 
flip flops will operate at 3 MHz even 
though 1.2 MHz 
is the maximum operating rate. 
I I/ \I Fig. 12. Displayed characters are 
generated by modulating these figures. 
The digit 1 is shifted to the 
center of the pattern. 

Fig. 13. Printed-circuit boards which make up the 
arithmetic unit are, left to right at top, 
side board, 
control logic, flip flop, core and 
drivers, core 
sense amplifiers and inhibit, flip flop, and side board. Large board 
at the lower left is the multilayer 
instruction board, and 
the program ROM is at the right. The magnetic card 
reader and its associated circuitry are at the bottom. 
14 I2 :I I , .. k 
Chapter 20 1 The HP Model 9100A computing calculator 253 Program read 
only memory 
The 32,768 bit read only program memory consists of 512 64-bit words. These words contain all of the operating subroutines, stored 
constants, character encoders, and CRT 
modulating patterns. 
The 512 words 
are contained in 
a 16 
layer printer-circuit 
board having drive and sense lines orthogonally 
located. A drive line 
consists of a reference line 
and a data line. Drive 
pulses are inductively coupled from both the reference line 
and data line into the sense lines. Signals from 
the data 
line either aid or cancel signals from 
the reference line producing 
either a 1 or 0 on the output 
sense lines. The drive and sense lines 
are arranged 
to achieve a bit density in the ROM data board of 1000 bits per square 
inch. The program ROM decoder/driver circuits 
are located 
directly above the ROM data board. Thirty-two combination 
sense ampli- fier, gated-latch circuits 
are located on each side of the ROM data board. The outputs of these circuits control 
the hard 
wired logic gates on the instruction logic board. Side boards The program ROM printed circuit 
board and the instruction logic board are 
interconnected by the 
side boards, 
where preliminary signal processing occurs. 
The keyboard The keyboard contains 63 molded plastic keys. Their markings will 
not wear 
off because the lettering is imbedded into the 
key body using a 
double shot injection molding process. 
The key and switch assembly was specifically 
designed to obtain a pleasing feel 
and the proper amount 
of tactile and 
aural feedback. Each 
key operates a single switch 
having gold alloy 
contacts. A contact closure acti- vates a matrix which encodes 
signals on six data lines and generates an initiating signal. This signal is delayed to avoid the effects of contact bounce. An electrical interlock prevents 
errors caused by 
pressing more 
than one key at a time. Magnetic card reader 
Two complete 196 step programs can be 
recorded on the credit card size magnetic program card. The recording process erases 
any previous information 
so that a card may be used over 
and over again. A program may be protected against accidental erasure by clipping off the corner of the card, Fig. 9, page 
249. The missing corner deactivates 
the recording circuitry in 
the magnetic card reader. Program cards are compatible among 
machines. Information is recorded in 
four tracks with 
a bit density of 200 bits per inch. Each 
six-bit program step is split into two time- 
multiplexed, three-bit codes and recorded on three of the four tracks. The fourth track 
provides the timing strobe. 
Information is read from the card and 
recombined into 
six bit codes for 
entry into 
the core memory. The magnetic card reading circuitry recognizes the ‚END™ program code as a signal 
to end the reading process. This feature makes it possible to enter sub- routines within 
the body of a main program or 
to enter 
numeric constants via the program card. 
The END code also sets 
the program counter to location 0-0, the most probable starting loca- tion. The latter feature 
makes the Model 9100A ideally suited to ‚linking™ programs that require more than 196 steps. 
Packaging and servicing The packaging of the Model BlOOA began by giving 
the HP indus- trial design group a volume 
estimate of the electronics package, 
the CRT display size 
and the 
number of keys on the keyboard. Several sketches 
were drawn 
and the best one 
was selected. The electronics sections were then 
specifically designed to fit in this case. Much 
time and effort were spent 
on the packaging of the arithmetic processing unit. The photographs, Figs. 11 and 14, attest to the fact that it 
was time well spent. The case covers 
are die cast aluminum which 
offers durability, effective RFI shielding, excellent 
heat transfer characteristics, and convenient mechanical 
mounts. Removing four screws 
allows the case to be opened and locked into position, Fig. 
14. This procedure exposes all important diagnostic test points 
and adjustments. The keyboard and arithmetic processing unit may be freed by removing 
four and seven screws respectively. 
Any component failures can 
be isolated by using a diagnostic 
routine or a special tester. 
The faulty assembly is then replaced and is sent to a service 
center for computer assisted diagnosis 
and repair. Reliability Extensive precautions have 
been taken to insure maximum relia- bility. Initially, wide electrical 
operating margins were obtained 
by using ‚worst case™ design 
techniques. In production all transis- 
tors are aged at 80% of rated power for 96 hours 
and tested before being used in the Model Y100A. Subassemblies are computer tested and actual operating 
margins are monitored to detect 
trends that could lead 
to failures. These data are analyzed and corrective action is initiated to reverse the trend. 
In addition, each calculator 
is operated in an environmental chamber 
at 55°C for 5 days prior 
to shipment to the customer. Precautions 
such as these allow Hewlett-Packard to offer a one year warranty in a field where 90 days is an accepted standard. 
254 Part 3 1 The instruction-set processor 
level: variations in the processor Fig. 14. Internal adjustments 
of the calculator are 
easily accessible 
by removing a few screws and lifting the 
top. ~ ‚I 100 A Internal programming of the 9100A calculator Extensive internal programming has been designed into the HP Model 9100A Calculator to enable the operator to enter data 
and to perform most arithmetic operations necessary for engineering and scientific calculation with 
a single 
key stroke or single program 
step. Each 
of the following operations is a hardware subroutine called by a key 
press or program 
step: Basic arithmetic operations Addition Subtraction Multiplication Division Extended arithmetic operations Square root Exponential-ex Logarithmic-ln x, log x Vector addition and subtraction Section 4 1 Desk calculator computers: keyboard processors with small memories 
Trigonometric operations 
Sin x, cos x, tan x Arcsin x, arccos x, arctan x Sinh x, cosh x, tanh x Arcsinh x, arccosh x, arctanh x Polar to rectangular and rectangular to polar coordinate 
transformation Miscellaneous Enter TI Absolute value 
of y Integer value of x In the evolution of internal programming of the Model 9100A Calculator, the first step was the development of flow charts of each function. 
Digit entry, Fig. 15, seemingly a 
trivial function, 
is as complex as most 
of the mathematical 
functions. From this functional description, 
a detailed program can be written which uses the microprograms and incremental instructions of the calcu- lator. Also, each program 
must be married to all of the other programs which make up the hard-wired software of the Model 9100A. Mathematical functions 
are similarly programmed defining a step-by-step 
procedure or algorithm for solving the desired mathematical problem. 
The calculator is designed so that lower-order subroutines may be nested to a level 
of five in higher-order 
functions. For instance, 
the ‚Polar to Rectangular™ function 
uses the sin routine which 
uses multiply which 
uses add, etc. 
Addition and subtraction The most elementary mathematical operation 
is algebraic addi- 
tion. But even this is relatively complex-it requires comparing 
signs and complementing if signs are unlike. Because all numbers in the Model 9100A are processed as 
true floating point numbers, 
exponents must 
be subtracted 
to determine proper decimal 
align- ment. If one of the numbers is zero, it is represented in 
the calcu- lator by an all-zero mantissa 
with zero exponent. 
The difference between the 
two exponents determines the offset, and rather 
than shifting the smaller number to the 
right, a displaced digit-by-digit 
addition is performed. It must also be determined if the offset is greater than 12, which is the resolution limit. Although the display shows 10 significant 
digits, all 
calculations are performed to 12 significant digits with the two 
last significant digits (guard digits) absorbing 
truncation and round-off errors. All registers are in core 
memory, eliminating the need for a large 
number of flip-flop registers. Even with 
the display in ‚Fixed 
Point™ mode, every computed result is in storage 
in 12 digits. 

Chapter 20 I The HP Model 9100A computing calculator 
255 ENTRY 0 Functions A REGISTER From EXP From CLEAR CLEAR KEYBOARD 
REGISTER SHIFT EXPONENT DIGITS LEFT I I STORE DIGIT IN STORE DIGIT IN LEAST SIGNIFICANT MOST SIGNIFICANT EXPONENT LOCATION 
1 I LOCATION POINT SET ? I 1 Yes I I 
EXPONENT FROM EXPONENT REA0 MOST DIGIT LOCATION 
y SIGNIFICANT 1 I EXIT I -- MOST SIGNIFICANT DIGIT LOCATION 
THIS LOCATION Fig. 15. Flow chart 
of a simple digit entry. Some 
of these flow paths 
are used by other calculator operations 
for greater hardware efficiency. 
Multiplication Multiplication is successive addition of the multiplicand as deter- mined by 
each multiplier 
digit. Offset in the digit position flip-flops is increased by one 
after completion of the additions by 
each multiplier digit. Exponents 
are added after completion 
of the product. Then the 
product is normalized to justify a carry digit which might 
have occurred. 
Division Division involves 
repeated subtraction of the divisor from 
the dividend until an 
overdraft occurs. At each subtraction without 
overdraft, the quotient digit is incremented by one at the digit position of iteration. When an 
overdraft occurs, 
the dividend is restored by 
adding the divisor. The division digit position is then incremented and the process continued. Exponents are subtracted after the quotient is formed, and the quotient normalized. Square root 
Square root, in 
the Model YlOOA, is considered a basic 
operation and is done by pseudo 
division. The method used is an extension of the integer relationship. 52i - 1 = n2 In square root, the divisor digit 
is incremented at each iteration, 
and shifted when an overdraft and restore occurs. This is a very fast algorithm for 
square root and is equal in speed to 
division. Circular routines 
The circular routines 
(sin, cos, tan), the inverse circular routines 
(arcsin, arccos, 
arctan) and the polar to rectangular 
and rectangu- lar to 
polar conversions are all accomplished by 
iterating through 
a transformation which rotates 
the axes. Any 
angle may be repre- sented as an angle between 0 and 1 radian plus additional 
infor- mation such as the number of times m/2 has been added 
or sub- tracted, and its sign. The basic algorithm for the forward circular 
function operates on an 
angle whose absolute value 
is less than 1 radian, but prescaling is necessary to indicate quadrant. 
To obtain the scaling constants, 
the argument is divided by 2m, the integer part 
discarded and the 
remaining fraction 
of the circle multiplied by 
257. Then m/2 is subtracted from the absolute value until the angle is less than 1 radian. The number of times m/2 is subtracted, the original sign of the argument, and the 
sign upon completion of the last subtraction make 
up the scaling constants. 
To preserve the quadrant information the scaling constants 
are stored in the core memory. 
i=l 
256 Part 3 1 The instruction-set processor level: variations 
in the processor The algorithm produces 
tan 0. Therefore, in the Model 9100A, cos 8 is generated as 1 diTGx and sin8 as tan 8 vTFi&z Sin0 could be obtained from the relationship sin8 = d-, for example, but the 
use of the tangent relationship preserves the 12 digit accuracy for very small angles, 
even in the range of 0 < 10-12. The proper signs of the functions are assigned from the scaling constants. 
For the polar to rectangular functions, cos 0 and sin 0 are com- puted and 
multiplied by 
the radius vector 
to obtain the X and Y coordinates. In performing 
the rectangular to polar function, 
the signs of both the X and Y vectors are retained to place 
the resulting angle 
in the right quadrant. Prescaling must also precede the inverse circular functions, 
since this 
routine operates on arguments 
less than or equal to 1. The inverse circular algorithm yields arctangent functions, making 
it necessary to use the trigonometric identity. If cos-l(x) is desired, the arcsin relationship is used and a scaling constant adds 
m/2 after completion of the function. For arguments greater than 1, the arccotangent 
of the negative reciprocal 
is found which yields the arctangent when 
m/2 is added. Exponential and logarithms 
The exponential routine uses a compound iteration 
.algorithm which has an argument range of 0 to the natural log of 10 (In 10). Therefore, to be able to handle any argument 
within the dynamic range of the calculator, it is necessary to prescale the absolute value of the argument by dividing it by In 10 and saving the integer part to be used as the exponent of the final answer. The fractional part is multiplied by 
In 10 and the exponential found. This 
number is the mantissa, and with 
the previously saved integer part as a 
power of 10 exponent, becomes 
the final answer. Section 4 I Desk calculator computers: keyboard processors with small memories 
The exponential answer 
is reciprocated in case the original argument was negative, and for use in 
the hyperbolic functions. 
For these hyperbolic functions, 
the following identities are used: e" - e-" sinh x = ___ 2 Natural logarithms 
The exponential routine in reverse is used as 
the routine for natural logs, with only 
the mantissa operated upon. Then the 
exponent is multiplied by In 10 and added to the 
answer. This 
routine also yields these loglo and are hyperbolic functions: 
In x In 10 Loglox = - cosh-l(x) = ln(x + dm) tanh-l(x) = lnp 1-x The sinh-l(x) relationship abdve 
yields reduced accuracy 
for negative values of x. Therefore, in the Model YlOOA, the absolute value of the argument is operated upon and the correct sign affixed after completion. Accuracy It can be seen from the discussion of the algorithms that extreme care has been taken to use routines that have accuracy commensu- rate with the dynamic range 
of the calculator. For example; the square root has a maximum possible 
relative error 
of 1 part in 
lo1" over the full range of the machine. There are 
many algorithms for 
determining the sine of an angle; most of these have points of high error. 
The sine routine in the Model 9100A has consistent low error 
regardless of quadrant. Marrying a full floating 
decimal calculator 
with unique mathe- 
matical algorithms results in accuracy 
of better than 
10 displayed digits. 
Section 5 Processors with stack memories (zero addresses per 
instruct ion) This section 
contains only 
computers which 
use a stack memory 
in their Pc and hence are 
denoted Pcstack. Although the im- plementation details differ, they are 
based on the common 
idea of a stack 
as described in Chap. 3, page 62. Several theory or 
language-based processors-IPL-VI 
and EULER-use a stack 
in Mp. However, for these language-based 
machines the stack is 
not the main 
design theme as it is with the other computers in Table 1. In fact, data in IPL-VI are organized (Chap. 30) about lists, which are a more general data structure than stacks. A 
stack permits push and pop operations to be performed on the 
top of the stack; a 
list permits 
push and pop operations to be performed on each cell of the list (they are 
then called insert Table 1 Pcstack computers Company or basis Disclosure 
Delivery Relative computer name 
autea date Ancestry power 
References English Electric 
KDF 9 /60 4/63 Georgec ... AllmR62, DaviG60, Burroughs (Paoli, Pa.) HambC62 /6 1 AndeJ62 D825* D830* extended performance B 85OOe 4/66* developed at labora- D825 tory producing D825, 
0830 Burroughs (Pasadena, Calif.) 
B 5000 B 5500 B 6500 B 7500 Theory or language- 
based: IPL-VI EULER ALGOL I PL-vc Argonne Laboratory 
/62 1/67/ 20-30 2/63 11/64 1 /68/ /67 successor to B 5000 B 5500 based with improved multi- and 
shared-programmed mapping extended performance B 6500 language: IPL-IV, 
V language: EU LER(ALG0L 
+ ) 1anguage:ALGOL 1 /2 AllrnR62. BartR61, Bock R63, Ca rlC63, 1-1.78-1.98 LoneW61, HaucE68 5-6 10 ShawJ58 We beH 67, W I rt N 66a, b AndeJ61 language: IPL-V 
HodgD64 a First edition 
of manual, or 
a paper, or 
the appearance in Adoms Computing Characteristics Quarterly. 
hStill evolving. 
B 8501 was discontinued in 1968. .George, University of New South Wales, interpreter using Polish 
notation and a stack. Circa 1957 [Hamblin, 19621. dProduced for command and 
control (military) 
applications. *B 8500 IS a system name: the 
Pc is a B 8501. ‚Reported. Actual delivery unknown. 
p Dual processor. 2 57 
258 Part 3 I The instruction-set processor level: variations in 
the processor T.console - Mp(#0:7)k3 dc'(#A:B) IKio(#1:4)-S4 K-T(console; typewriter) - K-T(#I :2; card; reader)+ 
K-T(#1:2; paper tape; reader)+ K-T(card; punch)+ K-T(#I:Z line; printer)+ K-Ms(#I :2; drum) I K-Ms(#1:16; magnetic tape) 
- 'Mp(core; 4 ps/w; 4096 w; (48.3) b/w) 'Pc(stack; 12 b/syllable; 6 b/char; data: si,sf,bv,w,char. string; (I - 2) syllable/instruction; Mps(- 4 w) ante- cedents: 'ALGOL language; descendants; 
'6 5000, B 6500, B 7500; technology: transistor; -41961 ... 1963)) 'S(from: 2 Pc,~ K; to: 8 Mp; concurrency: 4) 4S(from: 4 Kio; to: KT,KMs; concurrency: 4) Fig. 1. Burroughs B 
5000 PMS diagram. Section 5 1 Processors with stack memories (zero addresses per instruction) 
and delete, respectively). Thus 
a list is like a nested set of overlapping stacks. 
EULER (Chap. 32) uses a stack 
to store temporary data and subroutine calls both when compiling and 
when interpreting the compiled program. However, the lan- guage-based machines can 
still be studied profitably with the 
stack in mind. The following comments will be 
directed to the P.stack com- puters manufactured 
by both English Electric and Burroughs. 
There are 
three basic P.stack 
computer families: B 
5000 + B 5500 4 B 6500/B 7500; D825 + D830 + B 8500; and KDF9. Each root member was made available at about the same time by Burroughs (Pasadena, Calif.), 
Burroughs (Paoli, Pa.), and English Electric. The 
IBM Corporation later 
responded with a proposed Pc.stack, 
but the machine never entered the produc- tion phase. The Pc.stack is a 
major alternative to the main 
line organi- 
zation of 1 address per 
instruction (augmented with index reg- 
isters or general registers). 
It tries to capitalize on the 
hierarchi- cal character 
of computation to avoid having to give memory shuffling instructions 
explicitly. In Chap. 3, page 64, we gave a comparison 
of a trivial computation using 
a stack 
and a general-register organization, 
in order to make clear the case .P~(#A)~-T.console- -Pc(#B)3-S,consoIe- L('Real Time Device)- K(#l :4)-C4-S-K(#l :4)-S-K(#1 - SET 'Mp((core; 1.2 us/w) ](thin film; .6 ps/w); 16 kw; 51 b/w) 'S(32 Mp; 4(Pc,K,S); concurrency: 4) 3P~(sta~k; technology: integrated circuits; .- 1969; data: sf,df,i,char.string, boolean vector, address integer; 4,6,8 b/char) 4~ ( 'Data Communi cat ions Processor) 
'Identical peripheral structures possible with two switches 6See Figures 3, 4, and 
5. 'Kio('Input/Output Mu1 tiplexor) 'Kio('Rea1 Time Adapter) Fig. 2. B 6500, B 7500 PMS diagram. 
Section 5 I Processors with stack memories 
(zero addresses per instruction) 259 -L- K-S b. 1 K for 2 Ms(disk) -L-KK7S(2K; 5x1 -L- K c. 2 K for 5 Ms(disk) ' L(to: Kio('Input/Output Multiplexor)) K('Disk Peripheral Controller) 'X := (-K('E1ectronics Unit)-S---Ms(#1:5)4) 
46 ms: (2161395) kby/s; 1 Fig. 3. Burroughs B 6500, B 7500 Ms (disk) PMS diagrams. for stacks. However, we did not there attempt any analysis. It has been asserted [Amdahl et 
al., 1964al that the Pc.stack derives its power only from its having 
some fast-working mem- 
ory in the 
Pc, thus that it is dominated by the general-register organization. Our own feeling is 
that the compile and compiled program execution times for 
the Pc.stack are indeed impressive. 
However, no definitive analysis has been 
published, as far as we know. Pcstack iscertainly an organization that rates serious study by any computer designer. The PMS structure of the examples The PMS structure diagram of the 
B 5000 and B 6500/B 7500 (Figs 1 to 5) should be compared with Burroughs own 
structure representation (Chap. 22, page 268). The 
D825 structure is similar; it is given in Chap. 36, page 447. All the Burroughs computers in Table 1 have the multiprocessor structure. Burroughs was probably the first 
computer company 
to take matters of the structure and organization seriously. The 
D825 hardware and software were designed for military command 
and control 
applications which demand very high uptime and availability. As various computer components 
in the 
structures fail, continuous operation is 
possible at a reduced 
level through the fail-soft 
design. However, to our knowledge, no published account exists on how well 
this design works in practice from a performance and 
reliability viewpoint. 
The philosophy and details of the D825 software and hardware are discussed 
in Chap. 36. The structures in the 
B 6500, especially, allow 
Kio's to be freely assigned 
to any T 
or Ms, thereby 
achieving better equip- 
ment utilization. The S(16 
Mp; 16 P) is probably overdesigned in the 
Burroughs B 6500 computers. These structures generally have a 
maximum 4(P + Kio), although the design is based on 16(P + Kio). The Kio's (Chap. 22) may be overdesigned, too, since a K capable of 
controlling a simple T.card,reader can also control a complex 
Ms.disk or Ms.magnetic,tape. The PMS structure of 
the English Electric 
KDF9 (Fig. 6) is fairly simple. The 
16 K's 
for direct memory 
access appear -L'-K2-S3- M #0:7; maqnetic tape; 9 - 144 kchar/s; 6lR b/char; 200155618001 1600 char/in; ,forward and reverse motion I a. 1 K for 8 Ms(rnagnetic tape) S(2 K; IO Ms)-Ms(#0:9: magnetic tape)- 
- - L-K L-Ki- b. 2 K for IO Ms(magnetic tape) - L- K S(4 K; 16 Ms)-Ms(#0:15; magnetic tape)- 
- L- K - - L- L- K c. 4 K for 16 Ms(magnetic tape) 'L (to: Kio ( ' I nput/Output Mu1 t ipl exor)) 'K('Periphera1 Controller) 3S(1K; 8 Ms; bus) Fig. 4. Burroughs B 6500, B 7500 Ms (magnetic tape) 
PMS diagrams. 
260 Part 3 1 The instruction-set processor level: variations 
in the processor -L'- K-T(console; keyboard, printer& -L __ K--(card; reader) t -L __ K-T(card; punch) --f - L - K - T(paper tape; 
reader) t - L __ K - T(paper tape: 
punch) * - L ~ K - T(CRT; display) + - L- K - T(line; printer) 
--f 'L (to: Kio( 
'Small Peripheral Control 
)) Fig. 5. Burroughs B 6500, B 7500 peripheral K-T PMS diagrams. to be both overdesigned (or overly general) 
and there are too few of 
them. The limit of only 16(T + Ms) components is small, 
especially considering that the 
KDF9 is to be time-shared from several consoles. The ISP of the examples The comparison of Pc.stack, 
Pc.laddress, and 
Pc.general,reg- isters (page 64) makes the assumption that an unlimited +--K(#I )-S-Ms (magnetic tape)- T(typewri ter)- T(paper tape)- 
'Mp(core; 6 ps/w; 4 - 32 kw; 48 b/w) 2S(16 Mp; 16(P,K); concurrency: 1) 3Pc(stack; 8 b/syllable; 0 - 1 address/instruction; 6 b/char; technology: transistor; data: syllable, char, w, bv, si, di, sf, df, hw; 1-3 syllables/instruction; operators: +, -, x, /, A, v, @,+ ichar.string1, Mp t stack, stack 
t Mp; Mps ('Subrouti ne Jump Nesting StoreCO; 7]<n: 1 ?> stack: 'Nest i nq Store[ 0: l5]<0: 47> a?;thmetic stack; 'Q-store[O:15]<0:17,18:31 ,32:48> &store is used for indexing, and contains 
a counter, an increment, and a 
modifier) 1 Fig. 6. English Electric 
KDF9 PMS diagram. Section 5 I Processors with stack memories (zero addresses per instruction) 
hardware stack 
resides in Pc. The B 5500 has a local 
M.stack in Pc of 4 words. The 
size and number 
of stacks, and their 
use by software, are 
most important. 
The IPL-VI machine has any number of 
stacks since the front of each list is a stack. The KDF9 
(Fig. 6) has two independent stacks: one for arith- metic expression evaluation and one for holding 
subroutine return addresses. The DEC 338 P.display (Chap. 25) uses a stack for storing subroutine return 
addresses. Unfortunately, we have not been able to include a discussion of the "cactus stack" of the B 6500, which 
is a data structure more like 
a list [Hauck and Dent, 19681. The Hauck and Dent paper describes 
both the 
relationship to a Pc.stack and its relevance to program mapping and memory management for multiprogramming. The C('D825) 
parameters are given 
in Fig. 7. The D825 
ISP differs from other Pc.stack computers in that the 
data, d, 
for operations can 
be in either of two 
places, the stack or Mp. 
Consider the unary or binary operations: 
C (' Burroughs D825; mu1 t i processor structure; S(cross-point; 16 M; Ib(Pc,Kio)) Mp(4.33 JLS/W; 65 kw; (48,l parity) b/w); S(cross-point; 4 Kio; 64 (T,Ms)); T(console, paper tape, printer, card, time, communication link); Ms(drum, disk, magnetic tape); 
Kio(#l :4); Pc(#1:2; 12 b/syllable; stack; 0 - 3 addresses/instruction; multiprogrammed; data: (integer, floating, single char- acter, fractional precision word, boolean vector); opera- tions: (+, -, x, /, A, v, @, 7, round, {si) c {sf), abs, negate, -abs) : instruction-size: (I - 7) syllable; operation-code-size: 5/12 syllable; address-size: (7/12 + 0 - 6) syllable; operation forms: (d3 t dl b d2, d2 tu dl); variable addresses: (stack, MpCsyllable + BAR],Mp[syllable + BAR + X[A] + X [E] + X [C]]); Mps ('Stack/S, Index Registers [I : 15]/X[I : 151, 'Index Comparison Limit Registers[1:151, 'Base Address Registers/BAR, 'Program Address Register/PAR, 'Program Counter/PC))) 
Fig. 7. Burroughs D825 PMS diagram. 
Section 5 I Processors with stack memories (zero addresses per instruction) 
261 d, tu d, d,tdlbd2 In either of these cases d,, d,, or d, can be the top of Stack/S; or Mp[Address + Base Address 
+ [Xindex registers [A,B,C]]]. This flexibility 
allows the Pc to behave as 
a 0, 1, 2, or 3 address per instruction processor. The 6 5000 is more conventional 
than the 
D825 in its use of stacks (see references, Table 1). There are 
only load 
and store (that is, push and pop instructions) to transfer data be- tween Mp 
and one stack. Actually, 
the B 5000 has several 
im- portant features that make it worthy of study: 1 The stacks. 2 Data-type specification. A data 
type is declared by placing a type identifier with the data. 
Thus, for example, there is one add operation 
for both 
fixed and floating point, the data telling which addition is to take place. 3 Multiprogram mapping. 
Descriptors are used to access variables (scalars, vectors, 
and arrays). This indirect 
addressing technique 
allows multiprogramming; how- ever, the reader should note that the data are not pro- tected against other 
accesses (corrected in the B 6500). Failure of the Pc.stack for character processing. The 
B 5000 has a character 
mode to allow processing of string data, and the stack is not used in this mode. In effect, a 
separate string processing ISP is incorporated 
in the 
Pc. Multiprocessing. A B 5000 can have two Pc™s. A command structure for 
complex information processing The IPL-VI (Chap. 30) is discussed in Part 4, Sec. 4 page 348 as a language-based processor. 
Microprogrammed implementation 
of EULER on IBM System/360 EULER (Chap. 32) 
is discussed in Part 4, Sec. 4 page 348 as a microprogrammed, 
language-based processor. 

Chapter 21 Design of an arithmetic unit incorporating a nesting storel R. H. Allmark / 1. R. Lucking Summary This paper describes the arithmetic 
unit of a computer whose order code is based on the Reverse Polbh algebraic notation. 
The order code has been realised 
by causing the arithmetic 
unit to operate on data stored in the most accessible registers 
of a nesting store; 
these registers 
are of the transistor flip-flop type but 
are backed up by sixteen fast magnetic core registers. The 
functions are performed 
as micro-programmes of trans- fers between the registers in the arithmetic unit, 
and the necessary arrange- ment of transfer paths, logical gates and 
arithmetic circuits is described. The number system is binary, using the two's-complement representation 
of negative numbers. Automatic floating-point operations 
are included which use an autonomous unit 
to perform the shifts required. introduction The arithmetic unit 
of a general purpose digital 
computer contains circuits to perform at least the basic operations 
of addition, sub- 
traction, multiplication and division. In many machines 
it is possi- 
ble to 
use some of the registers in 
the arithmetic unit 
as temporary storage for the partial results arising during a calculation; thus the accumulator of a one-address machine 
is used to store the result of the last arithmetic operation. The arithmetic unit de- 
scribed in this 
paper uses a nesting store, 
operating on the last- in-first-out principle, for the storage of its data and 
partial results. The nesting store consists of a stack of cells, of which only the most accessible supply data to the arithmetic unit, 
the results are automatically returned to the most accessible cells 
and the 
original operands erased, less accessible information being moved 
into the cells made vacant by 
the operation. The computer and 
its order code on the Reverse Polish algebraic notation, 
and contains four groups 
of operations: a b Transfers between the arithmetic unit 
and the main store. 
Arithmetic, logical and manipulative functions 
on data in the nesting store. 
Conditional and unconditional jump instructions used 
to interrupt the normal sequencing of instructions. Instructions for controlling the operation of the various peripheral devices which may be 
attached to the machine. c d Main store transfers include instructions for transferring 
half and full-length words to the most accessible 
cell of the nesting store, information 
already in the 
stack being 
retained by transfer to the 
less accessible cells. The contents of the most accessible 
cell of the stack may be stored in the main store; they are then automatically erased 
from the stack while information 
is moved from the less accessible cells 
to a more accessible position. 
Arithmetic operations also feature the transfer of data in the nesting store 
so that the operands are destroyed, the results are left in the most accessible cell (or cells), 
and data 
not involved in the operation are moved to fill any vacated 
cells. Thus the programme for evaluating f = (a - b)/(c + de) may be written: fetch a, fetch b, subtract (forming a - b in the most accessible 
cell The arithmetic unit 
is part of a general purpose 
synchronous system, working in the parallel mode, 
with main core storage of (up to) 32, 768 48-bit words, and provision for 
the time sharing of up to 4 programmes. The order code of the computer is based 'Proc. IFIP Congr. 62, pp. 694-698, 1962. and erasing both a and b from the stack), fetch d, fetch e, (forming de in the most erasing d and e, and thus leaving 
a - b in the second most accessible cell), 262 
Chapter 21 I Design of an arithmetic unit incorporating a nesting store 
263 fetch c, add (forming c + de) divide (forming 
f), store as f(1eaving the nesting store 
in the same state as before the fetch a instruction). For instructions, the 48-bit word has 
been divided into 6 sylla- bles of eight bits 
each, and these are then treated 
as a 
continuous sequence of variable length instructions. Arithmetic operations 
are specified by single syllable 
instructions, but main store transfers require three 
syllables to accommodate both 
the address and the address modifying information of the word to which they 
refer; jump instructions also have three 
syllables. Two-syllable 
instruc- tions include 
the peripheral transfers, and instructions for process- 
ing address modifiers and performing shifts. The first syllable of every instruction contains 
two bits whose values specify 
the length of the instruction; the redundant case being used to differentiate between main store transfers 
and jump instructions. The first syl- lable of an instruction contains enough information 
to specify any arithmetic unit 
operation required; thus in 
the machine, each instruction is treated by two controls; the first or Store Control organising the fetching and storing of information in advance 
of the second or 
Arithmetic Unit Control which completes the in- struction on the information in the first syllable. Range of functions The allocation of bits to the 
instructions described above 
allows 64 possible functions, of which 59 are used to specify the wide range of operations needed in a 
general purpose 
computer. As well as the normal single-length fixed-point 
arithmetic oper- ations, functions 
have been 
provided for the addition and subtrac- tion of double-length numbers. 
These simplify the programming of multi-length operations 
as well as giving 
increased accuracy. For normal scientific and engineering calculations automatic float- ing-point facilities are available. A single length word may repre- 
sent a floating-point number with 
a 40-bit fractional 
part f, and an 8-bit characteristic c; the value of the number is then f2c-128. The fractional part is limited to the range -1 5 f < -y2, or 1 > .f 2 y2, or f = 0 when c is also zero. All floating-point opera- tions assume that operands are in this standard form and give correctly rounded results in 
standard form. Functions 
for the addi- tion and subtraction of double-length floating-point numbers have 
been provided, as these give increased accuracy and stability in many matrix operations. An increase in operating speed and a saving 
of instructions are effected by 
the use of instructions which 
re-order the position of information in the most accessible 
cells of the nesting store, in- 
cluding reversing 
and cycling operations. 
The normal logical oper- ations are provided. All arithmetic operations in the arithmetic unit 
are carried out 
on binary numbers using the two™s-complement notation for nega- tive numbers; instructions 
being provided for the conversion to and from binary of information stored 
as 6-bit characters in other radix systems. 
For the convenience of the programmer, double- 
length numbers are stored in the arithmetic unit with their 
more significant half in a 
more accessible cell; the sign of the less sig- 
nificant half is ignored and is set positive 
after all double-length 
operations. The nesting store 
Although the concept of a nesting store 
is similar to that of a rifle magazine where the addition of a cartridge displaces those already 
there, movement of information only occurs in the three most accessible cells 
of the nesting store, which 
are transistor flip-flop registers forming part of the arithmetic unit. 
The less accessible cells are core registers which are addressed in a 
sequential manner 
by a reversible counter. Reading from these cores reduces the count by one, thus selecting 
the next word; the read-out is de- structive so that the cores are in the correct state for a 
subsequent writing operation, which is the reverse of a read. The access time of the cores is reduced by providing separate counters 
and reading and writing mechanisms 
for the odd and even numbered rows of cores; thus when 
reading or 
writing from odd rows the addressing mechanism for the next even row is set, so that it is available for immediate use. Thus with 
a simple one core per bit system suc- 
cessive reads can be made at 1 pec intervals and writes at 2 pec intervals; as these operations are performed in parallel with the functioning of the arithmetic unit, their 
times do not 
increase the time required to complete 
the functions. The arithmetic unit 
As shown in Fig. 1, there are six full length transistor 
flip-flop registers in the arithmetic unit; there 
are also two 8-bit registers used when performing floating-point operations. 
The main facili- ties associated with these registers are as follows. W1, W2 and W3 are the three most accessible cells 
of the nesting store; transfers 
to the core part of the nesting store, 
being 
264 Part 3 I The instruction-set processor level: variations 
in the processor MAIN TRANSFERS 
A.U CONTROL PULSES COUNTER ./ / I I #™ # t _- I SET FROM ONES ICLfAR 1 STORE CONTROL TO STOPE CONTROL CLEAR AUXILIARY TRANSFERS AND SHlFTS RIGHT SHIFTS OF 0,1,2,S.8 OR-8 CHARACTERISTIC MODIFIER Fig. 1. Block diagram of the arithmetic unit. 
Full lines represent 
infor- mation transfers; dotted lines represent 
control pulses. All registers are 48-bits long 
unless otherwise stated. 
made via W3. W1 and W2, together with 
B1 and B2, form a double-length shifting register 
which may 
be used as two inde- pendent single-length shifting registers. 
B1 and B2 are the inputs to the 48-bit adder whose output may be routed to W1, W2, or to the 
characteristic difference 
register CD. The adder 
contains 13 carry-skip stages which 
reduce the 
carry propagation time to a 
maximum of 150 nsec. Subtraction is per- Section 5 I Processors with stack memories (zero 
addresses per instruction) formed by adding 
the minuend™s complement to 
the subtrahend with a carry inserted 
into the 
right-most adder stage. Nb acts as a buffer between store control and 
the arithmetic unit, and together 
with B1 and B2, is used in nearly 
every function. Arithmetic unit control 
interprets each 
instruction as a se- quence of timed pulses along lines which 
activate the various transfers etc., between the 
registers. The sequences have 
been constructed so that many operations 
are performed simultaneously, 
reducing the overall time to a minimum; thus 
the function sin- gle-length fixed-point add is performed by: i Transferring W1, W2, W3 to B2, B1 and Nb respectively, simultaneously commencing a read 
from the nesting store, 
clearing the carry inserted 
into the right-most adder stage and switching 
the adder™s output to W1. Adding and simultaneously transferring Nb to W2. ii Each step takes 0.5 psec and by 
the end of the last step, W3 has been refilled from 
the core nesting store. 
To speed up multiplication and 
division, these functions 
are carried out in a 
separate unit 
employing the stored carry principle, 
but the results are finally assimilated within the arithmetic 
unit. A similar arithmetic unit 
operating only on single-length 
num- bers could 
be designed using only four full-length registers. 
At least five registers are required to 
perform the function which inter- changes the contents 
of the two most accessible cells 
in the nesting store with those of the next most accessible 
pair. The sixth register 
enables all double-length arithmetic operations to be performed without writing information back 
into the nesting during the 
func- tion; this would 
have complicated 
the sequences and increased the time for the functions. When determining 
the arrangement of transfer paths between the various registers, 
it was found sufficient to consider only 
the double-length functions 
which required complicated 
or lengthy sequences; in particular the function for adding two double-length Hoating numbers had great influence. An overflow indication is set 
on fixed-point addition and sub- traction if the sign of the result differs from 
that expected, and 
on floating-point operations if the characteristic exceeds the maximum allowable; 
shifting may also cause 
overflow. Shift control Shifting operations are effected by transfers 
between W1 (and/or W2) and B1 (and/or B2), and back again. 
The shift transfer 
paths from the W to the B registers provide right 
shifts of 0, 1, 2, 5 
Chapter 21 1 Design of an arithmetic unit 
incorporating a nesting store 
265 or 8 places, and a 
left shift 
of 8 places; the paths from the B to the W registers provide the same shifts 
in the reverse direction. The two 
sets of shift paths are 
used alternately, those from the W registers being used first; all shifts 
are terminated 
using a path into the W registers. Shifts 
of a large number 
of places are accom- plished by 
a series of shifts of eight places in 
the appropriate 
direction until the number of places remaining is less than eight; if necessary the number is then transferred back 
into the W regis- ters: the remaining shifts, or 
the whole shift 
if the number of places is less than eight, is then completed by a transfer to the 
B registers and back again using 
two appropriate 
paths. With the shifts avail- 
able, extension of the B registers by 
two bits at 
the right-most end enables any shift 
to be 
performed without loss of accuracy. In double-length arithmetic shifts, the sign digit of the less sig- nificant word 
is by-passed. When a shift is to be performed, the number of places and 
the type of shift are transferred 
into a semi- autonomous unit, called the shift control, which is then supplied with a string 
of command pulses by the arithmetic unit control; 
shift control then re-routes these 
pulses to perform the transfers necessary to obtain the shift. When performing floating-point 
addition and subtraction, shifts are required to 
equalize the characteristics of the two numbers; the amount of shift is calculated by a 
modified subtraction, oper- 
ating on the characteristic 
positions of the two numbers. 
After the addition, the shift required to restore the result to standard form is determined by logical 
circuits which 
interpret the pattern 
of bits in W1 into shift information. The number of shifts performed during this standardising operation 
is made available to 
the arith- metic unit control 
for use in forming the correct characteristic 
of the result. The character 
conversion operations to, and 
from, binary are 
accomplished by shift 
control, using a method 
involving successive 
shifting of the character 
word, and adding or subtracting portions of the radix word. 
Examples of sequences To illustrate the working of the arithmetic unit, 
two sequences 
are described. 
a -D, (i.e. subtract the 
double-length fixed-point number in 
W1 and W2 from the number in 
W3 and the 
most accessible 
core register of the nesting store). Transfer W1, W2, 
W3 to B2, B1 
and Nb respectively, simultaneously reading from the core nesting store. 
i ii A dummy pulse. iii iv 2) vi Transfer the complement of W2 to B2 (but setting the 
sign of B2 positive), transfer W3 directly to B1 (W3 has by now 
been filled with fresh data), switch the adder™s output to W2, inserting a carry 
into the right- most adder stage, and read 
from the nesting store. 
Add. Transfer the complement of W1 to B1 and Nb to B2, switch the adder™s output to W1 and insert a carry 
into the 
right-most adder stage if W2 is negative. Add, simultaneously 
clearing the sign of W2. b + F (i.e. add 
the two single-length floating 
numbers in 
W1 and W2). i ii iii iv 21 vi vii viii ir X Transfer the complement of W1 to B1, transfer W2 to B2 and switch the adder™s output to register CD. Store the characteristic of W1 in the eight-bit register C and add. 
Clear the 
characteristic positions of W1, simultane- ously transferring CD into the shift number register in shift control. This latter operation is such that the 
shift register 
contains minus the difference in charac- teristics. Clear the characteristic of W2, and if W1 is about to be 
shifted, determined by the sign digit of CD, replace the contents 
of C by the characteristic of B2; thus C contains the larger Characteristic. Supply control pulses to shift control and thus 
perform the required right-shift of eight W1 or W2. Having completed 
the shift, transfer 
W1, W2 and W3 to B2, B1 and Nb respectively, simultaneously switch- ing the adder™s output to W1, clearing the carry into 
the right-most adder stage and reading from the core- nesting store. 
Add the fractional parts, 
simultaneously transferring Nb to W2. Supply control pulses to shift control so as to cause it to enter the 
standardization procedure 
and perform the shifts required. Store the complement of the number of left-shifts performed in (viii) in the characteristic position of B2, transfer C to the characteristic position of B1, switch the adder to W1. Perform a special add operation 
which only affects 
the characteristic positions of W1. The sum is thus formed in W1. Rounding the answer is carried out using two special control 
pulses which 
complete all floating- 
point operations, these call 
up logic to deal with the cases when the rounding operation 
necessitates re-standardization of the re- sult. 
266 Part 3 1 The instruction-set processor level: variations 
in the 
processor Conclusions The advantages of a machine incorporating 
a nesting store 
in the arithmetic unit 
are:- i ii The machine is simple to programme using the machine language. Programmes are faster, since many main store 
transfers are eliminated, and the access time of the nesting store 
is virtually zero. They are 
more compact because less infor- mation is required to 
specify many instructions. Section 5 1 Processors with stack memories (zero addresses per instruction) 
iii As the operation of the arithmetic unit 
is largely inde- 
pendent of the main store, their controls may readily 
be separated. This allows store control 
to process instructions whilst the arithmetic unit 
control processes a 
prior instruc- 
tion, thereby leading 
to faster execution 
of the programme. The main disadvantage 
is an increase in the order of complexity involved. References AllmR62; DaviC60; HaleA62 
Chapter 22 Design of the B 5000 system1 William Lonergan / Paul King Computing systems have conventionally been designed via the ‚hardware™ route. 
Subsequent to design, these systems have been handed over to programming systems people for the development of a programming package 
to facilitate the use of the hardware. In contrast 
to this, the B 5000 system 
was designed from the start as a 
total hardware-software system. 
The assumption was made that higher level programming languages, such as ALGOL, should 
be used to the virtual exclusion of machine language programming, 
and that the system should largely 
be used to control its own 
operation. A hardware-free notation 
was utilized to design a proc- 
essor with the desired word 
and symbol manipulative capabilities. 
Subsequently this model 
was translated into hardware 
specifica- tions at which time cost constraints were considered. Design objectives 
The fundamental design objective of the B 5000 system 
was the reduction of total problem through-put time. 
A second major objective was facilitation of changes both in programs and system configurations. Toward these objectives 
the following aspects of the total computer utilization problem were 
considered: Statement of problems in higher-level machine-independent languages; efficiency of compilation of machine language; speed 
of compilation of machine language; program debugging in higher- 
level languages; 
problem set-up 
and load time; efficiency of system operation; ease of maintaining and making changes 
in existing programs, 
and ease of reprogramming when changes are made in a system configuration. 
Design criteria 
Early in the design phase of the B 5000 system 
the following principles were 
established and adopted: 
Program should 
be independent of its location and unmodified as stored at object time; 
data should be independent of its location; addressing of memory within 
a program should 
take advantage 
of contextual addressing schemes 
to reduce redundancy; provisions ‚Datamation, vol. 7, no. 5, pp. 28-32, May, 1961. should be made for the generalized handling 
of indexing and subroutines; a full 
complement of logical, relational and control operators should be provided to enable efficient translation of higher-level source languages such as ALGOL 
and COBOL; pro- gram syntax should permit an almost mechanical translation 
from source languages 
into efficient machine code; 
facilities should be provided to permit 
the system to largely control its own 
operation; input-output operations should be divorced from processing and should be handled by an operating system; multi-programming and true parallel processing (requires multiple 
processors) should be facilitated, and changes in 
system configuration 
(within certain broad limitations) should not require reprogramming. 
System organization 
The B 5000 system achieves its 
unique physical and operational modularity through 
the use of electronic switches which function 
logically like telephone crossbar switches. 
Figure 1 depicts the basic organization of the system as well as showing 
a maximum system. Master control program 
A master control program 
will be provided with the B 5000 system. 
It will be stored on 
a portion of the magnetic drum. 
During normal operations, a small 
portion of the MCP 
will be contained in core memory. This portion 
will handle a large percentage 
of recurrent system operations. Other segments of the MCP 
will be called in from the magnetic drum, 
from time to time, as they are required 
to handle less frequently-occurring events, or system situations. 
Whenever the system is executing the master control program, 
it is said to be 
in the Control State. 
All entries to the Control 
State are made via ‚interrupts.™ A special operation 
is provided, which can 
only be executed when 
the system is in the Control State, to permit control to return to the object program 
it was executing at 
the time the ‚interrupt™ occurred. 
The following are a few 
typical occurrences 
which cause 
an automatic ‚interrupt™ in 
the system: An input-output channel 
is 267 
268 Part 3 [ The instruction-set processor level: variations in 
the processor Section 5 1 Processors with stack memories (zero addresses per instruction) 
Fig. 1. Organization of the B5000 system. available, an input-output 
operation has been completed 
or an indexing operation was attempted which violated the storage protection features built 
into the system. In addition to 
processing interrupt conditions, the master 
con- trol program handles fundamental parts 
of the total system opera- tion such as the initiation of all input-output operations, tanking 
of input-output areas when required, file control, allocation 
of memory, scheduling of jobs (priority ratings, 
system requirements of each object program, 
and the present system configuration 
are considered), maintenance 
of an operations log and maintenance of a system description. Operating modes The B 5000 can either operate 
with fixed-length words 
or with variable-length fields. These two 
modes of operation are called the word mode and the character mode. For certain operations, a 
processor operating on words 
is most desirable and 
for other opera- tions, a variable 
field length mode of operation is most 
desirable. By combining both abilities in 
one processor, a processor can operate in the mode most 
desirable for the operation at hand. In a B 5000 system, it is even possible 
for one processor to be 
operat- ing in the word mode 
and the other in 
the character mode. When operating in the word mode, 
a standard format for the data word is used as illustrated in 
Fig. 2. Note that the 
standard word is an octal floating point word. 
However, the mantissa is treated as an integer rather than 
as a fraction (heretofore 
the reverse has been common practice). This provides two benefits: first, an 
integer has 
the same internal repre- 
sentation as its unnormalized 
floating point correspondent; 
and, second, the range of numbers that can be expressed, rather than being from S+64 to 8-63, is 8+76 to S-51. The first feature eliminates 
Chapter 22 1 Design of the B 5000 system 269 First Char- acter Integer Part Second Third Fourth Fifth Sixth Seventh Eighth Char. Char- Char- Char- Char- Char- Char- acter acter acter acter acter acter acter F-Flag (1 bit) SE-Sign of Exponent (1 bit) Exponent (6 bits) Fig. 2. Data word 
- word mode. 
SO-Sign of Operand (1 bit) Integer Part 
(39 bits) the need for fixed-to-floating 
point conversion; integers and 
floating point numbers can 
be mixed in arithmetic calculations. The second expands the range where 
trouble with range 
is most often 
en- countered, namely, in numbers with 
extremely large 
magnitude. The flag serves a dual 
purpose. The function of the flag depends on how the program references the data word. If the data 
word is a single variable and not 
an element of an array, the flag identi- fies the word as being operand, 
that is, a data word. If the word is an element 
of an array, the flag may be used to identify 
this particular element 
as an element of data which is not to be 
proc- essed by 
the normal program (for example, a boundary point in 
mesh calculations). When operating in the character mode, each data word consists 
of eight alphanumeric characters 
as illustrated in 
Fig. 3. Programs in the character mode can address any character in a 
word. Fields 
can start at any position in a word. A processor in a 
single opera- tion can operate on fields 
of any length 
up to 
63 characters long; operations on fields 
of greater length can 
easily be programmed. For example, two 57 character fields could be compared in a 
single operation. There are two instances when the character mode operates with 
words of the type used in the word mode. 
Operations are provided in the character 
mode for converting numeric 
information in the alphanumeric representation to the 
standard word type of the word mode 
and vice versa. 
In both 
of these instances, the length of the alphanumeric fields being converted 
to or from 
the word mode type of word can be no greater than eight characters long. Again, conversion 
of fields of greater length can 
easily be pro- grammed. The purpose of the word mode 
is to provide the advantages of high-speed parallel operations, 
floating-point abilities and the inherent information density possible in a binary machine. 
In the first case, it is economically feasible to provide parallel operations 
in a word machine; the cost of parallel operations 
on variable length fields would be prohibitive. In the last case, a given size 
memory can contain 
over twenty percent 
more numeric 
informa- tion if that information is expressed in binary 
rather than 
binary- coded decimal, and 
over eighty percent more information 
than can be 
expressed in six-bit alphanumeric representation. The purpose of the character 
mode is to provide editing, scan- ning, comparison 
and data manipulative abilities (although addi- 
tion and 
subtraction are 
also provided). The type of editing facili- ties provided obviate the need for the artificial ﬁadd-shift-extract- storeﬂ type 
of editing. For 
example, operations are provided 
for generalized insertion of editing symbols (such 
as blanks, decimal points, floating 
dollar signs, etc.) and for the substitution or sup- pression of any unwanted characters. For 
those interested in the new area 
of Information Processing Languages, 
the character mode is particularly well suited to list structures. Program organization 
Programs in the B 5000 are composed of strings of syllables. A syllable is the basic unit of the program and is twelve bits in length. The term ﬁsyllableﬂ is used rather than 
instruction to distinguish it from conventional single-address or multi-address instructions. Each 
program word 
contains four syllables 
and they 
are executed sequentially in a left-to-right order within 
the pro- gram word, 
and sequentially by 
word. Branching is allowed to any syllable within a 
word. Before 
delving into 
some of the details of the internal operation of the B 5000 processor, it is necessary to discuss stacks, Polish 
notation, and 
the Program Reference 
Table. The stack The internal organization of single-address computers forces the wasting of both programming and running time for the storage and recall 
of the intermediate 
results in the sequence of compu- tation. The data 
must be placed into 
the proper 
registers and memory cells before the operation can be executed, and 
their contents must often 
be completely rearranged 
before the next operation can 
be performed. Multi-address computers are 
con- structed to make the execution of a few selected operations 
more efficient, but at the 
expense of building inefficiencies into all the rest. Automatic programming aids 
attack this problem indirectly: they relieve the programmer of the need to laboriously code his 
270 Part 3 1 The instruction-set processor level: variations in 
the processor Executed way around machine design, 
but they 
still must provide 
object coding to 
accomplish the storage and recall 
functions. In brief, conventionally designed 
computers, with 
or without automatic programming aids, require the wasteful expenditure of program- ming effort, memory capacity, and 
running time to overcome the limitations of their internal 
organization. The problem is attacked directly in 
the B 5000 by incorporation of a ﬁpushdownﬂ stack, 
which completely eliminates 
the need for instructions (coded 
or compiled) to store or recall intermediate results. In a B 5000 processor, the stack is composed of a pair of regis- ters, the A and B registers, and a memory area. As operands are 
picked up by the programs, they are placed in 
the A register. If the A register already contains a 
word of information, that word is transferred to 
the B register prior to loading the operand into the A register. If the B register is also occupied by information, then the word in B is stored in a 
memory area defined by 
an address register 
S. Then the word in A can be transferred to 
B and the operand brought into 
the A register. The new word coming 
into the stack has 
pushed down the information previously held in the registers. As each pushdown occurs, 
the address in the S register is automatically increased 
by one. The information con- tained in the registers is the last information entered into the 
stack; the stack operates 
on a ﬁlast in-first outﬂ principle. As information is operated on in the stack, operands are eliminated from the stack and results of operations are 
returned to the 
stack. As information in the stack is used up by operations being performed, it is possible to cause ﬁpushups,ﬂ i.e., a word is brought from the memory area addressed by 
the S register, and the 
address in the S register is decreased by one. 
To eliminate unnecessary pushdowns 
and pushups, the A and B registers both have indicators 
used for remembering whether the registers contain information or are empty. 
When an operand is to be placed in 
the stack and either 
of the registers is empty, no pushdown 
into memory occurs. 
Also, when an operation leaves one or both of the registers empty, no automatic pushup occurs. Polish notation 
The Polish logician, 
J. Lukasiewicz, developed a notation which 
allows the writing of algebraic or logical expressions which do 
not require grouping 
symbols and operator precedence conventions. 
For example, parentheses are 
necessary as grouping symbols in the expression A(B+ C) to convey the desired interpretation of the expression. In 
the expression A + B/C, the normal interpretation is A + (B/C), rather than 
(A + B)/C, because of the convention that Section 5 I Processors with stack memories (zero addresses per instruction) 
the / operator is of higher precedence than 
the + operator. The right-hand Polish notation used in the B 5000 is based on 
placing the operators to 
the right of their operands: A + B becomes AB+ in Polish notation. A + B + C can be 
written either 
as AB + C + , or as ABC+ +. In the expression ABC+ +, the first + operator says to add 
the operands B and C. The second + operator says to add A to the sum of B and C. Returning to 
the first examples above, A(B + C) can be written 
as BC +AX or ABC + x in Polish. The second example is written as BC/A+ or ABC/+. The exten- sion of Polish notation to handle equations is shown in the follow- ing example: 
Conventional notation 
Z=A(B-C)/(D+E) Polish notation ABC - x DE + /Z= The stack in use To illustrate the functioning of the stack, two simple examples 
are shown in Figs. 4 and 5. In the 
examples, the letters P, Q and R represent syllables in the program that cause the operands P, 
Q, and R to be picked 
up and placed in 
the stack. The symbols + and x represent syllables that cause the add and 
multiply operations to occur. The two examples represent different ways of writing P(Q+R) in Polish notation. The first example in Fig. 4 does not require 
pushdowns or pushups. The second example, 
shown in Fig. 5, requires a 
pushdown in the execution of the syllable R, and a pushup in 
the execution of the syllable x. The columns in the table represent the contents of the various registers 
after execution of the syllable listed in the first column. Independence of addressing One of the goals set in the design of the B 5000 was to make the programs independent of the actual memory locations 
of both the program itself and the data, in order 
to provide really automatic Polish Notation QR + P x ~ ~- Fig. 4 
Chapter 22 1 Design of the B 5000 system 271 Syllable Executed P Q Pushdown Execute R Polish Notation PQR + x Contents of Register A Register B Register S Cell 101 100 - P 100 - P Empty Q Empty Q 101 P R Q 101 P X 100 Fig. 5 program segmentation. Through 
automatic program segmentation, it is possible to have program size 
practically independent of the size of core memory. The systems analyst or programmer intending 
to do multi-processing is then no longer faced with 
the difficult task of planning what jobs are to be 
run together in order 
that system storage capacities 
are not exceeded. In achieving independence 
of addressing, a solution 
requiring large contiguous 
areas of memory was not deemed satisfactory. Each segment 
of the program and each data area should be com- pletely relocatable without 
modification to the program. It is then possible to load all the segments of a program or programs 
onto the drum at load time and call in 
the segments to any available 
space in core memory as needed during run time. 
If some segment of a program 
is overlaid by a 
subsequent segment 
of a program, the segment of the program destroyed in core memory is still available on the drum to be called in 
again if needed. Due to the 
very high 
program densities in the B 5000, the availability of high capacity drum storage on every system and automatic segmentation, a minimum 
B 5000 system has 
the capa- 
city for a program or programs 
equivalent to approximately 40,000 to 60,000 single address 
instructions. Of course, if an installation normally ran such large programs, 
the system would very likely 
not be a minimum system. 
However, the installation having an 
occasional need to run very large programs is not prevented 
from doing so by storage 
capacity. Processing speed now becomes a 
function of the size of core memory. If large programs 
are run in 
a system 
with small core memory, time will be consumed in recalling 
program segments from drum to core. If the core memory is expanded, less time will be spent 
in such activity and the program or programs 
will be speeded up, 
and no reprogramming 
is required. Program reference table 
The means of achieving independence of addressing in the B 5000 is called a Program 
Reference Table (PRT). The PRT is a 1,025 
word relocatable area 
in memory used primarily 
for storing con- 
trol words that locate data areas or program segments. There are also control words for describing input-output operations. These control words, called descriptors, contain 
the base address 
and size of data areas, program segments 
and input-output areas. A descrip- tor specifying an input-output operation also contains the desig- nation of the unit to be 
used and the type of operation to be performed. Operands may 
also be stored in the PRT, providing direct access to single values such as 
indices, counts, control 
totals, etc. In the 
word mode of the B 5000, every item of data is con- sidered to be either 
a single value 
or an element of an array 
of data. If it is a single 
value, it will be obtained directly by indexing 
a descriptor contained in 
the PRT. Program segments 
are described by program descriptors. 
In addition to core base 
address, the program descriptor contains 
the location in drum storage 
of the program segment 
and an indication if the program segment is currently in core 
memory starting at the address specified in the descriptor. Entry to a program segment 
is made via its program descriptor 
contained in the PRT. If the program segment 
is in core 
memory, entry will be made to the program segment. However, when entry 
is attempted to a program 
segment whose descriptor indicates 
that the 
segment is not in core memory, automatic entry to 
the Master Control Program will occur and the 
desired segment will then be brought in 
from the drum. 
Notice that in moving from one segment 
to another, it is not necessary to know whether the segment to be 
entered is currently in core 
memory. Branching within 
a program segment is self- relative, i.e., the distance to jump either forward or 
backward is specified, not the address to be 
jumped to. As a result 
of keeping all actual addresses of data and program in the PRT, the program itself does not contain any 
addresses, but only references to the PRT. To specify one of the 1,024 posi- 
tions in 
the PRT requires 
only 10 bits which contributes greatly 
to the high program density achieved in 
the B 5000. Since the PRT is relocatable, references 
to the PRT contained 
in the pro- gram are to relative locations, thus completely freeing the program from any 
dependence whatsoever on actual memory locations. 

272 Part 3 I The instruction-set processor level: variations 
in the processor Section 5 1 Processors with stack memories (zero addresses per instruction) 
Word mode program For (3), indexing of the descriptor by 
the item that is now the operand is obtained from the indexed address; 
for the descriptor action is after the indexing. In the case of (4), subroutine entry 
occurs to the subroutine addressed. A word of the three 
previous types may be left in the The word mode 
of the B 5000 processor has 
four types of syllables, second item in the stack occurs. For an 'Perand sY1lable, the The syllable is distinguished by the two high-order bits 
of each 12-bit syllable. The types of syllable and the identification bits are: 
00-Operator Syllable 
01-Literal Syllable 
10-Operand Call Syllable 11-Descriptor Call Syllable The first of these, the operator syllable, causes 
operations to 
be performed. The remaining ten bits 
of the operator syllable are the 
operation codes. There are approximately 
sixty different operations in the word mode. 
For those operations requiring 
an operand 
or operands, the processor checks 
for sufficient operands in 
the regis- ters; if they are not 
there, pushups from 
the stack in 
memory occur automatically. The literal syllable is used for placing constants 
in the stack to be used as 
operands. The ten bits 
of the literal syllable are transferred to 
the stack. This allows 
the program to contain 
inte- gers less 
than 1,024 as constants. The operand call 
syllable, and the 
descriptor call syllable ad- dress locations 
in the program reference table. The 
purpose of the operand call syllable is to place an 
operand in 
the stack; the purpose of the descriptor call syllable is to place the address of an operand, a descriptor, in 
the stack. There are 
four situations that arise, depending on the word read from the program reference table. 1 2 The word is an operand. The word is a descriptor containing 
the address of the operand. The word is a descriptor containing 
the base address 
of the data area 
in which the operand resides. The word is a program descriptor containing 
the base ad- 
dress of a subroutine. 
3 4 For (l), the operand call 
syllable has 
completed its 
action by placing an operand in 
the stack. The descriptor call 
syllable will 
cause the construction of a descriptor 
of the operand, replacing 
the operand by 
the constructed descriptor. 
For (2), the operand 
call syllable then reads the operand from the cell addressed. The descriptor call 
syllable has 
completed its 
action. registers upon return from the subroutine, in which instance the actions described 
above will 
take place, depending upon 
the type 
of syllable which 
initiated the 
subroutine. Essentially, the four types 
of action that occur for an 
operand call syllable are obtaining an operand directly, indirectly, 
from an array, or by computation. Sometimes in the use of the call syllables, it is not known which 
type of action will occur for a particular syllable when 
the program is created. This is particu- larly true for call syllables in subroutines. Programs in the word mode consist 
of strings of syllables which 
follow the rules of Polish notation. Variable length strings of call syllables and literal syllables, which place items of information in the stack, are followed by operator 
syllables which perform 
their operations on information in the stack. The indexing features of the B 5000 allow generalized indexing and at the 
same time provide complete storage protection. 
Data areas and program segments 
of different programs may 
be inter- mingled, but a program is prevented from storing outside of its data areas. The method 
of indexing allows 
any of the 1,024 words 
of the program reference table to be 
considered index registers. Multilevel indexing 
is provided, i.e., indices of arrays can them- 
selves be elements 
of arrays. The subroutine control provided 
in the B 5000 allows nesting of subroutines-even recursive 
nesting (a subroutine is a subrou- tine of itself)-arbitrarily deep. Dynamic allocation 
of storage for parameter lists and temporary working storage simplify the use of subroutines. Storage is automatically allocated and deallocated 
as required. Character mode program In the character mode of the B 5000 Processor, there is only one 
type of syllable, called the operator syllable. Program segments 
in the character mode are constructed of strings of these syllables. The character mode is designed to provide editing, formatting, 
comparison, and other 
forms of data manipulation. In doing 
so, the processor uses two areas 
of memory-the source 
and desti- nation areas. When a program switches from word mode 
to char- acter mode, two descriptors containing 
the base addresses 
of these areas are supplied. The source area or destination area 
may be 
Chapter 22 1 Design of the B 5000 system 273 changed at any time during character 
mode so that the program may act on several areas. 
parts; the last part specifies the 'peration to be performed and Conclusion The Burroughs B 5000 system has 
been designed as an integrated 
in the memory space required to store equivalent object 
programs; The character mode 'perator is into two hardware-software package which offers such benefits as savings the first part 'pecifies the number Of times the 'peration is to be multi-processing and parallel processing; and identical performed. Operations are 
provided for the transferring, deletion, 
comparison, and insertion 
of characters or bits. Also, there are 
operations which allow 
the repetition of syllable strings. This is quite useful for complex 
table look-up operations and for 
editing information which contains 
repeated patterns. 
programs on systems with different size memories and different system configurations with no loss in individual system References LoneW61; BartR61; BockR63; 
CarlC63; MaheR6l 
Section 6 Processors with mu I ti programm i ng The processors 
in this section have features which allow 
multi- ple programs to exist in the primary memory at the same time. The programs can 
be executed alternately by a single processor without having 
to wait for new programs to be input. The cost is only that of changing 
the processor state, which involves 
only a few instructions at most (and only one instruction on some systems, such as the CDC 6600). Since programs are subject 
to numerous unpredictable delays within a single run for inter- change with the external environment (either via Ms or T), substantial increases in Pc utilization can 
be achieved by 
multi- programming. If more than a single processor has 
access to Mp, the system is called a 
multiprocessor system. Time-shared computers are generally 
multiprogrammed. Alternatively, time-shared 
systems can be implemented by swapping programs, one 
at a time, into primary memory for interpretation. The Berkeley Time-sharing System (Chap. 24) uses both multiprogramming and program 
swapping. The Burroughs B 5000 (Chap. 22) is an early computer to have multiprogram capability. The idea of multiprogramming is so fundamental that it should be 
among the first concepts to be understood by the student of computing systems. A 
very nice review of memory mapping and storage allocation is presented 
in the paper Dynamic Storage Allocation Systems [Randell and Kuehner, 19681. Atlas The Atlas is one of the most important machines described in this book. The prototype was originally designed and con- structed at Manchester University. The Atlas 1 and Atlas 2 were produced by Ferranti Corp. (prior to becoming part of 1.C.T.l). Atlas 1 is the 
most interesting; it incorporates most of the features of the Atlas prototype. 
The Lincoln Laboratory 
TX-2 [Clark, 19571 
influenced some Atlas features: multiple index registers and interrupt processing of input/output devices. Atlas' detailed 
internal structure 
is described in a paper 
[Sum- ner et 
al., 19621. 
International Computers and Tabulators, U. K. Two original features, one-level storage and extracodes, have 
been copied in many other machines. A one-level 
store is 
com- mon to most new computers which are 
time-shared or 
multi- programmed; the scheme for memory paging in the SDS 940 is essentially 
that of Atlas. 
The extracodes 
feature allows ordinary machine operation codes to be used 
to call subroutines. 
Commonly used complex instructions (such as sin, cos, and monitor calls) can be written in a common operating 
system accessible 
to all users. Initially these subroutines 
were stored in a read-only memory. The ISP is straightforward and 
extremely nice. The 
extra- code idea appears in the 
SDS 900 series and was used in the SDS 940 system for defining common-user 
instructions. The IBM Systeml360 
SVC (supervisor call) 
instruction is an adapta- tion of the extracode. Atlas was about the earliest computer to be designed 
with a software operating system and the idea of user machine in mind. The operating system has been 
nicely described 
[Kilburn et al., 19611 and evaluated [Morris et 
al., 19671. 
In a letter to the authors of 
this book, F. H. Sumner makes 
the following comments 
on Atlas. The initial ideas and the preliminary research on the Atlas computer system started in the Department of Computer Science of the Uni- versity of Manchester in 1956. The team, under 
the direction of Professor T. Kilburn, was later supplemented by several members of the I.C.T. Computer Research Department, and the prototype machine was working in the department by the Autumn of 1961. The first production model became operational in January 1963. The significant features of the system can be summarised as: 1 The provision of 
a virtual address field greater than the real address space. 2 The implementation of a "one-level" store using 
a mixture of core store and drum store. 3 The interrupt system and the method of peripheral control. 
4 The realisation at the design stage 
that there would 
be a complex operating system and the provision in the hardware of specific features to assist such an operating system. 274 
Section 6 I Processors with multiprogramming 
ability 275 The method of peripheral control permitted 
the attachment of a large number of on-line peripherals with rapid 
response and entry 
into the operating system for a peripheral requiring attention. 
This, together with the multiprogramming features, makes the design ideal for the attachment of keyboards for the provision 
of multi- access operation. In the original design, provision 
for several such 
on-line typewriters was made, but at 
the production stage it was decided to remove these as an economy measure. 
In view of the subsequent development 
of on-line operation, this was rather an 
unfortunate decision. The Atlas computer at the University 
has now been in continuous operation for four years and it is expected to provide for the major part of the University's computing needs until 1971. During the period 
of its operation the provision 
of extensive monitoring and logging information has permitted the behaviour 
of the system to be studied in detail. The results of these studies have been extremely valuable 
in the design 
of a successor to the Atlas. 
Design of the B 5000 System The Burroughs B 5000 computer 
is described in Part 3, 
Sec. 5, page 257, 
Chap. 22. A user machine in a time-sharing system The Berkeley 
Time-sharing Computer (Fig. 1) is based on the SDS 930 (Chap. 24). 
The hardware modifications to the SDS 930, together 
with the operating system software, were sold by Scientific Data Systems 
as the SDS 940. The operating system and hardware modifications for multiprogramming make the 940 one of the first 
commercially available combined hardware- software time-sharing computers.' The description in Chap. 24 is concerned with the machine 
as it appears to the user. That is, the hardware and the oper- ating system software are both presented in the context 
in which they contribute to form a user 
machine. The 940 uses a memory map which is almost a subset 
of that of Atlas but is more modest than that 
of the 
IBM 360/67 [Arden et 
al., 19661 and GE 645 [Dennis, 1965; Daley and Dennis, 19681. A number of instructions are apparently built in via the programmed operator calling mechanism, based on Atlas extracodes (Chap. 23). The software-defined instructions emphasize the need for hardware features. For example, float- ing-point arithmetic is needed when several computer-bound programs are run. The SDS 945 is a successor to the 940, with slightly increased capability but at 
a lower cost. 'Time-shared computers consist of 
both hardware and 
a complex software operat- 
ing system. Adams Compute+ Chamcteristics Quarterly lists the deliveries of gen- 
eral-purpose time-shared 
computers as DEC PDP-6 hardware, October, 
1964 (software in early 1965); SDS 940 hardware (and 
Berkeley software) April, 
1966; GE 635, 645 
hardware, May, 1965 
(M.I.T.'s project MULTICS software, around 
1969); IBM System/360 Model 67 hardware, March, 
1966 (software, around 1968). M(content addressable; 
flip flop) Mp(#0:3)'4(4 Mp; 3 (P,K)) i('Map)-F'c2-S K--Ms(magnetic tape)- 
LT(paper tape)- K-S-T (Teletype)- K--Ms(drum: 2 dw; 1.3 x 10 w) K-Ms(moving head 
disk: 1.5 x 10' w) 6 E Pi0 'Mp(core; 1.75 us/w; 16384 w; (24,l parity) b/w) "Pc('Modified SDS 930). see Chgpter 42 Fig. 1. University of California (Berkeley) time-shared-computer PMS diagram. 
Chapter 23 One-level storage system1 
T. Kilburn / D. B. G. Edwards / M. J. Lanigan F. H. Surnner Summary After a brief survey of the basic Atlas machine, 
the paper describes an 
automatic system which in principle can be applied to any combination of two storage systems 
so that the 
combination can 
be regarded 
by the machine user as a single level. The actual 
system described relates 
to a 
fast core store-drum combination. 
The effect of the system on instruc- tion times 
is illustrated, and 
the tape 
transfer system 
is also introduced since it fits basically in through 
the same hardware. The scheme incor- porates a ﬁlearningﬂ program, a technique which can be of greater impor- tance in 
future computers. requisite transfers of information taking place automatically. 
There are a number 
of additional benefits derived from 
the scheme adopted, which include relative 
addressing so that routines can operate anywhere in the store, and a ﬁlock out,, facility to prevent interference between different programs simultaneously 
held in the store. 2. The basic machine 1. Introduction In a universal high-speed 
digital computer it is necessary to have 
a large-capacity 
fast-access main 
store. While 
more efficient oper- ation of the computer can 
be achieved by making 
this store all 
of one type, this step is scarcely practical for the storage capacities 
now being considered. For 
example, on Atlas 
it is possible to address lo6 words in the main store. In practice 
on the first instal- lation at Manchester University a total of lo5 words are provided, but though it is just technically feasible to make this in one level it is much more 
economical to provide a core store 
(16,000 words) and drum (96,000 words) combination. Atlas is a machine 
which operates its peripheral 
equipment on a time 
division basis, the equipment ﬁinterruptingﬂ the 
normal main program when 
it requires attention. Organization of the peripheral equipment is also done by 
program so that many pro- 
grams can be contained in the store of the machine at the same time. This technique can 
also be extended 
to include several main 
programs as well as 
the smaller subroutines 
used for controlling peripherals. For these 
reasons as well 
as the fact 
that some orders 
take a variable 
time depending 
on the exact numbers involved, it is not really feasible 
to ﬁoptimumﬂ program transfers 
of infor- mation between 
the two levels of store, i.e., core store 
and drum, in order to eliminate 
the long drum access time of 6 msec. Hence a system has 
been devised to make the core drum store 
combi- nation appear to the programmer as a single level 
of storage, the The arrangement of the basic machine is shown in Fig. 1. The available storage space 
is split into three 
sections; the private store 
which is used solely for 
internal machine organization, 
the central store which includes both core and drum store, 
in which all words are addressed and is the store available to 
the normal user, and finally the tape 
store, which is the conventional backing-up large 
capacity store of the machine. Both the private store 
and the main core store are 
linked with the 
main accumulator, the B-store, and the B-arithmetic unit. However the drum and tape stores only have 
acces5 to these latter sections of the machine via the main core store. The machine order code 
is of the single address 
type, and a 
comprehensive range of basic functions are provided by normal engineering methods. 
Also available to 
the programmer are a 
number of extra functions 
termed ﬁextracodesﬂ which give 
auto- matic access to and subsequent 
return from a large number 
of built-in subroutines. 
These routines provide 
1 A number of orders which would 
be expensive to provide in the machine both in terms of equipment and also time because of the extra loading 
on certain circuits. An example of this is the order: Shift accumulator contents +n places where n is an integer. The more complex 
mathematical operations, e.g., sin x, logx, etc., Control orders for 
peripheral equipments, card readers, 
parallel printers, 
etc., 2 3 ‚IRE Truns., EC-II, vol. 2, pp. 223-235, April, 1962. 4 Input-output conversion routines, 276 
r--- -----1 I ! i-i ! Fig. 1. trysut of I#dc mrelrina. 5 Special programs 
concerned with 
storage allocation to different programs being run sknuftaneously, monitoring routines for fault finding and costing purposes, 
and the detailed organization 
of drum and tape 
transfers. All this information 
is permanently required and hence is kept in part of the private store termed the 
ﬁfixed storeﬂ [Kilburn and Grimsdale, lWa] which operates on a ﬁread 
onlyﬂ basis. This store consists of a woven wire 
me& into which a 
pattern of small ﬁlinearﬂ ferrite slugs are inserted to represent digitai information. The information content can 
only be changed manually 
and will tend to differ only in 
detail between 
the different versions of the Atlas computer. In Muse this store is arranged in two units each of 4096 words, a unit consisthg of 16 columrrs of 256 words, each word being 50 bits. The access time to a word in any one column is about 0.4 psec. If a change of column address is required, this figure increases by about 1 pec due to switching wents in the 
read amp&rs. s\tbsequent accsssssin the new cthru~ revert to 0.4 pec. The store operates in mnj with a 
subsidiary core store of 1024 words which 
provides working 
space for the bed store programs, and has a cycle time 
of about 1.8 pec. There are certain safeguards aght a normal machine user to addre- in either 
part of the privstc store, thcwgh in effect he makes use of this stom &rot& the extracode facility. The central store of the madthe consists of a dnun and core store combination, whiuh Bas a maxi- edclPcssoble oopcity of about 10s weds. ];n R(ue the central store 
c&paoi2y is about !it@W words ferred ha blah 81 !W wads -/from the main core stom, which am&& of four mpuate stacks, each stack hwbg a wpadty of 4088Wonaa The &ip system provides a veay large capacity baddag store for the machine. The user aua &e@ transfers of v@&kr Lpmaunts of informWon between this store and the eatad &om In octual fa& suoh &ansfen are or@d by a fixedstcue program which initiates -c transfers of blocks of 512 WQlCdio Ween the OB 4 drums. Any part afthis *re CBn be trans- main core store. ﬁhe system cpn sima•-ly, each prodw.@gor dem thus be PmyMad $onr sither the an& mcke, tfiG drum, or the tape systepl. is no between tbse addrema, he priority system to allocate add~esses to the core stom, The dh.lsm has top prbxity sbce it delhrsrs a word every 
4 pet, the trpe next prio%itv since ~rdsopn.cuise every 11 pec h 8 ddcs u6es the core st~re for the reat of the available system newswily takes time to establish its at ea& cinnn ar tapa request. Thus the madhe is not slowed dmm in payway when aodnug or tape trunshs take place. Thtt of &am aad tape traders on machine speed is given in 
Appendix 1. To simplt•y the aontrol commands given to the drum, tip. and PBzfpherpaaqUtpHIent in tbs msrchiae, the rdtm all take the b b+ S or a+ B d the identification of t$e mquired eonmaad register is pvk?d by the address S, This type of storpgeis daatly widely Soaapered in &e machine but is termed collecthly the v-stm. En ilye o~ntnh machine &e main accumulator 
conbins a fast uMar [Uhrn -et at., 1tHhi~J d has built-in nrrtwplication nnd diviJiciH &&ties. ft cwn dasal with fked 
or hating poi& numbers and its operation is completely independent 
of the B-store and &~~c unit, Tbe B-store is a fast core store. (cycle time 0.7 pee) Qf 1W twenty-four bit words operating in a wosd selected ‚‚fast™™ B lines u;e+olso provided ia the hm of flipflo these, thwe am uwd as cm&ol lines, terbped mojn, extrscode, d inter- rupt con&& raapectively. The arrangement has the advantage that the &td hnmbers can be maatpWby &e Mwmai &type orders, and the existence of three controb permits the machine t~ swit&b wpidiy from one to another without having to transfer emtd &rs to the core store. Main control is used when the pridty, ead 90 &E? bWn fSlWlg0d thoz 00- btb 8- .81y partial flw wwitclbing mock [Edwards et al., 
278 Part 3 I The instruction-set processor level: variations 
in the processor Exponent V,8 bits including sgn central machine is obeying the current program, while 
the extra- code control 
is concerned with the fixed store subroutines. 
The interrupt control provides the means for 
handling numerous pe- ripheral equipments 
which ﬁinterruptﬂ the 
machine when they either require or are providing information. 
The remaining ﬁfastﬂ B lines are mainly used for 
organizational procedures, 
though B124 is the floating point accumulator exponent. 
The operating speed 
of the machine is of the order of 0.5 x lo6 instructions per second. 
This is achieved by the use of fast tran- sistor logic circuitry, rapid 
access to storage locations, and an 
extensive overlapping technique. 
The latter procedure is made possible by the provision of a number of intermediate buffer stor- age registers, separate access mechanisms 
to the 
individual units 
of core store 
and parallel operation 
of the main accumulator and B-arithmetic units. 
The word length throughout the machine is 48 bits which may be considered as 
two half-words of 24 bits each. All store transfers between the central machine, the drum and 
tape stores are parity checked, 
there being 
a parity digit associated with each half-word. In the case of transfers within 
the central store (ie., between main core store and 
drum) the parity digits 
associ- ated with a 
given word 
are retained 
throughout the system. Tape transfers are parity checked 
when information is transferred to and from the main core store, and 
on the tape 
itself a check 
sum technique involving the use of two closely spaced heads 
is used. The form of the instruction, which allows 
for two B-modifica- tions, and the allocation of the address digits is shown in Fig. 2a. Half of the addressable store locations 
are allocated to the central store which is identified by 
a zero in 
the most significant 
digit of the address. (See Fig. 
2b.) This address 
can be further subdivided into block address, 
and line address in a 
block of 512 words. The least significant digits, 0 and 1, make it possible to address 6 bit characters in a 
half word 
and digit 2 specifies the half word. 
The function number 
is split into several sections, each section 
relating to a particular set of operations, and these 
are listed in 
Fig. 2c. The machine orders fall into two 
broad classes, and these 
are 1 B codes: These involve 
operations between a 
B line specified by the BA digits in 
the instruction and a core store line 
whose address 
can be 
modified by the contents of a B line determined by the B, digits. There are a total of 128 B lines, one of which, Bo, always contains zero. Of the other lines 90 are available to 
the machine user, 7 are special registers previously 
mentioned, and a 
further 30 are used by extracode orders. A codes: These involve operations between the Accumulator and a core store line 
whose address 
can now be doubly 2 Mantissa x 40 bits lnc(uding sign 
Section 6 1 Processors with 
multiprogramming ability (0) 23 22 21~20 19 18 17 46 15 14 I3 12 1i1 IO 9 8 7 6 5 4 3 2 I -Line address .. OdBlock address- ~ .!- (core store and drum) I 1 I 0 O1 0 0 
t 0 0 O-Column-LLineaddress 1 MeshA address ™ 1 Mesh8 Address in fixed store . 1 Address in subsidiary store ___ ~~~_______ Address in Vstare Most signilicant hall word 0 Least significant half-word I Mast significant 
character 0 0 
Least significant character I I 47 46 
45 44 43 42 41 40 39 38 0000888888 ___ 0001 B codes 0040 8 test codes 001 t A codes Of00 OIOI Of 10 01 I1 4 codes and extrocode return 0 os 8 il86 __ 8 codes and extrocode return 
__ B type extracode A type extrocode (C) (d) Fig. 2. Interpretation of a word. 
(a) Form of instruction. (b) Allocation of address digits. (c) Function of decoding. (d) Floating-point 
number X8™. modified first 
by contents of B, and then 
by the contents of BA. Both fixed and floating point orders are provided, and 
in the latter case numbers 
take the form of XSY, the digit allocation of X and Y being shown in Fig. 
2d. When fixed point working occurs, 
use is made only of the X digits. 
3. 0rrcl-kiwl.toreconobpt The choice of system for the fast access store in a large scale computer is governed by a number of emdicting factom which include speed ard size requirements, eapnomic and technical difficulties. Previously the probkm bas been resow in two ex- treme cuses either by the provision of a very large core 
store, e.g., the 2.5 mebit [Papian, 19571 store at M.I.T., or by the use of a small core store (40,000 bits) eapanded to &10,000 bits by a dnun store as in the Ferranti Mercury [Lonsdale and Warburton, 19%; Kilbwn et al., 19561 computer. Each 
of these methd has its disadvantages, in the first case, that of expense, snd in the second case, that of inconvenience to the user, who is obliged to program traders of information between the two types of stom and this can be time 
consuming. In some instances it is Wble for an expert mechine user to mge his program so thnt the amwnt of time lost by the transfers in the two-level storage mangemcBt is not significant, but this sort of ‚‚optimtun™™ pqpmaing is aot very desirable. Suitable interpretative coding [Brooker, 19601 can permit the two-level system to appear as one level. The effect is, however, accompanied by an effective loss of machine speed which, in some programs 
and dependhg on details of machine design, can be quite were, varying typically, 
for example, be- tween one and three. 
The two-level storage sake has obvious economic 
advan- tages, and bconvenience to 
the machine user can 
be ellminated by &g the transfer arrangements completely automatic. 
In Atlas a comphdy automatic system has been provided with td- niques for minimizing the transfer times. In this way the core and drum are merged iato an appBpent single level 
of storage with 
good performance and at moderate cost. Some details of this ar- rangement on the Muse are now provided. The central store is subdivided into blocks of 512 words as shown by ttre address arrangeumnts in Fig. %b. The main cere store is also partitioned into blocks of this size which for identifiuation purposes are called pages. Assodated with each 
of these core 
store page positions is a ﬁpage address registerﬂ 
(P.A.R.) which contains the address of the block of information at present Occupying that page position. When access to any w01d in the central store 
is required the digits of the demanded block address are compared with the contents of all the page address registers. 
If an ﬁequiva- lenceﬂ indication 
is obtained then 
access to that particular page position is permitted. Since B block can occupy any one of the 32 page positions in 
the core store it is necessary to modify some 
digits of the demanded block address to conform 
with the page positions is which an equivalence was obtained. Thaw processes are necesearily time consum@ but by provid- ing a by-pass of this procedure for instruction acoews (since, in genera& instntctioa loops are all amtained ia the wne block) then most of &is tim~ cpn be overlapped with a UeeM pﬂtion of the machine or corn store rhythm. in thia wsly infomation in the core store is available to the mschine at the full speed d &e awe store and only rarely is the over-all machine speed 
rrffeeted by 
delays in the equivdwoe circuitry. If P ﬁnot equivalenceﬂ indication 
is obtakred when the de- manded kk address is co~lip~~~d with the conte~~ts of the P.A.R.™s &en tht address, which 
may have bn B-modiiBe& is first stored in a register which can be acd as a Iiw of the V-store. Thip permits the central machine easy access to this ad- dress. An ﬁintemptﬂ also occurs which swikcherr operntion of the machine over te the interrupt control, wbidh fh d the 
ciwe of tbc intarnapt and &en, in thtp htace, enters a bd store routisk ta organize the necessary trmdbm of infonaation between dkun and core store. A. Dficnrtm* On each drcun, one track is used to identify absolute 
bloak psi- tions around the dnmr periphery. The records on these tracks 
are read into the 
B registers which can be accd as lines of the V-store and this permits the present anglular ckum pition to be detmmbd, though dy in units of one blook. In this way the ti- Ildeded to tmnsbr my Mock while reading hoin &e drwns the dFnrn malation time is L2 msec ad the actual transfer time 2 msec. The time 
of a writing transfer to the drums hss been redd by writing 
the bld of idormation to the ht wailable empty bloak pitian oa any dmm. Thus the access time of the drum can be elhind pvkkd there are 
a masorable number of empty blocks on the dnun. This means, however, that transfers to/from the drtffn have to be carried out by 
refesenoe to a direc- tory and this is stored in the subsidiary store and 
up-dated when- ever a transfer occurs. &st action is to determine the absolute position on a dmm of the required block. The order is then given to carry out the transfer to an empty page position in the core store. The transfer occurs 
automatically as soon as the drum reaches the correct angular position. The page address regirtsr in the vacant @ion in the core store is Set to *specific block number for dram transfers. This technique sim- plifies the engineering with regard 
to the provision of this number ~anbe~~~time~uie~bet\msen2and14mge~~in~e Whrta the dwm transfer routine is eatered 
280 Part 3 I The instruction-set processor level: variations in 
the processor from the drum and also provides a safeguard 
against transferring 
to the 
wrong block. As soon as 
the order asking for a 
read transfer from the drum 
has been given the machine continues with the drum transfer program. It is now concerned with determining 
a block to be transferred back from 
the core store 
to the 
drum. This 
is necessary to ensure an empty core store page 
position when the next read transfer is required. The block in 
the core store 
to be transferred has to be carefully chosen 
to minimize the number of transfers in the program and this optimization process is carried out 
by a learning program, details 
of which are given in Sec. 5. The opera- tion of this program is 
assisted by the provision of the ﬁuseﬂ digits 
which are associated with each page 
position of the core store. To interchange information between the core store 
and drums, two transfers, a 
read from and a write to the 
drum are 
necessary. These have to be done sequentially but could occur in 
either order. The technique of having a vacant page position in 
the core store permits a read transfer to occur first and thus allows the time for the learning program to be overlapped either into 
the waiting period for the read transfer or 
into the transfer time itself. In the 
time remaining after completion of the learning program an entry 
is made into 
the over-all supervisor program for the machine, and a decision 
is taken concerning 
what the machine is to do 
until the drum transfer is completed. This might involve a 
change to a different main program. A program could ask for access 
to information in a 
page position while a drum or tape transfer is taking place 
to that page. This 
is prevented in 
Atlas by the use of a ﬁlock 
outﬂ (L.O.) digit which is provided with each 
Page Address Register. 
When a lock 
out digit is set at 1, access to that page is only permitted when 
the address has 
been provided either by the drum system, the tape 
system, or the interrupt 
control. The latter case permits all trans- 
fers from paper tape, punched card, 
and other peripheral equip- 
ments, to be handled without interference 
from the main program. When the 
transfer of a block has been completed 
the organizing program resets the L.O. digit to zero and access to that page Section 6 I Processors with multiprogramming ability 
position can then be made from the central machine. It is clear that the L.O. digit can also 
be used to prevent interference 
be- tween programs when several different 
ones are being held in the machine at the same time. In Sec. 3 it was stated that addresses demanding access to the core store could 
arise from three distinct sources, the central machine, the drum, and the 
tape. These 
accesses are complicated because of (1) the equivalence technique, and (2) the lock out digit. The various cases 
and the 
action that takes place are 
summarized in Table 1. The provision of the Page Address Registers, 
the equivalence circuitry, and the learning program have permitted 
the core store and drum to 
be Legarded by the ordinary machine user as a one- 
level store, and the system has 
the additional feature of ﬁfloating addressﬂ operation, Le., any block of information can be stored in any absolute position in either core or drum store. The minimum access time to 
information in this store 
is obviously limited by the core store 
and its arrangement and this is now discussed. B. Core store arrangement 
The core store 
is split into four stacks, each with individual address decoding and read and write mechanisms. The stacks are then combined in such a way that common channels into the machine for the address, read and write digits are time shared between 
the various stacks. 
Sequential address 
positions occur in two stacks alternately and a page position which contains a block 
of 512 sequential addresses is thus arranged 
across two stacks. In this way it is possible to read a pair of instructions from consecutive ad- 
dresses in parallel by increasing the size of the read channel. This 
permits two instructions to be completely obeyed in three 
store ﬁaccesses.ﬂ The choice of this particular 
storage arrangement is discussed in Appendix 2. The coordination of these four stacks 
is done by the ﬁcore stack 
coordinatorﬂ and some features of this are now discussed, 
starting with the operation of a single 
stack. Table 1 Comparison of demanded block address with contents 
of the P.A.R.™s resultant state 
of equivalence and lock out circuits 
Equivalence Lock out = 0 Sourw of address lE.Q.1 Not equivalence [N.E.Q.] [ Equioalence ) Lock out = 1 [E.Q. 6- L.O.] ~ ~ ~~~~ 
1 Central Machine Access to 
required page 
position Enter drum transfer routine 
Not available to this program 2 Drum System Access to required page 
position Fault condition indicated 
Fault condition indicated 
3 Tape System 
Access to required 
page position Fault condition indicated 
Fault condition indicated 

C. Operation Of U &@e rtedr Of corC8rt0rc The storage system employed is a cdncident currant M.I.T. system arranged to give paralkl read out of 50 digits. The reading opera- tion is de$tmctbe and each read phase of the stack cycle Is fol- lowed by a write phase during which the infonnaton read ont may be rewritten. 
This is achieved by 
a set of digit stpti&zors which am loaded during the read phase and are ad to control the inhibit current drivers dwkg the write phase. When new information is to be written into 
the store a similar sequence 
is followed, except that the digit staticizors 
are loaded with the MW information during the read phase. A diagram indicating the different types of stack cycle is shown in Fig. 3. I strobe I I Lg I I phose (0) :::::St-p7 r Rood phase I 1 I Write strobe I U Write phase 1 I I ,wo.l Stnrk - I I Write ! ‚ strobe I I U Write phose ! I I l--r @% b IC) r, = occess time; rc = cyclic time; Wo = woit for oddrens decoding and loading 
of oddreu register; Ww = woit for release of write hold UP. Rg. 3. Bask types of rtldr cycle. (a) Road orckr (s + A). (b) rmteonkr (a + S). (c) Road-writ~ W (&I + s + S). There is a small delay W, ( N 100 mpec) between the 
ﬁstack requestﬂ signal, SR, and the start of the rtwd phase to allow for setting of the address stab d the decodbg. The output informath from the store appears in the read strobe period, which is towards the end 
of the read phase. In general, the write phase starts as soon UL the read phase ends. However, the start of the write phase may be held up until the new information is available from the central machine. This delay 
is shown as W, in Fig. 3c. The interval 
T™ between the stack request 
and the read strobe is termed &e stack access time, and in practice this is approxi- mately one third of the cycle time T,. Both Tn and T, are functions of the storage ryatem and resuming that W, is zero have 
typical values of 0.7 and 1.9 pc respectively. A holdup gate in 
the request channel prevents 
the next stack request occurring 
before the end 
of the preceding write phase. D. opsrclh of the muin wm store wit), the umtral machine A scheme diagram of the essentials of the main core store con- trol system 
is shown in Fig. 4. The control signals SA, and SA, indicate whether 
the address presented is that of a single word or a pair of sequentially addressed instructions. 
Assuming that the 
flip-flop F is in the reset condition, either of these signals results 
in the loading of the buffer address register 
(B.A.R.). This loading is done by the signal B.A.B.A. which also indicates that the 
buffer register in the central 
machine has 
become free. In dealing with 
the 5st request the block address digits 
in the B.A.R. are compared with 
the contents of all the page address 
registers. Then one of the indications summarized in 
Table 1 and indicated in Fig. 
4 is obtaimd. Assuming access to the required store stack is permitted then a 
set C.S.F. signal is given which resets the flip-flop F. If this occurs before the next access request 
arises, then the 
speed of the system is not store-limited. In most cases SET CSF is generated when the equivalence operation 
on the demanded block address is complete, and the read phase of the appropriate stack (or stacks) has swed. Until this time the information held in the B.A.R. must not be allowed to change. In Fig. 5 a f?mv diagram is shown for the various cases which can When a 
single address request is accepted it 
is necesrary to obtain an ﬁequivalenceﬂ indication 
and form the page location digits -re ttZe stack request 
can be generated. The SET CSF sippnrl thm OCC\IES as soon as the read phase starto. zf a ﬁnot equiva- 
lentﬂ or ‚‚equivalent a& locked outﬂ indication is a stack request is not generated, and the contents of the 4A.R. are copied in to a line of &e V-store before SET CSF is pneratd. When access to a pair of addresses is reqwsted &e., an instruc- 
ariseinpmctice. 
282 Part 3 I The instruction-set processor level: variations 
in the processor Page oddress reg 
0 Page oddress reg 1 Equivalence EO NEQ EQEiLO Not instruction address 1 Instruction addressbl A register Cornporison circuit re Stock 0 Stack 1 7w-j Main core store 1 Stack Fig. 4. Main core store control. 
tion pair) the stack requests 
are generated on the assumption that these instructions are located in the same page position as 
the last pair requested, 
Le., the page position 
digits are taken from the page digit register. 
(See Fig. 
4.) In this way the time required 
to obtain the equivalent indication 
and form the page location digits is not included 
in the over-all access 
time of the system. The assumption will normally 
be true, except when crossing block 
boundaries. The latter cases are detected and corrected 
by com- 
paring the true position page 
digits obtained 
as a result of the Section 6 1 Processors with multiprogramming 
ability equivalence operation with 
the contents of the page digit register 
and a ﬁright pageﬂ 
or ﬁwrong pageﬂ indication 
is obtained. (See Fig. 4.) If a wrong page is accessed this is indicated to 
the central 
machine and 
the read out 
is inhibited. The 
true page location 
digits are copied into 
the page digit register, 
so that the 
required instruction pair 
will be obtained 
when next 
requested. The read out to the central machine is also inhibited for ﬁnot equivalentﬂ 
or ﬁequivalent and 
locked outﬂ indications. In Fig. 5 the waiting time indicated immediately 
before the stack request 
is generated can 
arise for a number 
of reasons. 1 2 The preceding write phase of that stack has not 
yet finished. The central 
machine is not yet ready 
either to accept infor- mation from 
the store, or to supply information to it. SA1 OR SA2 1 Walt for core store free 1 Wait for equivalence ond formotion of page diglts Not equivalent or equivolent ond locked Woitlsee text1 Copy to Vline BAR requ,est Stack SET CSF Start read Dhose WOlt for equivalence and formalion of page digits i Woit (see text) It l or equivalent compare page Not equivalent 
and locked I requests 1 digits with contents of I Page digit out I 1 SET CSF Copy pede digits to page digit SET CSF SET CSF SET CSF Fig. 5. Flow diagram of main core store control. 

3 It is necessary to ensure a 
certain minimum time between The eppro&mate times for various iastrustiono are given in SUCCesSive read strobes &Om the core store 
ScScks to dOW Table 2. These figures relate to the times between completing mtisfactoV operation 
of the PafitY CkCUib, Which take instructions 
when a long sequence of the same type of instruction be reduced, but as it is only poSIsible to get such a condition in practice obNg one instruction is overlapped in time with for a part Of the instruction timing it some part of three other instructions. This makes the detailed was not thought 
to be an economical proposition. 
timing complicated, 
and so the timing sequence 
is developed about 0.4 pec to chwk the information. Thip time could is while this is not ideal, it is necessary because The basic machine timing 
is now discussed. 4. Instruction times In high-speed computers, one of the main factors 
limiting speed 
of operation is the store 
cycle time. 
Here a number 
of tecbnlques, e.g., splitting the core store 
into four separate stacks and 
extracting two instructions in a single cycle, 
have been 
adopted despite 
a fast basic cycle time bf 2 pec in order to alleviate this situation. The time taken to complete 
an instruetion is dependent upon 1 The type of instruction (which is defined by the function 2 The exact location of the instruction and operand in the 
core or fixed store since this em affect the access time 3 Whether or not the operand 
address is to be 
modified 4 In the case of floating point accumulator 
orders, the actus1 numbers themselves 5 Whether dnun and/or tape transfers are taking place 
~git.4 slowly by 
first considering 
instructions obeyed one after another. It is convedient to make these instructiow a sequemce of floating point additions 
with both instruction and operand in the core store 
and with the operand 
address single B-modiW. To obey this instruction the central 
machine makes two re- quests to the 
core store, one 
for the instruction and the second for the operand. After the instruction is received in the machine the function part has to be dscaded and tlm operand address modified by the contents 
of one of the B registers More the operand request can be made. Finally, after the operand 
has been obmned the actual 
accumulator addition 
takes place to complete the instruction. The time 
from beginning to end of one instruction is 6.05 pec and an approximate timing schedule is as follows in Table 3. If no other action is permitted in the 
time required 
to complete the instruction (steps 1 to 8 in Table 3). then the different 
sections of the machine uw being used very inefliciently, eg., the accumu- lator adder is only used 
for less than 1.1 pec. However, the orga- nization of the computer is such 
that the different sections such 
as store stacks, accumulator and Mthmetic unit, can operate Floating Point Addition Floating Point Multiplication 
Floating Point 
Division Add Store Line to an Index Register 0 1 2 1.4 1.6 2.03 0. 1 or 2 4.7 0, 1 or 2 13.6 0 1 Add Index Register to Store Line and Rewrite to 0 Store Line 1 1.53 1 .$S 1.63 1.8 1.65 1.65 1.9 4.7 13.6 1.65 1.85 1.65 1.7 ~ 1.2 1.2 1.9 4.7 13.6 1.t5 1.85 
284 Part 3 I The instruction-set processor level: variations in the processor Table 3t and operands in the core store) 
Timing sequence for floating point addition (instructions 
Time interval Total between steps time 
Sequence ELSec Pec 1. Add 1 to Main Control 0 2. Make Instruction Request 0.3 (Addition time) 0.3 (Transfer times, equivalence 
time and stack access time) 1.75 3. Receive Instruction in Central Machine 2.05 4. Function decoding complete 
2.25 5. Request Operand 3.10 (Load register and decode) 0.2 (Single address modification) 0.85 (Transfer times, equivalence time 
and stack access time) 1.75 (Load register) 0.1 6. Receive Operand in Central Machine 4.85 7. Start 
Addition in Accumulator 4.95 (Average floating point addition, including shift round and 
stand- a rd i se) 1.1 
8. Instruction complete 
6.05 t In step 4, time is for single address modification. Times for no modification and two modifications are 0.25 psec and 1.55 psec respectively. at the same time. In this way several instructions 
can be started 
before the first has finished, 
and then 
the effective instruction 
time is considerably reduced. There have, of course, to be certain safe- guards when for example an instruction is dependent in any way 
on the completion of a preceding instruction. In the time sequence 
previously tabulated, by far the longest time was that between a request in the central machine for the core store and the 
receipt in the central machine of the infor- mation from that store. This effective 
access time of 1.75 psec is made up as shown in 
Table 4. It has been reduced 
in practice by the provision of two buffer registers, 
one in 
the central machine 
and the other in the core stack coordinator. 
These allow the equivalence and transfer times 
to be overlapped with the organi- zation of requests in the central machine. In this 
way, provided the machine can arrange to 
make requests fast enough, then the effective access time is reduced to 
0.8 pec. Further, since three accesses are needed to complete 
two instruc- tions (one for 
an instruction pair and one for each of the two operands) the theoretical minimum time of an instruction is 1.2 psec 3 ~0.8/2 and it then 
becomes store limited. Reference 
to Section 6 1 Processors with multiprogramming ability Table 3 shows that the 
arithmetic operation takes 
1.2 psec to complete so that, on the average, the capabilities of the store and the accumulator are well matched. Another technique for reducing store access time for instruc- tions has 
also been adopted. 
This permits the read cycles of the two stacks to start assuming that the 
same page will be referred to as in the previous instruction pair. This, 
of course, will 
normally be true and there 
is sufficient time to take corrective procedures 
should the page have been 
changed. The limit of 1.2 psec per instruction is not reduced 
by this technique, but the 
possibility of reaching this limit under other conditions is enhanced. A schematic diagram of the practical timing of a sequence of floating point addition orders is shown in Fig. 6. The overlapping is not perfect and in the time between 
successive instruction pairs the computer is obeying four instructions for 
25 per cent 
of the time, three 
for 56 per cent and 
two for 19 per cent. 
It is therefore to be expected that the 
practical time 
for the complete order 
is greater than 
the theoretical minimum time; it 
is in fact approxi- 
mately 1.6 psec. For certain 
types of functions the reading of the next pair of instructions before 
completing both 
instructions of the first pair would be incorrect, e.g., functions causing transfer 
of control. Such situations are recognized during the function decoding, and the 
request for 
the next instruction pair is held up until a suitable time. In a sequence of floating point addition orders 
with the operand addresses unmodified 
the limit is again 1.2 psec while the time obtained is 1.4 pec. For accumulator 
orders in which the actual 
accumulator operation 
imposes a 
limit in excess of 2 psec then the actual time 
is equal to this limit. 
Perhaps a more realistic way of defining the speed of the com- puter is to give the time for 
a typical inner 
loop of instructions. A frequently occurring operation 
in matrix work in 
the formation of the scalar product of two vectors, this 
requires a loop 
of five instructions: Table 4 Effective store 
access time Total time Sequence !J=c 1. Request in Central Machine 0 3. Equivalence complete and request made 
to selected 2. Request 
in Core Stack Coordinator 
0.25 stack 0.95 4. Information in Core Stack Coordinator 1.65 5. Information in Central Machine 1.75 
Chapter 23 1 on*lovel storage syskm 28!5 1 fylj Accumulator busy acc Stack request Read f:lyj Accumulator busy 1 Operand request I Equivalence I 21 Start second of pair 
Operand OCc Stack request lF~~~~~l Bmodificatmn reqr(est I Equivalence I Read f::'\ Acwnulator busy I 3 OCC Start Instruction Stock 
Operand Stack next pair request 
1.31 request , Function request request 
I I Equivalence Read I decode I Bmodification I I Equivalence I Start second of pair 4 5 IFd":zl 8 modification 6 Start Instruction next pair request I I 1$1 Equivalence Fig. 6. Timing diagram for a 
sequence of floating point addition orders. (Singleaddress modification.) 1 Element of first vector into accumulator. 
(Operand B-modi- fied.) Multiply accumulator by element of second vector. 
(Oper- and B-modified.) 3 Add partial product to 
accumulator. 4 Copy accumulator to store line containing partial product. 
5 Alter count to 
select next elements and 
repeat. 2 The time for this loop with instructions and operands on the core store is 12.2 psec. The value of the overlapping technique 
is shown by the fact that the time from starting the first instruction to finishing the second is approximately 10 psec. When the 
drum or tape systems are transferring 
information to or from the core store 
then the rate of obeying instructions which also use 
the core store will be affected. The affect is dis- cussed in more detail in Appendix 1. The degree of slowing down is dependent upon the time at which a drum or tape request occurs 
relative to machine requests. It also depends on the stacks used 
by the drum or tape and those being used by the central machine. The approximate slowing down is by a factor of 25 per cent during 
a drum transfer and by 2 per cent for each active tape channel. (See Appendix 1.) 5. The drum transfer 
learning program The organization of drum transfers has been 
described in Sec. 2A. After the transfer of the required block from 
the drum to 
the core store has been initiated, the organizing program examines the state of the core store, 
and if empty pages still 
exist, no further action 
is taken. However, 
if the core store is full it is necessary to arrange for an empty page to be made available 
for use at the next non- equivalence. The selection of the page to be transferred could be made at random; this could easily result 
in many additional trans- 
fers occurring, as the page selected could be one of those in 
current use or one 
required in the near future. 
The ideal selection, which 
would minimize the total number 
of transfers, could 
only be made by the programmer. To make this ideal selection the programmer 
would have to know (1) precisely how his 
program operated, which 
is not always the case, and (2) the precise amount 
of core store 
available to his program at any instant. This latter information is not generally available 
as the core store 
could be shared by other 
central machine 
programs, and almost certainly by 
some fixed store program organizing the input and output of information from 
slow peripheral equipments. 
The amount of core store required by 
this fixed store program 
is continuously varying 
[Kilburn et al., 19611. The only way the ideal pattern 
of transfers can be approached is for the transfer program 
to monitor the behavior of the main program and in so doing attempt to select the correct 
pages to be transferred to the drum. The techniques used for monitoring are subject to the condition that they must not 
slow down the operation of the program to such an extent that they offset any reduction in the number of transfers required. The method de- 
scribed occupies less than l per cent of the operating time, and the reduction in the number of transfers is more than 
sufficient to cover this. 
286 Part 3 1 The instruction-set processor level: variations in 
the processor That part of the transfer program which 
organizes the selection of the page to be transferred has 
been called the ﬁlearningﬂ pro- gram, In order for this 
program to have some data on 
which to operate, the machine has been designed to supply information 
about the use made of the different pages of the core store 
by the program being monitored. 
With each page of the core store there is associated a ﬁuseﬂ digit which is set to ﬁ1ﬂ whenever any 
line in that page is accessed. The 32 ﬁuseﬂ digits exist in two 
lines of the V-store and can be read by the learning program, 
the reading automatically 
resetting them to zero. The frequency with which these digits are read is governed by 
a clock which measures 
not real time 
but the number of instructions obeyed in the operation of the main program. This 
clock causes 
the learning program to copy the ﬁuseﬂ digits to a list in the subsidiary store every 
1024 instructions. The use of an instruction counter rather than 
a normal clock to measure ﬁtimeﬂ for the learning program is due to the 
fact that the operations of the main program may 
be interrupted at random for random lengths of time by the operation of peripheral equipments. 
With an instruction counter the temporal pattern of the blocks used 
will be the same on successive runs through the same part of the program. This 
is essential if the learning program is to make use of this pattern to minimize the number of transfers. When a nonequivalence occurs 
and after the transfer of the required block has been arranged, the learning program again adds 
the current values of the ﬁuseﬂ digits 
to the list and then 
uses this list to bring up to date two sets of times also kept in 
the subsidiary store. 
These sets consist 
of 32 values of t and T, one of each for each page of the core store. The value of t is the length of time since the block in that page has been used. The value of T is the length of the last period of inactivity of this block. The accuracy of the values of t and T is governed by 
the frequency with which the ﬁuseﬂ digits are inspected. The page to be written to 
the drum is selected by the appli- cation in turn 
of three simple tests 
to the values of t and T. 1 2 Any page for which t > T + 1, or That page with t # 0 and (T - t) max, or 3 That page with T,, (all t = 0). The first rule selects 
any page which 
has been currently 
out of use for 
longer than its last period of inactivity. Such a 
page has probably ceased 
to be used by the 
program and is therefore an ideal one to be transferred to 
the drum. The second rule ignores all pages with 
t = 0 as they are in current 
use, and then 
selects the one which, if the pattern of use is maintained, will not be Section 6 I Processors with multiprogramming ability 
required by the program for the longest time. If the first two rules fail to select a page the third ensures that if the page finally selected is wrong, in that it is immediately required again, then, as in this 
case, Twill become zero and the 
same mistake will 
not be repeated. For all 
the blocks on the drum a list of values of T is kept. The values of T are set when the block is transferred to the drum: T = time of transfer-value of t for transferred page When a block 
is transferred to the core store the value of T is used to set the value of T. T = time of transfer-value of T for this block = length of last period 
of inactivity For the 
block transferred 
from the drum t is set to 0. In order to make its decision the learning program has only to update two short lists and apply at the 
most three simple rules; this can easily be done during 
the 2 msec transfer time of the block required as a result of the nonequivalence. As the learning program uses only fixed and subsidiary store addresses it is not slowed down during the period of the drum transfer. The over-all efficiency of the learning program cannot be known until the complete Atlas system 
is working. However, the value of the method used has 
been investigated by simulating the behavior of the one-level store and learning program on the Mercury computer 
at Manchester University. This has been done 
for several problems using varying amounts of store in excess of 
the core store available. 
One of these was the problem of forming the product A of two 80th order matrices 
B and C. The three matrices were stored row by row 
each one 
extending over 
14 blocks, only 14 pages of core store 
were assumed to be available. The method of multiplication was b,, x 1st row of C = partial answer to 1st row of A b,, x 2nd row of C + partial answer = second partial answer, etc. Thus matrix B was scanned once, matrix 
C 80 times and each row of matrix A 80 times. Several machine users were asked to spend 
a short time writing 
a program to organize the transfers for a 
general matrix multipli- 
cation problem. In no case when the method was applied to the 
above problem 
were fewer than 357 transfers required. A program written specifically for this 
problem which 
paid great attention 
to the distribution of the rows of the matrices relative 
to block divisions required 234 transfers. The learning 
program required 274 transfers; the gain over the human programmer was chiefly 

Chapter 23 1 One-level storage 
system 287 due to 
the fact that the learning program could take full advantage 
of the occasions when the rows of A existed entirely 
within one block. Many other problems involving cyclic running 
of single or 
multiple sets 
of data were simulated, and in no case did the learn- ing program require more transfers than an 
experienced human programmer. A. Prediction of drum transfers Although the learning program tends to reduce the number of transfers required 
to a minimum, the transfers which do occur still 
interrupt the operation of the program for from 2 to 14 msec as they are initiated by nonequivalence interrupts. 
Some or all of this time 
loss could be avoided by organizing the transfers in 
advance. A very experienced programmer having sole use of the core store could arrange his 
own transfers in such a 
way that no unnecessary ones ever occurred 
and no 
time was ever wasted waiting for transfers to be completed. This 
would require a 
great deal of effort and would 
only be worthwhile for a program that was going 
to occupy the machine for a long time. 
By using the data accumulated by 
the learning program it is possible to recog- nize simple 
patterns in the use made by 
a program of the various blocks of the one-level store. In this way a prediction 
program could forecast 
the blocks required in 
the near future and 
organize the transfers. By recording the success or failure 
of these forecasts 
the program could be made self-improving. For the matrix multi- 
plication problem discussed above the pattern of use of the blocks containing matrix 
C is repeated 80 times, and a considerable 
degree of success could be obtained with a simple prediction 
program. 6. Conclusions A specific system for making 
a core-drum store combination 
appear as a single level store 
has been described. While this is the actual system being built for 
the Atlas machine the principles involved are applicable to combinations of other types of store. For exam- ple, a 
tunnel diode-fast core store combination 
for an even faster 
machine. An alternative which 
was considered for Atlas, 
but which was not as 
attractive economically, was a fast 
core-slow core store combination. The system too 
can be extended to three levels of storage, and indeed if 106 words of total storage had to be provided then it would be most economical to provide it on a third level of store such as a 
file drum. The automatic system does 
require additional equipment and introduces some complexity, since 
it is necessary to overlap the time taken for address comparison 
into the store and machine 
operating time if it is not to introduce any extra time 
delays. Simulated tests 
have shown that the organization of drum transfers are reasonably efficient and other 
advantages which accrue, such 
as efficient allocation 
of core storage 
between different programs and store lock out facilities are 
also invaluable. No matter how intelligent a 
programmer may be he can never 
know how many 
programs or peripheral 
equipments are in operation 
when his program is running. The advantage of the automatic 
system is that it takes into account the state of the machine as it exists at any particular time. Furthermore if as in normal use there is some sort of regular machine rhythm even 
through several programs, 
there is the possibility of making some sort of prediction with regard to the transfers necessary. This involves no more hardware and 
will be done by 
program. However, this stage will probably 
be left until results on the actual 
system are obtained. It can be seen that the 
system is both useful and flexible in that it can be modified or extended in 
the manner previously indicated. Thus despite the increase in equipment, 
the advantages which are derived completely 
justify the building of this automatic 
system. APPENDIX 1 TO THE CORE STORE 
ORGANIZATION OF THE ACCESS REQUESTS 
There are three sources of access requests to the core store, 
namely the central machine, the drum, and the tape 
systems. In deciding how the sequence of requests from all three sources are to be serialized and placed in some sort of order, a 
number of facts have to be considered. These are 1 All three sources are asynchronous in nature. 2 The drum and tape systems can make requests at a fairly 
high rate compared with the store cycle time 
of approxi- mately 2 psec. For example, the drum provides a request 
every 4 pec and the tape 
system every 11 pec when all 8 channels are operative. 3 The drum and tape systems can only be stopped in multiples 
of a block length, i.e., 512 words. This means that any system devised for accessing the core store 
must deal with both the average rates 
of drum and tape requests specified in 2. Only the central machine 
can tolerate requests being stopped 
at any time and 
for any length of time. From these facts a 
request priority 
can be stated which 
is a Drum request. b Tape request. c Central machine request. 
288 Part 3 1 The instruction-set processor level: variations in the processor I 1 Stack request of stored rnochine Order A machine request can be accepted by the core store, 
but because there is no place available 
to accept the 
core store 
information, its 
cycle is inhibited and further 
requests held 
up. In the case of successive division orders 
this time can be as long as 20 psec, in which 
case 5 drum requests could be made. To avoid having an excessive 
amount of buffer storage for the drum 
two techniques are 
possible: a When drums or tapes are 
operative do not permit 
ma- chine requests to be accepted until there 
is a place 
available to put the 
information. h Store the machine request 
and then 
permit a drum 
or tape request. The latter 
scheme has been adopted because it can be accommodated more conveniently and it 
saves a small amount of time. If the central machine is using the private store then 
it is desirable for drum and 
tape transfers to the core store not 
to interfere with 
or slow down 
the central machine in any 
way. When the central machine, drum and 
tape are sharing the core store 
then the loss of central machine speed 
should be roughly proportional to the activity of the drum or tape systems. This means 
that drum or tape requests must 
ﬁbreak™ into the normal machine request 
channel as and when required. The system which 
accommodates all these points is now dis- cussed. Whenever a drum or tape request occurs inhibit signals are applied 
to request channel into 
the core stack coordinator 
and also to the stack request channels 
from this coordinator. This 
results in a ﬁfreezingﬂ 
of the state 
of flip-flop F (Fig. 5) and this state is then inspected (Fig. 
7, point X). If the state is ﬁbusyﬂ this means that a machine order 
has been stopped somewhere between the loading of the buffer address register (B.A.R.) and the stack request. Normally this time interval can 
vary from 
about 0.5 pec if there are no 
stack request holdups, 
to 20 psec in the case of certain accumulator holdups. In 
either case sufficient time is al- lowed after 
the inspection to ensure that the 
equivalence operation 
has been completed. 
If an equivalence indication 
is obtained all 
the information relevant to this machine order 
(i.e., the line ad- 
dress, page digits, stack(s) 
required and type of stack order) 
are stored for future reference. Use is made here of the page digit 
register provided to allow the by-pass on the equivalence circuitry 
for instruction accesses. The core store 
is then made free for access 
by the drum 
or the tape. If the core store had been 
found to be 
free on inspection, the above procedure 
is omitted. i F flip-flop frozen 1 xp -Inspect state of F flip-flop I i 1s there o stored machine order 
7 I > -~ Busy Wait for equivalence completed 1 i 1 Store machine order Free F flip-flop Drum tape access to core store - Drum/tape priority Remove stock request Inhibit signals I- + Stock request 
for drum /top Drum/tape request I I hmrt stack request inhibits to reapply ADP~Y inhibits to stack request channels and to machine request channels (if these are not already applied) 
1 Has the stack request of 0 stored machine order been Stopped 
i No r-lx i Fig. 7. Drum and tape break in systems. A drum or tape access (as decided by 
the priority circuit) to the core store 
then occurs, which removes 
the inhibits on the stack request channels. 
When the stack request 
for the drum or tape cycle is initiated these inhibits 
are allowed to reapply. At this stage 
(Fig. 7, point Y), if there is a stored machine order it 
is allowed to proceed if possible. The inhibits on the machine request chan- 
nels are removed when the stack request 
for the stored machine 
order occurs. If there is no stored machine order 
this is done 
Chapter 23 I One-level storage system 289 immediately, and the central machine 
is again allowed access 
to the core store. However, another 
drum or tape request can arise before the stack request of the stored machine order 
occurs, in 
particular because this latter order may still be held up by the central machine. If this is the case the drum or tape is allowed immediate access and a further 
attempt is made to complete 
the stored machine order when this drum or tape stack request 
occurs. If the stored machine order 
was for an operand, the content of the page digit register will correspond 
to the location of this operand. The next machine request 
for an instruction pair 
will then almost certainly result in 
a ﬁwrong 
pageﬂ indication. This 
is prevented by arranging 
that the next instruction pair 
access does not by-pass the equivalence circuitry. The effect on the machine speed when the drum or tapes are 
transferring information to or from the core store 
is dependent upon two factors. First, upon 
the proportion of time during which 
the buffer register in the core coordinator 
is busy dealing with machine requests, and secondly, upon the particular stacks being used by the central machine 
and the drum or tape. If the computer is obeying a program 
with instructions and operands 
on the fixed or subsidiary store then the rate of obeying instructions is un- affected by drum or tape transfers. A drum or tape interrupt occurring when the B.A.R. is free prevents any machine address being accepted 
onto this 
buffer for 1.0 psec. However, if the B.A.R. is busy then the next machine request to the core store 
is delayed until 1.8 psec after the interrupt 
if different stacks are being used, or until 3.4 psec after the interrupt if the stacks are the same. When the 
machine is obeying a program with instructions and operands on the core store the 
slowing down 
during drum 
transfers can be by a factor 
of two if instructions, operands, and drum requests use the same stacks. It is also possible for the machine to be unaffected. The effect on a particular 
sequence of orders can be seen by considering the one discussed in 
Sec. 4 and illus- trated in Fig. 6. in this sequence the instructions are 
on stacks 0 and 1 while the operands are on stacks 2 and 3. if the drum or tape is transferring alternately to 
stacks 0 and 1 then the effect of any interrupt within 
the 3.2 psec of an instruction pair 
is to increase this time by 
between 0.5 and 3.4 pec depending upon where the interrupt occurred. The average increase 
is 1.8 psec and for a tape transfer with interrupts every 88 pec the computer can obey instructions at 98 per cent 
of the normal rate. During drum transfers the interrupts occur every 4 psec which would suggest a slowing down to 60 per cent 
of normal. However, for any regular sequence of orders the requests to the core store 
by the machine and by the drum rapidly become synchronized with the result in 
this particular 
case that the machine can still operate at 80 per cent of its normal speed. 
APPENDIX 2 METHODS OF DIVISION OF THE MAIN CORE STORE The maximum frequency with which requests can 
be dealt with by a single stack 
core store 
is governed by the cycle time of the store. If the store is divided into 
several stacks 
which can be cycled independently then 
the limit imposed 
on the speed of the machine 
by the core store 
is reduced. The degree of division which is chosen is dependent upon the ratio 
of core store cycle 
time to 
other machine opqrations and also upon the cost of the multiple 
selec- tion mechanisms required. , Considering a sequence 
of orders in 
which both the instruction and operand 
are in the core store, 
then for a single stack 
store the limit imposed on the operating speed by the store is two cycle times per order, Le., 4 psec in Atlas. This is significantly larger than the 
limits imposed by other 
sections of the computer (Sec. 4). If the store is divided into two stacks and instructions and operands are separated, then 
the limit is reduced to 
2 pec which is still rather high. The provision of two stacks permits the ad- 
dressing of the store to be arranged so that successive addresses 
are in alternate stacks. it is therefore possible by making requests to both 
stacks at the same 
time to 
read two instructions together, 
so reducing the number of access times to three per instruction 
pair. Unfortunately such 
an arrangement 
of the store 
means that operands are always on the same stacks as instruction pairs, and the limit imposed by 
the cycle time is still 2 pec per order even if the two operand requests in 
the instruction pair are to different stacks and occur 
at the same time. Division into any number of stacks with the addressing system working through each 
stack in turn cannot reduce 
the limit below 
2 psec since successive instructions normally occur in successive addresses and are therefore 
in the same stack. However, four stacks 
arranged in two pairs reduces 
the limit to 
1 psec as the operands can always be arranged to 
be on different stacks from the instruc- tion pairs. In order to reduce 
the limit to 0.5 psec it is necessary to have 
eight stacks arranged in two sets of four and to 
read four instructions at once, which would increase the complexity of the central machine. The limit of 1 pec is quite sufficient and further division with the stacks arranged in pairs 
only enables the limit to be more easily obtained by suitable location of the instructions and operands. The location of instructions and operands 
within the core store 
is under the control of the drum transfer program; 
thus when there 
290 Pari 3 1 The instruction-set processor level: variations in the processor 
20 156 Number of pages of Operands Fig. 8. Limit imposed by cycle time on operating speed for different divisions of the core store. 
Section 6 1 Processors with multiprogramming ability 
are several stacks 
instructions and operands are 
separated wherever possible. Under these conditions 
it is possible to calculate the limit imposed 
on the operating speed 
by the cycle time for different divisions of the core store. 
The results are shown in Fig. 8, for stacks 
arranged in pairs instructions 
are read 
in pairs and in all cases both instructions and operands are assumed to be 
on the core store. Operands 
are assumed to be selected 
at random from 
the operand space, 
for instance in the case of two stacks arranged as a pair, successive operand requests have equal probability of being to the same stack or to alternate stacks. The limit imposed by a 
four stack store 
is never severe com- 
pared with other limitations, for example the sequence of floating point addition 
orders discussed in Sec. 4 required 1.6 psec per order 
with ideal distribution 
of instructions and operands. Division into eight stacks, although it 
reduces the limit, will not have an equiv- alent effect on 
the over-all operating speed, and such a division was not considered to 
be justified. References KilbT62; BrooR60; EdwaD6O; KilbT56; 
mu, 60b, 61; LonsK56; PapiW57; FothJ61; HartD68; HowaDtil; 
62, 63; MorrD67; SumnF62 
Chapter 24 A user machine in a time-sharing system1 B. W. Lampson / W. W. Lichtenbqm / M. W. Pirtb Summoy This paper describes the design of the computer seen by a machine-language programmer in a time-sharing system developed at the 
University of California at Berkeley. Some of the instructions in this machine 
are executed by the hardware, and some are implemented 
by software. The user, however, thinks 
of them all as part of his machine, a machine having extensive 
and unusual capabilities, many of which might be part of the hardware of a (considerably more expensive) computer. Among the important features of the machine are the arithmetic 
and string manipulation instructions, the very general memory allocation and 
configuration mechanism, 
and the multiple processes which 
can be created by the program. Facilities are provided for communication among these processes and for the control of exceptional conditions. The input-output system is capable of handling all of the peripheral equipment in a dorm and convenient manner through files having sym- bolic names. Programs can 
access files belonging to a number of people, but each person can protect his own files from unauthorized access by others. Some mention is made at various points of the techniques of implemen- tation, but the main emphasis is on the appearance of the user™s machine. Introduction A characteristic of a time-sharing system is that the computer seen by the user programming 
in machine language 
differs from that on which the system is implemented [Bright, 
1964; Comfort, 1965; Forgie, 1965; McCullogh et al., 1965; Schwartz, 19641. In fact, the user machine is defined by 
the combination of the time-sharing hardware running in user mode 
and the software which controls 
input-output, deals with illegal actions which 
may be taken by a user™s program, and provides various 
other services. If the hard- ware is arranged in 
such a way that calls on the system have 
the same form as the hardware instructions of the machine [Lichten- berger and Pirtle, 19651, then the distinction becomes 
irrelevant to the user; he simply programs 
a machine with an unusual and powerful instruction 
set which relieves him 
of many of the prob- lems of conventional machine-language programming [Lampson, 1965; McCarthy et al., 19631. ‚Pm. IEEE, 54, vol. 12, pp. 1766-1774, December, 1966. In a time-sharing system which has 
been developed by and for the use of members of Project Genie 
at the University of California at Berkeley [Lichtenberger and Pirtle, 19651, the user machine 
has a number of interesting characteristics. The computer in 
this system is an SDS 930, a 24 bit, fixed-point machine with one index 
register, multi-level indirect addressing, a 14 bit address field, and 32 thousand words 
of 1.75 ps memory in two independent modules. Figure 1 shows the basic configuration 
of equipment. The memory is interleaved between 
the two modules so that processing and drum transfers may occur simultaneously. A detailed description of the various hardware modifications of the computer and their implications for 
the performance of the overall system has 
been given in a previous paper [Lichtenberger and Pirtle, 
19651. Briefly, these modifications include the addition of monitor and user modes in which, for user mode, 
the execution of a class of instructions is prevented and replaced by 
a trap to a system rou- 
tine. The protection from unauthorized access to memory has been 
subsumed in an address mapping scheme: both 
the 16 384 words addressable by 
a user program 
(logical addresses) and the 32 768 words of actual core memory (physical addresses) have 
been divided into 2048-word pages. A set of eight six-bit hardware regis- ters defines a map from the logical address space 
to the real memory 
by speclfying the real page which 
is to correspond to each of the user™s logical pages. Implicit in this 
scheme is the capability of marking each of the user™s pages as unassigned or read-only, 
so that any attempt to access such 
a page improperly will result 
in a trap. All memory references 
in user mode 
are mapped. In monitor mode, all memory references 
are normally absolute. 
It is possible, 
however, with any instruction 
in monitor mode, or even 
within a chain of indirect addressing, to specify use of the user map. 
Furthermore, in 
monitor mode 
the top 
4096 words are mapped through two additional registers called 
the monitor map. 
The mapping process is illustrated in 
Fig. 2. Another si@cant hardware 
modification is the mechanism for going between modes. Once the machine is in user mode, it can get to monitor mode 
under three 
circumstances: 291 
292 Part 3 I The instruction-set processor 
level: variations in the processor Magnetic m 1 processor I II interface Teletypes I L-- Memory 175esec I3x IO6 WORDS 51105 WDS/SEC I 1 General 1 Graphic display and light pen Fig. 1. Configuration of equipment. 
1 2 3 If a hardware interrupt occurs If a trap is generated by the user program as outlined. If an instruction with a particular configuration of two bits is executed. Such an instruction is called a system 
pro- grammed operator 
(SYSPOP). In case 
3, the six-bit operation field is 
used to select one of 64 locations in absolute core. 
The current address of the instruction is put into absolute location 
zero as a subroutine link, the indirect address bit of this link word is set, and another bit is set, marking the memory location in the link word as having come from user- mapped memory. The system routine thus 
invoked may take a parameter from the word addressed by the SYSPOP, since its 
address field is 
not interpreted 
by the hardware. The routine will Section 6 I Processors with multiprogramming ability 
address the parameter indirectly through location zero 
and, be- 
cause of the bit marking the contents of location zero 
as having come from mer mode, the user map will be applied to 
the re- mainder of the address indirection. 
All calls on 
the system which are not inadvertent 
are made in 
this way. A monitor mode program 
gets into user mode by 
transferring to an address with mapping 
specified. This means, 
among other things, that a SYSPOP can return to 
the user program simply by branching indirect 
through location 
zero. As the above discussion has perhaps indicated, the mode- changing arrangements are 
very clean and permit rapid 
and natu- ral transfers of control between user and system programs. Advan- 
tage has been 
taken of this fact to 
create a rather grandiose machine for the user. Its features 
are the subject of this paper. Basic features of 
the machine A user in the Berkeley time-sharing system, working at what he thinks of as the hardware language level, has 
at his disposal a 
machine with 
a configuration 
and capability which can be con- veniently controlled 
by the execution of machine instruction 
se- quences. Its simplest configuration is very similar 
to that of a POQ 3 0 4 I 5 2 6 3 7 4 8 5 9 6 10 7 11 I2 13 la l6K virtual core u15 32K real core (0) 0 23 13 j1010O11010110~ Virtual effective address 24654e joo01001( Mapping reglster 5 118 [go ,003 ;07m: Real effective address 44654a Read-only bit off (b) fl Fig. 2. The hardware memory map. 
(a) Relation between virtual and real memory for a typical 
map. (b) Construction of a real 
memory address. 
Chapter 24 I A user machine in a timesharing system 293 standard medium-sized computer. In this configuration, the machine possesses the standard 930 complement of arithmetic and 
logic instructions and, in addition, a set 
of software interpreted monitor and executive instructions. 
The latter instructions, which will be discussed more 
fully in 
the following, do rather complex input-output of many different kinds, perform many frequently used table lookup and string processing functions, implement floating point operations, 
and provide for the creation of more complex machine configurations. Some examples of the instructions available are: 
Load A, B, or X (index) registers 
from memory or store 
any of the registers. Indexing 
and indirect addressing 
are avail- able on these and almost all other instructions. Double word load and store are also available. The normal complement 
of fixed-point arithmetic and 
logic operations. Skips on 
various arithmetic and 
logic conditions. Floating point 
arithmetic and input-output. 
The latter 
is in free format 
or in the equivalent of Fortran E or F format. Input a character from a teletype or write a 
block of arbi- trary length on a drum file. Look up a string in a hash-coded 
table and 
obtain its 
posi- tion in the table. Create a new process and start it running concurrently with 
the present one at a specified point. Redefine the memory of the machine to 
include a portion 
of that which is also being used 
by another program. 
It should be emphasized that, 
although many 
of these instruc- 
tions are software interpreted, their 
format is identical to the standard machine instruction format, with 
the exception of the one bit which 
specifies a system interpreted instruction. Since the system interpretation of these instructions 
is completely invisible to the machine user, and since these instructions 
do have 
the standard machine instruction format, 
the user and his program make no distinction between hardware 
and software interpreted instructions. Some of the possible 192 operation codes 
are not legal in 
the user machine. Included in this category are those 
hardware in- structions which would halt 
the machine or interfere with the input-output if allowed to execute, and those software 
interpreted instructions which attempt to do things which are forbidden to the program. Attempted execution of one of these instructions will 
result in 
an ilkgal instruction violation. The effect of an illegal instruction violation 
is described later. Memory configuration The memory size 
and organization of the machine is specified by 
an appropriate 
sequence of instructions. For example, the user may specify a 
machine which 
has 6K of memory with 
addresses from 
0 to 13777,; alternatively, he may specify that the 
6K should include addresses 0 to 3777,, 14000, 
to 17777,, and 34oO0, to 37777,. The user may also specify 
the size and configuration of the machine™s secondary storage and, 
to a considerable extent, 
the structure of its input-output system. A full discussion of this capa- 
bility will 
be deferred to a later section. The next few 
paragraphs discuss the mechanism by 
which the user™s program may specify its memory size 
and organization. This 
mechanism, known as 
the process map to distinguish it from the hardware memory address 
mapping, uses a (software) 
mapping register consisting of eight 6-bit bytes, 
one byte for each of the eight 2K blocks addressable by the 14 bit address field of an in- struction. Each of these bytes 
either is 0 or addresses one of 
the 63 words in 
a table called the private memory 
table (PMT). Each user has his own private memory table. 
An entry in this table provides information 
about a particular 
2K block of memory. The block may be either local to the user or it may be shared. If the block is local, the entry 
gives information about 
whether it is currently in core or 
on the drum. This information 
is important to the system but need not concern 
the user. If the block is shared, 
its PMT entry points to an entry in another table called the shared memory table (SMT). Entries in this table describe blocks of memory which are 
shared by several users. Such blocks may con- tain invariant programs and 
constants, in 
which case they will 
be marked as read-only, or they 
may contain arbitrary data which is being processed by programs 
belonging to two different users. A possible arrangement of logical or virtual memory 
for a process is shown in Fig. 
3. The nature 
of each page has been noted in the picture of the virtual memory; this information 
can also be obtained by taking the corresponding byte of the map and 
looking at the PMT entry specified by that byte. The figure shows a large 
amount of shared memory, which suggests that the 
process might be a compilation, sharing 
the code for the compiler with 
other processes translating programs written in the same source language. Virtual pages 
one and two 
might hold tables 
and tem- porary storage 
which are unique 
to each separate compilation. Note that, 
although the flexibility of the map allows any block of code or data 
to appear anywhere in the virtual memory, it is certainly not 
true that a program can run regardless 
of which pages 
294 Part 3 I The instruction-set processor level: variations in 
the processor Page 5 1 UNASSIGNED 6 1 SHARED BL 3 16 K virtuol memory Entry block l:l: I Process Privote map memory table Fig. 3. Layout of virtual memory for a typical 
process. it is in. In particular, if it contains references 
to itself, such as branch instructions, then it must run in the 
same virtual pages 
into which it was loaded. Two instructions are provided which permit the user to read 
and modify his process 
map. The ability to read 
the process mapping registers permits the user to obtain 
the current memory assignment, and the 
ability to write the registers permits him to reassign memory in any way which suits his fancy. The system naturally checks each new map as it is established to ensure that the process is not attempting to obtain unauthorized 
access to memory which 
does not belong 
to it. When the user™s process is initiated, it 
is assigned only enough memory to contain the program data as initially loaded. 
For in- stance, if the program and constants occupy 3000, words, two blocks, say blocks 
0 and 1, will be assigned. At this point, the first two bytes of the process mapping register will be nonzero; the others will be zero. When the program runs, it may address memory outside of the first 4K. If it does, and if the user has specified a 
machine size larger than 4K, a new block of memory will be assigned to him which 
makes the formerly illegal reference legal. 
In this way, the user™s process may obtain 
more memory. 
In fact, it may easily obtain more than 16K of memory simply by ad- dressing 16K, reading and preserving the process mapping register, setting it with 
some of the bytes cleared to zero, and grabbing some more memory. 
Of course, only 
16K can be addressed at one time; this is a limitation imposed by 
the address field of the machine. Section 6 I Processors with multiprogramming ability 
There is an instruction which allows a process 
to specify the maximum amount of memory which 
it is allowed to have. If it attempts to obtain 
more than this amount, a memory violation will occur. A memory violation can also be caused by attempts 
to transfer into or indirect through 
unassigned memory, 
or to store 
into read-only memory. The effect of this violation is similar to the effect of an illegal instruction violation 
and will be discussed. The facilities just described are entirely sufficient for programs which need to reorganize the machine™s memory solely for internal purposes. In many cases, however, the program wishes to obtain access to memory blocks which have been created by 
the system or by other programs. For example, there may be a package of mathematical and utility routines in the system which the program would like to use. To accommodate this requirement, there 
is an instruction which establishes a 
relationship between a name and a certain process mapping function. This instruction 
moves the PMT entries for the blocks addressed by the specified process 
mapping function into the shared memory table so that they are generally accessible to all users. Once this correspondence has been established, there is another instruction 
which allows a 
different user to deliver the name and obtain in return 
the associ- ated process map. This 
instruction will, 
if necessary, make new entries in 
the second user™s PMT. Various subsystems and programs of general interest have 
names permanently assigned to them by the system. The user machine thus makes it possible for a number of proc- esses belonging to independent 
users to run with memory which 
is an arbitrary 
combination of blocks local to each 
individual process, blocks 
shared between 
several processes, and blocks per- manently available in the system. A complex configuration 
is sketched in 
Fig. 4. Process 1.1 was shown 
in more detail in 
Fig. 3. Each box represents a process, 
and the numbers within rep- resent the eight map bytes. The arrows between processes show the process hierarchy, which is discussed in the next section. Note that the PMT™s belong to the users, not to the 
processes. From the above discussion, it is apparent that the 
user can manipulate the machine memory configuration to perform simple memory overlays, to change data bases, or to perform other more complex tasks requiring memory reconfiguration. For example, the use of common routines is greatly facilitated, since 
it is necessary only to adjust the process map so that (1) memory references internal and external to the 
common routine are correct, and (2) the memory area in which the routine resides is read-only. In the simplest case, in which the common routine and the data 
base fit into 16K of memory, the map is initially established 
and remains static throughout the execution of the routine. 
In other cases where 
Chapter 24 I A user machine in a time-sharing system 295 the routine and data base do not fit into 16K, or where several common routines 
are concurrently employed, 
it may be necessary to make frequent adjustment 
to the map during execution. 
Multiple processes An important feature of the user machine allows the user program, which in the current 
context will be referred to as the controlling process, to establish one or more subsidiary processes. With a few minor exceptions, to be discussed, each subsidiary 
process has the same status as the controlling process. Thus, it may in turn estab- lish a subsidiary 
process. It is therefore apparent that the user machine is in fact a multi-processing machine. 
The original sug- gestion which gave rise 
to this capability 
was made by Conway 
[Conway, 19631, more recently the Multics system has included 
a multi-process capability [Corbato 
and Vyssotsky, 1965; 
Dennis and Van Horn, 1966; Saltzer, 19661. A process is the logical environment 
for the execution of a program, as contrasted to the physical environment, 
which is a hardware processor. It is defmed by the information which is re- quired for the program to run; this information 
is called the state vector. To create a new process, a given process 
executes an in- struction which has arguments specifying 
the state vector of the new process. This state vector includes 
the program counter, the 
central registers, and the process map. The new process may have a memory 
configuration which is the same as, or completely 
differ- ent from, that of the originating process. The only constraint placed on this memory 
specification is that the 
total memory available to the multi-process system is limited to 128K by the process mapping mechanism, 
which is common to all processes. Each user, of course, has his own 128K. This facility 
was put into the system so that the 
system could control the user processes. 
It is also of direct value, however, 
for many user processes. 
The most obvious examples 
are input-output buffering routines, 
which can operate independently of the user™s main program, communicating with it through memory 
and with 
interrupts (see the following). Whether the 
operation being 
buff- ered is large volume output to a disc or teletype requests for information about 
the progress of a running program, 
the degree of flexibility afforded by multiple processes far exceeds anything which could have been 
built into the input-output 
system. Fur- thermore, the overhead is very low: an additional 
process requires about 15 
words of core, and process switching takes about 
1 ms under favorable conditions. 
There are numerous other examples of the value of multiple processes; most, 
unfortunately, are too 
complex to be briefly 
explained. A process may create a number of subsidiary processes, each , of which is independent of the others and equivalent to them 
from the point of view of the originating process. Figure 4 shows two simple multi-process structures, 
one for each of two users. Note 
that each process has associated 
with it pointers to its 
controlling process and to one of its subsidiary 
processes. When a process has 
two immediate 
descendants, as in the case of processes 1.2 
and 1.3, they are chained together 
on a ring. Thus, 
three pointers, up, down, and ring, suffice to defme the process structure completely. The up 
pointers are, 
of course, redundant, 
but are convenient for the implementation. The process is identified by a process number which is returned by the system when it is created. A complex structure such as that in Fig. 5 may result from the creation of a number of subsidiary processes. The processes in 
Fig. 5 have 
been numbered 
arbitrarily to allow a clear description 
of the way in which the pointers are arranged. Note that the 
user need not be aware of these pointers; they 
are shown here to clarify the manner in which the multiple process mechanism 
is imple- mented. A process may destroy one of its subsidiary 
processes by 
execut- ing the appropriate 
instruction. For obvious reasons 
this operation 
is not legal if the process being destroyed 
itself has subsidiary 
PMT 1 1 M3 2 M4 3 M5 4 SMT1 5 SMT4 6 SMT2 7 M12 8 SMT6 9 SMT3 10 PMT 2 1 SMT1 2 SMT5 3 M7 4 M8 5 M9 6 SMT2 7 M13 8 SMT3 9 M14 IO M15 SMT 1 M1 2 MI6 3 M2 4 M1O 5 M11 6 M6 Fig. 4. Process and memory configuration 
for two users. (The processes are numbered for each user 
and are represented by 
their process map ping registers. Memory blocks are identified by drum addresses, which 
are written M1, M2, . . . .) 
296 Part 3 I The instruction-set processor level: variations 
in the processor 
Section 6 I Processors with multiprogramming ability 
Fig. 5. Hierarchy of processes. processes. It is possible to find out what processes are subsidiary to any given one; this permits a 
process to destroy an entire tree 
of sub-processes by 
reading the tree 
from the top 
down and de- stroying it from the bottom up. The operations of creating and destroying 
processes are entirely separate from those 
of starting and stopping 
their execution, for which two more operations are provided. A process whose execu- 
tion has been stopped 
is said to be suspended. To assure that these various processes 
can effectively work 
together on a common task, several means 
of interprocess com- munication exist. The first allows the controlling process to obtain 
the current status 
of each of its subsidiary processes. This 
status information, which 
is read into 
a table by the execution of the appropriate system instruction, includes the current state 
vector and operating status. 
The operating status of any process may 
be 1 Running 2 Dismissed for input-output 3 Terminated for memory violation 
4 5 Terminated for illegal violation, or 
Terminated by the process itself A second instruction 
allows the controlling process to become 
dormant until one of its subsidiary processes 
terminates. Termina- tion can occur in 
the following four ways: 1 2 3 Because of self-termination Because of a memory violation 
Because of an illegal instruction violation Interactions described 
previously provide no 
method by which 
a process can attract the attention 
of another process which 
is pursuing an independent 
course. This 
can be done with a 
program interrupt. Associated with 
each process is a 20-bit interrupt mask. If a mask bit is set, the 
process may, 
under certain conditions (to be described in the following), be interrupted; Le., a transfer to a fixed address will 
be simulated. The program will 
presumably have at this fixed address the location of a subroutine 
capable of dealing with the interrupt and returning to 
the interrupted 
com- putation afterwards. The mechanism is functionally almost identi- cal to 
many hardware interrupt systems. A process may cause 
an interrupt 
by delivering the number of the interrupt to 
the appropriate 
instruction. The process causing 
the interrupt continues undisturbed, 
but the 
nearest process which 
is either on the same level 
as the one causing the interrupt 
or above it in 
the hierarchy of processes, and which has 
the appro- priate interrupt armed, will be interrupted. This mechanism pro- 
vides a very flexible way for processes 
to interact with 
each other without wasting any time in 
the testing of flags or similar frivolous 
activities. Interrupts may be caused not only by 
the explicit action of processes, but also by the occurrence of several special conditions. The occurrence of a memory violation, 
attempted execution of an illegal instruction, an unusual input-output condition, the ter- mination of a subsidiary process, or 
the intervention of a user at a console (by pushing a 
reserved button) all may cause 
unique interrupts (if they have been 
previously armed). In this way, a process may 
be notified conveniently of any unusual conditions 
associated with other processes, the process itself, or a console user. 
The memory assignment algorithm discussed previously 
is slightly modified 
in the presence of multiple processes. When a process is activated, one of three options may be specified: 1 Assign new memory 
to the 
process entirely independently of the controlling process. Assign no new memory 
to the process. Any attempt to obtain new memory will cause 
a memory violation. 
2 
Chapter 24 1 A user machine in a time-sharing system 297 3 If the process attempts to obtain new memory, scan upward through the process hierarchy until the topmost process is reached. If at any time during this scan a 
process is found for which the address causing 
the trap is legal, propagate 
the memory assigned to it down through the hierarchy to the process causing the trap. Option 3 permits a 
process to be started with a subset 
of memory and later to reacquire some of the memory which was not given to it initially. This 
feature is important because 
the amount of memory assigned to a process influences 
the operating efficiency of the system and thus the speed with 
which it will be able to respond to teletypes or 
other real-time devices. The input-output system The user machine has a straightforward 
but unconventional set of input-output instructions. The primary emphasis in 
the design of these instructions has 
been to 
make all 
input-output devices interface identically with a 
program and to provide as much flexibility in this common interface 
as possible. Two advantages result from this uniformity: 
it becomes 
natural to write programs which are 
essentially independent 
of the environment in which they operate, and the implementation of the system is greatly simplified. To the user the former point 
is, of course, the important 
one. It has been common, for example, for programs 
written to be controlled from a teletype to be driven instead 
from a file on, let us say, the drum. A command exists which permits the recognizer for the system command language and all 
of the subsystems to be driven in this 
way. This device 
is particularly useful for 
repeti- tive sequences 
of program assemblies and for background jobs which are 
run in 
the absence of the user. Output which normally goes to the teletype is similarly diverted 
to user files. Another application of the uniformity of the file system is demonstrated in some of the subsystems, notably the assembler and the 
various compilers. The subsystem may request the user to specify where he wishes the program listing to be placed. The user may choose anything from paper tape to drum to 
his own teletype. In the absence of file uniformity each 
subsystem would 
require a 
separate block of code for each possibility. In fact, however, 
the same input-output instructions are 
used for 
all cases. The input-output instructions communicate with jiles. The system in turn associates 
files with the various physical devices. Programs, for 
the most part, do not have to account for the pecu- liarities of the various actual devices. Since 
devices differ widely in characteristics 
and behavior, the flexibility of the operations available on files is 
clearly critical. 
They must range from single- 
character input 
to the output 
of thousands of words. A file is opened by giving its name as an 
argument to the appropriate instruction. Programs thus refer 
to all files symboli- cally, leaving 
the details of physical location 
and organization to the system. r'f authorized, a 
program may refer 
to files belonging to other users by 
supplying the name of the other user as well 
as the file name. The owner of a file determines who 
is authorized to access it. 
The reader may compare this file naming mechanism with a more sophisticated one [Daley and Neumann, 19651, bearing in mind the fact the file names can be of any length and can be manipulated (as strings of characters) by the program. Access to files is, in general, 
either sequential or 
random in nature. Some devices (like a 
keyboard-display or a 
card reader) are purely sequential, while others (like a 
disk) may be either 
sequentially or 
randomly accessed. There are accordingly 
two major 1/0 interfaces to deal with these different qualities. 
The interface used in conjunction with a 
given file depends on whether the file was declared to be a random or a sequential 
file. The two major interfaces are each broken 
down into 
other interfaces, pri- 
marily for reasons 
of implementation. Although the distinction between sequential and random files is great, the subinterfaces are not especially 
visible to the user. Sequential J;k The three instructions CIO (character input-output), WIO (word input-output), and BIO (block input-output) are used 
to commu- nicate with a sequential file. Each instruction takes 
as an operand ajile number. This number is given to the program when 
it opens a file. At the time of opening a 
file it must be specified whether the file is to be read from or written onto. Whether any given device associated with 
the file is 
character-oriented or 
word- oriented is unimportant; the system takes care of all necessary 
character-to-word assembly or 
word-to-character disassembly. There are actually 
three separate, full-duplex physical 
inter- faces to devices in the sequential file mechanism. Generally, these 
interfaces are invisible to programs. They exist, of course, for reasons of system efficiency 
and also, because of the way in 
which some devices are 
used. The interfaces are: 
Character-by-character (basically 
for low-speed, character- oriented devices used for 
man-machine interaction) Buffered block 
1/0 (for medium-speed 1/0 applications) Block 1/0 directly from user core (for high-speed situations) 

298 Part 3 I The instruction-set processor level: variations in 
the processor It should be pointed out 
that there 
is no particular relation be- tween these interfaces 
and the 
three instructions CIO, WIO, and 
BIO. The interface used in a given 
situation is a function of the device involved and, sometimes, of the volume of data to be trans- mitted, not 
of the instruction. Any interface may be driven by any 
instruction. Of the three subinterfaces under discussion, the last two are straightforward. The character-by-character interface is, however, somewhat different and deserves some elaboration. Devices 
associ- ated with 
this interface are generally (but not necessarily) used for man-machine interaction. Consider the case of a person 
com- municating with 
a program by means 
of a keyboard-display (or a teletype). He types on the keyboard and the information is transmitted to 
the computer. The program may wish to make an immediate response on the display screen. In 
many cases this response will consist 
of an echo of the same character, 
so that the user has 
the feeling of typing directly onto 
the screen (or 
onto the teleprinter). So that input-output can be carried out when the program is 
not actually in main memory, 
the character-by-character input 
interface permits 
programs a choice of a number of echo tables; it further permits 
programs a choice of grade of service by per- mitting them 
to specify whether a given 
character is an attention (or break) character. Thus, for example, the program may 
specify that each character typed 
is to be echoed immediately and that 
all control 
characters are to result in activation of the program regardless of the number of characters in the input buffer. Alter- natively, the program may 
specify that no characters are echoed and every character is a break character. 
By changing the specifi- cation the program can obtain an appropriate (and 
varying) grade of service without putting undue 
load on the system. Figure 6 Output interrupt routine Fig. 6. The character-oriented interface. 
Section 6 I Processors with 
multiprogramming ability shows the components of the character-by-character interface; 
responsibility for 
its operation 
is split between the interrupt 
called when the device signals for attention and the 
routine which proc- esses the user™s 1/0 request. The advantage of the full-duplex, character-by-character mode of operation is considerable. The character-by-character capability 
means that the user can interact 
with his program in the smallest possible unit-the 
character. Furthermore, 
the full-duplex capa- bility permits, among other things (1) the program to substitute 
characters on 
strings of characters as echoes for those received, 
(2) the keyboard and display to be used simultaneously 
(as, for example, permitting a character typed on a keyboard to pre-empt the operation of a process. 
In the case of typing information in during the output 
of information, a simple algorithm prevents 
the random admixture of characters which might 
otherwise result), 
and (3) the ready detection of transmission errors. Instructions are included to enable 
the state of both input and 
output buffers to be sensed and perhaps cleared 
(discarding un- 
wanted output 
or input). Of course, it is possible for a 
program to use any number of authorized physical devices; in 
particular, this includes 
those devices used as remote consoles. A mechanism is provided to permit output which is directed to 
a given 
device to be copied on all other devices which are output linked to it 
(and similarly for 
input). This is useful when communication among users is 
desired and in numerous other situations. The sequential file has a 
structure somewhat similar to that of an ordinary magtape file. It consists of a sequence of logical records of arbitrary length and number. On some devices, such 
as a card reader 
or the teletype, a file may have only one logical record. The full generality 
is available for drum files, which are the ones most commonly used. The logical record is to be con- trasted with 
the variable length physical record of magtape or the fixed length record 
of a card. Instructions are provided to insert or delete logical records and increase or decrease 
them in length. Other instructions permit the file to be ﬁpositionedﬂ almost in- stantaneously to a specified logical 
record. This 
gives the sequen- tial file greater flexibility than one which is completely unaddressa- 
ble. This flexibility is only possible, 
of course, because the file is on a 
random-access device 
and the 
sequential structure 
is main- tained by pointers. The implementation is discussed in the follow- ing. When reading a sequential file, CIO and WIO return certain unusual data configurations when they encounter an end 
of record or end of file, and BIO terminates transmission on either of the conditions and returns the address of the last word transmitted. In addition, certain flag bits are set by the unusual conditions, 
and an interrupt may be caused if it has been armed. 

Chapter 24 I A user machine in a time-sharing system 299 The implementation 
of the sequential file scheme for auxiliary storage is illustrated in Fig. 7. Information is written on the drum in 256-word physical records. 
The locations of these records 
are kept track 
of in 64-word index blocks 
containing pointers 
to the data blocks. For the file shown, the first logical record 
is more 
than 256 words long but ends in 
the second 256-word block. 
The second logical record 
fits in the third 
256-word block 
and the third logical record-in the 4th data block-is followed by an end of file. If a file requires more than 
64 index words, 
additional index blocks are chained together, both forward and backward. Thus, 
in order to 
access information in 
the He it is necessary only 
to know the location of the first index block. 
It may be worthwhile to point out that all users share the same drum. Since the system has complete control over the allocation of space on the drum, there is no possibility of undesired interaction among users. Available space for new data blocks or index blocks 
is kept track 
of by a bit table, illustrated 
in Fig. 
8. In the figure, each column represents one of the 72 physical bands 
on the drum allocated 
for the storage of file information. Each row represents one of the 64256-word sectors around 
a band. Each bit 
in the table 
thus represents one of the 4608 data blocks available. The bits are set 
when a 
block is in use and cleared when the block becomes 
avail- able. Thus, if a new data block is required, the system has only to read the 
physical position 
of the drum, use this position to index in the table, 
and search a 
row for 
the appearance of a 0. The column in which a 
0 is found indicates the physical track on which a block is available. Because of the way the row was 
chosen, this block is immediately accessible. This 
scheme has two advantages over its alternative, which 
is to chain unused 
blocks together: It is easy to find a block in an optimum position, using the algorithm just described. 1 EOR/ EOF Fig. 7. Index blocks and pointers to data blocks. 64 words 72 bits I Fig. 8. Bit table 
for allocation of space on the drum. 2 No drum operations are required 
when a new block is needed or an old one 
is to be released. It may be preferable to assign the new block so that it becomes accessible immediately 
after the block last assigned for the file. This scheme will speed up 
subsequent reading of the file. Random $la Auxiliary storage files can also be treated as extensions of core memory rather than 
as sequential devices. Such 
files are called 
random fiZes. A random file differs from a sequential 
file in that there is no logical record 
structure to 
the file and that 
information is extracted from or written into 
the random file by addressing a specific word or block 
of words. It may be opened like a sequen- tial file; the only difference is that it need not be 
specified as an output or an input file. Four instructions are 
used to input 
and output words and blocks of words on a random 
file. To permit the random file to look even more like core memory, an instruction enables one 
of the currently 
open random files to be specified as the secondury memory file. Two instructions, LAS (load A from secondary memory) and SAS (store A in secondary memory), act like ordinary load and store instructions with one level 
of indirect addressing (see 
Fig. 9) ex- cept, of course, that the data are 
in a random 
file instead of in core memory. Random files are implemented like sequential files except that end of record indicators are 
not meaningful. 
Although as many index blocks are used up as required by the size of a random file, only those data blocks which actually contain 
information will be 
attached to a 
random file. As new locations 
are accessed, new 
data blocks are attached. 
Subroutine $lea Whereas it makes little sense to associate, say, a card reader 
with a random file, a sequential 
file can be associated with any physi- 
300 Part 3 1 The instruction-set processor level: variations 
in the processor Main memory Secondary memory STAx ADDR ADDR 
- ~- (a) Instruction 16345 16345 1234567 Effect (234567-A (b) Fig. 9. Load and store form main and 
secondary memory. (a) Instruc- tions. (b) Addressing. cal device in the system. In addition, a sequential file may be associated with a subroutine. Such a file is called a subroutine $le, and the subroutine may thus be thought of as a ﬁnonphysicalﬂ device. The subroutine file is defined by the 
address of a subroutine together with 
information indicating whether it 
is an input 
or an output file and whether it 
is word or character oriented. An input operation from a subroutine file causes the subroutine to 
be called. When it returns, the contents of the A register is taken to be the input requested. Correspondingly, 
an output operation causes the subroutine to be called with the word or character being 
output in A. The subroutine is completely unrestricted in the kinds of processing it can 
do. It may do further 
input or output and 
any amount of computation. It may even call itself if it preserves the old return address. Recall that for sequential 
files the system transforms 
all infor- mation supplied by the user to the format required by the particu- Section 6 1 Processors with 
multiprogramming ability lar file; hence, the requirement that the 
user, in opening 
a sub- 
routine file, must specify whether the file is to be character or word oriented. The system will 
thereafter do 
all the necessary packing and unpacking. Subroutine files are the 
logical end-product of a desire to de- couple a program from its environment. Since they can do 
arbi- trary computations, they can provide buffers of any desired com- plexity between the assumptions a program has made about 
its environment and the true 
state of things. In fact, they 
make it logically unnecessary to provide an identical interface for all the input-output devices attached to the system; if uniformity did not exist, it could be simulated with the appropriate subroutine 
files. Considerations of convenience and efficiency, of course, militate against such 
an arrangement, but it suggests the power inherent in the subroutine file machinery. Summary The user machine described was designed to be a flexible founda- tion for development and experimentation in man-machine sys- tems. The user has been given the capability to 
establish configura- tions of multiple processes, and the 
processes have the ability to communicate conveniently with each other, with central 
files, and with peripheral 
devices. A given user may, 
of course, wish only to use a subsystem 
of the general system (e.g., a compiler or 
a debugging routine) for his particular job. In the course of using the subsystem, however, he may become dissatisfied with it and wish to revise or even rewrite the subsystem. The features of the user machine not 
only permit this activity but make it easier. References BrigHM; ComfW65; ConwM63; CorbF65; DaleR65; DennJ66; ForgJ65; LampB65; LichW65; McCaJ63; McCuJ65; SaltJ66; SchwJ64 
The instruction-set 
processor level: special-function processors This part contains 
descriptions of processors that do not interpret general pro- gramming languages; that is, they are 
not Pc™s. They 
are all p™s, however, since they have 
an interpreter that determines not only the operations to be taken, 
given the current instruction, but the 
next instruction to be obtained. A Pi0 (Sec. 1) is a processor 
that controls T and Ms components. 
It manages block or vector transmission between Ms or T and Mp. A P.array (Sec. 2) processes both vectors and two-dimensional matrices. By recognizing these data as fundamental units, programs (or algorithms) 
can be expressed efficiently in terms of primitive operators. The chief 
advantage of these P™s is their ability to take advantage of the data structure for parallel interpretation, thereby increasing processing speed. A microprogram processor (Sec. 3) is designed to interpret and process a data- type which is a 
program. In effect, this processor is a 
computer within another computer, programmed 
to act as an interpreter. A language processor 
(Sec. 4) interprets a data-type derived from the primitives of a programming language. In contrast, a conventional processor interprets a language based on fundamental hardware implementation primitives. The difference is clearly 
apparent as increased complexity of 
the language processors. 301 

Section 1 Processors to control terminals and secondary memories (i nput-output processors) The first three chapters 
of this section show the evolution of 
the IBM Data Channels (io processors) from 1958 (the 
7094 II) to the present (the 1800, which came 
after the 
360). The processor approach 
for controlling 
T and Ms components, while 
more general, should 
be contrasted with 
the specialized one- 
instruction controls in the B 5000 (Chap. 22) and Burroughs D825 (Chap. 36). The fourth chapter, on the DEC 338, shows a processor that controls cathode-raytube display consoles. 
The graphic termi- nals are the first T™s of sufficient 
complexity to utilize a proc- 
essor of their 
own. The 
first CRT displays used the Pc (e.g., on Whirlwind); then small 
Pc™s were adapted to the task; the DEC 338 is one 
of the earliest special P.display™s 
that ap- pea red. There is no example 
in this section of a 
specialized P for 
message concentration and switching. For computer systems 
multiple remote inputs are still recent enough 
so that either the main Pc handles the task, via specialized K, or small 
Pc™s are committed to it. However, in the telephone industry there 
has been a very substantial development by 
the Bell System 
of the Electronic Switching 
System (ESS), which uses specialized C™s to control switching 
(routing). In computer systems, we can expect the use of such specialized processors 
to increase in the near future. The IBM 7094 II The IBM 709, 
a member of the 
IBM 701-7094 II family, is 
one of the first computers to have an io processor (IBM name: Data Channel) in its structure. Chapter 41 discusses the two 
Data Channel types: the early 7607 and the later 7909. The 
7909 Data Channel ISP, and a K which it controls, are given in Ap- pendix 2 and 3 of Chap. 41. The principal difference 
is that Pc controls the Pi0 (‚7909) which 
in turn controls the K, which in turn controls a 
T or Ms; the Pc controls the Pi0 (‚7607) 
and the K; the K controls the 
T or Ms. The series is discussed in Part 6, Sec. 1, page 515. The structure of System/360 Part I-outline of the logical structure 
The io processors 
(Selector and Multiplexor Channels) 
in the System/360 have evolved from the IBM 
701-7094 II Series. Part 6, Sec. 3 presents the ISP and PMS structures for these proc- essors. Depending on thecomputer model, the implementations 
are realized by a microprogrammed processor interpreting a shared control program 
for both Pio™s and Pc, or by a hardwired Pio. The multiple Pio™s in a 360 Multiplexor Channel, 
though logically independent, are 
implemented as a single, shared physical processor. 
The IBM 1800 The Pio™s in this structure are presented 
in Chap. 33, and the structure is discussed in Part 5, Sec. 2, page 396. The Digital 
Equipment Corporation 
DEC 338 display processor The DEC 338 is an 
early P.display. It directly interprets a 
stored program to control a T.display. Earlier T.displays were con- trolled by Pc (Whirlwind, Chap. 6), or by a special K.display without stored-program 
capability, or by a general-purpose Pio. The last method outputs fixed length blocks containing data to be interpreted 
by T.display as points, vectors, characters, curved line segments, etc. The 
control of T.display first by Pc, then by a K, then by a Pio, and finally 
by a P.display has been observed as an evolution [Myer and Sutherland, 19681. Myer and Sutherland also 
observe that the 
evolution is about to become a closed cycle 
because the generality of a Pc is needed 
to control a T.display. Note that the 338 has a very extensive ISP. In fact, the 
P.display™s ISP is more extensive than the companion Pc of the PDP8 (Chap. 5). There are some display tasks which require 
Pc, for example, compiling programs 
(pictures), calculating elaborate light-pen tracking figures, making coordinate and curved lines to straight-line vector approximation transforma- tions, and communicating 
with other system components. 
303 
304 Part 4 I The instruction-set processor 
level: special-function processors 
Another approach 
to the design of a P.display 
is based on a P.microprogram which is shared among many T.displays [Rose, 19671. Yet another alternative, which has 
not yet been tried, is to incorporate a Pi0 (P.display) 
as a special 
mode in a conventional Pc. Thus the P would 
interpret either conven- 
tional Pc instructions or P.display instructions. P.display is the interpreter for the output 
of pictures or graphics. The 338 utilizes data 
space efficiently simply 
because the data are long variable-length strings (word vectors). The 
instruction requires almost no space to specify the data opera- 
tions and 
addresses; data are 
interpreted directly or 
immedi- ately in the instruction rather 
than via instruction addresses. Another feature 
which allows 
a program to be efficiently encoded is the stack 
mechanism for storing 
subroutine link- ages. Subroutines in P.displays are 
actually programs which form part of 
a more complete picture. Subroutines 
are actually subpictures. Although 
the stack mechanism allows for recursive picture calls, the stack is 
used principally to save space 
and to allow multiple T.displays to use common picture programs. A problem in the 338 which is common to all multi-P struc- tures is 
intercommunication among the 
P™s. Pc is the control- 
ling P, as is the case with most Pc-Pi0 structures. 
The P(™338) has no trap to itself but relies on an interrupt 
signal to Pc. The Pc processes both tasks which P.display might process, given 
Section 1 I Processors to control terminals and secondary memories 
an interrupt 
system, and other tasks beyond P.display™s capa- bility. A clock 
should be built into the 338. The brightness or 
in- tensity of 
a picture is determined 
both electronically (see the mode instructions 
for controlling intensity) 
and by the rate 
at which the pictures are 
repeated. A clock would 
allow the time 
when pictures are started or drawn to be specified; 
thus the 
intensity would be independent of picture length. The 338 requires more hardware than a simpler Pc. However, a large 
amount of this hardware is used to control the genera- tion of characters and lines. The lines 
(vectors) are drawn using a DDS (Digital Differential Analyzer) technique. Perhaps 
one-half of the registers could 
be eliminated if the 
338 were not a P. A simpler alternative was constructed about 
a similar computer, the 
PDP-9, by Bell Telephone Laboratories and DEC, using the 
approach of making the 
display only 
a K. A more elaborate Pc interrupt system with reduced overhead 
time would enable 
Pc to take on the 
specialized program control 
functions in the 338. Such a scheme 
might pass the program or instruction counter 
parameter directly 
from P.display to Pc. In this way, Pc or P.display would alternatively process part of a single instruction stream, depending 
on the 
task. Despite the problems of this early P.display, it has a sophis- tication which successors appear 
to be following. 
Chapter 25 The DEC 338 display computer Introduction The C(disp1ay; ‚DEC 338) is a C(™DEC PDP-8) with a 
P.display which can connect to 
T( #1:8; CRT; display; area: 9.375 x 9.375 in.2). The PMS structure is shown in Fig. 1, Chap. 5, describing the PDP-8. The Pc ISP is 
given in Appendix 1 of Chap. 5. The C(338), although designed to stand alone, 
is generally used as a satellite to a larger 
C, via an L(Dataphone). The rationale 
for using a C as a T is based on the bandwidth and storage require- 
ments needed to maintain graphical picture 
displays. A human being manipulating pictures 
(rotation, scale change, and conver- sion of internal linked 
data structure to a picture structure) re- quires short response time; this requirement places 
high processing 
demands on larger C™s. Thus this C(disp1ay) is a preprocessor for larger, more 
general C™s. The actual T(CRT) 
is a 16-inch CRT with a 
93/,-inch square viewing area covered 
by 1,024 x 1,024 (XY) points. The diameter 
of the points is -0.015 inch. The spot 
is magnetically deflected 
and focused. All eight T(CRT)™s can be driven together 
or used ™ Eaecuted by Pc to start F! display Executed by Pc to stw I? display ™ Data state ‚statesﬂ 4 control stoteﬂstated™ Stote transitions occur approximately each 
Mp cycle Control state instructions Fig. 1. DEC 338 instruction-interpretation state 
diagram. independently. A photomultiplier connected through a 
fiber-optic bundle link is used as a light pen (a photosensitive sensor) 
to detect spots on the T. The light pen 
allows the P.display to detect whether a 
user has ﬁpointed toﬂ a displayed 
spot. Pc and P.display access 
the same Mp; 
the total data rate 
avail- able from Mp is 
one 12-bit word/l.5 microseconds. The instruction times of P.display are a function 
of the point plotting times 
of the T(CRT):0.3 microsecond to the next incremental unintensified point (approximately 
0.010 inch away); 
1.2 microseconds to an incremental intensified point; and 35 
microseconds to a point 
plotted at a random position. The state (registers) of C.display is given in the ISP description of Appendix 1 of this chapter. There 
are four parts of the state: the control registers for Program Flow State, 
the Picture State 
(or position of beam), Console and Light-pen State, and Mp State. 
The instruction 
interpreter is fairly simple and 
is best described 
by the state diagram (Fig. 1). The instructions are given in Tables 1 and 2. The remainder 
of the chapter 
discusses the P.display instructions and 
the Pc 
instructions for communicating with 
P.dis- Play* Principle of operation The actual picture 
is held stationary 
by repeatedly displaying (intensifying) a particular point, 
line, etc. The number 
of times a figure has to be displayed so that it appears stationary and 
does not flicker depends on the CRT phosphor, the figure, and environ- 
mental parameters. 
The generally accepted range 
is a plotting 
rate of 20 - 50 plots/second; thus a complete picture 
has to be drawn 
in 50 - 20 milliseconds. If we assume a 30-Hz plot rate, about 28,000 points can be plotted 
in vector mode (or 
280 - 1120 inches, depending on the spacing). About 1,OOO characters can 
be dis- 
played in 30 milliseconds using character mode. When the 
light pen is used, a display program is required to ﬁtrackﬂ the pen. The 
pen™s position is determined by displaying 
known points. 
The pen, 
of course, detects the points when it is present at the 
displayed points 
position; therefore the 
program knows the location of the pen. The parameters of interest for a 
display vary, 
depending on the application. However, the general parameters 
are: 305 
306 Part 4 I The instruction-set processor level: special-function processors skip skip if 
not in sector count 0 + +1 scale 14 -1 1 Group number (0:l) set unit 0 It pen Section 1 I Processors to control terminals 
and secondary 
memories skip if 
PB (0 5) = 0 count Intensity Intensity -~ Table 1 Instruction Op Code DEC 338 control-mode instruction 
set Bits 0:2 Parameters Mode Conditional skip Conditional skip Arithmetic compare PB Arithmetic compare PB Skip on flags Count Set slaves Spare 3 4 5 6 7 8 9 10 11 sett Scale Scale (0:l) set It pen It pen set 1 Intensity (0:2) Intensity I stop clear set Scale 
1 Scale (0:1) 1 set It pen 1 It pen 1 push clear 1 ;,e; 1 enter sector Data-State 
Memory field 
(0:2) set Scale 
~ Scale (0:1) ~ set It pen ~ It pen 1 inhs Data-Mode inh Scale, It pen Push-Buttons (0:5)/PB 
(0:5) test Push-Buttons (6: 11)/PB (6: 11) test 0 ~ Push,Buttons (0:5) 0 lo Push-Buttons (6: 11) ~1 0- +1 o+ -1 set unit 1 1 It pen 1 Intensity t Set; allow instruction bits to specify new value. A two-word instruction, second word 
contains low-order 12 bits for 
DAC (jump address). 7 Skip can be for true or false. 8 Inhibit restoration of bits 1 Picture 3 a Display area b c Spot size 
d Resolution e Linearity f Short-term and long-term stability 
Phosphor type (intensity and 
color as function of time) 4 2 Figure plotting (generation) characteristics 
a Data types: points, 
lines (vectors), graphs, characters (from a 
fixed set), characters (from a defined set), curved- 
line segments, etc. b Plotting time Transformation and internal 
representations a Space to encode (specify) a figure b Scale change, rotation, 
coordinate-system transformation abilities c Ability to communicate between a displayed data structure and an internal representation 
of a picture Light-pen or graphic input capability 
Chapter 25 1 The DEC 338 display computer 307 inta inhb escc inh Instructions and their interpretation 
in P(display) Two instruction-set types are interpreted 
in the P.display: Data State, in which instructions 
specify display information; 
and Con- 
trol State, 
in which instructions 
specify program control 
informa- tion (e.g., jumps, modes, 
etc.). A state diagram for the interpre- tation process is given in Fig. 1. Y coordinate X coordinate Data-state instructions There are seven instructions (which DEC calls modes) that can 
be executed while P.display 
is in data state. The instructions (modes) are really substates of data state. The 
instructions (actually 
character 1 Table 2 DEC 338 data-made instruction set character 2 more like data) are interpreted 
for the mode. When all the data- mode instructions have been 
interpreted, an escape instruction returns the P.display to control state. A control instruction 
is issued to select a 
mode and simultaneously place 
the display in data state. 
blank Increment mode. This mode 
is used to draw curves and alpha- numeric characters and other small symbols. Two instructions are stored per word. 
An instruction will cause 
the beam position to be moved one, two, 
or three times, in 0.010-inch increments, in one of eight directions. Direction 
0 is to the right, direction 
1 is up and to the right, etc. character point increment vector vector continue short vector 6-bit character 7-bit character graph plot spare esc 6 - 35 1.5 + 2 x (.9 - 3.6) 1 - 150 1 - 1,200 1.8 - 24 3.75 + 4.5+ 6 - 35 X/vf Y or X coordinate 1 of 2 2 of 2 1 1 of 2 2 of 2 1 of 2 2 of 2 1 1 1 1 Intensify; turn on beam. Inhibit; do not set value 
into Y or X coordinate. e Escape; enter control state. d0 + move 1 and escape: 1, 2, 3, + move 1. 2, 3. '0 + set Y and increment X; 1 + set X and increment y. 8 directions. same as bits 
0 - 5 int 1 cmoo;v 1 move directione int f Delta Y esc 1 -C 1 Delta X int esc 1 z I int 1 -t 1 Delta Y I 2 I esc Delta X 
308 Part 4 1 The instruction-set processor level: special-function processors 
Vector mode. The vector mode is used to draw straight-line 
seg- ments. This two-word 
instruction causes the beam position to be 
moved along a line represented 
by an 
11-bit delta y and 
an 11-bit delta x. Vector continue mode. This mode 
is used to draw a straight line 
to the 
edge of the screen. It is similar to vector 
mode but causes the line to be extended until an ﬁedgeﬂ 
is encountered. Short vector mode, The short vector 
mode is used to draw figures composed of short line segments. A one-word instruction specifies a 5-bit 
delta y and a 5-bit 
delta x quantity. It is transformed within the display to the same format as vector mode and operates in 
the same manner. The preceding modes move 
the beam by counting the X and Y position registers. The counting 
is done at 1.2 microseconds per step on an intensified move and at 0.30 microsecond per step on a nonintensified move. 
Point mode. Point mode 
is used for random point 
plotting. A two-word instruction specifies new Y and/or X coordinates to be 
placed into the 
Y and X position registers. Graph-plot mode. This is used to draw 
curves of mathematical functions. A one-word instruction has data for the Y or X position register; at the same time, X or Y, respectively, is incremented by a count 
of one, two, four, or eight, depending on the scale factor. Point and graph-plot modes operate at a rate depending upon 
the position of the new point with respect to 
the previous point. If a point 
is only one-eighth of the screen away, 
the delay for beam-settling time is 6 microseconds; otherwise the settling time 
is 35 microseconds. Character generation option instructions. The alphanumeric char- acters or special symbols which make up a character set are stored in Mp in increment 
mode or 
short vector 
mode. These 
characters can be arbitrarily 
defined. A &bit (or 7-bit) 
character code in the instruction is used to locate a 
word in a table in Mp called 
the dispatch table. 
The base address 
of the table is specified by the Starting Address Register/SAR(0:5). SAR may be loaded by instructions from the Pc. The SAR represents the most significant 6 bits of a 15-bit memory address. 
The character 
code represents 
the least significant 6 (or 7) bits. A seventh SAR bit, corresponding to the 
octal position 100, 
is used with &bit 
characters as a case bit (Le., uppercase or lowercase 
characters) and may be set 
or cleared with a control 
character. Section 1 I Processors to control terminals and secondary memories 
A word in the dispatch table has the following format: Bit 0: If bit 0 is a 1, bits I to 11 are used to perform a control 
function as specified by particular control instructions. 
If bit 0 is a 0, bits 2 to 11 are combined with 
SAR to specify the address at which the character definition program starts. (The address bit 2 is common to both the SAR and bit 2 of the dispatch word and so may be specified in either place or in both places.) Determines the mode in which the character 
is to be 
displayed. If bit 1 is a 0, the increment mode is used to plot the character used; if bit 1 is a 1, the short vector mode is used to plot the character. Bit 1: Control-state instructions There are 
six control-state instructions. 
Parameter. Parameter is used to set values in scale, light-pen, and 
intensity registers. Mode. Mode is used to set up the 
data-state mode (or data-mode 
instruction). Mode also 
is used to stop the display. Conditional skip. The skip instruction tests the state 
of the P.display and the pushbuttons. Miscellaneous. These instructions include 
both tests and additional 
parameter control. Display jump and 
push -jump 
subroutine instructions. 
The display jump instruction 
has 15 address bits, so that a jump 
may be executed to 
any location in the display file within the 32-kw memory. The display subroutine instructions are 
push-jump (an extension of the jump instruction) 
and pop, the return from subroutine. The push-jump works as follows: The current state 
of the display (Light Pen Enable, 
Data Mode, Scale, 
and Intensity) is stored, along with the return address, in two 
successive locations 
in the first 4,096 words of memory. The locations are determined 
by the pushdown pointer, PDP. This pointer is initially set 
by a Pc instruction. The normal jump 
is then executed. To return from a subroutine, 
the pop instruction 
is executed. It has no address 
bits. Its function 
is to return the 
display to a 
previous state by sending the last words on the push-down stack 
back to the display. The stack approach to subroutining as implemented on the 338 has certain advantages over the jump to subroutine instruction 
normally used 
in Pc™s: 
Chapter 25 1 The DEC 338 display computer 309 1 Memory space is conserved since 
return address locations 
are not required 
in each subroutine in memory. 
A subroutine can 
be called any number 
of times before 
return to the main routine. 
Since the state of the display is saved on the stack and subsequently restored, subroutines 
are truly transparent; that is, after the 
return they 
leave the state 
of the display program the same as before the subroutine call. The subroutines can 
either retain 
the same state or change the state 
of the display by using one or more 
of the ﬁinhibit restoreﬂ bits available in 
the pop instruction. The program- mer can elect independently 
to inhibit restoration of mode, light pen, and scale, or intensity information. 
2 3 4 Instructions in Pc for communicating with P(display) Instructions in 
Pc communicate with P.display. The physical con- 
nection is by the S(™I/O Bus). The in-out transfer instructions in 
Pc are 
used to initialize and 
read the state of P.display. P.display state initialization 
from Pc instructions Set Push Down Pointer from AC Set Display Address Counter from AC Set Push Button 
contents from AC Set miscellaneous 
flag and status bits 
from AC Set character generator SAR address P.display status to 
Pc instructions Read Push 
Down Pointer 
into AC Read X register into AC Read Y register into AC Read Display Address Counter into 
AC Read Status words 
1, 2, 3, 4, 5 into AC (60 miscellaneous bits of flags, modes, etc.) 
Picture debugging modes. These modes aid programmed and pic- ture debugging. A bit can be set 
to override the nonintensify bit in data-mode instructions. 
When this bit is a 1, all points and vectors are plotted, whether they are 
to be intensified or 
not. The 
search enable instruction forces 
the display to run until a particu- 
lar instruction type is found. The instruction type 
is specified 
by the search enable instruction. 

310 Part 4 I The instruction-set processor level: special-function 
Drocessors APPENDIX 1 DEC 338 DISPLAY PROCESSOR ISP DESCRIPTION Section 1 I Processors to control terminals and 
secondary memories P. display State Program Blow State DACB: 14> PDP6l:I 1> I n te rna 1 ,Stop Appendix 1 DEC 338 Display Processor ISP Description (partially complete) Display Address 
Counter; holds memory address of display Push Down Pointer to stack holding subroutine 
return addresses c7enotes halt by a 
P.display instruction instruction External Jtop denotes a request by Pc for P.display to halt Datastate and ControlJtate are two mutually exclusive states. lines, and characters to be displayed on T. data type being interpreted. registers and switching to a specific data mode, Datastate instructions are interpreted by P.dispZay as points, !There are 7 modes for specifuing the data types. The DataYFnode register holds the 
ControlJtate instructions include jwv to subroutines using 
the stack, controlling 
P.displq state Data-State Control ,State := 7 Data-State DataJode/DM4:2> SAR<D : 5> Picture Btate 
x4:12> Y4:12> Vertical&dgeflag/Vef Hor izontal,edge,il 
ag/Hef Edge- In terrupt/E I CHSZ lntensityd):2> Xdirnens ional: 12 Y,dirnension<D:l> Beam Console snd Light Pen State PushJluttons/PBQ): 11> Push,ButtonJii t/PBH Manual J nterrupt/M I L ightJen,Fi nd/LPF LighLPebEnable/LPE Mp State M (0 :7] [0:4095] 4: 1 I> Instruction Format instruction/i<O:lD en teru da taus tate pbdense := iqll> := iCD specifies interpretation of DataJtate 
instructions Starting Address Register: base register of a dispatch table for catting character display subroutines 
beam position; onZy integers in range o s XIY s denotes if beam is within a displayable area set when beam moues outside the 
display area z~~+~~~~~~~~~-™ are plotted Character Size, 0 indicates 6 bit character set 1 indicates 7 bit character set used to set increrfent size for Dataaode instructions, incre- ments are x zSCa e brightness of displaued points maximum dimension 0.f plotting area, 9.375, 28.75, 37.5, 75.0 in on, to displav a point or line; automatically turned off at instruction comvletion register with tights: can be complemented manually or by flag is set by manually striking any push button key which is used to interrupt Pc and becomes one 
when struck stops the display 
and interrupts Pc whenever the Light Pen has seen a displayed spot 
and the LighLPe%EnabZe is a one 
a bit to 
enable the LightPeqFind flag 
to cause an interrupt processor ppimar,u memory for P.display and Pc The individual instructions ,fields 
are defined 
below. instruction type has its own bit field assignments. common bits for seveml instructions Each push button control bits 
Chapter 25 I The DEC 338 display computer 311 APPENDIX 1 DEC 338 DISPLAY PROCESSOR ISP DESCRIPTION (Continued) pb,cleay := i<b pbdomp 1 ement i= i6> pbdelectd):5> := i4:11> scale&hange/sc := io> scale (size) control bits s ca 1 eUva 1 ue /sv<O : 1 > 1 ight&en&hange/lpc := id> light pen test control bits 1 ightYpenYbit/lpb := iQ> := i<4:5> Instruction Interpretation 
Process (7 InternalJtop V 7 Externa1,Stop) + fetch (instruction[O:I] c M[DAC:DAC+l] ; DAC c DAC + 1; next (ControLState A (instructioKD:l> = 2)) + (DAC c DAC + 1); (DatbState A ((Data Mode = 0) V (OatkMode = 2) V 2 w data 2 w instruction (Data Mode = 3))) + (OAC + DAC + I): next ~nstruct ion,execut ion) execute Instruction Set and Instruction Execution Process The following instruction set definition is not complete. the miscellaneous and conditional skip instructions, 
It does not include the complete character instruction definition or Most of the instructions 
are microcoded. Instruct ion-execut ion := ( Control Instructions parame ter<0 : 1 I> : = i [o] <O : 1 I> pa rame te r,opcode := (i<O:P = 000) parameter,intensity,change := parameter49 parameter, i ntens i tv<D r2> := parameteK9: 1 1> parameter,opcode A ControlJtate + ( scale,change + (Scale t scale,value); 1 ight-pen-change + (Light,Pen,Find +-T 1 ight,pen,bi t); intensity,change + (Intensity t parameter,intensi ty)); mode<0:ll> := i<O:Il> modedpcode := (i<O:2> = 001) mode,s top,code := modeb> modeslear@ush&utton,flag:= mode<4> mode,datapde,change := moded> mode,setd):Z> := mode<6:8> modeslear,sector := mdde- mode&l ear,coord i nate := mode<lO> modedpcode A ControlJtate + ( modedtop,code + (InternalJtop c 1); mode~lear~ush,button,flag + (PushAuttonJit c0): modeJatamde&hange + (Datadode c modedet); rnodesleardector + (X=S:2> c 0; Y=S:2> ~0); mode&learjoordinate + (XS:12> -0; Yb:12> +O); ebterdatadtate + (DataJtate 1)); set parameter instruction fonnat set parameter execution set mode instruction fomt set mode execution 
312 Part 4 I The instruction-set processor level: special-function processors 
Section 1 1 Processors to control terminals and 
secondary memories APPENDIX 1 DEC 338 DISPLAY PROCESSOR ISP DESCRIPTION (Continued) I PB,l<D;lI> ;=I i<o:11> grouv 1 push button test and set instruction .format .for grouv 2 (not defined) is for Wsh Buttons 6 to 11 PBJ instruction execution Push Buttons 0 to 5 PR,ldpcode := (PE,ld:2> = 100) PB,l&pcode h Contro1,State + ( pbdense &? (pb,select<0:5> = (PB<O:5' A pb,select<0:5>)) - ( Skip test DAC + OAC + 2); pb,clear + (PB<o:5> tPB4:5> A pb selectcD:5>); next pb,complement + (PB4:5> cPB4:5> + pb>elect4:5>)); jump[0:1]~0:11> := i [O:I]CO:II> j umwp jumbpush := i [old> jump,fieldd:2> := i [0]4:11> := (i [03~:2> = 010) jump-op A Control,State + ( scaledhanqe + (Scale Cscale-value): light~enshange + (LightPeLFind + Iight,pen,bit); DAC jump,fieldoi[l]: jump,push + ( M[PDP + l] t DAC<O:2XLPFoScaIeOData,ModeOIntensi ty: M[PDP + 23 t DAC<3:14>; PDP t PDP + 2); pop<o:ll> := i[0]<0:11> pop,op,code := (i<O:2> = 011) pop,inhibit,mode := DOp<b pop,inhibit,scale,pen := pop<9> pop,inhibit,intensity := pop<lO> pop,op,code h Control,State + ( DAC<3: l4> t M[PDP] : DAC<O:D t M[PDP-I] ; pop-inhibi t-intensi ty + (Intensity c M[PDP-1]6: 1 I>) ; pop,i nh i b i t,mode + [Da taJ4ode c M [PDP- 1k6: A>) : pop-inhibi t,scale,change + ( Scale M[PDP-1]<4:5> LPF t M[PDP-1]<3>); PDP tPDP - 2; next scale,change + kale c scale,value): liqht~en~chanqe + (LPF lightsen,bitl: enter-datadnode + (Datadode t 1)); Data 'do& Instructions point[0:1]<0.11> := i [o:I]<o:II> point-intensity := point [034> poi nt ,i nh i b i t ,y := point [0]<1> poi nt -y4: 9> := point [O)Q: 11> point 24 :9> := point[l]Q:ll> poi n t &scape poi nt,inh i b i t-x := point [13a1> := point [I]<I> ,+wm and stack push down (subroutine calling) instruction format jump and push dm erecution stack pop instruction format; subroutine return pop execution point data instruction format I 
Chapter 25 I The DEC 338 display computer 
313 APPENDIX 1 DEC 338 DISPLAY PROCESSOR ISP DESCRIPTION (Continued) (DatkMode = 000) A Datadtate + ( 7 point,inhibit.,x+ (X e point,X); 7 point,inhibit,y + (Y c point-Y); point-intensify + (Beam c 1); point-escape + (Data-State c 0)); vector-intensify := vectorC01<(~ vecto rues cape 
:- vector[~l<~> vectorydy<O: lo> := vector[~I<l: 1 I> vector,dx<O:lD> := vector[ll<l: 1 I> vector(01 <O : I I> :- i [O : 13<0 : 1 I> (Data-Mode = 010) A Datadtate + ( Y c Y + vector-dy; X c X + vector-dx; vector-intensify + (Beam c 1); vector-escape + Data-state c 0); vector continueC0: 1]<0:11> := i LO: I KO: i i> (Data-Mode = 011) A Data-State + ( Y c Y + si gnaxtend (vector-dy) ; X t X + s igndxtend (vector-dx) ; vector-intensify + (Beam e 1); vector-escape + (Data-State c 0)); short,vector<O: 11> := i [O]<O:ll> short,vector,intens i fy :- short,vector<CD short-vector-escape :=- short,vector<b> short-vector-dx :.= short,vector<E: 11> short-vector-dy :- short,vector<l:P (Data-Mode = 100) A Data-State + ( X c X + sign,extend(short,vector,dx); Y Y + sign,extend(short,vector,dy); short-vector-intensify + (Beam e 1); short-vector-escape + (Data-State e 0)) ; i ncrement<O :9 increment-intensi fy :- increment<(Y i ncremnt-di recti on/i KO I P i ncremnt,count/i KO: I> icle := (ic = 0) icl :- (ic = 1) ic2 := (ic = 2) ic3 := (ic - 3) := i ncrementc3 :5> := incremnt<l:D (Data-Mode = 001) A Data-State + ( increment c i<O:P; next plot-increment-vector; next 
increment e i<b:l 1>; next plot,incremnt,vector) point data execution vector data instruction format vector data execution not correct, since the vector 
from point Y,X to Y+ vectordy, Xt vector,& is plotted vector continue instruction format sme as vector vector continue execution not correct, as vector continues plotting until e&e is round short vector 
instruction format short vector execution 
increment instruction fonnat; 2 increment/instruction 1 of 8 directions count 1 and escape to ControlState count 1 count 2 count 3 increment instruction execution 
314 Part 4 I The instruction-set processor level: special-function processors 
Section 1 1 Processors to control terminals and secondary memories 
APPENDIX 1 DEC 338 DISPLAY PROCESSOR ISP DESCRIPTION (Continued) plot,increment,vector := ( icle + (move-lgosition; Contro1,State -1); move 1 and escape icl + (move,l,position); move 1 ic2 + (move,l&osition; next move,l+osition) move 2 ic3 + (move-lgosition; next move,lGosition; next move 3 move,l ,pos i t ion) Move,l,position := ( sub process for moving beam (id = 0) + (X tX + Scale); (id = I) + (X CX + Scale; Y CY + Scale); (id = 2) + (Y tY + Scale); (id = 3) + (Y tY + Scale; X tX - Scale); (id = 4) + (X tX - Scale); (id = 5) + (Y CY - Scale; X CX - Scale); (id = 6) + (Y CY - Scale); (id = 7) + (Y CY - Scale; X - X + Scale); increwntdntensify +Beam +I) 1 of 8 positions characterd):ll> := id):]]> 6,bit [O:I]d):5> := characterQ):ll> 7,bi t6: 1 I> := character<5: 1 I> (OataJode = 101) A DataJtate + ( (CHSZ = 0) + ( X,Y tf(M[SWRc6,0it[0]3,M); X,Y tf (M[SAROb,bi t 11 ]],M)); (CHSZ = I) + (X,Y tf(Y[SARW&it],Y))); character instmction format character instruction execution; pZot function; see text graph data instruction format graph,plot<O:ll> := i [O]G:ll> grapkp lot,escape<O> graph-pl ot,x,yd)> graph,plot,data<O:SD := graph,plot<Z:ll> : = graphup I otQ> : = graphup I ot<P (Data-mode = 110) A DataJtate + ( graph data execution -, graph,plot,x,y + (X t X + Scale'; Y t graph,plot-data; Beam t 1); graph,pIot,&y + (Y CY + Scale'; X tgraph,plot,data; Beam t I); graph,plot,escape + (Data-State c 0)) end Instruction~execution ) I 
Section 2 Processors for array data 
Two array processors are discussed 
in this section. Concep- 
tually, they are an outgrowth 
of both the parallel, distributed computer [Holland, 19591, and the matrix-interpreter-based programs for general-purpose computers. NOVA is a very low cost special processor. 
ILLIAC IV is a very general array proces- sor. Another approach, 
the ILLIAC Ill [McCormick, 19631 stores information on 
photographic media, so that optical processing 
(inherently parallel) can be used. 
NOVA NOVA is a proposed, non-general-purpose machine 
based on the belief that efficient, special-function processors can be built to solve particular problems. It is reasonable 
to assume that there are problems 
for which NOVA, with its 
cyclic memory, would perform no worse than a processor with a random-access memory. Unless 
the opera- tions performed on the arrays were extremely simple or re- stricted, a single system might not always work very efficiently. By using a variable-speed cyclic 
memory to match the operation time in the form of an address transformation or renaming 
mechanism, the access problems might be avoided. 
NOVA represents a 
particular'idea for effective utilization of hardware and is presented 
to remind us that a memory now 
considered obsolete 
may perform nicely for a restricted appli- cation. The ILLIAC IV computer D. L. Slotnick is responsible 
for the ILLIAC IV 
computer. The idea for a computer with a number of parallel data operators 
or processing elements appeared some 
time ago in the SOLO- MON computer [Gregory and McReynolds, 19631. The tech- nology of the first and second generation made 
SOLOMON impractical to build. ILLIAC IV was 
designed at the Univer- sity of Illinois under a contract to the Department of 
Defense's Advanced Research 
Projects Agency.' The processing elements 
are constructed 
from third-generation technology although some medium- and large-scale integrated circuits are used in the design. The design 
is about the most ambitious ever undertaken. The direct and indirect effects 
should be numerous. 'The University 
of Illinois monitored 
the contract to the Burroughs Corporation, 
Paoli. Pa. 315 
Chapter 26 NOVA: a list-oriented computer1 Joseph E. Wirsching Since the advent of the internally-stored program computer, those of us concerned with problems involving massive amounts of com- putation have 
taken a one-operation, one-operand approach. But there is a very 
large class of problems involving massive amounts of computation that may be thought of as one-operation, many- operand in nature. 
Some familiar examples are numerical integra- 
tion, matrix operations, 
and payroll computation. This article proposes a 
computer, called 
NOVA, designed to take advantage of the one-operation, many-operand concept. NOVA would use rotating memory instead of high-cost random access memory, 
reduce the number of program steps, and reduce the number of memory accesses to program steps. In addition it is shown that NOVA could execute typical 
problems of the one- operation, many-operand type in times comparable to that of modern high-speed random access computers. Rotating memories were used in early computers because of low cost, 
reliability, and ease of fabrication. These machines have 
been replaced 
by machines with more costly random access memories primarily to increase computing speed as the result of a decrease in access time to both operands 
and instructions. In general, the four or 
more instructions must be brought from the memory to the instruction register once for 
each pair in 
the lists. This seems to be a great waste 
when only one arithmetic operation is involved. Indeed it 
is, when one 
considers that the majority of computing work consists of the performance of highly repetitive operations that are merely combinations of the simple example given. Attempts have been made to alleviate this waste 
by incorporating ﬁinstruction 
stacksﬂ and ﬁrepeatﬂ commands into the instruction execution units 
of more recent computers. Example 2. Consider three lists (a™s, b™s and c™s), where we wish to compute (a + b) x c for each trio. There are two 
distinct methods by which 
this can be accomplished: first, by forming (a + b) x c for each trio 
of numbers in the list, or second, by forming a new list consisting of (a + b) for each a and b, and then 
multiplying each c by the corresponding member of the new list. Clearly the second method is wasteful of memory space and wasteful of programming steps. Next, let us take a look at the 
memory requirements for these two examples. First, the instructions are kept in 
a high-speed random access memory, and while the bulk of the variables need The NOVA approach Let us take two simple examples and use them to compare 
con- ventional computing techniques with 
those proposed for NOVA. not be kept in 
a random access memory, 
they must be brought to one 
before the algorithm can be performed. This extra transfer 
may entail more instructions to perform the logistics. Thus the simplicity of the overall program is directly related to 
the size Example 1. Consider two lists (a™s and b™s) of which the corre- sponding pairs are to be added. With a conventional computer this is done with a program that adds the first a to the first b, the second a to the second b, etc., and counts the operations. The working part of such a program might 
consist of the following instructions: Fetch a Add b Store (a + h) Count, Branch, and Index lDatarnation, vol. 12, no. 12, pp. 41-43, December, 1966. of the memory. The variables (a™s, b™s, etc.) are usually stored in consecutive memory locations. Except for indexing this ordering 
of the data 
is not exploited. In NOVA, lists of variables are kept on tracks of a rotating bulk memory. When called for, the lists of variables are streamed through an arithmetic unit 
and the results immediately replaced on another track 
for future use. This process takes maximum ad- vantage of the sequential ordering of the variables. Instructions need only be brought to the instruction execution 
unit once for each pair of lists rather than 
once for each operand; 
thus the instructions need not 
be stored in a 
random access memory but may also be stored on the rotating bulk memory. This departure from the requirement for random access memory significantly 316 
Chapter 26 I NOVA a list-oriented computer 317 reduces the cost of the computer, without sacrificing speed of problem solution. 
Solution of a network problem Before going further into 
the structure of NOVA, let us consider a significant example, 
which shows that NOVA is well suited to the solution of differential equations 
using difference 
methods over a rectangular network. 
Let Fig. 1 represent an artificial network used as a model for 
some physical process. 
Generally speaking, the method of advanc- ing the variables at a mesh point (i, k) from one time step to the next involves only 
information from the neighboring mesh points. A typical hydrodynamics problem will require a 
list of 10 to 20 variables (physical 
quantities) at each mesh point. The traditional computer solution involves 
listing these 
variables to each point 
in a 
contiguous fashion 
and in 
a regular sequence with respect 
to the rows and columns of the array. If the total array does not fit into the fast memory, three adjacent columns (or rows) are brought to the fast memory; as 
a new column is calculated, the next column in sequence is brought in 
from bulk memory and the 
oldest of the three is written to bulk memory. In this fashion one proceeds across the array. This 
process is then repeated until 
some significant physical 
occurrence happens 
and the problem is ended. In NOVA, the variables are organized into separate 
lists rather than by mesh point. From a 
computational standpoint 
this is possible since the main memory 
of NOVA may be essentially unlimited in size, at least exceeding the size of the largest present 
network problems. One then proceeds to execute operations 
on 123. 1.. J Fig. 1. Two-dimensional array. original Lists u0,o v0,o U0,l V0,l u0,2 V0,z .. u1,o v1.0 U1,l V1,l U1.Z vk,2 .. Uj,k .. .. .. UJ,K VJ,K V Shifted Down by 1 - v0,o V0,l v0,z ‚0,K v1.0 VlJ y,k VJ,K-I VJ,K V Shifted DownBy2 - - v0,o V0,l ﬁ0,K-1 v0,K v1,o Vja-1 VJJ-2 VJJ-1 VJ,K V Shifted Down By K y-l,k VJ-LK Fig. 2. Lists of variables. lists of variables rather than 
single variables, 
performing a 
single operation for all mesh points in 
the array in sequence. 
Let us look more closely 
at the variables and their 
possible combinations. Let Ui,k and Vj,k be variables associated with the array of Fig. 1. These variables are listed sequentially by column 
in Fig. 2, along with 
further lists of the Vcolumn shifted by 
various increments. With some concentration, one 
discovers in Fig. 2 that an arith- metic operation between Uj,k and Vi,* is simply a matter of taking the two columns as 
they exist and operating 
on them in 
pairs. To combine Uj,k with a nearby neighbor, 
Vj,k--l, the V column is shifted down one place, 
at which time the 
proper neighboring 
variables are found opposite one 
another for the entire network. At certain boundaries of the array some elements have 
no proper neighbors. In NOVA these boundary elements 
must be handled separately in 
the same way 
as they must be handled separately 
in a conventional machine. 
In NOVA, calculations at boundaries may be temporarily inhibited by having a 
third input to the 
arith- metic unit 
which allows the calculation of a result for 
a pair 
of operands to proceed 
or not, as appropriate. This third input 
is defined as ﬁconditions,ﬂ 
and is brought as a bit string to the arith- metic unit concurrently 
with the operands. This bit string 
may contain any number from one to several bits for each pair of operands. 
318 Pari 4 I The instruction-set processor level: special-function processors 
Section 2 1 Processors for array data 
Further observation shows not only that it is possible to obtain 
the nearest neighbors 
easily by shifting the columns of variables with respect to one another, 
but that 
any neighbor relationship 
can be 
obtained. In general, 
for an operation with a neighbor kn rows away and krn columns away, the lists are offset by fn k m- K, where K is the number of rows in the array. Many problems 
(for example, payroll and inventory records) 
are essentially list-structured but do not require 
offsetting of vari- ables. Clearly 
the NOVA structure is well suited for the solutions of these problems also. Structure The most difficult 
problem to be solved in the proposed computer is to synchronize movement of the columns of data that 
require offset. Buffers 
of various types could 
be used to solve this problem; 
they could range all 
the way from rotating memory devices or delay lines 
to core memories. The former are simple, direct, and low in cost but are limited in their general capabilities. 
On the other hand, a number of small random access buffer 
memories could be used for offsetting 
lists of variables and for facilitating special functions such 
as boundary calculations but at a higher equipment cost. Figure 3 shows a block diagram of the organization of NOVA. The rotating memory, which might 
be a disc 
or drum, would be Fig. 3. Block diagram of NOVA computer. \ CONTROL RESULTS TO F& , , MEMORY 02 m ARITHMETIC CIRCUITRY CO N DlTlONS CONDITIONS TO MEMORY Fig. 4. Buffering in arithmetic unit. 
composed of several hundred tracks, each storing several thousand 
words, with a total capacity between one 
and two 
million words. Each track would have an individual read-write 
head. The heads would be organized in such a way as to attain a high word-transfer 
rate, perhaps 
as high as one million words 
per second. With this in mind an 
ideal execution 
time for one addition 
would be the time required to 
move two operands from the disc to the arith- metic unit; i.e., 1-2 microseconds. The disc synchronizer 
would be capable of simultaneously reading 
two lists of operands, writing one list of results, and reading one list and writing one 
list of conditional control information. 
In addition, instructions 
would be read from another channel in 
small blocks. 
The bit string of conditions coming from the memory is used to control individual operations 
on pairs 
of operands in the lists, and in essence each bit (or bits) is a subordinate part 
of the indi- vidual operations. Conditions going 
to the 
memory are the sub- sidiary result 
of the operation of one list upon another. 
These bit strings may be used later as control during another 
list operation. They want 
also to contain 
information on the occurrence of an overflow or underflow, or on the presence of an illegal operand, etc. Figure 4 shows a 
suggested organization 
for the arithmetic unit 
that incorporates five sets of alternating buffers. Two sets are for lists of operands coming from the memory, one set 
for lists of results going 
to the 
memory, and two 
sets for ﬁconditionsﬂ (condi- 
tional control information) 
coming from and going to the memory. 
Chapter 26 I NOVA a list-oriented 
computer 319 BUFFER These buffers should be equivalent in length 
to the 
number of words on 
a track 
of the rotating memory. The loading and unloading of the buffers to 
and from the rotat- ing memory is dependent on the timing of the rotating memory, whereas the loading and unloading 
of the buffers to and from the arithmetic unit 
is guided solely by the rate 
at which the arithmetic can be performed. Here again it may also be possible to take advantage of the streaming nature of the operands by designing 
an ﬁassembly-line™™ arithmetic unit 
in which more 
than one pair 
of operands could 
be in 
process at the same time. 
With this kind of unit it may be possible to execute additions 
at a rate equal 
to the word-transfer rate from the rotating memory; however, 
a multiplication or division 
of two lists may 
require several revolu- 
tions of the memory. The timing diagram 
of Fig. 5 shows several typical instructions being 
carried out. A certain amount of look- ahead is required, but there is ample time 
for this, since instruc- 
tions are prepared 
for execution at an average 
rate of less than one per revolution of the rotating memory. While a detailed 
cost estimate has not been made for 
a simple prototype NOVA, a quick estimate would be $50,000 for a head- per-track disc and 
$50,000 for the arithmetic and 
control section, 
making a total 
of $100,000. For a buffering scheme such 
as the one shown in Fig. 
4 the cost would 
be considerably higher 
but would be offset by increased versatility. REVOLUTIONS OF ROTATING MEMORY 1121 314 15 16 Conclusions In the previous paragraphs 
we have demonstrated that NOVA is capable of handling network problems 
at a significantly lower cost 
than contemporary computers, and 
at a comparable speed. 
The availability of such a machine as NOVA would stimulate further 
Fig. 5. Timing diagram of buffers, rotating memory, and arithmetic unit. 
Dotted line shows movement of data into a device: solid line shows movement out. interest in 
the one-operation, many-operand 
approach to compu- tation and 
no doubt would uncover many 
other problems to which it could be applied. Because NOVA makes it possible to easily establish neighbor- 
relationships between mesh points that are further 
away than nearest neighbors, 
it may be possible to develop new differencing techniques for 
the solution of coupled sets of differential equations. 
This may increase 
the accuracy or shorten the time required for 
their solution. The memory, arithmetic, and other units needed for NOVA are commercially available 
now. No new technology would 
be required to fabricate a prototype 
model. In view of the potential advantages of such a machine, it seems clear that construction of a model would justify the minimal development 
costs. 
Chapter 27 The ILLIAC 
IV computer1 George H. Barnes / Richard M. Brown / Maso Kato David J. Kuck / Daniel L. Slotnick / Richard A. Stokes Summary The structure of ILLIAC IV, a parallel-array computer con- taining 256 processing elements, is described. Special features include multiarray processing, multiprecision arithmetic, and fast data-routing interconnections. Individual 
processing elements execute 
4 x lo6 instruc- tions per second to yield an effective rate of lo9 operations per second. 
Array, computer structure, look-ahead, machine 
lam page, parallel processing, speed, thin-film memory. 
Index terms 
Introduction The study of a number of well-formulated but computationally massive problems is limited by the computing power of currently available or 
proposed computers. Some involve 
manipulations of very large matrices 
(e.g., linear programming); others, the solution of sets of partial differential equations over sizable 
grids (e.g., weather models); and others require extremely fast data correlation techniques (phased array 
signal processing). 
Substantive progress in these areas requires 
computing speeds several orders 
of magn- tude greater than conventional computers. At the same time, signal propagation speeds 
represent a serious 
barrier to increasing the speed of strictly sequential 
computers. Thus, in recent 
years a variety of techniques have been introduced 
to overlap the functions required in sequential processing, e.g., multiphased memories, program 
look-ahead, and pipeline arith- 
metic units. Incremental speed 
gains have been achieved 
but at considerable cost 
in hardware 
and complexity with accompanying 
problems in machine checkout and reliability. The use of explicit parallelism 
of operation rather than 
over- lapping of subfunctions offers the possibility of speeds which in- crease linearly with 
the number of gates, and consequently has 
been explored in several designs [Slotnick 
et al., 1962; Unger, 1958; Holland, 1959; Murtha, 19661. The SOLOMON computer [Slotnick et al., 19621, which introduced a large degree of overt parallelism 
into its structure, had four principal features. 1 A large array 
of arithmetic units was controlled by a single 
'IEEE Trans., C-17, vol. 8, pp. 746-757, August, 1968. 
control unit so that a single 
instruction stream sequenced 
the processing of many data streams. Memory addresses and data common to all of the data processing were broadcast from the central control. Some amount of local control 
at the individual processing element level was obtained by permitting each element 
to enable or disable 
the execution of the common instructions according to 
local tests. 
Processing elements in 
the array had nearest-neighbor con- nections to provide moderate coupling for data exchange. Studies with the original SOLOMON computer indicated 
that such a 
parallel approach was both feasible and applicable to a variety of important computational 
areas. The advent of LSI cir- cuitry, or at least medium-scale versions, with gate times of the order of 2 to 5 ns, suggested that a SOLOMON-type array of potentially lo9 word operations per second could 
be realized. In 
addition, memory technology had advanced 
sufficiently to indicate that lo6 words of memory with 200 to 500-11s cycle times could 
be produced at acceptable cost. The ILLIAC IV Phase I design study during 
the latter part of 1966 resulted in the design discussed 
in this paper. 
The machine, to be fabricated by the Defense Space 
and Special Systems Division of Burroughs Corporation, Paoli, Pa., 
is scheduled for installation in early 
1970. Summary of the ILLIAC IV The ILLIAC IV main structure consists of 256 processing 
elements arranged in four reconfigurable 
SOLOMON-type arrays of 64 processors each. The individual processors have a 240-ns ADD time and a 400-11s MULTIPLY time for 64-bit 
operands. Each processor requires approximately 
lo4 ECL gates and is provided with 2048 words of 240-ns cycle time thin-film memory. Instruction and addressing control 
The ILLIAC IV array possesses a common control unit which decodes the instructions and generates control signals for all 320 
Chapter 27 I The ILLIAC 
IV computer 321 processing elements in 
the array. This eliminates 
the cost and complexity for decoding and timing circuits 
in each element. 
In addition, an index register and address adder 
are provided with each 
processing element, so that the final operand address a, for element i is determined as follows: a, = a + (b) + (c,) where a is the base address 
specified in the instruction, (b) is the contents of a central 
index register in 
the control unit, and (ci) is the contents of the local index register of the processing ele- ment i. This independence in operand addressing is very effective 
for handling rows and columns 
of matrices and other 
multidimen- sional data structures [Kuck, 19681. Mode control and 
data conditional operations 
Although the goal of the ILLIAC IV structure is to be able to 
control the processing of a number of data streams with a 
single instruction stream, 
it is sometimes necessary 
to exclude some data streams or to process 
them differently. This is accomplished by 
providing each 
processor with an ENABLE flip-flop whose value controls the instruction execution 
at the processor level. The ENABLE bit is part of a test result register 
in each 
processor which holds 
the results of tests conditional on local data. Thus in ILLIAC 
IV the data conditional jumps of conventional computers are accomplished by processor tests which enable or disable local execution 
of subsequent commands in 
the instruction stream. Routing Each processing 
element i in the ILLIAC IV has data routing connections to 
4 of its neighbors, processors i + 1, i - 1, i + 8, and i - 8. End connection is end around 
so that, for a single array, 
processor 63 connects to processors 0, 62, 7, and 55. Interprocessor data transmissions of arbitrary distance are ac- complished by 
a sequence of routings within a 
single instruction. 
For a 64-processor array the maximum number of routing steps 
required is 7; the average overall 
possible distances is 4. In actual programs, routing by distance 
1 is most common 
and distances greater than 2 are rare. 
Common operand broadcasting 
Constants or other operands used in 
common by all the processors are fetched and 
stored locally 
by the central control and broadcast to the 
processors in conjunction with the instruction using them. This has several advantages: 
(1) it reduces the memory used for storage of program constants, 
and (2) it permits overlap 
of common operand fetches with other 
operations. Processor partitioning Many computations do not require 
the full 64-bit precision 
of the processors. To make more efficient use 
of the hardware and 
speed up computations, each 
processor may be partitioned 
into either two 32-bit or eight 8-bit subprocessors, 
to yield 51232-bit or 2048 %bit subprocessors 
for the entire ILLIAC IV set. The subprocessors are not completely 
independent in that they share a common index register and the 64-bit data routing paths. 
The 32-bit subprocessors have 
separate enabled/disabled modes for indexing 
and data routing; the 8-bit subprocessors do not. 
Array partitioning The 256 elements of ILLIAC IV are grouped into four 
separate subarrays of 64 processors, each subarray having 
its own 
control unit and capable of independent processing. The subarrays may be dynamically united to form two arrays 
of 128 processors or one 
array of 256 processors. The following advantages 
are obtained. 
1 Programs with moderately dimensioned vector 
or matrix variables can 
be more efficiently matched to the array size. Failure of any subarray 
does not preclude continued proc- 
essing by the others. 2 This paper summarizes the structure of the entire ILLIAC IV system. Programming techniques and 
data structures for ILLIAC 
IV are covered in 
a paper 
by Kuck [1968]. ILLIAC IV structure The organization of the ILLIAC IV system is indicated in Fig. 
1. The individual processing elements 
(PES) are grouped in four 
arrays, each containing 64 elements and a 
control unit (CU). The four arrays 
may be connected together under 
program control 
to permit multiprocessing or single-processing operation. The system program resides in 
a general-purpose computer, a 
Burroughs B 6500, which supervises program loading, array configuration 
changes, and 1/0 operations internal to the 
ILLIAC IV system and to the external world. 
To provide backup memory for the ILLIAC IV arrays, a large parallel-access 
disk system (10 bits, lo9 bit per second access rate, 40-ms maximum latency) is directly coupled to the 
arrays. There is also provision for real-time 
data connections directly to the 
ILLIAC IV arrays. 
322 Part 4 1 The instruction-set processor 
level: special-function processors 
PARALLEL DISK ACCESS Section 2 I Processors for array data 
GENERAL PURPOSE COMPUTER 0-6500 - *------ REAL TIME LINK 4 TO PERIPHERALS AND COMPUTER NET Fig. 1. ILLIAC IV system organization. Array organization The internal structure 
of an array is indicated in Fig. 2. The 64 processing elements in each array are arranged in a string and 
are controlled by 
the control unit 
(CU) which receives 
the instruc- tion string, generates 
the appropriate 
control signals and address parameters of the instructions, and transmits them to 
the indi- vidual processing elements for execution. In addition, each 
CU can broadcast via the common data bus operands 
for common 
use (e.g., constant). Full word length (64 bits) communication 
exists between the processing elements for exchange of information by 
organized rout- 
ing of words along the string array. Direct routing connections 
exist for nearest neighbors and also for processing 
elements 8 units away. Routing 
for intermediate distances are generated 
via se- quences of routes of + 1, - 1, + 8, or - 8. The end 
connections of the string are circular, but can 
be broken and 
connected to the ends of other arrays when the system is organized in one 
of the multiarray configurations. All processing elements of an array execute, 
of course, the same instruction in unison under the control of the CU; local control is provided by the mode bit in each processing element which enables or disables 
the execution of the current instruction. The control unit 
is able to sense the mode bits of all processing ele- ments under 
its control and thereby 
monitor the state of operation. Multiarra y 
configurations To permit more optimal matching of array size 
to problem struc- 
ture, the 
four arrays may 
be united in three different configura- 
tions, as shown 
in Fig. 3. To enlarge the arrays, the end 
connections of the PE strings are decoupled and 
attached to the 
ends of the other arrays to form strings 
of 128 or 256 processors. For multiarray 
configurations all CUs receive the same instruction string 
and any data centrally accessed. The control units execute 
the instructions independently, however, with inter-CU synchronization occurring 
only on those instructions in 
which data or control information must cross array boundaries. This simplifies 
and speeds up the in- struction execution in multiarray configurations. The multiplicity of array configurations introduces complexities in memory ad- dressing which will 
be discussed in a later section. Control unit The array control 
unit (CU) has the following five functions. 1 2 To control and decode the instruction streams 
To generate the control pulses transmitted to the processing elements for instruction execution To generate and broadcast those components of memory addresses which 
are common to all processors To manipulate and 
broadcast data words common 
to the calculations of all the processors 3 4 ROUTING NETWORK COMMON DATA BUS (MEMORY ADDRESS AND COMMON OPERAND1 ___ ___ I 1l ll 1 PE61 ___ PEO PE 1 I r ' __. __-2IJT' CONTROL UNIT BUS UNSTRUCTION AND COMMON OPERANDS) Fig. 2. Array structure. 
Chapter 27 I The ILLIAC 
IV computer 323 F• PE 0 631 127 9 I PE PE 0 63: 127 191; 255 FOUI WADRANT ARRAYS PE 0 63! 127 -1 ["I I"] SINGLE OLMMIANT ARRAYS Fig. 3. Multiarray configurations. 5 To receive and process trap signals arising 
from arithmetic faults in the processors, from internal 1/0 operations, and from the B 6500. The structure of the control unit is shown in Fig. 4. Principal components of the CU are 
two fast-access 
buffers of 64 words each, one associatively addressed, 
which holds current and pending instructions (PLA), and 
the other a 
local data buffer (LDB). The four 64-bit accumulator registers 
(CAR) are central to communi- cation within the CU and 
hold address indexing information 
and active data for logical manipulation 
or broadcasting. The CU arithmetic unit 
(CULOG) performs addition, 
subtraction, and Boolean operations; more complex 
data manipulations are rele- gated to the 
PE's. To specify and control array configurations, there are three 4-bit configuration 
control registers whose use will 
be described in another section. Instruction processing All instructions are 32 bits in length 
and belong to one of two classes: CU instructions, which 
generate operations local 
to the CU (e.g., 
indexing, jumps, 
etc.), and PE instructions, which 
are decoded in 
the CU and then transmitted 
via control pulses to all the processing elements. Instructions 
flow from the array memory 
upon demand in 
blocks of 8 words (16 instructions) into the 
in- struction buffer. 
As the control advances, individual instructions 
are extracted from the instruction buffer 
and sent to the 
advanced instruction station 
(ADVAST) which decodes them and executes 
those instructions local 
to the 
CU. In 
the case of PE instructions, ADVAST constructs the necessary address 
or data operands and stacks the result in a queue 
(FINQ) to await transmission to the PES. PE instructions are taken from the bottom of the stack to the ha1 instruction station (FINST) which controls 
the broadcast of address or 
data and 
holds the PE instruction during 
the execu- tion period. 
The use of the PE 
instruction queue permits overlap 
between the CU and PE instruction executions; 
the amount of overlap depends, of course, on the distribution of CU and 
PE instructions. As in all overlap strategies, careful attention to the 
instruction sequence by 
the programmer or compiler can result in consider- 
able speedup 
of program execution. 
The instruction buffer holds 
a maximum of 128 instructions, sufficient to hold the inner loop of many programs. For such 
loops, after initial 
loading, instructions 
are fetched 
from the buffer with minimal delay. 
A variety of strategies for instruction buffer loading were 
ex- amined, and the following straightforward 
approach was taken. When the instruction counter is halfway through 
a block of 8 INSTRUCTION ASSOCI4TlVE BUFFER LOCAL BUFFER AI gpy, SEOUENCER .. CONTROL SIGNALS COMMON 0414 BUS 110 REWEST MOW FIF FROM PES TO PES FROMIM FROM PES Fig. 4. Control-unit block diagram. 
324 Part 4 I The instruction-set processor level: special-function 
Drocessors Section 2 I Processors for array data 
words (16 instructions), 
fetch of the next block 
is initiated; the possibility of pending jumps to different blocks is ignored. If the next block is found to be already resident in the buffer, no further action is taken; else fetch of the next block from the array memory is initiated. On arrival of the requested block, the instruction buffer is cyclically filled; the oldest block 
is assumed to be the least required block in the buffer and is overwritten. Jump instruc- tions initiate the same procedures. Fetch of a new instruction block from memory 
requires a delay of approximately three memory cycles to cover the signal trans- mission times between the array memory and the 
control unit. On execution of a straight line 
program, this delay is overlapped with the execution of the 8 instructions remaining 
in the current block. In a multiple-array configuration, instructions are fetched 
from the array memory specified by the program counter, and broadcast simultaneously to all the participating control units. Instruction 
processing thereafter is identical to that for single-array operation, 
except that synchronization of the control units 
is necessary whenever information, in the form of either data 
or control 
signals, must cross array boundaries. CU synchronization must be forced at all fetches of new instruction blocks, upon all data routing operations, all conditional program transfers, and all configuration- changing instructions. With these exceptions, the CUs of the several arrays 
run independently 
of one another. 
This simplifies the control in the multiple-array operation; 
furthermore, it permits 
1/0 transactions with the separate array memories without steal- ing memory cycles from the nonparticipating memories. Memory addressing Both data and 
instructions are stored in 
the combined memories of the array. However, the CU has access to the 
entire memory, while each PE can only directly reference 
its own 
2,048-word PEM. The memory appears as a two-dimensional array 
with CU access sequential along rows and with PE access down its own column. In multiarray configurations the width of the rows is increased by multiples of 64. The resulting variable-structure addressing 
problem is solved by generating a fixed-form 20-bit address in the CU as shown in Fig. 5. The lower 6 bits identify 
the PE column within 
a given 
array. The next 2 bits indicate the array number, and the 
remain- ing higher-order bits 
give the row value. The row address bits 
actually transmitted to 
the PE memories are configuration- dependent and are 
gated out 
as shown. Addresses used by the PE™s for local operands 
contain three 
components: a fixed address contained in 
the instruction, a CU I Row Artov Column Single array Y Address bits (12) to PES Fig. 5. Memory address structure. index value 
added from one of the CU accumulators, and a local 
PE index value added at the 
PE prior to transmission to its own memory. CU data operations . The control unit can fetch either individual words or blocks of 8 words from 
the array memory to the local data buffer. In addi- 
tion, it can fetch 1 bit selected from the 8-bit mode register of each processing element to form a 64-bit 
word read into 
the CU accumulator. The CU program counter (PCR) and the configura- tion registers 
are also directly addressable by the CU. Data manipulations (+ , -, Boolean) are performed on a selected CAR and the 
result returned to 
the CAR. Data to be broadcast to the processing elements is inserted into the FINQ along with the accompanying instruction and transmitted to the 
PES at the appro- priate time. 
Configuration control With the 
variety of array configurations for 
ILLIAC IV, it is necessary to specify and control the subarrays which are conjoined and to designate the instruction and data 
addressing. For this purpose each CU has three 
configuration control registers 
(CFC), each of 4-bit length, where each bit corresponds to one 
of the four subarrays. The CFC registers may be set by the 
B 6500 or a CU instruction. CFCO of each CU specifies the array configuration in which it is participating by means of a 1 in the appropriate bits of CFCO. CFCl specifies the instruction addressing 
to be used within the array. In a united configuration it is thus possible for the instruc- tion stream 
to be derived from any 
subset of the united arrays. CFC2 specifies the CU data addressing form in a 
manner similar to the CFC 1 control of instruction addressing. 
Chapter 27 I The ILLIAC IV computer 325 The addressing indicated by both CFCl and CFC2 
must be consistent with the actual configuration designated 
by CFCO, else a configuration interrupt is triggered. Trap processing Because external demands on the arrays will 
be preprocessed through the B 6500 system 
computer, the interrupt system for the control units is relatively straightforward. Interrupts are provided to handle B 
6500 control signals 
and a variety of CU or array faults (undefined instructions, 
instruction parity error, 
improper con- figuration control instruction, 
etc.). Arithmetic overflow and under- 
flow in any of the processing elements is detected and 
produces a trap. The strategy of response to an interrupt is an effective 
FORK to a single-array configuration. 
Each CU saves its 
own status word automatically and independently 
of other CU™s with which it may previously have been 
configured. Hardware implementation 
consists of a base interrupt address register (BIAR) which is dedicated as a pointer 
to array storage into which status information will 
be transferred. Upon receipt of an interrupt, the contents of the program counter and 
other status information and the contents of CAR0 are stored in the block pointed to by the BIAR. In addifion, 
CAR 0 is set to contain the block address used by 
BIAR so that subsequent register saving 
may be programmed. Interrupt returns are accomplished through a special instruction 
which reloads 
the previous status word and CAR 0 and clears the interrupt. Interrupts are enabled through 
a mask word in a special regis- ter. The interrupt state 
is general and not unique 
to a specific trigger or 
trap. During the interrupt 
processing, no 
subsequent interrupts are responded to, although their presence is flagged in the interrupt state 
word. The high degree of overlap in the control unit 
precludes an 
immediate response to an interrupt during the instruction which generates an arithmetic fault in some processing 
element. To alleviate this 
it is possible 
under program control 
to force non- overlapped instruction execution permitting access to definite fault information. Processing element (PE) The processing element, shown in Fig. 
6, executes the data com- putations and local indexing 
for operand fetches. It contains the following elements. 1 Four 64-bit registers 
(A, B, R, S) to hold operands 
and results. A serves as the accumulator, B as the operand register, R as the multiplicand and data 
routing register, and S as a general 
storage register. An adder/multiplier (MSG, PAT, CPA), 
a logic unit (LOG), and a barrel 
switch (BSW) for 
arithmetic, Boolean, and shifting functions, respectively. 
A 16-bit index register (RGX) 
and adder (ADA) for memory 
address modification 
and control. An 8-bit mode register (RGM) 
to hold the results of tests and the 
PE ENABLE/DISABLE state information. As described earlier, the PES may be partitioned into subproc- essors of word lengths 
of 64, 2 x 32, or 8 x 8 bits. Figure 7 shows the data 
representations available. Exponents 
are biased and rela- tive to base 2. Table 
1 indicates the arithmetic and logical opera- 
tions available for 
the three operand precisions. PE mode control Two bits of the mode register (RGM) control the enabling or disabling of all instructions; 
one of these is active only in 
the 32-bit precision mode 
and controls instruction execution on 
the second operand. Two other bits of RGM are set whenever 
an arithmetic fault (overflow, underflow) occurs in 
the PE. The fault bits 
of all PES are continuously monitored by 
the CU to detect 
a fault condi- 
tion and initiate a CU 
trap. Data paths Each PE has a 64-bit wide 
routing path to 
4 of its neighbors 
(kl, ?8). To minimize the physical distances involved 
in such routing, 
the PES are grouped 8 to a cabinet (PUC) in the pattern shown in Fig. 8. Routing by 
distance 58 occurs interior to a PUC; routing by distance +1 requires no more 
than 2 intercabinet distances. CU data and instruction fetches require 
blocks of 8 words, which are accessed in parallel, 1 word per PUC, into a CU buffer (CUB) 512-bit wide, 
distributed among the PUCs, 1 word per Table 1 PE data operations 
Operation time per element 
~~ Operation 64 bit 2 x 32 bit 8 x 8 bit +, - 200 ns 240 ns 80 ns X 400 ns 400 ns - 2200 ns 3040 ns Boolean 80 ns Shift 80/240 nst 160 ns t (Single length)/(double length) 
326 Part 4 I The instruction-set processor level: special-function processors 
Section 2 I Processors for array data 
NEWS DRIVERS/ RECEIVERS R REGISTER 
™ (RGR) * 1 CONTROL UNIT MIR CDB 1 1 DRIVERS MODE RECEIVERS (RGM) AND - REGISTER Jl A REGISTER (RGA) LEADING DETECTOR 1 ADDRESS (MAR) y REGISTERS kMEMORy 1 Fig. 6. Processingelement block diagram. 
Chapter 27 I The ILLIAC IV computer 327 S E ( 15) F(481 64 BIT 81 32 BIT 82 B3 B4 B5 I B6 B7 CUI CUZ cu3 16 48 0 BIT cu4 S: SIGN •:EXPONENT F : MANTISSA MEMORY REWEST PEMl PEMZ PEM, Fig. 7. ILLIAC IV data representation. 
PEM4 Fig. 8. (a) Electrical connectivity for 
routing. (b) Physical layout. Fig. 9. 1/0 data path. 

328 Part 4 1 The instruction-set processor 
level: special-function processors 
Section 2 1 Processors for array data 
cabinet. Data is transmitted to the 
CU from the CUB on a 512-line bus. Disk and on-line 1/0 data are transmitted 
on a 1024-line bus 
which can be switched among the arrays. Within each 
array, parallel connection 
is made to a selected 
16 of 64 PES, 2 per PUC. 
Maximum data rate 
is one 1/0 transaction per 
microsecond or lo9 bits per second. 
The 1/0 path of 1024 lines 
is expandable to 
4096 lines if required. Processing element memo y (PEM) The individual memory attached to each processing element is a thin-film DRO 
linear select 
memory with a 
cycle time of 240 ns and access time of 120 ns. Each has a capacity of 2048 64-bit 
words. The memory is independently accessible by 
its attached PE, the 
CU, or 1/0 connections. DiskIfile subsystem The computing speed and 
memory of the ILLIAC IV arrays re- quire a substantial secondary storage 
for program 
and data files as well as 
backup memory for programs whose 
data sets exceed fast memory capacity. The disk-file subsystem consists 
of six Bur- roughs model 
IIA storage units, each 
with a capacity 
of 1.61 X los bits and a maximum latency of 40 ms. The system is dual; each half has 
a capacity of 5 x 1OX bits and 
independent electronics capable of supporting a transfer 
rate of 500 megabits per second. The data path 
from each of the disk subsystems becomes 1024 
bits wide at its interface with the array. Figure 9 
shows the organization of the disk-file system. B 6500 control computer The B 6500 computer is assigned the following functions. 1 2 3 Executive control of the execution of array programs Control of the multiple-array configuration operations Supervision of the internal 1/0 processes (disk 
to arrays, etc.) External 1/0 processing and supervision Processing and supervision of the files on the disk file sub- system Independent data 
processing, including compilation 
of ILLIAC IV programs 4 5 6 To control the array operations, 
there is a single interrupt line and a 16-bit 
data path both 
ways between the B 6500 and each 
of the control units. 
In addition, the B 6500 has a control and 
data GPC-IOC DISK TEST PEM-DISK TEST PEM TEST CU TEST PE TEST 0 TO BE TESTED PARTIALLY TESTED [zzl TESTED Fig. 10. System diagnostic sequence. path to the 1/0 controller (IOC) which supervises 
the disk, and also direct connections to the array memories. Reliability and maintenance of the ILLIAC IV The progress in computer components from vacuum 
tubes to semi- conductors over several 
generations has improved the mean-time- between-failures for computers from tens of hours to several thou- sand hours. By using larger scale integration, a 
tedfold increase 
Chapter 27 1 The ILLIAC IV computer 329 in number 
of gates per system should 
be possible with comparable 
reliability. It is only by virtue of high-density integration (50- to 100-gate package) that the 
design of a three-million-gate 
system can be contemplated. Reliability of the major part of the system, 256 processing elements and 256 memory units, is expected to be 
in the range of lo5 hours per element and 
2 x lo3 hours per memory unit. The organization of the ILLIAC IV as 
a collection 
of identical units simplifies its maintenance 
problems. The processing ele- ments, the memories, and some part of power supplies are designed to be pluggable and replaceable to 
reduce system down 
time and 
improve system availability. The remaining problems are (1) location of the faulty subsys- tem, and (2) location of the faulty package in 
the subsystem. Location of the faulty subsystem assumes 
the B 6500 to be fault-free, since this 
can be determined by using the standard 
B 6500 maintenance routines. 
The steps to follow are shown in Fig. 10. The B 6500 tests the control units 
(CU) which in 
turn test all 
PES. PEMs are tested 
through the disk channel. This capability for functional partitioning of the subsystems simplifies 
the diag- nostic procedure 
considerably. References HollJ59; KuckD68; MurtJ66; SlotD62; UngeS58 
330 Part 4 I The instruction-set processor level: special-function processors 
Section 2 1 Processors for array data 
APPENDIX 1 Al. CLASSIFIED LIST OF CU INSTRUCTIONS AI. 1 Data transmission ALIT BIN BINX BOUT BOUTX Indexed block store. CLC Clear CAR. COPY DUPI DUPO EXCHL Add literal (24 bit) to CAR. Block fetch to CU 
memory. Indexed (by PE index) block fetch. Block store from 
CU memory. Copy CAR into CAR of other quadrant. Duplicate inner 
half of CU memory ad- dress contents into 
both halves of CAR. Duplicate outer 
half of CU memory ad- dress contents into both 
halves of CAR. Exchange contents of CAR with CU mem- ow address contents. 
LDL LIT LOAD LOADX ORAC SLIT STL STORE STOREX TCC W TCW A1.2 Skip and test CTSB(: ") 4 Instructions: 4 Instructions: Load CAR from CU memory address con- 
tents. Load CAR with 64-bit 
literal following the instruction. Load CU memory from 
contents of PE memory address 
found in 
CAR. Load CU 
memory from 
contents of PE memory address 
found in 
CAR, indexed 
by PE index. OR all CARS in array 
and place in CAR. Load CAR with 24-bit literal. Store CAR into CU memory. Store CAR into PE memory. Store CAR into PE 
memory, indexed 
by PE index. 
Transmit CAR counterclockwise between CUs in 
array. Transmit CAR clockwise between CUs in array. Skip on nth bit of CAR. If Tis present, skip if 1; if F is present, skip if 0. If A is pres- ent, AND together bits from all CUs in array before testing; if absent, OR together bits from all GUS in array before testing. CTSBT, CTSBTA, CTSBF, CTSBFA. 
Skip on CAR equal to CU 
memory ad- 
dress contents. The 
letters T, F, and A have the same meaning as 
in CTSB above. 
EQLT, EQLTA, EQLF, EQLFA. 4 Instructions: 4 Instructions: LESS(:> A J 4 Instructions: ONES( i> 4 Instructions: ONEX(: 4 Instructions: SKIP(:' ") 4 Instructions: SKIP 8 Instructions: Skip on 
index portion 
of CAR (bits 40 through 63) equal 
to bits 40 through 63 of CU memory address contents. 
The letters T, F, and A have the same meaning 
as in CTSB above. EQLXT, EQLXTA, EQLXF, EQLXFA. Skip on index part of CAR (bits 40 through 63) greater 
than bits 40 through 
63 of CU memory address 
contents. The letters T, F, and A have the same meaning 
as in CTSB above. 
GRTRT, GRTRTA, GRTRF, GRTRFA. Skip on index part of CAR (bits 40 through 
63) less than bits 40 through 
63 of CU memory address contents. 
The letters 
T, F, and A have the same meaning as in CTSB above. LESST, LESSTA, LESSF, LESSFA. Skip on CAR equal to all 1's. The letters T, F, and A have the same meaning 
as in CTSB above. ONEST, ONESTA, ONESF, ONESFA. 
Skip on 
bits 40 through 63 of CAR equal 
to all 1's. The letters T, F, and A have the 
same meaning 
as in CTSB above. ONEXT, ONEXTA, ONEXF, ONEXFA. 
Skip on T-F flip-flop previously set. The letters T, F, and A have the same meaning 
as in CTSB above. SKIPT, SKIPTA, SKIPF, SKIPFA. Skip unconditionally. Skip on index 
portion of CAR (bits 40 through 63) less than limit portion (bits 
1 through 15). The letters T, F, and A have the same 
meaning as in CTSB above. 
If I is present, the index portion 
of CAR is in- cremented by the increment portion of CAR (bits 16 through 
39) while 
the test is in progress; if I is not present, no incre- menting takes place. 
TXLT, TXLTI, TXLTA, TXLTAI, TXLF, TXLFI, TKLFA, TXLFAI. 
Skip on 
index portion 
of CAR (bits 40 through 63) equal to 
limit portion of CAR (bits 1 through 15). See CTSB for the 
meaning of T, F, and A; see TXL above for the meaning of I. 
Chapter 27 1 The ILLIAC IV computer 331 8 Instructions: 8 Instructions: ZER( "1 4 Instructions: 4 A1.3 Al.4 ZERX(: "1 Instructions: Transfer of control TXET, TXETI, 
TXETA, TXETIA, TXEF, 
TXEFI, TXEFA, TXEFIA. Skip on index portion 
of CAR (bits 40 through 63) greater than limit portion of CAR (bits 1 through 15). See CTSB for the meaning of T, F, and A; see TXL above for the meaning of 1. TXGT, TXGTI, TXGTA, TXGTAI, TXGF, TXGFI, TXGFA, TXGFAI. Skip on CAR all 0's. See CTSB for the meaning of T, F, and A. ZERT, ZERTA, ZERF, ZERFA. Skip on index portion 
of CAR (bits 40 through 63) all 0's. See CTSB for the 
meaning of T, F, and A. ZERXT, ZERXTA, ZERXF, ZERXFA. EXEC EXCHL HALT JUMP LOAD LOADX STL Route RTE A1.5 Arithmetic ALIT CADD CSUB INCRXC A1.6 Logical CAND CCB CEXOR Execute instruction found in 
bits 32 through 
63 of CAR. Exchange contents of CAR with contents 
of CU memory 
address. Halt ILLIAC 
IV. Jump to address found in 
instruction. Load CU memory address contents from contents of PE memory address found 
in CAR. Load CU 
memory address 
contents from contents of PE memory address 
found in CAR, indexed by 
PE index. Store CAR into CU memory. Route. Routing distance 
is found in address 
field (CAR indexable), 
and register con- nectivity is found in 
the skip field. Add %-bit literal to CAR. Add contents of CU memory address to CAR. Subtract contents 
of CU memory address 
from CAR. Increment index word in CAR. AND CU memory to CAR. Complement bit of CAR. Exclusive OR CU memory to CAR. CLC COR CRB CROTL CROTR CSB CSHL CSHR LEAD0 Clear CAR. OR CU memory 
to CAR. Reset bit of CAR. Rotate CAR left. Rotate CAR right. Set bit of CAR. Shift CAR left. Shift CAR right. Detect leading ONE in CAR of all quad- rants in 
array. Detect leading ZERO in CAR of all quad- rants in 
array. OR all CARS in array and place in CAR. LEADZ ORAC A2. CLASSIFIED LIST OF PE INSTRUCTIONS A2.1 Data tramisdon LDA LDB LDR LDS LDX LDCO LDCl LDC2 LDC3 LEX ONES STA STB STC STR STS STX 
SWAPA SWAP SWAPX Load A register. Load B register Load R register. Load S register. Load X register. Load CAR 0 from PE 
register. Load CAR 1 from PE 
register. Load CAR 2 from PE 
register. Load CAR 3 from PE register. Load exponent of A register. Load all ONES into A register. Store A register. Store B register. Store C register. Store R register. Store S register. Store X register. Interchange inner and outer contents 
of A register. Interchange the contents of A register and B register. Interchange outer operand 
of A register and inner operand 
of B. A2.2 Index operations Set I on comparison 
of X register and op- erand. The presence 
of L means set 
I if X is less than operand; 
the presence of E means set I if X is equal to operand; the presence of G means set 
I if X is greater than operand. 
If Z is present, increment X while performing test; if I is absent, do not 
increment X. 
332 Part 4 I The instruction-set processor level: special-function processors 
Section 2 1 Processors for array data 
6 Instructions: JX{ i7 11 6 Instructions: 
XI XI0 A2.3 Mode setting, EQB ' GRB LSB CHWS 3 Instructions: 3 Instructions: 3 Instructions: l(:Iz 3 Instructions: 3 Instructions: J{, i} Z 0 15 Instructions: L IX{2 I] IXL, IXLI, IXE, IXEI, IXG, IXGI. Set J on comparison of X register and op- erand. See above for meaning of L, E, G, and I. JXL, JXLI, JXE, JXEI, JXG, JXGI. 
Increment PE index (X register) by bits 
48 through 63 of operand. Increment PE index of bits 48 through 
63 of operand plus one. 
/comparisons Test A and B for equality bytewise. Test B register greater than A register bytewise. Test B register less than A register bytewise. Change word size. Set 1 if A register is less than operand. L means test logical; A means test arithmetic; 
M means test 
mantissa. ILL, IAL, IML. Set 1 if A register is equal to operand. See above for meaning 
of L, A, and M. ILE, IAE, IME. Set 1 if A register is greater than operand. See above for meaning of L, A, and M. ILG, IAG, IMG. 
Set 1 if A register is equal to all zeros. ILZ, IAZ, IMZ. 
Set 1 if A register is equal to all ONES. ILO, IAO, IMO. Set J under conditions specified in set of instructions immediately above. 
6 Instructions: 6 Instructions: 3 Instructions: 3 Instructions: ISN SETE SETEO SETF SETFO SETG SETH SET1 SETJ SETCO SETC 1 SETC2 SETC3 IBA JSN A2.4 Arithmetic 
ADB SBB ADD SUB JLL, JAL, JML, JLE, JAE, JME, JLG, JAG, JMG, JLZ, JAZ, JMZ, JLO, JAO, JMO. Set 1 on comparison 
of X register and op- erand. See Section A2.2 for meaning of L, E, G, and I. IXL, IXLI, IXE, IXEI, IXG, 
IXGI. Set J on comparison of X register and op- erand. See Section 
A2.2 for meaning 
of L, E, G, and 1. JXL, JXLI, JXE, JXEI, JXG, 
JXGI. Set 1 on comparison of S register and op- erand. See Section A2.2 for meaning of L, E, and G. ISL, ISE, 
ISG. Set J on comparison of S register and op- erand. See Section A2.2 for meaning of L, E, and G. JSL, JSE, JSG. Set I from the sign bit of A register. Set J from the sign bit of A register. Set E bit as a logical function of other bits. Set El bit similarly. 
Set F bit similarly. Set F1 bit similarly. 
Set G bit similarly. Set H bit similarly. Set 1 bit similarly. Set J bit similarly. Set Pth bit of CAR 0 similarly. Set Pth bit of CAR 1 similarly. Set Pth bit of CAR 2 similarly. Set Pth bit of CAR 3 similarly. Set 1 from Nth bit of A register; bit 
num- ber is found in address field. Set J from Nth bit 
of A register; bit 
num- ber is found in address field. Add bytewise. 
Subtract operand 
from A register bytewise. Add A register and operand as 64-bit 
operands. Subtract operand from 
A register as 
64- bit quantities. S} Add operand to A register. The R, N, M, S specify all possible variants of the arith- metic instruction. 
The meaning of each letter, if present in 
the mnemonic, is R round result N normalize result 
M mantissa only 
S special treatment of signs. 
Chapter 27 I The ILLIAC IV computer 333 16 Instructions: 
ADM, ADMS, ADNM, ADNMS, ADN, ADNS, ADRM, ADRMS, ADRM, 
ADRNMS, ADRN, ADRNS, ADR, 
ADRS, AD, ADS. ADEX Add to exponent. DV{R, N, M, S} Divide by operand. See AD instruction for meaning of R, N, M, and S. 16 Instructions: DVM, DVMS, DVNM, DVNMS, 
DVN, DVNS, DVRM, DVRMS, 
DVRNM, DVRNS, DVRN, 
DVRNS, DVR, DVRS, DV, DVS. Extend precision after floating point ADD. Extend precision after floating point 
SUB- TRACT. LEX Load exponent 
of A register. ML{R, N, M, S} Multiply by 
operand. See AD instruction for meaning of R, N, M, and S. MLM, MLMS, MLNM, MLNMS, MLN, MLNS, MLRM, MLRMS, MLRNM, 
MLRNMS, MLRN, MLRNS, MLR, MLRS, ML, MLS. EAD ESB 16 Instructions: SAN Set A register negative. 
SAP Set A register positive. SBEX Subtract exponent of operand from expo- nent of A register. SB{R, N, M, S} Subtract operand from A register. See AD instruction for meaning of R, N, M, and S. SBM, SBMS, SBNM, SBNMS, 
SBN, SBNS, 
SBRM, SBRMS, SBRNM, SBRNMS, 
SBRN, SBRNS, SBR, 
SB, SBS. 
In 32-bit mode, perform 
MULTIPLY and 
leave outer result in 
A register and inner result in B register, with both results ex- tended to 64-bit format. 
16 Instructions: 
NORM Normalize A register. MULT A2.5 Logical AND A register with operand. 
The left- the A register, the right-hand set, 
on the operand. The meaning of these variants is not present 
use true N use complement Z use all ZEROS 0 use all ONES. hand set of letters specifies a variant on 16 Instructions: 
CBA CHSA { EOR { 16 Instructions: LEX 16 Instructions: RBA RTAL RTAML RTAMR RTAR SAN SAP SBA SHABL SHABR SHAL SHAML SHAR SHAMR AND, ANDN, ANDZ, ANDO, NAND, NANDN, NANDZ, 
NANDO, ZAND, ZANDN, ZANDZ, 
ZANDO, OAND, OANDN, OANDZ, 
OANDO. Complement bit 
of A register. Change sign of A register. Exclusive OR A register with 
operand. EOR, EORN, EORZ, EORO, NEOR, 
NEORN, NEORZ, 
NEORO, ZEOR, ZEORN, ZEORZ, ZEORO, OEOR, OEORN, OEORZ, 
OEORO. Load exponent 
of A register. OR A register with operand. OR, ORN, ORZ, ORO, NOR, NORN, NORZ, NORO, ZOR, ZORN, ZORZ, ZORO, OOR, OORN, OORZ, OORO. Reset bit A register to ZERO. 
Rotate A register left. 
Rotate mantissa of A register left. Rotate mantissa of A register right. 
Rotate A register right. Set A register negative. 
Set A register positive. Set bit of A register to ONE. Shift A and B registers double-length left. Shift A and B registers double-length right. 
Shift A register left. 
Shift A register mantissa left. Shift A register right. Shift A register mantissa right. 
Section 3 Processors defined by a microprogram Processors defined by a microprogram have only recently come into existence, although Wilkes suggested 
the idea in 1951. The discussion in Chap. 3 (page 71) suggests reasons 
why this controversial idea has taken so long to be adopted. Microprogramming and the design of the control circuits in an electronic computer Chapter 28 is an extension 
of an earlier paper 
by Wilkes. It includes an example of a 
microprogrammed processor (page 
337). In the earlier paper, The Best 
Way to Design an Automatic Computing Machine [Wilkes, 1951a1, the essential ideas of microprogramming were first outlined. The observation 
that an instruction 
set, or ISP, should be looked at as a program to be interpreted is the basis of micro- 
programming. The idea of an ISP is our acknowledgment that we, too, view a processor 
as a program. There is little to 
say about this chapter; it is historical, 
yet timely and 
well written. Microprogramming, 
like other 
of Wilkes™ ideas, is 
present in many of our computers. chines they have designed. 
This formal ruse can be used 
to make the design seem difficult but well founded-certainly not arbitrary, Kampe truthfully admits to making decisions in a somewhat arbitrary fashion. The SD-2 microprogram structure, unlike that of the IBM Sys- tem 360 models, has a P.microprogram which is similar to the 
external Pc which it defines. As such, the main 
question about 
this design is whether it is cheaper to have a single, 
hard- wired Pc rather than a computer within a computer. The Packard Bell 440 [Boutwell and 
Hoskinson, 19631 is an 
example of a better-known Pc whose internal P resembles 
the SD-2. The authors of this book feel 
that, when the internal and 
external P™s are so similar, it may be better to have a single 
Pwhich suits both needs. To gain speed and still define 
powerful functions, Mp 
could be made up of both the 
conventional Mp 
and a small, fast Mp. The Hewlett-Packard HP 9100A computing calculator The HP 9100A 
(Chap. 20) 
is discussed in Part 3, 
Sec. 4, page 235. The design of a general-purpose microprogram- 
controlled computer with elementary structure 
The SD-2 
computer (Chap. 29) 
is described by Kampe in a casual but highly communicative fashion. Most 
engineers tend to be somewhat formal and stuffy when describing the ma- Microprogrammed implementation of EULER On the IBM System 360™Mode1 30 This microprogrammed processor in Chap. 32 is also discussed 
as a language processor 
in Part 4, Sec. 4, page 348. , 334 . \ 
Chapter 28 Microprogramming and the design of the control circuits 
in an electronic digital computer1 M. V. Wilkes / J. B. Stringer 1. Introduction Experience has shown 
that the 
sections of an electronic digital 
computer which are easiest to maintain are those which 
have a simple logical 
structure. Not only 
can this structure be readily borne in 
mind by a 
maintenance engineer when 
looking for 
a fault, 
but it makes it possible to use fault-locating programmes and to test the equipment without the 
use of elaborate test 
gear. It is in the control section 
of electronic computers 
that the 
greatest degree of complexity generally arises. This is particularly so if the machine has a comprehensive order code 
designed to make it simple and fast in operation. 
In general, for each different order in the code some special equipment must be provided, and the 
more complicated the function of the order the more complex this equipment. In 
the past, fear 
of complicating unduly 
the control circuits of the machines has prevented the designers of electronic machines from 
providing such facilities as orders for floating-point 
operations, although experience 
with relay machines and with 
interpretive subroutines has shown how 
valuable such 
orders are. This paper describes a 
method of designing the control circuits 
of a machine 
which is wholly logical 
and which enables alterations 
or additions to the order code to be made without ad hoc altera- tions to the circuits. An outline of this method 
was given by 
one of us [Wilkes, 1951~1 at the 
Conference on Automatic Calculat- ing Machines at the University of Manchester in 
July 1951. The operation called 
for by 
a single machine order can 
be broken down 
into a sequence of more elementary operations; 
for example, shifting a number in 
the accumulator one place 
to the 
right may involve, 
first, a transfer 
of the number to an auxiliary shifting register, and secondly, 
the transfer of the number back 
to the accumulator along 
an oblique path. These elementary 
operations will be referred to as micro-operations. Basic machine operations, such as 
addition, subtraction, 
multiplicatio-tc., are thought of as being made 
up of a micro-programme of micro- . 'Proc. Cambridge Phil. Soc., pt. 2, vol. 49, pp. 230-238, April, 1953. operations, each micro-operation being called 
for by a 
micro-order. The process of writing a micro-programme for 
a machine order 
is very similar 
to that of writing a programme 
for the whole calculation in terms 
of machine orders. For the method to be 
applicable it is necessary that the 
machine should contain a suitable 
permanent rapid-access storage device in which the micro-programme can be held-a diode matrix is proposed in the case of the machine discussed as an example 
below-and that means should 
be provided for executing the micro-orders one after the other. It is also necessary 
that provision should be made for 
conditional micro-orders which play a role in micro-programming similar 
to that played by conditional orders in ordinary 
programming. Since the only feature of the machine which 
has to be designed specially for any particular set of machine orders 
is the configura- tion of diodes in the matrix, or 
the corresponding configuration in whatever equivalent device 
is used, there is no difficulty in making changes 
to the 
order code 
of the machine if experience shows them to be 
desirable; in 
fact, the 
design of the machine in the first place can be carried out completely without a 
firm decision on the details of the order code,being taken, as long as 
care is taken to provide accommodation 
for the greatest number 
#of micro-orders that are likely to be required. It would even be possible to have a number 
of interchangeable matrices providing 
for different 
order codes, so that the 
user could choose the one most suited to his particular requirements. 2. The system will 
be described in relation to a parallel machine 
having an arithmetical unit designed along conventional 
lines. This will contain a set 
of registers and an adder together with 
a switch- 
ing system which 
enables the micro-operations in 
the various machine orders to be 
performed. Some of the micro-operations will be simple transfers 
of a number 
from one register 
to another with or without shifting 
of the number one place to 
the left or Description of the proposed system 335 
336 Part 4 I The instruction-set processor level: special-function processors 
the right, while 
others will also involve the use of the adder. Any particular micro-operation can be performed by applying pulses simultaneously to the appropriate gates of the switching system. 
In certain cases it may be possible for two or more micro-opera- tions to take place 
at the same time. It will be convenient to 
regard the control system as 
consisting of two parts. A register is needed to hold the address of the next order due to 
be executed, and another to hold the current order 
while it is being executed, 
or at any rate during part 
of that time. Some means of counting the number of steps in 
a shifting operation or a multiplication must also be provided. One method 
of meeting these requirements is to provide a group of registers and an adder together with 
a switching system which enables transfers 
of num- bers, with or without addition, 
to be made. This part of the control system will 
be called the control register unit. In any case the operations which need to be performed on the numbers standing 
in the control register 
unit during 
the execution of an order are, 
like the operations performed in the arithmetical unit, regarded as being made 
up of a sequence of micro-operations, each of which is performed by 
the application of pulses to appropriate 
gates. The other part of the control system is concerned with 
control of the sequence of micro-orders required to 
carry out each 
machine order, and with the operation of the gates required for the execu- tion of each micro-order. This 
will be called the micro-control unit; it consists of a decoding tree, two 
rectifier matrices and two regis- ters (additional to those of the control register 
unit) connected 
as indicated in 
Fig. 1, which shows how the pulses used to operate the gates in the arithmetical unit 
and control register 
unit are generated. A series of control pulses from a pulse generator are 
applied to 
the input of the decoding tree. Each 
pulse is routed to one of the output 
lines of the tree, according to the number standing in 
register I. The output 
lines all 
pass into a rectifier 
matrix A and the outputs 
of this matrix 
are the pulses which operate the various gates associated 
with micro-operations. Thus one input line of the matrix corresponds 
to one 
micro-order. The address of the micro-order is the number which must 
be placed in register I to cause the control pulse 
to be routed to 
the corre- sponding line. 
The output lines from the tree also pass 
into a second matrix 
B, which has its 
outputs connected to 
register 11. This matrix has 
wired on it the 
address of the micro-order to be performed next in 
time so that the address of this micro-order is placed in register 11. Just before 
the next control pulse 
is applied to the 
input of the tree a connexion 
is established between register I1 and register I, and the address of the micro-order due to be executed next is transferred into register I. In this way the de- coding tree is prepared to route the next incoming control pulse Section 3 I Processors defined 
by a microprogram 
I Matrix B c_____- -7 r------1 Contro pulses - To arithmetical From unit, control condltiona' registers, etc. flip-flop Fig. 1. Microcontrol unit. to the correct output line. Thus application 
of pulses alternately to the input of the tree and to the 
gate connecting 
registers I and I1 causes a 
predetermined sequence 
of micro-orders to be executed. It is necessary to have 
means whereby 
the course of the micro- programme can be made conditional on whether 
a given 
digit in one of the registers of the arithmetical unit 
or control register 
unit is a 1 or a 0. The means of doing this is shown at X in Fig. 1. A two-way switch, controlled 
by a special flip-flop called a condi- tional flip-flop, is inserted between 
matrix A and matrix B. The conditional flip-flop can be set by an earlier 
micro-order with any 
digit from any one of the registers. Two separate addresses are wired into 
matrix B, and the 
one which passes into register I, 
and thus becomes the address of the next micro-order, is determined by the setting of the conditional flip-flop. Conditional micro-orders play 
the same part in the construction of micro-programmes as conditional orders play in the construction of ordinary programmes; apart from their obvious uses in micro- 
programmes for such operations 
as multiplication and division, they enable repetitive 
loops of micro-orders to be used. If desired, two branchings may be inserted in the connexions between matrix A and matrix B, so that any one of four alternative addresses for the next micro-order 
may be selected according 
to the settings of two conditional flip-flops. Another possibility is to 
Chapter 28 1 Microprogramming and the design of the control circuits in an electronic digital computer 337 make the output from the decoding tree branch before it enters matrix A so that the nature 
of the micro-operation that is per- formed depends 
on the setting of the conditional flip-flop. The micro-programme wired 
on to the matrices contains 
sec- tions for 
performing the operations required by each order in 
the basic order code 
of the machine. To initiate the operation it is only necessary 
that control in 
the micro-programme should 
be sent to the 
correct entry 
point. This is done by placing the function digits of the order in 
the least significant part of register 11, the other digits in this register being made zero. 
The micro-programme is constructed so that when this number 
passes into register I, control in 
the micro-programme is sent to the 
correct entry 
point. The switching system in the arithmetical unit may either be designed to permit a large variety 
of micro-operations to be per- formed, or it may be restricted so as to allow only 
a small number of such operations. In a machine with a 
comprehensive order code there is much to be 
said for 
having the more flexible switching system since this will 
enable an economy to be 
made in 
the number of micro-orders needed in the micro-programme. A similar remark 
applies in connexion with the degree of flexi- bility to 
be provided when 
designing the switching system for 
the control register unit. If the specification of the machine allows the same number of registers to be used in the arithmetical and 
control sections, 
the construction of these two sections may 
be identical except 
as far as the number of digits is concerned. In a new machine 
under construction in the Mathematical Labora- 
tory, Cambridge, 
the registers are being constructed 
in basic units 
each containing five registers and an adder-subtractor together 
with the associated switching system. It is hoped that it will be possible to use identical units in 
the arithmetical 
unit and in the control register unit. 3. Etample An example will now 
be given to show the way in which a micro- programme can be drawn up for a machine with a 
single-address order code covering 
the usual operations. It is supposed that the 
arithmetical unit contains 
the following registers: A multiplicand register B accumulator (least 
significant half) C accumulator (most significant 
half) D shift register The registers in the control register unit are as follows: E register connected to the access circuits of the store; the address of a storage location 
to which access is required is placed here sequence control register; contains 
address of next order due F to be 
executed G register used for 
counting It was assumed when drawing 
up the micro-programme that there was an adder-subtractor in 
the arithmetical unit 
with one 
input permanently connected to register D, and a similar adder-sub- 
tractor in the control register 
unit with one 
input permanently connected to register G. For convenience it was assumed that the 
switching systems in each 
case were comprehensive 
enough to provide any micro-operation required. 
It was further supposed that the arithmetical unit 
provided for 20 
digits and 
that the numbers 0, 1 and 18 could be introduced at will into one of the registers or the adder of the control register unit. Two conditional flip-flops are used. All micro-operations including 
those involving access 
to the store are supposed to take the same amount of time. Reference will be made to this point in 
r54. Table 1 gives the order code 
of the machine, and Table 2 the micro-programme. Each line 
of Table 2 
refers to one micro-order; 
the first column gives the address of the micro-order, the second column specifies the micro-operations called 
for in the arithmetical unit of the machine, and the third column specifies the micro- Table 1 Notation: Acc = accumulator Accl = most significant 
half of accumulator Accz = least significant half of accumulator C(X) = contents of X (X = register or storage location) n = storage location n Order Effect of order A n S n V n T n C(Acc) + C(n) to Acc C(Acc) - C(n) to Acc C(Accz).C(n) to Acc, where C(n) 2 0 C(Acc1) to n, 0 to Acc W TI C(n) to ACC~ U n C(ACC~) to n R n L n C(ACC).~"+' to ACC C(ACC) .Z-(n+l) to ACC G n I n 0 n If C(Acc) < 0, transfer control to n; if C(Acc) 2 0, ignore (i.e., proceed serially) Read next character on input mechanism into 
Send C(n) to output mechanism 
338 Part 4 1 The instruction-set processor level: special-function processors Section 3 1 Processors defined 
by a microprogram Table 2 Notation: A, B, C, . . . stand for 
the various registers 
in the arithmetical and 
control register units (see 03 of the text). 'C to D' indicates that 
the switching circuits connect the 
output of register 
C to the input 
of register 
D; '(D+A) to 
C' indicates that the output of register 
A is con- nected to the one input of the adding unit (the output of D is permanently connected 
to the other input), and 
the output of 
the adder to register C. A numerical symbol n in quotes (e.g., In') stands for the source whose output is the number n in units of the least significant digit. 
Ari thmeticul unit Control register unit Conditional fliP-.fEop Next micro-order Set Use 0 1 0 1 2 3 4 A5 S6 H7 V8 T9 u 10 R 11 L 12 G 13 I 14 0 15 16 17 18 19 20 21 22 23 24 25 26 
27 28 29 30 31 32 33 34 35 36 
37 C to D C to D Store to B Store to A C to Store C to Store B to D C to D Input to Store Store to Output (D+Store) to C (D- Store) to C D to B (R)t C to D D to C (R) D to C (L)$ B to D D to B (L) '0' to B B to C '0' to c B to D D to B (R) C to D (R) D to C (D+A) to C BtoD D to B (R) C to D (R) D to C (D-A) to 
C F to G and E (G+'l') to F Store to G G to E E to decoder EtoG E to G E to G (G-'l') to E (G-'l') to E '18' to E E to G (G-'l') to E 1 2 3 4 16 17 0 27 25 0 19 22 18 0 0 0 0 0 20 21 11 23 24 12 26 0 28 29 30 31 28 28 34 35 36 0 0 - 1 0 0 32 33 33 37 t Right shift. 
The switching circuits in the arithmetic unit are arranged 
so that the 
least Significant 
digit of register C is placed in the most significant place of register B during right shift micro-operations, 
and the most significant digit of register C (sign digit) is repeated (thus making the correction 
for negative numbers). $ Left shift. The switching circuits are similarly arranged to pass the most significant 
digit of register B to the least significant place of register 
Cduring left shift 
micro- operations. 
Chapter 28 I Microprogramming and the design of the control 
circuits in 
an electronic digital computer 339 operations called 
for in the control register unit. The fourth col- umn shows which conditional 
flip-flop, if any, is to be set and the digit which is to be 
used to set it; for example, 
(1)C, means that flip-flop number 1 is set by the sign digit of the number in register 
C, while (2)G, means 
that flip-flop number 2 is set by the least significant digit of the number in 
register G. In the 
case of uncon- ditional micro-orders columns 
5 and 7 are blank and column 6 contains the address of the next micro-order 
to be executed. In the case of conditional micro-orders column 5 shows which 
flip-flop is used to operate the conditional switch 
and columns 6 and 7 give the alternative addresses to which control 
is to be 
sent when the conditional flip-flop contains a 
0 or a 1 respectively. Micro-orders 0 to 4 are concerned with the extraction of orders from the store. They serve 
to bring about the transfer of the order from the store to register E and then cause the five most significant 
digits of the order to be placed in 
register I1 with the 
result that control is transferred to one of the micro-orders 5 to 15, each of which corresponds 
to a distinct order in 
the machine order code. In this way the sequence of micro-orders needed to perform the particular operation called 
for is begun. The way in which the various operations are performed can be followed from 
Table 2. In the section dealing 
with multipli- cation, it is assumed that numbers lie 
in the 
range -1 < x < 1 and that negative numbers 
are represented in 
the machine by 
their complements with respect to 2. It will be noted that the process of drawing up a micro-programme 
is very similar 
to that 
of draw- ing up an ordinary programme 
for an automatic computing 
ma- chine and the 
problems involved 
are very much alike. 4. The timing 
of micro-operations The assumption that all micro-operations take 
the same length of time to perform 
is not likely to be borne out in practice. In 
particular in a parallel machine 
it may not be possible to design an adder 
in which the carry propagation 
time is sufficiently short to enable an addition to be 
performed in substantially 
the same length of time as that taken for a simple 
transfer. It will be neces- sary, therefore, to arrange 
that the 
wave-form generator feeding 
the decoding tree should, when suitably stimulated by a 
pulse from 
one of the outputs 
from matrix A, supply a somewhat 
longer pulse 
than that normally required. Other operations may take many times 
as long to perform as an 
ordinary micro-order; 
for example, access 
to and from the store (particularly if a delay store 
is used) and operation of the input 
and output devices of the machine. The sequence of operations in 
the micro-programme must 
therefore be interrupted. One 
way of doing this is to prevent pulses from 
the wave-form generator reaching 
the decoding tree during 
the waiting period. 
This method, although 
quite feasible, appears to involve just the kind of complication which the present system is designed to avoid. A more attractive system is to make the machine wait 
on a conditional 
micro-order which transfers 
control back to itself unless the associated conditional flip-flop is set. Setting of this flip-flop takes place when the operation is com- pleted, and 
control then goes to the next micro-order 
in the 
se- quence. The machine is thus in a condition 
of ‚dynamic stop™ 
while waiting for the operation to be completed. This 
system has 
the advantage that no complication is introduced into the units sup- plying the wave-forms to the 
decoding tree and 
that the 
control equipment required is similar to that 
already provided 
for other purposes. 5. Discussion It will be seen that the 
equipment needed 
to execute a 
compli- cated order in the machine order 
code is of the same form as that required for a simple one, namely outlets from the decoding tree and diodes in the matrices. Quite complicated orders 
can, there- fore, be built into the machine without difficulty. In particular, 
arithmetical operations on numbers expressed 
in floating binary form and other similar operations can be micro-programmed and it is found that they do not involve very 
large numbers 
of micro- orders. For example, a micro-programme providing 
for the float- ing-point operations 
of addition, subtraction, and multiplication 
needs about 70 micro-orders. The switching system in the arith- metical unit 
must, of course, be designed with these operations 
in view. The decoding tree and matrices of a parallel machine 
with 40 digits in the arithmetical unit and 
provision for 
256 micro-orders would only 
amount to about 15% of the total equip- ment in the machine, so that it appears 
that such a machine can 
well be provided with built-in facilities of considerable complexity. The number of micro-orders needed in a 
complicated micro- programme can 
sometimes be reduced by 
making use of what might be called micro-subroutines. For example, when two num- bers have to be 
added together in a 
floating binary machine, 
some shifting of one of them is usually necessary before 
the addition can take place. 
By making the micro-orders for 
this shifting opera- 
tion serve also 
when a multiplication is called for, considerable saving is effected. Four registers is the bare minimum needed in 
the arithmetical unit in order to enable the 
basic arithmetical operations to 
be performed. If any extension or refinement 
of the facilities provided is required, it may be necessary to increase the number of registers. 
340 Part 4 1 The instruction-set processor level: special-function 
processors For example, four registers 
are not sufficient to enable a succession 
of products to 
be accumulated without 
the transfer of intermediate results to the store, since 
the accumulator must be clear at the 
beginning of a multiplication. The addition of one register enables 
the accumulation of products to 
be provided for in the micro- programme. If this register 
is associated with the outlet from the store, it also enables some of the waiting time 
for storage access to be eliminated. To do this the micro-programme is arranged to call for a number from the store as soon 
as it is known that the number will be required and to continue with other necessary micro-operations before 
finally proceeding to use the number. The ‚dynamic stop™ 
would occur just before the number is required for use. Another way 
of saving time is to arrange, in 
the case of those orders which permit it, 
for the next order to be extracted from the store before 
the operation currently being 
performed has been completed. The minimum number of registers required in 
the control register unit of the machine for the simplest mode of operation is three. If extra registers 
are provided facilities 
similar to those provided by the B-lines in the machine at Manchester University could be included in the micro-programme. Section 3 1 Processors defined 
by a microprogram 
6. All the discussion so far has 
been with reference to 
parallel ma- chines because 
the technique described in this 
paper is most adapted to that type 
of machine. It is, however, possible to design a serial machine along the same lines. In a parallel computer with an asynchronous arithmetical unit 
every gate requires only 
one kind of wave-form to operate it and the timing of that wave-form is not critical. In 
a serial machine, 
on the other hand, 
different gates require different wave-forms and the same gate may require different wave-forms at different times; 
further, all these 
wave- forms must 
be critically timed. These 
complications may be handled by including in the micro-control unit a third matrix, C, for selecting the appropriate wave-form for 
each micro-order. The main wave-form, routed by the decoding tree and matrix A, opens a gate which is fed by a wave-form 
selected by matrix C. This enables a wave-form 
of correct duration 
to be applied to any selected gate in 
the arithmetical or control sections 
of the ma- chine. Microprogramming applied to serial machines References WilkM5la; BoutE63; FlynM67; GreeJ64, 
66; MercR57; Patz67; RosiR69; TuckS67; WilkM58b, 69; WebeH67 
Chapter 29 The design of a general-purpose microprogram-controlled computer 
with elementary structure1 
Thomas W. Kampe Summary This paper presents the design of a parallel digital 
computer utilizing a 20-psec core memory 
and a diode storage microprogram unit. The machine is intended as an on-line controller and is organized for ease of maintenance. A word length of 19 bits provides 31 orders referring 
to memory loca- tions. Fourteen bits are 
used for 
addressing, 12 for base address, one 
for index control, and one for indirect addressing. A 32nd order permits the address bits to be decoded 
to generate special functions 
which require no address. The logic of the machine is resistor-transistor; the arithmetic 
unit is a bus structure which permits many variants of order structure. In order to make logical decisions, 
a ﬁgeneral-purposeﬂ logic unit has been incorporated so that the 
microcoder has as much freedom in this area 
as in the arithmetic unit. 
Introduction This paper discusses the logical design 
of a binary, parallel, real- 
time computer. 
Only those aspects of packaging and circuitry which bear directly on this topic 
will be considered. Since the specifications for 
the job a computer is to perform are not 
enough to fix the design, the logical designer 
is faced with an undetermined system. One of his main functions is to analyze the system in its natural environment, 
i.e., with malfunctions, operator errors, etc., and to supply the remainder of the side conditions which do fix the design. In this discussion, the exposition will be directed toward 
the design philosophy 
which led to a machine now 
being built. 
In order to accomplish this, 
we shall consider 
the functional require- 
ments, their analysis in terms of the state of the art, the basic design decisions, 
and, finally, a description of the computer as it stands. ‚IRE Trans., EC-9, vol. 2, pp. 208-213, June, 1960. Functional requirements The design of the computer (known, for a 
variety of reasons, as the SD-2) was undertaken to supply a computer capable of mod- erately fast arithmetic with perhaps 
five decimal places of accu- racy and 3000 or more words of storage. Furthermore, the com- puter must reside in a hostile environment (a small house, 
0ﬂ to 85°C temperature), withstand 
severe shocks, and be maintained by men with 
only two weeks training on 
the system. The volume limitation is 40 cubic feet. 
Within this space must reside the control computer, memory, power 
supplies, complete maintenance 
facilities, and sufficient input/output equipment to handle 20 shaft position outputs, 30 such inputs, numerous switch settings, 
and 20 or more display or relay 
signals. The final specification (or blow) was 
that 15 months were available from the start of preliminary design to the delivery of an operating instrument with 
debugged program. Design analysis The maintenance requirement 
was evidently the major problem. In order to achieve the simplicity required, two 
design criteria were necessary. First, the computer had to be readily understood. This 
implied that the usual clever logical tricks such 
as intensive time sharing 
of control and arithmetic were 
undesirable. Second, if built-in maintenance facilities were to be kept sim- ple, the machine must be designed with 
this in 
mind. Since temperature and 
reliability were important, an 
extremely conservative approach had 
to be taken with respect to component performance. With the 
schedule requirements, a machine which could be designed and released in pieces was needed. Since the control system is usually the most troublesome part of a computer to design, a simple control was needed. 341 
342 Part 4 I The instruction-set processor level: special-function 
processors The volume available, together with the 
schedule, required a logical design 
with natural 
packaging properties in the sense that it should break, in a 
natural way, into logical packages 
of a reason- 
able size having a minimum of interpackage communication. Design decisions The need for 2000 
operations per second poses a serious access 
problem with a serial memory, unless one resorts to several simul- taneously operating control units 
which are neither small nor simple. Hence, a random access memory seemed 
advisable. Mag- netic core memories at 85°C are a problem, but they can 
be built, provided memory cycle time is not too 
short. The memory was chosen as 4096 words of core storage, 
with a 20-psec cycle time. The requirement for training a man in two weeks to maintain the machine argues for a 
simple-structured parallel machine. 
Providing that much use is made of asynchronous transfer, there are a variety of simple maintenance 
methods, particularly 
if a bus structure is adopted. Also, asynchronous, or 
semi-asynchronous, parallel machines require only average performance of a set of components, not of any particular component; 
the central limit theorem of statistics can come to the 
aid of reliability. This 
ap- proach was finally adopted. The simplicity of both design and understanding is aided by the use of a microprogram control system. Further, maintenance 
is made rather simple by two provisions on 
the maintenance con- sole. The first of these is a manner of going through the micro- program on a step-by-step basis. While this tests little of the dynamics, it can often locate totally defective parts, 
and it helps factory checkout immeasurably. 
The second is a means of taking out the microprogram unit and substituting a set of switches. This permits 
a maintenance man 
to exercise specific registers, 
or the memory, at will. This is a powerful tool, and is almost free 
with a microprogram control. Finally, 
and rather 
pragmatically, microprogramming permits ﬁlast minuteﬂ changes in machine 
operation without seri- ous hardware modifications. This approach was chosen. 
Regardless of the control used, 
at various times in the process of executing orders, 
decisions must 
be made. Occasionally these are on a single 
bit, more 
often on two, 
and occasionally on more than two. If one excludes order decoding, 
only such functions 
as zero detection require 
the use of more than two 
bits. At this point, 
the logical designer is faced with a rather sticky decision: whether to design a specific set 
of decision logic, 
which is cheap to build 
Section 3 
1 Processors defined 
by a microprogram but sometimes messy, or to use some microcontrolled logic- generating scheme. In this case, the latter alternative was taken. A unit, called 
(for several obscure 
reasons) the alteration unit, 
was designed which amounted to 
a three-address, one-bit 
unit. It can generate any 
Boolean function of two binary variables 
and transmit this 
value to another 
variable. A special set of logic was needed for detecting zeros. Because of the rather wild nature of the inputs, it seemed desirable to include a trapping mode. The logic for 
this was made an adjunct 
to the 
alteration unit. The circuitry chosen was resistor-transistor logic, 
which yields either Sheffer stroke or 
NOR logic, as one prefers, high or low true logic, and p-n-p or n-p-n transistors. In this case, the com- bination was high true logic and p-n-p transistors, so that the 
logical operation is Sheffer 
stroke. Because of temperature and reliability requirements, the maximum frequency available was a 250-kc square wave. This gave a 
cycle time of 4 pec available for asynchronous transfer in any sequence 
of logic. An index register seemed advisable because of the amount of data processing. Thus, additions 
were needed 
for indexing, 
arith- metic, and counter advance. It seemed undesirable to have more than one parallel adder, so that an adder 
accessible to all registers was chosen. This 
was another argument 
for a 
bus structure. Because of the multiplicity of problems being handled 
simul- taneously, one index register was not really enough. 
Rather than add another register, indirect addressing was chosen. 
At this point, one needs 12 bits for address, one for index tagging, and one to specify whether the address is direct or in- 
direct, or 14 bits for operand selection. Thirty-two orders 
was a 
tight minimum, so the minimum word 
length was 19 bits. Since this was consistent with 
five decimal place accuracy, it 
was tenta- tively chosen. 
It was decided, however, to design a structure basically suited to any length 
word. Shifting is necessary to multiply and divide and is required on two registers, yet shift registers for asynchronous operation are 
complex. Hence, it 
was decided to 
put the shift facility on the data transfer bus. By providing complementing here, subtraction 
could be generated. It was decided to use two-complement arithmetic, first because of the simplicity of the multiply-divide logic, and second because it avoids the whole negative zero question. 
The precise number of microsteps needed was determined by a trial microprogram. The machine was 
designed for up to 512 microsteps although only 
384 are now used. Eight bits were 
in 
Chapter 29 1 The design of a general-purpose microprogram-controlled computer with elementary structure 
343 I. I I OUTPUT DISTRIBUTOR ‚ I I a register, called 
J, and one 
was a flip-flop, TO, in the alteration 
unit, thus allowing fixed sequence with a 
one-bit micropro- 
grammed choice. This, incidentally, 
is the genesis of the name ﬁalteration unit.ﬂ J ARITHMETIC UNIT c- The SD-2 computer Figure 1 is a block diagram of the computer. There 
will be, pres- ently, a block-by-block description of the computer. The two boxes on the left were 
added to facilitate input and output. The output 
buffer holds 
20 words, and outputs all values in a 
4.8-msec cycle, 
thus providing 
for nearly continuous outputs. 
The output 
distributor is a selection 
system which allows 
the programmer to transmit the contents of the accumulator onto one 
of eight channels to control external 
devices. The ﬁinputsﬂ line 
represents up to 
32 channels which can be read 
into the accumu- lator. The numbers 8 and 32 are purely 
arbitrary; the upper limit of 32 is a microcode convenience only. The alteration 
unit, in addition to its decision making 
duties, has several 
other functions. It has a five bit counter, 
used for 
microsubroutines, which 
can be set to any value chosen or 
to any 
number on the arithmetic unit. The alteration unit 
can sense when 
it goes from all zeros to all ones. In addition, the flip-flops con- I I BUFFER I [ ORDER I I I I I I I I I I 1 I I TRAP SIGNAL I Fig. 1. Computer block diagram. - FROM MEMORY I ELSETERE p-™ d BUS ] Fig. 2. Arithmetic flow. trolling initial carry 
in the adder, end 
carry in shifting, and 
mem- ory read or write control are in this 
unit. Figure 2 is a block diagram of the arithmetic unit. 
Information may be put onto 
the b bus from any register, 
or from outside 
sources, such 
as inputs, or constants from the microprogram unit; thence to the 
shift unit, and 
finally to the 
d bus. From the d bus, it may be sent to other places, such as 
the output distributor, microprogram register, etc., 
or to an arithmetic register. Data and addressing between memory and the arithmetic 
unit have their own private channels, leaving 
the bus free during 
memory operation. The memory buffer 
and address register 
are a part of the arithmetic 
unit. Figure 3 is an expanded view of this unit. Capital letters 
stand for registers, small letters for logical 
entities. Registers A, B, C and E are simply storage registers, and are used as the Accumu- lator, B-line, Counter and Extension (least significant arithmetic) register. The Distributor, D, is the memory buffer, 
and is often used as working storage. Registers 
F and G are the inputs to the 
adder logic. The a logic is the algebraic sum of (F) + (G); e is a rather weird logic, (e = F + G, which is used in generating the extract order); 
f, which yields 
FG + FG, is used for 
the ﬁexclusiveﬂ or generation; c is the carry logic; 
g is a constant 
emitter, under 
microprogram control; and 
h is a set 
of gates used for input. As a number 
moves from b to d, one of five operations may be performed; uiz., normal, shift left one bit, shift right one 
bit, complement or shift left 5 bits. The last is used for 
automatic fill and in 
connection with the microprogram unit control. 
As an example, to add the number in 
the A and D registers, three microprogram steps would be needed. First, 
transfer A to G, D to F, and finally a to A; 12 psec would 
be required. 

344 Part 4 I The instruction-set processor level: special-function processors 
n . aL] MEMORY -I I 0 MEMORY Fig. 3. Arithmetic unit detail. 
Figure 4 is a diagram of the microprogram unit. The eight-bit J register, augmented by the TO flip-flop of the alteration unit, 
is decoded for up to 512 steps. Students of microprogramming will recognize the Wilkes model 
in its pure form [Wilkes and Stringer, 19531. The ﬁnextﬂ value 
of the microprogram register may be chosen in one 
of three ways. First, the value may be controlled by the microprogram itself. 
Second, five bits of the bus, corresponding 
to the 
order portion of the word, may be entered; the other three bits 
are set to zero. In this 
manner, the order decoding is accomplished. Third, all 
eight bits 
of the J register may be filled from the d bus. In practice, the order is shifted five bits to the 
left, pre- senting eight bits of the address to get the J register. In this 
manner, one may generate ﬁno addressﬂ commands. In principle, the programmer may start on any 
microstep which amuses him; in practice, 
only a limited number 
of these will yield no-address orders, 
the other steps being 
used for parts of add, subtract, order procure, etc. 
The author has no doubt, 
however, Section 3 1 Processors defined 
by a microprogram that someone will find a useful 
reason for popping into the middle of divide or some other command. There is no feature 
of a ma- chine, however pathological, which cannot be exploited by a programmer. The actual decoding of these nine bits is accomplished partly by logic, and partly by current switching 
of the clock pulse. A diode matrix 
is used to convert the microsteps into control signals. No more than 15 micro operations may be called out on a single 
step, including selection 
of the next microorder. When stepping the microregister, a ploy 
is used to reduce 
the number of diodes. Instead of specifying the next step, the micro- coder specifies the bits of J which he wishes to reverse. Instead of the minimum latency coding of earlier days, the microcoder of the SD-2 must do minimum diode coding. This 
is roughly anal- ogous to asking for a fast, 
efficient computer program containing a minimum of 1™s. The author, as well as 
others, has 
spent endless hours trying to devise a 
computer program to do 
such microcoding, 
with no 
results. One may note in 
passing that the man who wrote 
the micro- code, Tomo Hayata, has for several years specialized 
in advanced 
programming problems. Wilkes™ views,l 
that logical design will 
in the future be done by programmers, seem 
to be verified here. Because of the limited microarithmetic 
available here, micro- coding of the highest order is a must, since 
each microstep is 4 psec of time. For simple orders 
(e.g., extract), the processes of order procure, indexing (but not indirect 
addressing), operand procure 
and exe- cution can be compressed into the time for two memory cycles, Le., 40 psec. Each indirect 
reference adds another 
memory cycle ‚Private communication; Aug. 17, 1959. Fig. 4. Microprogram unit. 
Chapter 29 I The design 
of a general-purpose microprogram-controlled computer with elementary structure 
345 INPUT GATES ZERO DETECT d STORAGE FLIP-FLOPS OUTPUT e FLIP-FLOPS to this time. Only on 
multiply, divide, and 
shift does the ultra- simple structure begin to be 
expensive in time. 
If the temperature requirement 
were not 
imposed, the clock frequency could 
be doubled, materially improving 
the perform- ance of the machine on multicycle orders. Figure 5 is a block diagram of the alteration 
unit. It consists of gates which permit entry of conditions within 
the computer 
or the outside world, flip-flops used as working 
storage, flip-flops, including TO, to make its conclusions known 
to all 
and sundry, a five-bit tally register 
(I), a circuit 
to detect 
a zero 
on the d bus, and the trap logic. There are 
as many 
as 20 input gates, 9 storage flip-flops and 10 
output flip-flops, exclusive of TO. The 1 register can change its 
contents in one 
of two ways, viz., counting down by 
one, or by accepting an entry from the d bus. It may transmit intelligence in two 
ways, viz., to the b bus, or 
by notifying 
the input gate system that, should anyone 
care, it has just counted past zero. The zero detector signals the truth 
of the statement 
that d is identically zero. In practice, it checks only 
the lower digits, not the sign. This is related to the 
existence of the number -1 in a two-complement 
system, which 
is the system™s answer to the negative zero 
of a one™s complement logic. The trap logic is as 
follows: one of the output signals of the alteration unit 
signals whether or not the system is receiving trap signals; if it is not, the trap logic makes 
a note of callers. When the system is again accepting those signals, it transmits whether or not signals have been received, and 
resets its memory to zero. The timing is such that no trap signal will 
ever be lost. + TRAP LOGIC + LOGIC UNIT - --+ - + The lines going into the logic unit are actually two 
busses. Any logic source may 
read to either bus. The logic unit has four 
control wires from 
the microprogram unit, specifying which 
of the 16 Boolean functions of the two busses is to be put on the output bus. This value is then routed 
to the 
appropriate logic destination. The output flip-flops have inputs from the logic unit, and their 
outputs go to various control points 
in the machine. Three major points are: (1) establishing whether a memory cycle is read/restore or erase/write; (2) setting the initial carry 
in the adder; and 
(3) determining what value 
shall shift into the vacant spot 
on a left 
or right 
shift. The initial carry 
is used for more 
than simply adding one to 
a value; since 
the logic is two complement, 
but the 
one comple- 
ment one is transmitted on the bus, the initial carry 
is, in general, 
one during subtraction and zero during addition. Microprogram details Figure 6 gives circuit details 
of the microprogram decode 
system. The nine flip-flops used are broken into two 
groups, one of four, the other of five flip-flops. These are decoded into, respectively, 16 and 32 
wires. In each group, one and 
only one wire goes nega- tive. When the clock signal, 
of 2 pec width, is applied to the 
emitters of the first set of 16 gates, 
it is passed by 
the selected gating transistor. From 
the collector of this transistor, it is routed to the 
emitter of a set 
of 32 transistors; again, only one can pass current. Thus, the clock signal 
is routed to one of 16 x 32 x 512 lines. Diodes on the selected line 
then cause this signal to be 
routed to appropriate gates in the arithmetic 
or alteration unit. 
By appropriate placement of diodes, a microstep 
can operate 
a variety 
of gates, the number of which is limited by 
the current available. Some of the microcontrol wires return to the 
J register so that the microcoder may control the selection of the next microstep. This register 
is so designed that the actual 
change of state is inhibited until the 
clock goes 
negative. While each 
output of the decoding trees 
may go to 16 
bases, only one transistor 
of the 16 will have a 
signal on 
the emitter; 
thus only one must be driven. 
From an engineering point 
of view, the control of a computer is an elaborate timing 
system. A microprogram unit is thus a 
programmable timing generator. 
The gating transistor/diode de- 
coding system is but one of many ways 
to achieve this. Wilkes has 
observedl that, with the diode system, one has an Fig. 5. Alteration unit. 
IM. V. Wilkes, private communication; Aug. 17, 1959. 
346 Part 4 I The instruction-set processor level: special-function processors 
I 4 - SELECT n i SELECTED V Section 3 1 Processors defined 
by a microprogram 
Fig. 6. Details of the microdecode system. acute packaging problem. He and his co-workers have been led 
to consider the use of switch-core decoding 
[Wilkes et al., 1958al. Eachusl and his co-workers have evolved yet another 
switch- core system which does not depend on coincident current switch- ing. Order code Since the order code 
is only a small 
problem in the design of a microprogrammed machine (GOTT SEI DANKE), there is little need to dwell 
on it. There are 
several comments of design interest, however. We were unable, with this structure, 
to get the multiplication below five microsteps per iteration, 
nor the divide below six, thus costing respectively 
20 and 24 psec per bit dealt with. 
Moreover, division required some precalculations (overflow detect) and some 'Dr. Joseph Eachus 
of Minneapolis-Honeywell, private conversation; 
Sep- tember, 1959. postcalculation (obtaining 
a rounded quotient with 
a correct re- mainder) which further boosted its 
time. Because of the asynchronous nature of transfer, it is not possible to read into 
and out 
of a register simultaneously. 
Hence, shifting one register requires 
two steps, or 
8 psec per bit, and double-length shifting requires 
16 psec. This is painful. Because of the short words, four double-length orders 
were microprogrammed: add, subtract, clear 
and add, and store. These take a total of 60 psec to execute. A rich collection of branch orders was included. BRanch Un- conditionally, BRanch Negative, and BRanch Zero are self- explanatory. BRanch on 
B is the tally loop 
order which 
decreases (B) by one, and branches if it does not 
go negative. BR1, BR2, BR3, and BR4 are sense toggle branch; if the toggle is set, it is turned off and the 
program branches. These sense toggles are actually storage 
flip-flops T1, T2, T3, and T4 of the alteration unit. 
These may 
be set by other orders. T1 is also used as 
an overflow mark. 
Chapter 29 I The design 
of a general-purpose microprogram-controlled computer with elementary structure 
347 The machine has a ﬁdynamicﬂ idle. 
When it 
is halted, either 
externally or by order, this fact is observed by 
the microprogram, through the alteration unit, 
whereupon the microprogram goes into a tight loop, 
continuously asking, 
ﬁCan I go? Can I go? Can I go?. . . 
.ﬂ Two forms of halting are provided. 
In ﬁHalt 
and Display,ﬂ registers are presented; in the other halt, 
the console lights are left unaltered. A manual halt is equivalent to 
halt and display. For an addressed order, 
bit positions one through 
five are sent into the microprogram unit. During order procure, 
the micro- program examines 
bits zero and 
six for indirect addressing and index modification. 
A nonaddress order is recognized by the binary equivalent 
of 31 in the order bits; 
the microprogram unit 
causes the order 
word to shift left 
5 bits, and the 8 high bits of the ﬁaddressﬂ field enter the J register. Conclusion This paper is not intended to be 
an argument 
in favor of the general acceptance of the SD-2 structure as an 
ideal. Like all computers, the SD-2 is a state-of-the-art device, 
intended not only to meet the needs of the problems at hand, but 
also, more impor- tantly, to meet the side conditions of its use. In a vague analogy, 
the computer specification is like a partial differential equation. The logical designer must choose 
the boundary conditions 
and solve the problem, or at least approximate 
the solution. With today™s emphasis on system 
speed performance, 
some serious mental gear-shifting on the designer™s part is required in order to design a simple machine. 
It goes against 
the grain of instinct and experience. 
A posteriori, the SD-2 could have been made even simpler, particularly 
with respect to 
several peripheral areas not 
discussed in the paper. Several conclusions 
can be drawn here, however, 
The bus structure is easy to fabricate and maintain; this has been proven on the MILSMAC, a breadboard 
for the SD-2. It is a highly flexible structure, permitting 
wide variation in order 
code with no change in arithmetic unit. At the same time, the components are cascaded 
to a point where one 
has the absurd situation 
of fast-switching in a relatively 
slow computer. A designer of a bus-structured 
machine would do well to consider alternatives, such as 
multiple busses, accumulators, etc., 
to permit more parallelism when speed 
is important. The use of a special-purpose 
logic unit, such as the alteration 
unit of the SD-2, gives 
a freedom of design not possible with a 
special-purpose logic. At the same time, it uses more parts, is slow in handling multiple variable 
problems, and requires a great deal 
of control input. It appears to be a weapon 
of opportunity. The use of microprogramming is much the same as the general logic unit. Its 
flexibility and speed 
of design are unquestionable. Also, it uses more parts than a special-purpose control. 
There is no real substitute for a special-purpose 
design. The use of generalized elements in 
computer design can be justified only by the side conditions, 
never by the basic specification. 
Where simplicity and speed of design are major items, their use seems indicated. Wilkes once presented a 
paper on the best way to design a computer and launched 
the microprogramming notions. The author would like to comment 
that if ease and reliability 
of design are criteria, he was absolutely 
correct. References KampTGO; WilkM53a; WilkM58a 
Section 4 Processors based on 
a programming 
language Programming-language-based processors are described 
in Chap. 3 (page 73). 
Three examples are 
presented in this 
sec- tion. Two of the languages, FORTRAN and EULER, are algebraic 
languages operating on conventional data 
types, whereas IPL-VI is more like a conventional machine 
language operating on unconventional data 
types (i.e., list structures). A peculiar 
fea- ture of IPL-VI is 
its conception of data as program (as well as of program 
as data) and the multiprogramming 
organization to which this led. A command structure for 
complex information processing The IPL-VI processor (Chap. 30) discussed in Part 3, Sec. 5, is an outgrowth of the 
IPL series of programming languages by Newell, Shaw, 
and Simon. The paper seriously 
treats both 
the language and the merits of 
casting a language in a hardware processor. IPL-VI was never 
implemented in hardware. (A partial IPL-V processor for the 
CDC 3600 was built at the 
Argonne National Laboratory.) A hardware processor for IPL-VI in the third generation would undoubtedly exist as an interpreter in a microprogrammed processor. System design of a FORTRAN machine 
This paper 
(Chap. 31) presents a way to map 
a software pro- gram into hardware. The machine™s passes (or modes) 
corre- spond to activities one 
would see when compiling, loading, and executing a FORTRAN program. BCD format is used for the arithmetic. The symbol 
table is simply organized 
and, therefore, has 
to be searched. A more serious approach 
for the 
actual implementation of such a 
machine might follow the lines of EULER (Chap. 32). A microprogrammed implementation of EULER on IBM System 360IModel 30 This very clearly written paper describes a processor 
to imple- ment an ALGOL-like language 
[Wirth and Weber, 19661. An earlier processor was proposed to directly execute ALGOL [Anderson, 19611. It is implemented using 
the Model 30 IBM System/360 P.microprogrammed. We include the paper both because it describes the Model 30 and because of EULER. The P.language operates 
like a conventional compiler and operating system. The description presents 
clearly the process of compiling before execution. 
The microprogramming aspects of the Model 30 are typical of other IBM 
System/360 models. The IBM approach to a P.microprogrammed is significantly different 
from that 
in Kampe™s SD-2 (Chap. 29). In the 360 a microprogram instruc- tion is encoded in a long word (60 to 100 bits, depending on the model) with a number of microcoded operations 
which can be selected 
in parallel. The 
SD-2 uses a short word, and only one operation 
is encoded in a single instruction. 348 
Chapter 30 A command structure for complex information processing1 1. C. Shaw / A. Newell / H. A. Simon / T. 0. Ellis The general-purpose digital 
computer, by virtue of its large ca- 
pacity and general-purpose nature, has opened the possibility of research into the nature 
of complex mechanisms per se. The chal- lenge is obvious: humans carry out information processing of a complexity that is truly baffling. Given the urge to understand either how humans 
do it, or alternatively, what kinds of mecha- nisms might accomplish the same tasks, the computer is turned to as a basic 
research tool. The varieties of complex information processing will 
be understood when they can 
be synthesized: when mechanisms can be created that perform the same processes. The last few years have seen a number of attempts at synthesis of complex processes.  these have included 
programs to discover proofs for 
theorems [Newell 
et al., 1956, 1957b1, programs to synthesize music [Brooks et al., 1957b], programs to play chess 
[Bernstein et al., 1958; Kister 
et al., 19571, and programs to simulate the reasoning of particular humans [Newell et al., 19581. The feasi- bility of synthesizing complex processes hinges 
on the feasibility of writing programs of the complexity needed to specify these processes for a 
computer. Hence, 
a limit is imposed by the limit of complexity that the human programmer can handle. 
The measure of this complexity 
is not absolute, for it depends 
on the programming language he uses. The more powerful the language, the greater will be the 
complexity of the programs he can write. 
The authors™ work has sought 
to increase the upper limit of com- plexity of the processes specified 
by developing a series 
of lan- guages, called information 
processing languages (IPL™s), that re- duce significantly the demands made upon 
the programmer in his communication with the computer. Thus, 
the IPL™s represent a series of attempts to construct sufficiently powerful languages to permit the programming of the kinds of complex processes previ- ously mentioned. The IPL™s designed so far have been 
realized interpretively on current computers 
[Newell and Shaw, 1957al. Alternatively, of course, any such language 
can be viewed as a set of specifications for a general-purpose computer. An IPL can 
be implemented far more expeditiously in a 
computer designed to handle it 
than by interpretation in a 
computer designed with a quite different com- mand structure. 
The mismatch between the IPL™s designed and current computers 
is appreciable: 150-machine cycles are needed to do 
what one feels should take only 2 or 3 machine cycles. (It will become apparent that 
the difficulty would not be removed by ﬁcompilingﬂ instead 
of ﬁinterpreting,ﬂ to resurrect a set of well-worn distinctions. The operations that are mismatched to current computers 
must go on 
during execution of the program, and hence cannot 
be compiled out.) The purpose of this paper is to consider an IPL computer, 
that is, a computer constructed 
so that its machine language is an information processing language. This 
will be called language 
IPL-VI, for it is the sixth in the series of IPL™s that have been 
designed. This 
version has not been 
realized interpretively, 
but has resulted 
from considering hardware requirements in the light of programming experience with the 
previous languages. 
Some limitations must be placed on the investigation. This 
paper will be concerned only with the central computer, 
the command structure, 
the form of the machine operations, and the 
general arrangements of the central hardware. 
It will neglect completely input-output and secondary storage 
systems. This does not mean these are unimportant or that they present only simple problems. The problem of secondary storage 
is difficult enough for current computing 
systems; it is exceedingly difficult for IPL systems, since in such systems initial memory is not organized 
in neat block-like packages for ease of shipment to the 
secondary store. Nor is 
it the 
case that one would place an order for the IPL 
computer about to 
be described without further 
experience with it. Results are not entirely predictable. 
IPL™s are sufficiently differ- ent from current computer 
languages that their utility can be evaluated only after much 
programming. Moreover, since 
IPL™s are designed to 
specify large complicated programs, the utility of the linguistic devices 
incorporated in them cannot 
be ascer- tained from simple examples. One more caution is needed to provide a proper setting 
for ‚Proc. WJCC, pp. 119-128, 1958 
349 
350 Part 4 1 The instruction-set processor 
level: special-function processors 
this paper. 
Most of the computing world is still concerned with essentially numerical processes, either because the problems themselves are numerical or because nonnumerical problems have 
been appropriately arithmetized. 
The kinds of problems that the 
authors have been concerned 
with are 
essentially nonnumerical, and they have tried to cope 
with them without 
resort to arithmetic 
models. Hence the IPL™s have not been 
designed with a view 
to carrying out arithmetic 
with great efficiency. Fundamental goals and devices 
The basic aim, then, 
is to construct 
a powerful programming language for the class of problems concerned. Given the amount and kind of output desired from the computer, a reduction in 
the size and complexity of the specification (the program) that has to be written in order 
to secure this 
output is desired. The goal is to reduce 
programming effort. This is not the same as reducing 
the computing effort required to produce the desired output from the specification. Programming feasibility must take precedence over computing economics; since 
it is not yet 
known how to write a program that will enable a computer to teach 
itself to play chess, it is premature to ask whether it 
would take such a computer one hour 
or one hundred 
hours to make a move. 
This is not meant 
as an apology, but as support for the contention that, in seeking to write 
programs for very large 
and complicated tasks, the overriding initial concerns must be to attain enough flexibility, abbreviation, and automation of the underlying computing proc- esses to make programming 
feasible. And these concerns 
have to 
do with 
the power of the programming language rather than the 
efficiency of the system that executes the program. In the next section a straightforward description 
of an IPL computer is begun. To put the details in a proper setting, 
the remainder of this section will be devoted to the basic devices 
that IPL-VI uses to achieve 
a measure of power and flexibility. These devices include: organization 
of memory into list structure, provision for 
breakouts, identity of data with program, two-stage interpretation, invariance of program during execution, provision for responsibility assignments, 
and centralized signalling of test results. List structure The most fundamental and characteristic feature 
of the IPL™s is that they organize memory into list structures whose arrangement is independent of the actual physical geometry of the memory cells and which undergo 
continual change 
as computation proceeds. In all 
computing systems the topology of memory, the character- Section 4 I Processors based 
on a programming language 
istics of hardware and program that determine what 
memory cells can be regarded as ﬁnext toﬂ a given cell, plays a 
fundamental role in the organization of the information processing. This is obviously true for serial memories like tape; it is equally true from random access memories. 
In random access memories the topo- logical structure is derived from the possibility of performing arithmetic operations on the memory addresses that make use of the numerical relations 
among these addresses. Thus, the cell with address 1435 is next to cell 1436 in the specific sense that the second can be reached from the first by 
adding one to 
the number in a 
counter. In standard computers 
use is made of the static topology based on memory addresses to facilitate programming and computation. Index registers and relative addressing schemes, 
for example, make use of program arithmetic and depend 
for their efficacy upon an orderly matching of the arrangement of information in memory with the 
topology of the addressing system. When memory is 
organized in a list structure, the relation between information storage 
and topology is reversed. The topol- ogy of memory is continually modified to adapt to the changing needs of organization of memory content. No arithmetic operations on memory addresses 
are permitted; the topology is built on a 
single, asymmetric, modifiable, ordinal relation 
between pairs of memory cells which is 
called adjacency. The system contains processes that make use of the adjacency relations 
in searching memory, and processes that change these relations 
at will inex- pensively in the course of processing. A list structure can 
be established in computer memory by 
associating with each word in memory an address that determines what word is adjacent to it, as far as all the operations of the computer are 
concerned. Memory space of an additional address 
associated with 
each word is given up, so that the 
adjacency relation can be changed as quickly as a 
word in memory can be changed. Having paid this price, however, 
many of the other 
basic features of IPLs are obtained 
almost without cost: unlimited hierarchies of subroutines; recursive 
definition of processes; vari- 
able numbers of operands for processes; 
and unlimited complexity of data structure, capable 
of being created and 
modified to any 
extent at execution time. Breakouts Languages require grammar-fixed structural features so that they can be interpreted. Grammar 
imposes constraints on what can be 
said, or said simply, 
in a language. However, the constraints created by fixed grammatical format can be 
alleviated at the 
cost of intro- ducing an 
additional stage 
of processing by devices that allow one 
Chapter 30 I A command structure for complex information processing 
351 to ﬁbreak outﬂ of the format and to 
use more general modes of specification than the format permits. 
Devices for 
breakouts ex- change processing time for flexibility. Several devices 
achieve this in IPL-VI. Each is associated with some part of the format. As an illustrative example, 1PL-VI has a 
single-address format. Without breakout devices, this format would permit an informa- 
tion process to operate on only 
a single operand as input, and would permit the operand of a process to be specified only by 
giving its address. Both 
of these limitations are removed: the first by using a special communication 
list to store operands, 
the second by allowing the address for 
an operand to refer either to the operand itself or to any process that will determine the operand. The latter 
device, which 
allows broad freedom in the method of specifying an operand, 
illustrates another 
important facet of the flexibility problem. Breakouts are of great importance in re- 
ducing the burden of planning that is imposed on the programmer. It is certainly possible, in principle, to 
anticipate the need for particular operands at particular stages of processing, and to pro- 
vide the operands in such a way that their addresses are known to the programmer at the appropriate 
times. This 
is the usual way 
in which machine coding 
is done. However, such plans 
are not obtained without 
cost; they must be created by the programmer. Indeed, in writing 
complex programs, 
the creation of the plan of computation is the most difficult part of the job; it constitutes the task of ﬁprogrammingﬂ that is sometimes distinguished from 
the more routine ﬁcoding.ﬂ Thus, 
devices that exchange computing time for a reduction 
in the amount of planning required 
of the programmer provide significant increases 
in the flexibility and power of the language. Identity of data with programs In current 
computers, the data 
are considered 
ﬁinert.ﬂ They are 
symbols to be operated upon by 
the program. All ﬁstructureﬂ of the data is initially developed in 
the programmer™s head and 
encoded implicitly 
into the 
programs that work with the data. The 
structure is embodied in the conventions that determine what 
bits the processes will 
decode, etc. An alternative approach 
is to make the data 
ﬁactive.ﬂ All words in the computer 
will have the instruction format: there will be ﬁdataﬂ programs, and the data 
will be obtained by executing these programs. Some 
of the advantages of this alternative are 
obvious: the full range of methods of specification available for programs 
is also available for data; a list of data, for example, may 
be speci- fied by a list of processes that determine the data. 
Since data are only desired ﬁon 
commandﬂ by the processing programs, 
this approach leads to a computer that, although still serial 
in its control, contains 
at any given moment 
a large number 
of parallel active programs, frozen 
in the midst of operation and waiting 
until called upon 
to produce the next operation or piece of data. This identity of data with 
program can be attained only if the proc- essing programs 
require for their operation no information 
about the structure of the data programs, only information 
about how to receive the data from them. Two-stage interpretation To identify the operand of an IPL-VI instruction, a designating 
operation operates 
on the address part of the instruction to pro- duce the actual operand. 
Thus, depending 
on what designating operation is specified, the address part may itself be the operand, 
may provide 
the address of the operand, or may 
stand in a 
less direct relation to 
the operand. The designating operation 
may even 
delegate the actual specification of the operand to 
another desig- nating operation. Invariance of program during execution 
In order to carry out generalized recursions, it is necessary to provide for the storage of indefinite amounts of variable informa- tion necessary for 
the operation of such routines. In 1PL-VI all the variable information is stored externally 
to the associated routine, so that the 
routine remains unmodified 
during execution. The name of a routine can 
appear in the definition of the routine itself without causing 
difficulty at execution time. Responsibility assignments The automatic handling 
of such processes as erasing a 
list, or searching through a 
list requires some scheme for keeping track of what part of the list has been processed, and what part 
has not. For example, in erasing a program containing a 
local sub- 
routine that appears more than once within 
the program, care must be taken to erase 
the subroutine once and 
only once. This is accomplished by a 
system for assigning responsibility 
for the parts of the list. In general, the responsibility code 
in IPL-VI handles these 
matters without any 
explicit attention from the programmer, except in 
those few 
situations where 
the issue of responsibility is the central problem. Centralized signalling of test results 
The structure of the language is simplified by having 
all conditional processes set a switch to 
symbolize their output 
instead of pro- ducing an immediate conditional transfer 
of control. Then, a few specialized processes are defined that transfer control on the basis of the switch setting. 
By symbolizing and retaining the conditional 
352 Part 4 I The instruction-set processor level: 
special-function processors 
information, the actual transfer can be postponed to the most convenient point in the processing. The flexibility obtained by this device proves especially useful in dealing with the transmission of conditional information 
from subroutines to the routines that call upon them. General organization 
of the machine The machine that is described can profitably be viewed as a 
ﬁcontrol computer.ﬂ 
It consists of a single 
control unit with 
access to a large random-access memory. This memory should contain lo5 words or more. If less than lo4 words are available in the primary memory, there will probably be too frequent occasions for transfer of information between primary and secondary storage 
to make the system profitable. 
The operation of the computer is entirely nonarithmetic, there being no arithmetic unit. 
Since arithmetic processes are not used as the basis of control, as they are in standard computers, 
such a unit is inessential, although 
it would be highly desirable for the computer to have access to one if it is to be given arithmetic tasks. The computer 
is perfectly capable of proving theorems in logic or playing 
chess without an arithmetic adjunct. Memory The memory consists 
of cells containing words of fixed length. Each word is divided into two parts, a symbol 
and a link. 
The entire memory is organized into a list structure in 
the following way. The link is an address; if the link of a word a is the address of word b, then b is adjacent to 
a. That is, the link of a word in a 
simple list is the address of the next word in the list. The symbol part of a word may 
also contain an address, and this may be the address of the first word of another list. As indi- cated earlier, the entire topology of the memory is determined by the links and by addresses located in 
the symbol parts of words. The links permit the creation of simple lists of symbols; the links and symbol parts together, 
the creation of branching list structures. The topology of memory is modified by changing addresses in links and symbol parts, thereby changing 
adjacency relations 
among words. The modification of link addresses 
is handled directly by various list processes without the attention of the programmer. Hence, the memory can be viewed as consisting of symbol occurrences connected together 
by mechanisms or struc- ture whose character 
need not be specified. 
The basic unit of organization is the list, a set of words linked together in 
a particular order 
by means 
of their link parts, in the Section 4 1 Processors based on 
a programming language 
way previously explained. The address of the first word in the sequence is the name of the list. A special terminating symbol T, whose link is irrelevant, is in the last word on every list. A simple list is illustrated in Fig. 1; its name is L,,,, and it contains two symbols, S, and S,. The symbols in a 
list may 
themselves designate 
the names of other lists. (The symbols themselves have a special format, 
so that they are not 
names of lists but designate the names in a 
manner that will be described.) Thus, 
a list may be a list of lists, and each of its sublists may be a list of lists. An example of a list structure is shown in Fig. 2. The name of the list structure is the name of the main list, 
L,,,,. L,,, contains two sublists, L,,, and L,,,, plus an item of information, l,, that is not a name of a list. L,,, in its 
turn consists of item I, plus another sublist, L,,,, while L,,, contains just information, and is not broken out further into sublists. Each of these lists terminates in a 
word that holds the symbol T. Available space list A list uses 
a certain number 
of cells from 
memory. Which cells it uses is 
unimportant as long as the right linkages are set up. In executing programs that continually create new lists and destroy old ones, two requirements arise. When creating a list, cells in memory must 
be found that are 
not otherwise occupied and so are available for the new list. Conversely, when a list is destroyed (when it is no longer needed in the system) its cells become avail- able for other 
uses, but something must 
be done to 
gain access to these available 
cells when they are needed. 
The device used 
to accomplish these 
two logistic functions is the available space list. All cells that are 
available are linked together into the 
single long list. Whenever cells are needed, they 
are taken from the front of this available 
space list: whenever 
cells are made available, they are inserted on the front of the available space list just behind the fixed register that holds the link to the first available space. The operations of taking cells from 
the avail- able space 
list and returning cells to the available space list in- volve, in each case, only changes 
of addresses in a pair of links. s2 T Fig. 1. A simple list. 
Chapter 30 I A command structure for 
complex information processing 353 Communication list Available space list CIA list ~ CClA list list 
. Camporator Memory Fig. 2. A list structure. Organization of central unit 
Figure 3 shows the special registers 
of the machine and the 
main information transfer paths. 
Four addressable registers accomplish 
fixed functions. These are 
shown as 
part of the main memory, 
but would be fast access registers. Lo L, L2 L3 Communication list, Lo. The system allows 
the introduction of unlimited numbers of processes with variable numbers of inputs and outputs. The communication of inputs and outputs among processes is centralized in 
a communication list with known name, Lo. All subroutines find their inputs on 
this list, 
and all subroutines put their outputs on the same list. 
Available space list, L,. All cells not currently being 
used are on the available space list: cells can be obtained from it when needed 
and are returned to it when they 
are no longer being used. List of current instruction addresses 
(CIA), L,. At any given moment in 
working sequentially through a program, there will be a whole hierarchy of instructions that are 
in process or interpreta- tion, but whose interpretation has not been completed. These will include the instruction currently being interpreted, 
the routine to which this instruction 
belongs, the superroutine to which this routine belongs, and so on. The CIA list 
is the list of addresses of this hierarchy of routines. The first symbol on the list gives the address of the instruction currently being interpreted; 
the second symbol gives the address of the current instruction in the next higher routine, etc. 
In this system 
it proves to be preferable to keep track of the current instruction being interpreted, 
rather than the next one. 
List of current CIA lists, L,. The control sequence 
is complicated in this computer by the existence of numerous programs which 
become active when 
called upon, and whose processing 
may be interspersed among other processes. Hence, a single CIA list does not suffice; there must be such a list for each program that has not been completely 
executed. Therefore, 
it is necessary also 
to have a list that gives the names of the CIA lists that are 
active. This list is L,. Besides these special addressable 
registers, three nonaddress- able registers are needed to handle the transfers of information. Two of these, R, and R,, are each 
a full 
word in length, and transfer information 
to and from memory. Register R, receives input from memory; R, transmits output to memory. The com- parator that provides the information for all tests takes 
as its input for comparison the symbols in R, and R,. This pair of registers also performs a 
secondary function 
in regenerating 
words in memory: the basic ﬁread™ operation from memory 
is assumed to be destructive; a nondestructive ﬁreadﬂ merely shunts the word received from memory in E, to R, and back, by means 
of a ﬁwriteﬂ operation, to the 
same memory cell. A register, A, which holds a single address, 
controls references 
to the 
memory, that is, specifies 
the memory address at which a ﬁreadﬂ or ﬁwriteﬂ operation is to be performed. References 
to the 
four addressable 
registers, Lo to L,, can be made either 
by A or directly 
by the control unit itself; other memory cells can be referred to only by A. Finally, the computer has a single 
bit register which is used to encode and retain test 
results. Fig. 3. Machine information transfer paths. 

354 Part 4 1 The instruction-set processor level: special-function processors 
The environment How input-output, secondary storage, 
and high-speed arithmetic could be handled with 
such a machine will be indicated. The machine manipulates symbols: it can construct 
complex structures, search them, and tell when two 
symbol occurrences are identical. 
These processes are sufficient to play chess, prove theorems, or 
do most other tasks. The symbols it manipulates 
are not ﬁcoded™; 
they simply form a set of arbitrary distinguishable entities, 
like a large alphabet. This computer can manipulate 
things outside itself if hardware is provided to make some of its symbols refer to outside objects, and other symbols refer 
to operations on these objects. It could do high-speed arithmetic, for example, if some of its symbols were names of words in memory encoded as numbers as in the usual computer fashion, and others were names of the arithmetic opera- tions. In such 
a scheme these words would not be in the IPL language; they would have some format of their own, either fixed or floating-point, binary or decimal. 
They might occupy 
the same physical memory as that used by the control computer. Thus 
the IPL language would deal with 
numbers at one remove, by their names, in much the same manner as the programmer deals with numbers in a 
current computer. 
A similar approach can 
be used for manipulating printers, input devices, etc. The word and its interpretation All words in 
IPL have the same format, shown in Fig. 4. The word a is divided into two major parts: the symbol part, bcde, and the 
link, f. It has been observed that the programmer never deals explicitly with the link, although it will be frequently represented 
explicitly to show how manipulations are being accomplished. 
Since the same symbol can appear in many words, the symbol occurrence of the symbol in the word a will be discussed. A symbol occurrence consists of an operation, 
b, a designation a Location of word b Operation code c Designation code d Address field e Responsibility code f Link to next word Fig. 4. IPL word format. Section 4 I Processors based on a 
programming language operation, c, an address, d, and a responsibility 
code, e. The opera- tion, b, takes as operand a single symbol 
occurrence, which is called s. The operand, s, is determined by applying the designation operation, c, to the address, d. Thus, the process determined by a word is carried out in two 
stages: the first-stage operation (the designation operation) determines an operand 
that becomes the input to the second-stage operation. 
The responsibility 
bit The single bit, e, is an essential piece of auxiliary information. The address, d, in a symbol 
may be the address of another list structure. The responsibility code in 
a symbol 
occurrence indicates whether this occurrence is ﬁresponsibleﬂ for the structure designated by d. If the same address, d, occurs in more 
than one word, only 
one of these will indicate responsibility for 
d. The main function of the responsibility code is to provide a way of searching a branching list structure so that every part of the structure will, sooner 
or later, be reached, and so that no part will be reached twice. The need for a definite assignment of responsibility for 
the various parts of the structure can 
be seen by considering the process of erasing a list. Suppose that a list has a sublist 
that appears twice on 
it, but that does not appear anywhere else in memory. When the 
list is erased, the sublist must 
be erased if it is not to be lost forever, and the 
space it 
occupies with it. 
However, after the sublist has been erased when an 
occur- rence of its name is encountered on 
the other list, it is imperative that it not 
be erased again 
on the second encounter. Since the words used 
by the sublist would have 
been returned 
to the avail- able space 
list prior to the second encounter, only chaos 
could result from erasing it again. The responsibility code would indicate responsibility, in erasing, for one and 
only one 
of the two 
occur- rences of the name of the sublist. Detailed consideration of systems of responsibility is inappro- priate in this paper. It is believed that an adequate system can be constructed with 
a single bit, although a system 
that will handle merging lists 
also requires a responsibility 
bit on the link f. The responsibility code 
is essentially automatic. The programmer does not need to 
worry about it 
except in those cases 
where he 
is explicitly seeking 
to modify structure. Interpretation cycle 
A routine is a list of words, that is, a list of instructions. Its name 
is the address of the first word used in 
the list. The interpretation of a program proceeds according to a very simple cycle. 
An instruc- tion is fetched to the control unit. 
The designation operation 
is decoded and executed, placing 
the location of s in the address 
Chapter 30 1 A command structure for complex information processing 
355 register, A, of Fig. 3. Then operation b is decoded and performed on s. The cycle is then repeated using f to fetch the 
next instruc- tion. The operation codes The simple interpretation cycle previously described provides none of the powerful linguistic features 
that were outlined 
at the beginning of the paper: hierarchies of subroutines, data programs, breakouts, etc. These features 
are obtained through particular b and c operations that modify the sequence of control. The opera- tion codes will 
be explained under 
the following headings: 
the designation code, sequence-controlling operations, 
save and delete operations, communication 
list operations, signal operations, list operations, and 
other operations. The designation code The designation operation, c, operates on the address, d, to desig- nate a symbol occurrence, s, that will serve 
as input, or operand, for the operation b. The designation operation places the address of the designated symbol, s, in the address register. The designation codes 
proposed, based 
on their usefulness in coding with the 
IPL™s, are shown in Appendix 1. The first four, c = 0, 1, 2, or 3, allow four degrees 
of directness of reference. They are 
usable when the programmer knows in advance where 
the symbol, s, is located. To illustrate their definition, consider an instruction a,, with parts 
b,, e,, d,, and e,, which can collec- tively be called 
s,. The address part, d,, of this instruction may be the address of another instruction d, = a,; the address part, d,, of a, may be the address of a,, etc. The code c, = 1 means that s is the symbol whose address 
is d,, that is, the symbol s,. In this case the designating operation 
puts d,, the address of s,, in the address register. 
The code c, = 2 means that s is s,; hence, the operation puts d,, the address of s3, in the address register. The code c, = 3 puts d,, the address of s4, in the address register. Finally, c, = 0 designates as 
s the actual symbol in a, itself; hence, this means that b is to operate on s,. Therefore, this operation places 
u1 in the address register. The remaining two designation operations, c = 4 and 5, intro- duce another kind of flexibility, for they allow the programmer to delegate the designation of s to other parts of the program. When c1 = 4, the task of designating s is delegated to the symbol of the word d, = u2. In this case, s is found by applying the designation operation, c2 of word a,, to the address, d,, of word u2. An operation of this kind permits the programmer to be 
unaware of the way in which the data are 
arranged structurally 
in memory. Notice 
that the 
operation permits 
an indefinite 
number of stages of delegation, since 
if c, = 4, there will be a further delegation of the designation operation to e, and d, in word a,. The last designation operation, 
c = 5, provides both for dele- gation and a breakout. 
With c, = 5, d, is interpreted as a process that determines s. Any program whatsoever, having its initial 
instruction at d,, can then be written to specify s. When this program has 
been executed, 
an s will have been designated, 
and the interpretation will continue by reverting 
to the 
original cycle, that is, by applying b, to the s that was just designated. It is necessary to provide a convention 
for communicating the result of process d, to the interpreter. 
The convention used is that d, will leave 
the location of s in L,,, the standard communication cell. 
Sequence-controlling operations Appendix 2 lists the 35 b operations. The first 12 of these are the 
ones that affect the sequence of control. They 
accomplish 5 quite different functions: executing a 
process (b = 1, lo), executing variable instructions (b = 2), transferring control within a routine 
(b = 3, 4, 5), transferring control 
among parallel program struc- tures (b = 0, 6, 7, 8, 9,), and, finally, stopping the computer A routine is a list of instructions; its name 
is the address of the first word in the list. To execute a routine, 
its name (Le., its 
name becomes the s of the previous section) is designated and 
to it is applied the operation b = 1, ﬁexecute s.ﬂ The interpreter must keep track of the location of the instruction that is being executed in 
the current routine and return 
to that location after 
completing the execution of the instruction (which, in general, 
is a subroutine). 
All lists end in a 
word containing b = 10, which terminates the list and returns control 
to the 
higher routine in 
which the subroutine just completed occurred. 
(The symbol T is really any symbol with b = 10.) Figure 5 provides a simple illustration of the relations between routines and 
their subroutines. In the 
course of executing the routine L,, (i.e., the instructions that constitute list L,,), an in- struction, (1,0, L,,), is encountered that is interpreted as ﬁexecute L,,.ﬂ In the course of executing L,,,, an instruction is encountered that is interpreted as ﬁexecute L,,.ﬂ Assuming that L,,, contains no subroutines, its instructions 
will be executed 
in order until 
the terminate instruction is reached. Because of the 10 in its 
b part, this instruction returns control to 
the instruction that follows L,, in Lz0. When the final word in L,, is reached, the operation code 
the instruction following L,,,. (Only the b part, b = 10, of the terminal word in a routine 
is used in the interpretation; the 
c and (b = 11). 10 in its b part returns control to Ll0; which then continues 
with 
356 Part 4 1 The instruction-set processor level: special-function processors 
L10 Fig. 5. A simple subroutine hierarchy. 
d parts are 
irrelevant.) This 
is a standard subroutine linkage, 
but with all the sequence control centralized. 
The operation code b = 2, ﬁinterpret 
s,ﬂ delegates the inter- 
pretation to the word s. The effect of an instruction containing 
b = 2 is exactly the same as if the instruction contained, instead, 
the symbol, s, that is designated by its c and d parts. One can 
think of the instruction with 
b = 2 as a variable 
whose value 
is s. Thus, a routine can 
be altered by modifying the symbol occur- rence s, without any 
modification whatsoever 
in the words belong- 
ing to the routine itself. The three 
operations, b = 3, 4, and 5, are standard transfer operations. The first is an unconditional transfer; 
the two others 
transfer conditionally 
on the signal bit. As mentioned earlier, 
all binary conditional 
processes set the signal either ﬁonﬂ or ﬁoff.ﬂ In order to describe operations 
b = 0, 6, 7, 8, 9 the concept of program structure must be defined. A program structure is a rou- tine together with all its subroutines and designation 
processes. Such a structure corresponds to a 
single, although perhaps 
com- plex, process. 
The computer 
is capable of holding, at a given time, any number 
of independent program structures, and can 
interrupt any one 
of these processes, from 
time to time, in order to execute 
one of the others. All of these structures are 
coordinate, or parallel, and the operations h = 0, 6, 7, 8, 9, are used to transfer control, 
perhaps conditionally, 
from the one that is currently active 
to a new one or to the previously active one. 
In this sense, the com- puter being described may be viewed as a serial control, parallel 
program machine. The execution of a particular routine in 
program structure A will be used as an example. 
Operation b = 6 will transfer 
control to an independent program structure determined 
by s; call it B. Section 4 I Processors based 
on a programming language 
The machine will 
then begin to execute B. When it encounters 
a ﬁstop 
interpretationﬂ operation (b = 0) in 
B, control will be returned to the program structure, A, that was previously active. But the ﬁstop interpretationﬂ operation, 
unlike the ordinary ter- mination, b = 10, does not mark the end 
of program structure B. At any later point in 
the execution of A, control may again 
be transferred to B, in which case execution 
of the latter program will be resumed from 
the point where 
it was interrupted by the earlier ﬁstop 
interpretationﬂ command. The operation that ac- complishes the second transfer 
of control from A to B is h = 7, ﬁcontinue parallel program s.ﬂ Thus, b = 0 is really an 
ﬁinterruptﬂ operation, which returns control to 
the previous structure, but 
leaves the structure it interrupts in condition 
to continue at a later point. There can 
be large numbers 
of independent program struc- tures all ﬁopen for businessﬂ 
at once, with a 
single control passing from one to the 
other, determining 
which has access 
to the proc- essing facilities, and gradually executing all of them. Operations 
b = 8 and 9 simply allow 
the interruption to be 
conditional on the test switch. 
Notice that the 
passage of control from one structure to another is entirely decentralized; it depends upon 
the occurrence of the appropriate b operations in the program structure that has control. When control is transferred to a parallel 
program structure, either of two outcomes is possible. Either a ﬁstop 
interpretationﬂ instruction is reached in 
the structure to which control has been transferred, or execution 
of that structure is completed and 
a termination reached. In either 
case, control is returned to the program structure that had it previously, together with 
informa- tion as to whether it was returned by interruption or by 
termina- tion. Thus, b = 0 turns the signal bit on when it returns control; 
b = 10 in the topmost routine 
of a structure turns 
the signal off. The operation, b = 11, simply halts. Processing continues from the location where 
it halted upon receipt of an external signal, ﬁgo.ﬂ Save and delete operations The two 
operations, b = 12 and 13, are sufficiently fundamental to warrant extended treatment. 
For example, consider a word, 
L,,,,, that contains the symbol I,: Location Symbol Link LlOO 11 t The link of L,,,, t, indicates that the 
next word holds 
the termination Operation, b = 10. The ﬁsaveﬂ operation (b = 12) 
Chapter 30 I A command structure 
for complex information processing 357 provides a copy of I, in such a way that I, can later 
be recalled, 
even if in the meantime the 
symbol in Lloo has been changed. 
After the ﬁsaveﬂ operation has been performed 
on s = L,,,, the result is: Location Symhol 
Link ~~ LlOO. ........... 11.. .............. Lzoo Lpoo., ............... .I1.. ............... t A new cell, which 
happened to be L,,,, was obtained during 
the ﬁsaveﬂ operation 
from the available space list, 
L,, and a copy of I, was put in it. The symbol in L,,, can now be changed without losing I, irretrievably. Suppose a different 
symbol is copied, for example, 12, into L,,,. Then: Location Symbol Link LlOO. ............... .I2 .................. L2oo Lpoo.. ................ .11.. ................ t Although I, has been replaced 
in L,,,, I, can be recovered by performing the ﬁdeleteﬂ operation, 
b = 13. Before the ﬁdeleteﬂ operation is explained, it will be instructive 
to show what happens 
when the ﬁsaveﬂ operation on L1,, is interated. If it is executed again, it will make 
a copy 
of I,. Therefore: Location Symbol 
Link ~ ~ LlOO., ............. .Ip.. ................. L300 L3oo 12 Loo Lzo o..... ........... -11. .................. t ..... ... .............. Notice that the cell L,,,,, in which the copy of symbol I, is retained, was not affected 
at all by this second ﬁsaveﬂ 
operation. Only the top cell in 
the list and the new cell from the available space list are involved in the transaction of saving. The same process is performed no matter how long the list that trails out below L,,,; thus, the save operation can be 
applied as many times 
as desired with 
constant processing time. The ﬁdeleteﬂ 
operation, b = 13, applied to the 
symbol I, in L,,,, will now 
be illustrated. 
This operation puts the symbol and link of the second word 
in the list, L,,,, into the 
first cell, L,,,, and puts L,,, back on the available space 
list, with the following result: Location Symbol Link .................. ............... L*OO. 12.. Lzoo Lzoo. 11.. t .............. ................. The result is the exact situation obtained 
before the last ﬁsaveﬂ was performed. In the 
description of the ﬁdeleteﬂ operation up to this point, only the changes it makes in the ﬁpush-downﬂ list, in this case L,,,, have been considered. 
The operation does more than this, however; ﬁdelete sﬂ also erases 
all structures for which 
the symbol s (II and I, in the examples) is responsible. When a copy of a symbol is made, e.g., the operation that initially replaced 
I, by I, in L,,,, the copy is not assigned responsibility for 
the symbol (e = 0 was set 
in the COPY). Thus, no additional erasing would be required in the particular ﬁdeleteﬂ 
operation illustrated. 
If, on the other hand, 
the I, that was moved into Lloo had been 
respon- sible for 
the structure 
that could be reached through it (if it were 
the name of a list, for example), 
then a second 
ﬁdeleteﬂ operation, putting I, back into L,,,, would also erase 
that list and put all its cells back on the available space 
list. Thus ﬁdeleteﬂ is also equivalent to ﬁeraseﬂ a 
list structure. Communication list operations In describing a 
process as a list of subprocesses, the question of inputs and outputs 
from the processes has been entirely 
by-passed. Since each subroutine 
has an arbitrary and 
variable number of operands as input, and provides to the routine that uses it an arbitrary number of outputs, some scheme of communication is required among routines. 
The communication list, 
L,, accom- plishes this function in IPL. That the 
inputs and outputs 
to a routine 
be symbols is required. This is no real restriction since a 
symbol can be the 
name of any list structure whatever. Each routine 
will take as its inputs the 
first symbols in the L, list. That is, if a routine 
has three inputs, then the 
first three symbols in L, are its inputs. Each routine 
must remove its inputs 
from L, before terminating with 
b = 10, so as to permit 
the use of the communication list by subsequent 
routines. Finally, each routine 
leaves its outputs at the 
head of list Lo. The b operations 14 through 19 are used for 
communication in and out 
of L,. Their one common feature is that, whenever they 
put a symbol in L,, they save the symbol already there, that 
is, they push down 
the symbols already ﬁstackedﬂ 
in Lo. Likewise, whenever a 
symbol is moved from 
L, to memory, 
the symbol below 
it in L, ﬁpops upﬂ to become the top one. (To be precise, 
the 
358 Part 4 I The instruction-set processor level: special-function processors 
responsibility bit travels with a symbol when it 
is moved. Hence for example, b = 16 and 17, do not, unlike 
the ﬁdeleteﬂ 
operation, erase the structure for which 
lL, is responsible.) The four operations, 
b = 14, 15, 16, 
and 17, are the main in-out 
operations for Lo. Two options are provided, depending 
on whether the programmer wishes to retain the s in memory (b = 14 and 16) or destroy it (h = 15 and 17). (The move in 
operation 15 has the same significance 
as in I6 and 17; the responsibility bit moves with the symbol, and the symbol previously 
in the location of s, is recalled.) Operation b = 18 is a special 
input to aid 
in the breakout designation operation, 
c = 5. Recall that the latter operation re- quires d to place 
the location of s, the symbol it determines, 
in Lo. Operation 18 
allows the process d to accomplish this. Operation b = 19 provides the means for creating structures. 
It takes a cell, for example, 
L,,,, from available space, and puts 
its name, 
as the symbol (0,0, L,,,), in the location of the designated symbol, s. The symbol s, previously in this location is pushed down and saved. Signal operations Ten 6 operations are primarily involved 
in setting and 
manipu- lating the signal bit. Observe that the 
test of equality (b = 20 and 21) is identity of symbols. Since 
there is nothing in the system that provides a natural ordering of symbols, inequality tests 
like s > lL,, are impossible. (E, means the symbol in Lo.) It is neces- sary to be able to 
detect the responsibility bit (b = 22), since there are occasions when 
the explicit structure of lists is important, and not just the information they designate. Finally, although 
the signal bit is just a single switch, it is necessary to have two symbols, one 
corresponding to ﬁsignal onﬂ and the other 
to ﬁsignal offﬂ (b = 26 and 27), so that the 
information in 
the signal can be retained for later use (b = 28 and 29). The sense of the signal is not arbitrary. In general ﬁoffﬂ is used to mean that a process ﬁfailed,ﬂ ﬁdid not 
find,ﬂ or the like. Thus, in operations h = 6 and 7, the failure to 
find a ﬁstop 
interpreta- tionﬂ operation 
sets the signal to ﬁoff .ﬂ Likewise, the end of a list will by symbolized by setting the signal to ﬁoff.ﬂ List operations Both the ﬁsaveﬂ and ﬁdeleteﬂ operations are used to manipulate lists, but besides these, several others are needed. The three 
opera- tions, 6 = 30, 31, 32, allow for search over 
list structures. They 
can be paraphrased 
as: ﬁget the 
referent,ﬂ ﬁturn 
down the sublist,ﬂ and ﬁget the 
next word 
of the list.ﬂ They 
all have in common that they replace a 
known symbol 
with an unknown symbol. This 
Section 4 1 Processors based 
on a programming language 
unknown symbol 
need not exist; that is, the symbol referred to may contain a 
b = 10 operation, which 
means that the end 
of the list has been reached. Consequently, 
the signal is always set ﬁonﬂ 
if the symbol is found, and ﬁoffﬂ if the symbol is not found. One 
of the virtues of the common signal 
is apparent at this point, since, 
if the programmer knows that the 
symbol exists, he will simply 
ignore the signal. Instruction formats 
that provide for additional addresses for 
conditional transfers would 
force the programmer to attend 
to the condition even if it only meant leaving a 
blank space in the program. To illustrate how these search operations 
work, Fig. 
6 shows a list of lists, L,,,, and a 
known cell, L,,,. Cell L,,, contains the reference to the 
list structure. The programmer does not know how the list, L,,,, is referenced. He wants to 
find the last symbol on the last list 
of the structure. 
His first step is (30, 1, L,,,) which replaces the reference by the name of the list, L,,,. He then searches down 
to the end of list L,,, by doing 
a series of opera- tions: (32, I, Ll,,). Each of these replaces 
one location on the list by the next one. 
In fact, 
a loop 
is required, since the length of the list is unknown. Hence, after 
each ﬁfind the next wordﬂ opera- 
tion, he must transfer, on the basis of the signal, back to the same operation if the end of the list hasn™t been reached. 
The net result, when the end 
of the list is reached, is that the location of the last word on list L,,, rests in L,,,. Since in this 
example he wants to go down 
to the end of the sublist of the last word on the main list, he next performs (31, 1, Lloo). This operation replaces 
the location of the last word with the name of the last list, 
L,,,,. Now the search down 
the sublist is repeated until the end is again reached, at this point the location of the last symbol on 
the last list is in L,,,, as desired. The sequence of code follows: Location Symbol 
Link The operations, b = 33 and 34, allow for 
inserting symbols in a list either before or after the symbol designated. The lists in this system are one-way: although there 
is always a way of finding the symbol that follows a designated 
symbol, there is no way of finding the symbol that precedes a designated 
symbol. The ﬁinsert beforeﬂ operation 
does not violate this rule. 
In both operations, 
Chapter 30 1 A command structure for complex information processing 359 Direct designation 
operations Figure 7 shows the information flows for 
c = 2, an operation 
that is typical of the first four designation operations. 
These flows follow a simple, fixed interpretation sequence. Assume that instruction (-, 2, L,,,) is inside the control unit. The contents of L,,,, are brought into R,, the input register, then transferred to R,, the output register, and back to L,,, again. The d part of R, now contains the location of s, and this location is transferred from R, to the 
address register. 
Execute subroutine 
(b = 1) When ﬁexecute sﬂ is to be interpreted, the address register 
already contains the location of s, which was brought in during the first stage of the interpretation cycle. L,, the current instruction address list (CIA), holds the address of the instruction containing the ﬁexecuteﬂ order. 
A ﬁsaveﬂ operation is performed on L,, and s is transferred into L,, which ends the operation. The result is to have the interpreter 
interpret the first instruction on the next sublist, and to proceed down it in the usual fashion. Upon 
reaching the terminate operation, 
b = 10, the delete operation is performed on E,, thus bringing back the original instruction address 
from which the subroutine was executed. Now, when the interpretation cycle is resumed, it will proceed down the original list. Thus, the two operations, save 
and delete, perform the basic work in keeping 
track of subroutine linkage. Parallel programs 
A single program structure, that is, a routine with all its sub- routines, and their subroutines etc., requires a CIA list in order to keep track of the sequence of control. In order to have a number of independent program structures, a CIA list is required for each. L, is the fixed register which holds the name of the current CIA Fig. 6. Example of finding last 
item of last sublist. 
33 and 34, a cell is obtained from the available space list and inserted after the word holding the designated symbol. (This is identical with 
the first step of the ﬁsaveﬂ operation.) 
In the ﬁinsert beforeﬂ operation (b = 33) the designated symbol, s, is copied into the new cell, and 1L, is moved into the previous location of s. In ﬁinsert 
afterﬂ (b = 34), the designated symbol is left unchanged, and lL, is moved into the 
new cell. In both cases lL, is moved, that is, it no 
longer remains 
at the head 
of the communication list. Other operations This completes 
the account of the basic complement of operations for the IPL computer. These form a 
sufficient set of operations to handle a wide range of nonnumerical problems. To do arith- metic efficiently, one would either add another set of b™s covering the standard arithmetic 
operations or 
deal with 
these operations 
externally via a 
breakout operation 
on b (not formally defined 
here) that would move a 
frill symbol into a special register 
for hardware interpretation relative 
to external machines: adders, printers, tapes, etc. 
The set of operations has not been 
described for reading and writing the various parts of the word: b, c, d, e, and f (although it may be possible to automatize 
this last completely). These operations rarely 
occnr, and it seemed best to 
ignore them as well as the input-output operations in the interest of simple presenta- tion. Interpretation This section 
will describe in general terms 
the machine interpre- 
tation required to carry out 
the operation codes prescribed. There is not enough space to be exhaustive, therefore selected 
examples will be discussed. LIOO Fig. 7. Information transfers in c = 2 operation. 
360 Part 4 1 The instruction-set processor level: special-function processors 
After 8,1,Lloo LIoorI list. The name of the CIA list for the program structure which is to be 
reactivated on completion or interruption of the current program structure is the second item on the L, list, etc. Therefore, the L, list is appropriately called the current 
CIA list. The ﬁsaveﬂ and ﬁdeleteﬂ 
operations are used to manipulate L, analogously to their use with L, previously described. Appendix 3 gives a more complete schematic representation 
of the interpretation cycle. It has still been necessary to represent only selected b operations. LmTl Data programs In the section on list operations a 
search of a list was described. There the data 
were passive; the processing program dictated just what steps were taken in covering 
the list. Consider a similar situation, shown in Fig. 
8, where there is a working cell, L,,,, which contains the name of a list, L,,,. L,,, is a data program. There is a program that wants to process the data 
of L3,,,, which is a sequence 
of symbols. This program knows 
L,,,. To obtain the first symbol of data, it does (6,1, L,,,), that is, ﬁexecute the parallel program whose 
name is in L,,,.ﬂ The result is to create a CIA list, L,,,, put its name in 
L,,,, and fire the program. Some sort of processing will 
occur, as indicated by the blank words 
of L,,,. Presumably this has something to do with determining what the data are, 
although it 
might be some bookkeeping on 
L,,,™s experi- ence as a data file. Eventually L,,, is reached, which contains (0, 1, This operation stops 
the interpretation, and 
returns con- trol to the original processing program. The first symbol of data is defined to be lL8,,. The processing program 
can designate 
this by 4L,,,, since the sequence of c = 4 prefixes in L,,, and L, pass along the interpretation 
until it ultimately becomes IL,,,. Now the processing program 
can proceed with 
the data. It 
remains Before 8.i.Lloo Lu3o~LGT L Section 4 1 Processors based on 
a programming language 
completely oblivious to the 
processing and structure 
that were involved in determining what was the first symbol of data. Simi- larly, although it 
is not shown, 
the processing program 
is able to get the second symbol of data at any time 
simply by doing 
a ﬁcontinue parallel program lL,,,ﬂ (b = 7). One virtue 
of the use of data programs is the solution it offers for ﬁinterpolated™ lists. In working on 
a chess program, for example, one has various 
lists of men: pawns, pieces, pieces 
that can move more than one square, such as 
rooks, queens, etc. One would like a list of all men. There already exists a list of all pieces and a 
list of all pawns. It would be desirable to 
compose these 
lists into a single long list without losing the identity of either of the short lists, since they 
are still used separately. In other words form 
a list whose elements are 
the two lists, but such that, when this list of lists is searched it 
looks like a single long 
list. Further, and this is the necessary condition for doing 
this successfully, one cannot 
afford to make the program that uses this list 
of lists know the structure. The operation ﬁexecute 
sﬂ (b = 1) is precisely the opera- tion needed to 
accomplish this task in a 
data program. It says ﬁturn aside and go down the sublist s.ﬂ Since it does not have 
the opera- tion b = 0, it is not ﬁdata.ﬂ It is simply ﬁpunctuationﬂ that describes the structure of the data list, and allows the appropriate symbols to be designated. Figure 
9 shows a data list of the kind just described. The authors have taken 
the liberty of writing in the names of the chessmen. The stretch 
of code that follows shows 
the use of a data program for a ﬁtable look upﬂ operation. The table 
has arbitrary arguments, 
each of which has a 
symbol for 
its value. A,, A,, etc. have been used to represent 
the arguments. To find the value corresponding 
to argument A,, for example, A, is put in the communication cell 
with (14, 0, A,). Then the data 
program is executed with 
(6, 0, J&). Control now lies with the table, which tests each argument 
against the symbol in the communication lists: Le., A,, and sets the signal accordingly. The program stops 
interpreting (b = 8) at the word holding the value only 
if the arguments are the same. In this case it would stop, designating 
L,,,. If no entry was found, of course, control would return to the 
inquiring program 
with the signal off. Locution Symbol Link LlOO. , . . . . . 
. . . 
. . . 
. . 
Fig. 8. Example of a data program. t 
Chapter 30 I A command structure 
for complex 
information processing 361 L,,, I j,O. Lzoo H 1,o,L,oo T O,O, Queen +, O,O, K-Rook + Fig. 9. Application of a data program to chess. Conclusions The purpose of this paper has been 
to outline a command structure for complex information processing, following 
some of the concepts used in a series 
of interpretive languages, called IPL™s. The ulti- mate test of a command structure is the complex problems 
it allows one to 
solve that would not have been solved if the coding language were not 
available. At least two different factors 
operate to keep problems from being solved on 
computers: the difficulty of specification, and the effort required to do the 
processing. The primary features of this command structure have 
been aimed at the specification problem. The authors have tried to 
specify the language requirements for complex coding, and then 
see what hardware 
organization allowed 
their mechanization. All the features of delegation, indirect refer- encing, and breakout imply a good 
deal of interpretation for each machine instruction. Similarly, the parallel program structure requires additional 
processing to set up CIA lists, and when a data symbol is designated, there is delegated interpreting through 
several words, each of which exacts its toll of machine time. If one were 
solely concerned with machine efficiency, one would require the programmer to so plan and arrange his program that direct and uniform processes 
would suffice. Considering the size of current computers 
and their continued rate 
of growth toward megaword memories and microsecond operations, it is believed that the 
limitation already 
lies with the programmer with his limited capacity 
to conceive and plan complicated programs. The authors certainly 
know this to be true 
of their own efforts to program theorem proving programs and chess playing programs, where the IPL 
languages or their equivalent in flexibility and also in power have been 
a necessary tool. 
Considering the amount of interpretation, and the fact that interpretation uses the same operations as are available to the programmer; e.g., the save and delete operations, one can 
think of alternative ways to realize an IPL 
computer. At one extreme are interpretive routines on current computers, 
the method that the authors have been 
using. This is costless in hardware, 
but expensive in computing 
time. One could also add special opera- 
tions to 
a standard repertoire 
to facilitate an interpretive 
version of the language. Probably 
much more 
fruitful is the addition of a small 
amount of very fast 
storage to speed up the interpreter. Finally, one could wire 
in the programs for the operations to get even more speed. It is not clear 
that there is any arrangement more 
direct than the wired in 
program because of the need of the inter- preter to use the whole capability of its own operation code. References ShawJ58; BernA58; BrooF57b; 
KistJ57; NeweA56, 57a, 57b, 58 
APPENDIX 1 c Nature of operation for (a) = b c d e. 0 1 2 3 4 5 APPENDIX 2 b OPERATIONS c OPERATIONS (DESIGNATING OPERATIONS) (a) is the symbol s. d is the address of the symbol s. d is the address of the address of the symbol s. d is the address of the address of the address of the symbol s. d is the address of the designating instruction 
that deter- 
mines s. d is the address (name) of a process 
that determines s. b Nature of operation SEQUENCE-CONTROL OPERATIONS 
0 1 Execute process named s. 2 Interpret instruction s. 3 Transfer control to location s. 4 Transfer control to location s, if signal is on. 5 Transfer control to location s, if signal is off. 6 Execute parallel program s; turn signal on 
if stops; off if not. 7 Continue parallel program s; turn signal on 
if stops; off if not. 8 Stop interpreting, 
if signal is on. 9 Stop interpreting, if signal is off. Stop interpreting; return to 
previous program structure. 10 Terminate. 11 Halt; proceed on 
go. SAVE AND DELETE OPERATIONS 12 Save s. 13 Delete s (and everything for 
which s is responsible). 
362 Part 4 I The instruction-set 
processor level: special-function processors 
Section 4 I Processors based 
on a programming language 
COMMUNICATION LIST OPERATIONS 
14 15 16 17 18 19 Copy s into communication list, saving 
IL,. Move s into communication list, 
saving 1L,. Move lL, into 
location of s, saving s. Move IL, into location 
of s, destroying s. Copy location 
of s into communication list, saving 
IL,. Create a new symbol in 
location of s, saving s. SIGNALLING OPERATIONS 
20 21 22 23 24 25 26 27 28 29 Turn signal 
on if s = lL,, off if not. Turn signal 
on if s = lL,, off if not; delete IL,. Turn signal 
on if s is responsible, off if not. Turn signal on. 
Turn signal 
off. Invert signal. Copy signal into location of s. Copy signal into location of s, saving s. Set signal 
according to s. Set signal 
according to s; delete s. APPENDIX 3 THE INTERPRETATION CYCLE 1. Fetch the current 
instruction according 
to the current instruc- tion address (CIA) of the current CIA list. 
2. Decode and execute the c operation: If c = 3 replace d by d part of the word at address d, reduce c to c = 2 and continue. If c = 2 replace d by d part of the word at address d, reduce c to c = 1 and continue. If c = 1 put d in the address register 
and go to step 3. If c = 0 put CIA in the address register and go to step 3. If c = 4 replace c, d by the c, d parts of the word at address d and go to step 2. If c = 5 mark CIA ﬁincomplete,ﬂ 
save it, set 
a new CIA 
= d, and go to step 
1. 3. Decode and execute the b operation: (Some of the b operations which affect the interpretation cycle follow.) If b = 0 turn the signal on, 
delete CIA and go to step 4. If b = 1 save CIA, set a 
new CIA = d part of s and go to step 1. If b = 2 replace b, c, d by s and go to step 2. If b = 3 replace CIA by the d part of s and go to step 1. If b = 10 delete CIA. LIST OPERATIONS 30 31 32 Replace s by the symbol designated by s, and turn signal on; 
if symbol doesn™t exist 
(b = lo), leave s and turn signal off. Replace s by the symbol in d of s and turn signal on; 
if symbol doesn™t exist, leave 
s and turn signal off. Replace s by the location of the next symbol 
after d of s and turn signal on (s replaced by ﬁ0, 4, (f, part of d of s)ﬂ); if next symbol does 
not exist, leave s and turn 
signal off. If no CIA ﬁpops 
upﬂ turn signal off, delete CIA and go to step 4. If ﬁpopped upﬂ 
CIA is marked ﬁincompleteﬂ fetch 
the cur- rent instruction again, 
move lL, into 
address register and go to step 3. Otherwise go to step 4. 33 34 Insert 1L, before s (move symbol from 
communication list). Insert IL, after s (move symbol 
from communication list). 4. Replace CIA 
by the f part of the current 
instruction and go to step 1. 
Chapter 31 System design of a FORTRAN machine™ Theodore R. Bashkow / Azru Susson / Arnold Kronfeld Summary A system design is given for a computer capable 
of direct execution of FORTRAN language source 
statements. The allowed types 
of statements are the FORTRAN DO, GO TO, computed 
GO TO, Arith- metic, READ, PRINT, arithmetic IF, 
CONTINUE, PAUSE, DIMENSION and END statements. Up to two 
subscripts are allowed for variables and no FORMAT 
statement is needed. The programmer™s source program 
is converted to a slightly modified form while being loaded 
and placed in a Program Area in 
lower memory. His original variable 
names and statement numbers are retained in a Symbol Table in upper memory, which also serves 
as the data storage area. 
During execution of the program each 
FORTRAN statement is read and interpreted at 
basic circuit speeds since the machine is a hardware interpreter for these statements. The machine corresponds 
therefore to a ﬁone-pass, load-and-goﬂ compiler 
except, of course, that there 
is no translation to a different machine language. It is estimated that the 
control circuitry 
for this machine 
will require on the order of 10,000 diodes and 100 flip-flops. This does 
not include arithmetic circuitry. Tern Digital computer system, digital 
machine design, 
direct .ion of FORTRAN, FORTRAN computer system, FORTRAN lan- machine, hardware interpreter. Introduction The algebraic languages, 
in particular 
FORTRAN in this country, have had enormous impact on 
the utilization of computers for scientific and engineering computation. They were 
designed in large part to overcome the annoyance of lengthy learning time and the 
laborious attention to detail needed to 
use a basic machine language. These annoyances 
are overcome by 
providing a language which is closer to English in form, and freer of ﬁbookkeepingﬂ details, than the usual machine languages, and by 
providing a machine language program, called 
a compiler or translator, 
to convert from the source program written by a user 
to an object program execut- able by a computer. Thus 
the original drawbacks are overcome but the discrepancy between the external language of the user and the 
internal language of the machine leads to at least two others. The compilation run of the machine, during which the ‚IEEE Trans., EC-16, vol. 4, pp. 485-499, August, 1967. language translation 
is accomplished, is a waste of time and money to the 
user since he must pay 
for this time 
though he gets no problem answers from 
it. Secondly, the user has specified 
the logical flow and arithmetic details of his solution 
in the source language. However, when 
the machine ﬁhangs upﬂ or when he attempts to debug 
his program, all 
he finds displayed on the machine console is the machine language. (On large machines he gets equivalently 
an esoteric print-out in 
a symbolic form 
of machine language.) To overcome these difficulties one could 
use an interpretive translator of the source language instead, 
but the historical deficiencies of interpreters, loss of memory space and loss of speed of execution have caused this solution to be shunned. Another solution is also possible-design 
a machine which executes an algebraic language directly 
as its ﬁmachine 
language.ﬂ This approach 
is based on a 
recognition that once the allowable syntax and associated semantics 
of language statements have been 
firmly specified it is a matter of choice whether to 
write a compiler, to write an interpreter or to build 
an interpreter out 
of hardware. The software choice has 
been almost overwhelmingly to write 
a compiler. Since the choice of hardware interpreter, 
or machine, 
has not been 
made, and in fact has hardly 
been explored to any 
great extent, 
a study has been made in order to 
see if this choice leads to a system 
which is competitive with 
the usual software system. It should be understood that such a machine has not been constructed. However, the design2 is sufficiently complete that construction seems feasible. 
Language-design philosophy 
Since the machine language is to be an algebraic one it 
seemed reasonable to choose a 
simple subset 
of the most commonly used one, FORTRAN. This eliminates 
the necessity for 
inventing still another such language 
and allows attention to 
be focused on 
machine design. In fact, the subset chosen 
is quite close to that known as ﬁPreliminary FORTRAN for 
the IBM 1620,ﬂ which is complete enough to be quite useful, but which does not include 2See ha1 technical report for Contract AF 19(628)-2798. 363 
364 Part 4 1 The instruction-set 
processor level: special-function processors 
such innovations 
as subroutines, etc. In addition, the usual ﬁbuilt 
inﬂ subroutines SIN (x), COS (x), etc., are not included. Their in- clusion would require additional effort for 
their hardware imple- mentation which did not 
appear to be worth expending at this time. The FORTRAN statement types which are accepted by the machine as machine language are in the table that follows.™ Stutement Comment a=b GO TO n The value of the arithmetic expression 
b is stored in the memory location referenced by the variable name a, which may have up to two subscripts. Program control is transferred to the 
statement numbered n. GO TO (nl, n2, . . 
. , nm), i Program control is transferred to one of the statements numbered nl, n2, . . . , n, depending on the value of i at the time 
this statement is executed. PAUSE Program control 
is transferred to 
the statement numbered nl if the algebraic expression e is negative, to 
that num- bered n2 if e is zero, and to that 
numbered n3 if e is positive. Program execution is halted until restarted by console switch. 
DO n i = ml, m2, m3 All statements following this one 
in the program, including the statement num- bered n. are executed repeatedly. 
The first execution is with i equal ml, i is in- cremented by the value of m3 before each succeeding execution. 
This continues until i is greater than m2 at which time pro- gram control 
is transferred either to the statement following n or to that statement 
required by the DO sequencing rules for DO nests. If m3 is not given 
it is under- stood to 
be 1. CONTl NUE END This statement has the effect of the ﬁno operationﬂ instruction in conventional machines. Program control 
goes to the 
next statement in the program unless 
the CONTINUE is the last statement in the range of a DO. In this case normal DO sequencing takes 
place. This statement generates 
a control signal to start execution of the program. Some familiarity with the FORTRAN language is assumed. Section 4 1 Processors based 
on a programming language 
READ, List These statements cause data to 
be read PRINT, List 
or printed, respectively, in accordance with the specified list of variables which may be subscripted; however, the ﬁimplied DOﬂ feature has not been implemented. 
No FORMAT control is available with this machine, therefore no statement number need be 
given. This statement has the effect of reserv- ing memory space for the subscripted variables G. Each u stands for a 
variable name followed by parentheses enclosing one or two 
constants. DIMENSION u, u, . . . No distinction is made in 
this machine between 
fixed (integer) and floating point (real) 
variables. These may 
have names of any length, starting with any alphabetic character. 
Fixed point constants 
may be specified, in a 
program or as data, as any combination 
of one to four numeric characters preceded 
by a + or - sign. however, these are converted to an internal 
decimal floating point number and so there are no restrictions on ﬁmixed modeﬂ expressions. Statement numbers must be unsigned fixed point constants, 
which are not so converted since they only affect program control and not arithmetic processing. Floating point comtants are specified in the form of a mantissa 
of one to four numeric symbols preceded by a decimal point (and a + or - sign). These are followed by the character E and a single 
(positive or negative) digit 
representing the power of ten in the usual scientific 
notation. These constraints on number size and format are made to 
simplify certain circuits and could easily be relaxed if desired. The restriction to a two-subscript maximum for 
subscripted variables is similarly motivated. Internally, all 
numerical data require three 
%bit words (Fig. 1). The first two words contain the four-digit mantissa, packed two per word in a 4-bit code for each digit. A decimal point 
is assumed to exist to the left of the most significant 
digit. The most significant 
two bits of the third word are zero. The third bit is 0 if the mantissa is positive, or 
1 if it is negative, and similarly the fourth bit is 0 or 1 if the exponent is, respectively, positive or negative. 
The single exponent digit occupies 
the least significant four bits 
of this word. All other characters 
occupy a full 
8-bit word of which the two most significant 
are 1™s. Any numeric characters 
which are symbols of a variable, e.g., the ﬁ2ﬂ in ABZX, also occupy a full word of this type. Statement 
numbers are simply packed 2 digits per word and always occupy 2 full words. 
Before proceeding with 
the description of the overall charac- 

Chapter 31 I System design 
of a FORTRAN machine 365 +0.5739 E-4 in three consecutive words in memory Mantissa i Word1{1011 1011 1011 Ill11 Word 2 [I 0 10 11 11 11 10 10 11 I Word 3 { I 0 I 0 I 0 I 1 I 0 1 1 1 0 1 0 I - Not used Exponent -\\ Exponent sign Number sign Fig. 1. Data format in memory. teristics of a machine that loads and executes the language speci- 
fied above, it may be well to indicate two 
basic design goals. 1 The card deck or tape containing the Hollerith or BCD version of the English language form of a source program should be the only deck or tape required at any time to 
execute the program. Once this program is loaded into memory and execution started, any 
look ﬁinto the machineﬂ should reveal 
infor- mation in the same form 
in which it was entered. Thus 
if the program is executing X = A + B, then one should find ﬁXﬂ, ﬁ=ﬂ, ﬁAﬂ, ﬁ+ ﬂ, ﬁRﬂ, at least in their BCD form. 2 The second goal has been compromised somewhat 
as far as the internal representation of the program is concerned in the interest of execution speed. 
However, all such 
compromises have been kept 
to a minimum. In addition, the mechanisms by which one can take such looks ﬁinto the machineﬂ are such as to conceal these 
com- promises. Memory organization The machine is, in effect, 
a hardware version of on ﬁone-pass- 
load-and-goﬂ compiler and it operates in two 
modes. In the load mode FORTRAN 
statements are read. They 
are analyzed as re- 
quired and stored in memory. When the last statement has been stored, the execution mode is entered and program execution begins at the first executable statement that was read. The input/ 
output device for 
the machine design is a Flexowriter Model SPD. Programs are assumed to be punched onto 
a paper tape, one statement per 
line, followed by a ﬁcarriage returnﬂ which gen- erates a paper tape symbol to separate statements. 
When this tape is read into memory, blanks are automatically ﬁsqueezed out.ﬂ The memory around which the machine is designed is a 4096- word, 8-bit-per-word, random-access 
core mem0ry.l It is treated by the control circuits 
as though it consisted of three distinct regions. 1 Input/output (I/O) buffer: One statement 
at a time is loaded sequentially into memory locations 0-99. The six-bit paper tape codes are first converted to internal (often different) 
six-bit memory codes and stored in the six least significant 
positions of the &bit words. The carriage return symbol is encoded into a special ﬁend-of-statementﬂ 
symbol repre- sented in the paper 
as ﬁ$.ﬂ When this symbol is read the tape is also automatically stopped. Symbol table 
area: Memory locations 4095 and sequentially downward in 
memory hold the programmer™s names for variables, statement numbers, etc., as well as 
ﬁpointersﬂ to machine addresses, plus empty (before execution) locations 
for data. Program area: Memory locations 100 and sequentially up- ward hold the FORTRAN program, in a slightly 
modified form. 2 3 Operating modes The load mode circuits control 
the input of FORTRAN statements. They place certain 
information in the Symbol Table Area and the 
modified form of the FORTRAN statements in the Program Area. It is while in this 
mode that the necessary searches for variable names take place 
and machine addresses are assigned. These ad- dresses replace portions of the variable names in the statement as it appears in 
the Program Area. Similar processing 
replaces programmer-assigned statement number 
references in the Program Area with various internal ﬁpointersﬂ for control 
of GO TO, DO, and IF statements. This modification is done so that statement execution in the execute mode can proceed at high speed. In short, 
the FORTRAN statement in the Program Area is modified 
to the extent that variable names are replaced by actual data addresses and statement number 
references are replaced 
by actual addresses of statement locations in the Program Area. This translation 
is done once only, when the statement is analyzed in the load mode. It might be noted here that 
because of the ﬁone-pass™™ nature of the translation (a given statement is analyzed only once), certain ‚5-ps cycle time, EE Co Model 781. 
366 Part 4 I The instruction-set processor level: special-function processors 
of the pointers correspond 
to indirect addresses. Figure 2 shows a sketch of the overall system control and Tables 2 to 7 show to what extent the original statements have been 
altered. Loading a program A program, which is punched in 
a paper tape, is loaded into memory by 
energizing the tape 
read circuit which 
reads a state- ment on the tape, including the end-of-statement symbol &, into the 1/0 buffer. The read circuit 
is then de-energized. The least significant 6 bits of each word of the buffer hold 
the internal BCD representation of each symbol. A scan circuit (Fig. 
3) now picks up each symbol in the state- ment from left to right and as each symbol is decoded it reacts as follows. 1 If the first symbol is a digit, control is turned over to a Statement Number Load 
circuit. This circuit shifts the statement number 
digit by digit into a register (SHR). The maximum allowable length 
of a statement number 
is 4 digits and all statement numbers are carried 
internally in this form, i.e., a programmer's statement number 
13 is carried in 2 words as 0013. A search is now made 
of the Symbol Table area. One 
of three possibilities exists: a The statement number 
is not found in the Symbol Table. 
I 1/0 buffer T Execute 7 t Input- Program 
output Read /print -- Section 4 1 Processors based on 
a programming language 
It is put into the Symbol Table followed 
by the value of the current Program location. The statement number 
is also 
put.into the 
Program Area starting at this location 
and the 
Program Counter incremented appropriately, 
Le., by 2 since two 8-bit words are used. The statement number 
is found in the Symbol Table because it has been previously referred to by an IF or GO TO. The 
current value of the Program Counter is placed into 
the two memory locations following the statement number. 
(These were left blank 
when the statement number 
was previously processed.) 
The state- ment number 
is put into the Program Area and the 
Program Counter is incremented. The statement number 
is found in the Symbol Table because it has been previously referred to by a DO statement. A description will be deferred until the DO statement loading is described since 
the circuit's behav- ior is more meaningful in that context. b c 2 After a 
statement number 
has been processed in this fashion 
or if the first symbol in 
the statement was not a digit (no statement number 
was assigned) then the scan circuit con- tinues to pick up each symbol from left to right until it 
is able to classify the statement as to type. It then turns over control to the 
appropriate loading circuit as indicated in Fig. 3. All of these loading circuits 
put the statements into 
the Pro- gram Area after replacing 
variable names and statement number 
references in the program with addresses or pointers. They also replace reserved names such as GO TO or CONTINUE 
with a single 8-bit code (token). Each unique 
variable name in 
the pro- gram, however, 
is also 
stored in 
the Symbol Table once using an 8-bit code 
for each symbol. For nonsubscripted variables 
the three words following 
the name are reserved for the data that 
will be associated with this name when the program is executed. Sub- scripted variable names are found in DIMENSION statements which must precede the use of these variables 
in the program. In this case 
as many locations following 
the name are reserved as have been computed 
from the DIMENSION statement. The name in the Symbol Table is preceded by a special symbol a, to indicate that it is a subscripted variable. 
In addition, the first of the two subscript values in the DIMENSION statement is also stored immediately following the name. This number is needed during program execution for constructing the proper element 
of the array specified by a subscripted variab1e.l The address of 'A pointer to 
the next available location 
in the Symbol Table is also stored for speed in 
Symbol Table searching. 
Fig. 2. FORTRAN computer system. 
Chapter 31 I System design 
of a FORTRAN machine 367 PRINT Process ‚ PRINT IF J Process PAUSE Process PAUSE CONTINUE Process IF ™ CONTINUE Paper-tope control program switch . I- - Process end statement Process DIMENSION Process P- ARITHMETIC Off I L Fig. 3. Load processing sequence and 
control. DO Process DO Process GO TO COMPUTED GO TO Process COMPUTED GO TO ﬁOn CKT hD- Process the data location replaces 
all symbols of the variable name in the Program Area except for the first. This symbol, which must 
be alphabetic, is retained in the Program Area as an indicator 
that this is indeed a variable. All special symbols such as (,), +, -, etc. are 
simply stored sequentially 
in the Program Area in the &bit BCD form as they appear in the original statement. Statement numbers in 
IF and GO TO statements are 
similarly replaced by the address in the Symbol Table which holds the address in the Program Area of the statement having that number. Note that this is an indirect 
address to the statement. Statement 
numbers in DO statements are dealt 
with somewhat differently as will be explained later. 
Because variable names and statement number references can appear many times in a program, these 
searches of the Symbol Table are controlled by two special circuits, 
the Variable Match Unit (VMU) 
and the Statement Match Unit (SMU). These circuits indicate either 
that the name or statement number is already in the Symbol Table or 
it is not. Thus 
the first appearance of a variable name, statement number, 
or reference 
to a statement number 
causes it to be put 
into the Symbol Table. Subsequent references merely utilize these previously assigned 
data or Program addresses. Therefore each name 
or statement number is stored in the Symbol Table only 
once with an 
exception noted below. In general, 
the programmer™s statement is altered only in the above described fashion. However, for ease of execution the computed GO TO has its index parameter name, i.e., the ﬁiﬂ in GO TO (nl, n2. . . . , nm), i, changed from the position following 
the parenthesis to a position 
preceding the parenthesis, The DO statement requires the most complex loading algo- rithm. Basically, the idea 
is to place 
the DO statement itself, essentially unchanged, into the Program Area but to extract the 
368 Pari 4 I The instruction-set processor 
level: special-function Drocessors range statement number (which specifies the last statement in the range of the DO) and put it 
into the 
Symbol Table. It is there preceded by a 
special symbol A, designating it 
as being referenced 
by a 
DO, and followed by the Program Area address 
of the corre- sponding DO statement. The DO statement in the Program Area has its original statement number replaced 
by a special symbol, A, and an internal 
address which 
is determined as follows (see Table 6). a If this DO is one of a nest of DO™s, the internal address is the Program Area address of the X token of the next preceding DO statement, This is easily found by a 
Symbol Table search 
for the range statement number since 
there is an entry in the Symbol Table corresponding to every DO statement. Thus for a DO nest three deep all ending in 
statement number 100, for example, there will be three entries in 
ﬁDO nest orderﬂ of the number 0100 each fol- lowed by the corresponding DO statement Program Area 
address. If this DO is the first of a nest 
of DO™s, or if it is the only DO specifying a 
particular range 
statement number, then this internal address is the program address 
of the next statement outside the DO range, Le., the address to which control should go if this DO or DO nest is satisfied. b This outside address is found by 
the Statement Number Load 
circuit at the time the 
last statement in the range appears 
in the 
1/0 buffer for loading. The circuit first detects that a matching statement number in the 
Symbol Table is 
preceded by a 
A. It then extracts and saves the Program Area address of the first DO and the last DO, if there is a nest, or simply 
the only address if there is just one. The statement number is put in the Program Area 
as always. In addition, 
the Program Area address 
of the h token of the last DO in the nest is also put in the Program Area 
immediately following it. In addition, 
a special flip-flop, the LSFF, is set. The loading circuit 
for each statement type 
allowed to be the 
last statement in a DO range, tests this 
LSFF after it has loaded the statement into the Program Area. 
If it is on, the current contents of the Program Counter, the address of the next statement outside the DO range are used as 
the internal address in the first (or only) DO of the nest. It should be noted that this DO range statement number together with 
its own Program Area 
location will also appear in the Symbol Table without a preceding A. This is necessary because it is possible (and even 
legal in some cases!) 
to have an IF or GO TO refer to it also. The method used to design 
the circuits which implement these Section 4 I Processors based on a programming language 
functions is the same in each case. From the English language description of the function a sequential circuit 
state diagram is constructed. The circuit is then synthesized from the state diagram using established methods. 
The state diagrams of the Arithmetic Statement Loading circuits and 
the Variable Match Unit, 
which are used during Loading, are shown in the Appendix. The hardware implementation of the state diagram of the Variable Match Unit is also described there. 
Executing a program When the 
END statement signaling the end of a source 
program is encountered by the scan unit, the 
machine leaves its load mode, 
executes an automatic RESET, and enters the execution mode. (Reset forces 
the address 100 into the 
Program Counter.) Pressing the console start button causes statement execution to begin at the first executable statement which is always found at memory address 100. There is a separate statement 
execution circuit for each statement 
type. In addition, 
the Statement 
Number proc- 
essing circuit reacts 
to a digit 
as the first symbol in 
a statement. Each of these circuits is in an initial state when execution begins. 
One and only one can leave its initial 
state when the first symbol of a statement is read from memory. The responding circuit then 
retains control 
as it executes the statement until the 
$ (end of statement symbol) is read from memory. 
It then returns to its initial state. 
The first symbol of the next statement, as indicated by the Program Counter, is read and causes some circuit to leave its initial state, etc. 
Thus the first symbol of a statement acts like the ﬁoperation codeﬂ 
portion of a conventional computer instruc- tion word. The first symbol must be (since the load circuitry causes this) one 
of the 8-bit tokens for 
the various statement types, or 
a digit 
of a statement number, or the alphabetic character 
of the variable on the left of the ﬁ=ﬂ symbol of an arithmetic statement. 
The tokens are represented in 
this paper shown in Table 1. Table 1 Statement type Token GO TO n GO TO (nl, n2, . . . , nm), i IF (e) nl, nz, n3 PAUSE DO n i = ml, m2, m3 CONTINUE READ PRINT GO TO COMGOTO IF PA US E DO CONTINUE READ PRlNT 
Chapter 31 1 System design of a FORTRAN machine 369 It is possible, however, for the DO execution circuitry to leave its initial 
state either 
by reading of the DO or by reading 
of the A token immediately 
following it. The former causes 
DO initial- ization, the latter causes DO indexing and testing 
as will 
be described later. The action of the execution circuits is briefly given 
below. Statement number processing When the first symbol of a statement is a digit this 
circuit is energized. If there are 
only four 
digits (packed into two 
memory words) the circuit returns to its initial 
state and the remainder of the statement is executed. If there are eight digits (packed 
into four memory words), 
the last four digits (the address of the A of the last, or only, DO in a nest) 
are saved in a register, 
SSAR. The LSFF is turned on, the circuit returns 
to its initial 
state and the remainder of the statement is executed. If the remainder of the statement is not an IF, GO TO, or DO statement, the 
execution circuitry in control executes 
the statement and then 
tests for the LSFF being on. If it is on, the Program Counter contents are 
re- placed with the SSAR contents, the LSFF is reset, and 
the circuit returns to its initial 
state. In 
this case 
the SSAR holds the program address of the h token of the innermost DO. When this A is read, DO indexing and testing take 
place. If the LSFF is off, the circuit returns 
to its initial state. GO TO n The GOTO token energizes 
this circuit. The four-digit address 
(packed into two 
memory words) 
immediately following the token is extracted. The contents 
of this address are put 
into the Program Counter and the circuit returns 
to its initial 
state. Exump2e.l GO TO 15$ (Table 2). GO TO (nl, nq, . . . , nm), i The COMGOTO token energizes this 
circuit. The initial alpha- betic symbol of i, now immediately following the token, is read and discarded and 
the four-digit address 
immediately following is extracted. The contents of this address 
(the current value of i) are put into a register and decremented by one. 1 If the result is zero, the four-digit address following 
the left parenthesis is extracted. The contents of this address are put into the Program Counter and the circuit returns 
to its initial 
state. ‚All examples are written as though this statement 
or statements were the 
first in the program. Table 2 Symbol table Program area Address contents Address contents 
4095 00 Machine form for 0100 0102 0103 4094 15)Statement 
15 0101 0250 0251 GOTO 40 Address of the address 93 I of Statement 15 $ 00 Statement 15 15 I in the program If the result is nonzero, the four-digit address following the left parenthesis 
is read and 
discarded. The register is decre- mented by one again. If the result is zero, the four-digit address following 
the next comma is treated as in 1 above. If the result is nonzero, the four-digit address following 
the next comma 
is read and discarded. The register is decre- mented by one again. 
Steps 3 and 4 above are repeated until 
the register is zero. If the right parenthesis 
is read while 
the register is nonzero an error condition has been found 
and will be indicated. Exumple. GO TO (5, 10, lSO), ITALY2 (Table 3). IF(+,, n2, n3 The IF token energizes this circuit. 
The left parenthesis immedi- ately following the token is read. Control is then given temporarily to the Arithmetic Statement execution circuit. The latter 
circuit is forced to 
the state 
in which it would be if it were ready 
to evaluate an 
expression to the 
right of the equal sign in an Arith- 
metic Statement. A special F/F, the IFFF, is also set to 1. The expression e of the IF statement is read and evaluated until 
the final right parenthesis 
of the IF statement is read. Since the Arith- metic Statement 
circuit was not allowed to read 
the initial left parenthesis, it 
would normally 
go to an error condition under these 
circumstances of ﬁunbalanced™ parentheses. However, sensing 
that the IFFF is set to 
1, it resets the IFFF, places the value of the expression e just evaluated into the 
accumulator, returns 
to its own initial state, and 
re-energizes the IF statement circuit. The ac- cumulator is equipped to sense its own contents and energizes one 

370 Part 4 I The instruction-set processor level: special-function processors Section 4 I Processors based on a programming language Table 3 ExampZe. IF(A - B) 10, 20, 202 (Table 4). Symbol table Program area Address contents 
Address contents 4095 4094 4093 4092 409 1 4090 4089 4088 4087 4086 4085 4084 4083 4082 408 1 4080 4079 4078 4077 4076 I m 1 A L Y 00 Representation of 05 I Statement 5 02 50 00 10 03 Address of 50 I Statement 10 01 50 05 Address of 53 1 Statement 150 0100 0101 0102 0103 0102 0103 0104 0105 0106 0107 0108 0109 0110 0111 
0112 0250 0251 COMGOTO I 40 Address of the 90 ! data for ITALY ( 40 Address of the address 85)of Statement 5 40 Address of the address 81 I of Statement 10 40 Address of the address 77)of Statement 150 ) * 00 05 0350 00 0351 10 0553 01 0554 50 
of three signal lines 
depending on whether the number is zero, positive, or negative. The IF circuit senses these lines 
and reacts as follows. 1 If the accumulator signal is negative, the next four-digit address (n,) is extracted. The contents of this address 
are put into the Program Counter and 
the circuit returns 
to its initial 
state. If the accumulator signal is zero, the next four-digit address is skipped over. The four-digit address 
following the next coininas (nz) is treated as in 1 above. If the accumulator signal is positive, the next 2 four-digit addresses and the intervening comma are skipped over. The four-digit address 
following the next comma (n3) is treated as in 1 above. 2 3 PA USE The PAUSE token energizes this 
circuit. The end of statement symbol, $, is read and discarded. All execution circuits 
are forced to a state 0 and automatic reading of the memory ceases. A START signal, initiated by a console 
switch, is required to return 
these circuits 
to state 0 and to initiate memory reading at the location specified by the current contents 
of the Program Counter. Example. PAUSE $ (Table 5). DO n i = m,, m2, m3 (or DO n i = m,, m2) This circuit is energized (i.e., caused to leave its initial state) either 
by a DO token or by the h token. Its action 
is different in these two cases and will be described separately. Table 4 Symbol table Program area Address contents Address contents 
4095 4094 4093 4092 409 1 4090 4089 4088 4087 4086 4085 4084 4083 4082 4081 4080 A B 00 10 03 Address of 50 I Statement 10 00 20 0100 0101 0102 0103 0104 0105 0106 0107 0108 0109 01 10 0111 0112 01 13 0114 0115 01 16 0117 0118 IF ( A 40 94 B 40 90 ) - 40 Address of the address 81 I of Statement 20 40 Address of the address 81 I of Statement 20 t 0350 00 0351 10 0441 00 0442 20 
Chapter 31 1 System design of a FORTRAN machine 371 Table 5 d Symbol table 
Program area Address contents (not applicable) 0100 PAUSE 0101 $ 1 The circuit is energized by the DO token: The h token and the four-digit address immediately following are read 
and discarded. The initial alphabetic symbol of i is read and discarded and the four-digit address immediately following is extracted and saved in a register called 
SAR. The = symbol is read and discarded. The initial value 
m, of this statement can 
be either purely numeric or it may be the name of a variable. a If it is purely numeric the load circuitry will have re- placed it with 
the internal machine representation of the number. Therefore this number 
is simply read and stored in the Symbol Table starting at the address given in the SAR register. If it is the name of a variable, the initial alphabetic 
symbol is read and discarded. The four-digit address following is extracted. The contents of this address 
are treated as in a above. In either event then, i, is given the value ml as required. The remainder of the DO statement including the $ sym- bol is read and discarded and the 
circuit returns to 
its initial state. h 2 The circuit is energized by the X token: The four-digit address immediately following is extracted and saved in the SSAR. The initial alphabetic symbol of i is read and dis- carded, the four-digit address immediately following is put into the SAR and the 
contents of this address are placed in the accumulator. (This is the current value of i.) The = symbol and all symbols up to and including the next comma are read and discarded. The final value, 
m2, may be numeric or the name of a variable. a If it is numeric, this value 
is placed in a 
numeric register, SHR. h If it is the name of a variable the initial symbol read and discarded. The contents of the four-digit address 
following is extracted and placed in 
the numeric register SHR. The next symbol 
is read. This will be a comma if m, has been specified 
or ,i if m, has not been specified. 
If it is a comma either the 
following purely numeric value is added to the contents of the accumulator or the contents of the following four-digit address 
is added. c If it is the $ symbol then the 
contents of the accumulator are incrementedby 
one. In either event, after 
the current value of i has been incremented 
by either m3 or one, the contents of the accumulator are put in 
the Symbol Table starting at the address given in the SAR. Now the final value, saved in the SHR, is subtracted from the accumulator. If the accumulator signal is positive then the value of i must be greater than the 
final value of m2. Therefore the address in the SSAR is placed in the Program Counter and the 
circuit returns to its initial state. 
The address in the SSAR will either be the address of the h token of a preceding DO in the nest or it will be the address of the next statement outside the DO nest depending on 
which DO statement is being executed. 
If the accumulator signal is not positive then the 
value of i is less than or equal to 
m2 and the 
circuit just returns to its initial 
state. Thus the next statement after 
the DO statement will be executed. Example. (See Table 6.) DIMENSION B(20, IO)$ DO 5 DO 5 J= N, M$ 5 A = B(K J)S IT = 1, 100, L$ CONTINUE The CONTINUE token energizes this circuit. The 1 symbol is read. If the LSFF is not on, the circuit returns 
to its initial state. If the LSFF is on, it is turned off. The contents of the SSAR re- place the contents of the Program Counter and the 
circuit returns to its initial 
state. Thus 
if this statement is either not labeled or is not the last statement in 
a DO range, its execution has no effect on 
the program. The example assumes the usual case where it is the last statement in 
a DO range. Example. (Table 7) DO 5 I = 1, 150$ 5 CONTINUE$ READ, list. (PRINT, list.) The READ token energizes 
this circuit which then energizes the Flexowriter read circuits. Data from paper tape 
is read into the I/O buffer until the end-of statement symbol, #, is stored. The data must be punched as one to 
four decimal digits for fixed point numbers or one to four decimal digits 
preceded by a decimal point for floating point numbers. The latter may also be followed by 
372 Part 4 I The instruction-set processor level: special.function processors 
Section 4 1 Processors based on 
a programming language 
Table 6 Symbol table Program area Symbol table Program area Address contents Address 
contents - _______~ Address contents Address contents 
4095 4094 4093 4092 409 1 4090 4089 3488 3487 3486 3485 3484 3483 3482 348 1 3480 3479 3478 3477 3476 3475 3474 3473 3472 3471 3470 3469 3468 a B 34 1 Next free symbol 88 1 Table Address Machine form of the constant 20 A 00 Machine form 05 J Statement 5 I T L A 00 05 01 Address of 2nd 21 I DO in nest J 0100 0101 0102 0103 0104 0105 0106 0107 0108 0109 01 10 0111 01 12 01 13 01 14 0115 01 16 0117 0118 01 19 0120 0121 0122 0123 0124 0125 0126 0127 0128 0129 0130 DO h 01 Address of Statement 57 1 following the DO nest I 34 Address of data 81 I for IT 00 01 04 - - 
01 00 04 L 34 77 $ DO h 01 Address of preceding 01 I DO in the nest J 34 68 w 34 64 - - 3467 3466 3465 3464 3463 3462 346 1 3460 3459 3458 3457 3456 3455 3454 3453 3452 345 1 3450 0131 0132 N 0133 0134 0135 0136 M 0137 0138 0139 0140 00 0141 05 0142 
0143 0144 A 0145 0146 0147 0148 0149 
0150 0151 0152 0153 0154 0155 0156 0157 M 34 60 00 05 01 Address of last 21 1 DO in the nest A 34 52 B 40 91 ( 1 34 81 $ - - J 34 68 ) $ the letter E and a single positive or negative digit indicating 
a power of ten. Numbers must be separated by a comma to distin- guish them, since no FORMAT information is available and the 
read circuits ‚‚squeeze outﬂ blanks. The first set of digits starting at the beginning of I/O buffer, memory address 0, is read into 
a 24-bit 
register (which is the size of the three &bit 
memory words required for data). Numerical Both registers 
are set to zero 
initially. If the first character is a minus sign, the bit in the mantissa sign position 
of X is set to one. (The internal form of data representation was described earlier 
in the section on Language-Design Philosophy.) If it is a plus sign no action is required since a zero in 
the mantissa sign position 
indicates a positive mantissa. 
Further action depends 
on the next character. 1 If the next character is numeric (or if there was no sign given and the first character is numeric) this must he a fixed information in the 1/0 buffer is in a 
6-bit code. 
The two 
most significant bits are 0 if the code is for a 
numeric character. The Q placing of information into the 24-bit register 
is easier to under- 
stand if we consider it as a 16-bit mantissa register M, which can hold four decimal 
digits, and an 8-bit sign and exponent register X, which can hold 2 bits of sign information and an exponent digit. 
point constant. 
The four bits of numeric information are gated to 
the least significant four positions of register M. If the next character is numeric, M is shifted left four 
posi- tions and this character is also gated to the least significant 

Chapter 31 1 System design 
of a FORTRAN machine 373 Table 7 Symbol table Program area Address contents Address contents 4095 A 0100 DO 4094 00 0101 h 4093 05 0102 01 4092 01 0103 22 4091 01 0104 I 4090 I 0105 40 4089 0106 89 
4088 0107 4087 0108 00 4086 00 0109 01 4085 05 0110 04 4084 01 0111 , 4083 16 0112 01 0113 50 0114 04 
0115 % 0116 00 0117 05 0118 01 Address of the 0119 01 I DO statement 01 20 CONTINUE 0121 ? 0122 position. This 
continues until the 
comma is read. The nu- meric code for four 
is now gated to the 
least significant four 
positions of X. Since the arithmetic unit assumes a decimal 
point at the 
left of all data, this action insures that a fixed point number is properly interpreted. If the next character after the sign (if there is one) is a decimal point 
this must be a 
floating point number. 
In this case the following digits are stored into M as indicated above, but three shifts of M are always taken, whether or not four digits are stored in 
M. This is required to insure 
proper interpretation of the number. If a comma follows 
the series of digits no further action is taken. If an E follows then the digit following it is placed in 
the least significant 4 positions of X. If a minus sign 
is found following 
the E a setting of the exponent sign position 
of X precedes this action. The comma is then read. 2 After this first piece of data has been 
placed in 
M and X, the alphabetic character 
following the READ token is read and 
dis- carded. The next 4 digits are used as 
the address in which the most significant 
two digits in 
M are stored and it 
is then decre- 
mented appropriately 
to store 
the remainder of the data. The remaining data in the 1/O buffer are then 
stored one 
by one in sequence at the addresses given 
by the remainder of the READ list. A subscripted variable on this list 
requires additional 
arithmetic operations to compute the correct address from the current index values 
and the original DIMENSION information 
stored in the Symbol Table. These 
operations will be given later in the Arithmetic Statement description. When the $ token in the I/O buffer is reached, the next char- acter in the READ list is read. If this character is also the $. token then the 
circuit returns to its initial state. If, however it is not, then the Flexowriter is again energized such as 
to read data into 
the 1/0 buffer, and processing proceeds as before until reading 
of the $ of the READ statement returns 
the circuit to its initial 
state. The PRINT statement circuit operates 
in almost exactly inverse 
fashion and will not be described in detail. The list variables are used in sequence 
to extract data 
from the proper memory locations 
and place it in 
the M and X registers. The contents 
of these regis- ters are then put sequentially into the 1/0 buffer, together with 
6-bit codes for the decimal point, 
plus and minus signs, commas, and the E symbol at appropriate places. All data are 
thus output in floating point form. When the $. token is read, the Flexowriter print circuits are energized and 
the circuit returns 
to its initial 
state. Example. READ, A, B, C(I, I)$ PRINT, B, C(I, I)+$ The appearance 
of the Symbol Table and Program Area should 
be apparent from previous examples. Since 
this would add little 
to the 
description of circuit action 
they will be omitted. a=b The Arithmetic Statement execution unit is energized by any 8-bit alphabetic character 
code. This first character of the variable name represented above as ﬁaﬂ is discarded. Then either 
the following four-digit data address is saved or 
the data address of a subscripted 
variable is computed and saved in a register. After reading and 
discarding the = symbol, the circuit executes the expression h in accordance with 
the given sequence of arithmetic operator symbols, +, -, *, /, which are used to control 
the arithmetic 
unit. The partial results at any time 
during the 
execution are stored in the 1/0 buffer area which is, of course, otherwise unused during 
374 Part 4 I The instruction-set processor level: special-function processors 
Arithmetic Statement 
execution. These storage areas 
for partial results are called di,, di,, where i specifies the ﬁlevelﬂ at which computation is taking place,, i is equal to zero until a left paren- thesis is encountered which increases the current value of i by 1. An exception occurs 
if the left parenthesis 
immediately follows the = symbol. In this case the level remains 
at zero. It is also necessary to store 
control information 
which relates to these par- tial results. Two control values 
are required at every level. 
The count of left parentheses 
at any i level is stored as a 
number, Zi. Before i is incremented, the incompleted arithmetic 
operations still re- 
quired at the current level are indicated 
by giving an indicator 
t, the value 1, 2, or 3. Also needed are indicators t + and t * to distinguish + from - and * from /. To clarify the significance of these control 
values an analysis will 
be made of the following ex- pression, which contains some unneeded but legitimate sets of parentheses: A = ((B + (C/((D + E*(F)))) + G))$ 1 The circuit reads and saves the address of A, then reads and discards the = which puts the circuit at the level i = 0. The first two left parentheses cause 
I, to be set to 2. The value of B is stored in 
do,,. The plus sign followed by a left parenthesis cause the indicator to to be set to 1 to indicate 
the condition ﬁB + (ﬂ. Since we might in other 
cases find ﬁB - (ﬂ, to is set to zero 
to indicate the plus sign. 
The left parenthesis 
also causes 
i to be incremented to one and since it is the only one at this level, I, is also set to 1. The value of C is stored in d,,,. The division symbol 
followed by a left parenthesis causes 
t, to be set to 2 to indicate the condition ﬁC/(ﬂ. Since we might 
find ﬁC*(ﬂ in other cases, t * The left parenthesis 
also causes i to be incremented to 2 and the 
next left parenthesis 
increments 1, to 2. The value of D is stored in d,, and the value of E put into d,,, respec- tively. The multiplication symbol followed 
by a left paren- thesis causes 
t, to be set to 3 to indicate 
the condition ﬁD + E * (ﬂ. t + , and t * , are each set to zero to indicate 
the plus and multiplication symbols, respectively. The left parenthesis before 
the F causes i to be incremented to 3 and Z:, to be set to 1. The value of F is placed in 
d3,. The Arithmetic Statement circuit 
always puts the final value computed at any level into the arithmetic unit 
regis- ter, SR. It does this whenever Zi = 0 for any i. Clearly Zi must be decremented by one for each right parenthesis. 
‚Basic circuit operation at any level is described in the earlier report. See page 363, footnote 2. 2 is set to 1 to indicate 
the division. 3 4 Section 4 I Processors based 
on a programming language 
Therefore the first right parenthesis after 
the F causes 1, to equal 
zero. This condition 
causes the value stored in 
d,, to be placed in the SR. The value of i is decremented to 2. tz being 3 (and t + = t * , = 0) causes the computation, d,, + d,, * SR to be stored in dzo. The next two paren- theses after F caiise I, to equal 
zero. Therefore, this 
result is placed in 
the SR. The value of i is decremented to 1. Since t, is equal to 
2 and t * , is equal to 1 the computation d,,/SR is made and stored in d,,. The final parenthesis after 
the F causes 1, to equal zero. Therefore this result 
goes to SR. i is decremented to zero. Since to is one and t + , is zero the computation, d,, + SR, is made and the result is stored in 
do,. The +G causes the computation do, + G to be made and stored in do,. The final two parentheses cause 
1, to be zero; therefore the value in 
do, is placed in 
SR. (If another right parenthesis were found, this would cause an error condition 
to be indicated.) The 3 symbol causes 
the contents of SR to be stored at the previously saved memory address for 
A. Any subscripted variable addresses are computed 
easily from the initial DIMENSION statement information, saved 
in the Sym- bol Table, 
and the current value of the subscripts. Assume the first data location for an array 
A(1, J) is stored at a location Abase + 1. If the DIMENSION statement read 
DIMENSION A(5, 10) then the computation, Abase + 5 * (J - 1) + I, gives the correct data address for any nonzero value of I and J. (This is true only if a complete data word is stored per 
memory word; 
in this machine the expression is slightly more complicated.) In this 
machine the partial result locations 
d,, and d,, are actually 3 words long, 
of course, to accommodate the data. An additional word is used to store control information 
where 4 bits are used for ti. t + ,, and t * and the 
remaining 4 bits for the Zi count. The i counter therefore is actually incremented 
or decre- 
mented by 7 instead of one. Thus 
at any level, of which there can be 14 since the 1/0 buffer is 100 words long, 
the li count can be as great as 15. This is more than adequate 
since it allows for 210 left parentheses, 
which is much longer than the 
1/0 buffer length. Since the appearance of the Symbol Table and Program Area would add little to this discussion, an example will be omitted. Conclusion We have 
illustrated in some 
detail that a machine for direct trans- lation of a simple algebraic language 
is possible. It would therefore 
Chapter 31 I System design of a FORTRAN machine 375 seem that further investigation be made of the economic position of this solution 
vis-k-vis the software compiler 
solution. Unfor- 
tunately, the present authors are not 
sufficiently versed in compiler construction to make such a comparison. The actual construction of such a machine as an independent unit is probably not reasonable except 
under particular 
circum- stances in which only small one-shot scientific 
problems form the bulk of the computing. However, as an adjunct to 
a larger general purpose machine, 
it may well serve a need as a 
hardware inter- 
preter for widely 
used higher level 
languages. As a result 
of a fairly 
complete design of the control circuits 
of this machine, it is estimated that 10,000 diodes 
and 100 flip-flops would be needed for these alone (not including 
arithmetic circuits). The design techniques used are simple and straightforward but rather expensive. These designs should probably 
only be consid- ered for 
use with integrated circuitry. References AndeJ61; BashT64; International Business Machines Corporation, General Information Manual; 
FORTRAN, Form 
F28-807401, December, 1961; IBM 1620 FORTRAN: Preliminary Specifications, 
Form J2R-4200-2, April, 1060 APPENDIX™ The variable match unit (VMU) (Fig. 4) The Symbol Table at the end of the load mode should contain all variable 
names used by the program, together with 
empty locations reserved for 
data associated with these names. The Pro- gram Area at the end 
of the load mode should have a program in which all variable 
names have been 
modified in that only the first letter is retained, followed by the Symbol Table address of the data associated with this name. Since any variable name may appear many times in a program, a search is required, during the loading, to see if the name already exists in the Symbol Table. The search of the Symbol Table 
(ST) consists of comparing each name there with 
the variable name in 
the statement being loaded. 
All statements are loaded by an appropriate circuit 
of Fig. 3 from the 1/0 buffer and into the Program Area of the memory. There- fore the variable name in 
the Statement exists physically in 
the I/O buffer. It is the function of the VMU to make this search 
when ener- 
gized or ﬁcalledﬂ by the loading circuits 
for DIMENSION, DO, computed GO TO, READ, PRINT, 
IF and Arithmetic statements in which variable names appear. The output 
action of the VMU ‚Symbols used in this Appendix are described in Table 8. is to set either the OK, AOK or EOL flip-flops. These flip-flops respectively indicate that the ST either: holds the variable in question as a result of previous loading, 
or that the 
variable is subscripted and has been previously loaded by the DIMENSION statement loading circuit, or that the End-of-List (EOL) token was 
found, indicating the absence of the variable in the ST. The state diagram for this circuit is shown in Fig. 4. When triggered by the START VMU signal in state 0, the circuit goes to state 1, the next clock pulse 
sends it to 
state 2 from which it starts its search 
of the ST. In going from 1 to 2, the 1/0 Counter (CIO) contents are saved in register SCIO since the name may have to 
be scanned again. The Symbol Table 
Counter (STC) is initialized to 4095 since the ST is scanned sequentially downward. If a character of a variable name in 
the 1/0 buffer is found in the corresponding position of a name in 
the ST, the character is said to be matched. The VMU proceeds from state 2 to state 
3 if the first character of the name under 
scan matches. 
Otherwise the state changes from 2 to 8, if the NO MATCH 
signal is given. The MATCH or NO MATCH signals are generated 
as a result 
of comparing the contents of the ST location undergoing 
the scan (the contents reside in the Memory Buffer Register, MBR), with the contents of the register COMP which has the character from the 1/0 buffer. The first character is put into COMP by the calling circuit, thereafter the VMU picks them up in the 3-4 transition. The ClO and STC counters are incremented 
and decremented, respectively, and the VMU oscillates between states 3 and 4 as 
long as matching continues. This 
comparison process will 
termi- nate when, either an arithmetic operator 
So, is read from the 1/0 buffer sending the circuit to state 6 from state 3, or the ST contents cause a NO MATCH 
signal with respect to the contents of the COMP unit causing the transition from state 4 to 5. In state 6, if a digit is next read from the ST, corresponding in position 
to the appearance of the operator from the 1/0 buffer clearly the names are the same and the 
OKFF is set to 1, and the transition from 6 to 0 is macle. On the 
other hand, if another alphameric character in 
the ST corresponds to an operator, So, in the 1/0 buffer, the names are not the same and the 
transition from 6 to 5 is made. In state 5 the circuit just reads to the end of the nonmatching name in the ST. A digit at the end of this name causes the transition 5-7 during which the STC is stepped over the 3 data locations to the next ST entry and the 
CIO reini- 
376 Part 4 1 The instruction-set 
processor level: 
special-function processors 
Section 4 I Processors based on a programming language 
// / a/ STC READ (ST d/SET OKFF READ (STC) c VMU/- :OL/SET EOLFF /NO MATCH/- Sy/t STC c/ /READ (STC) READ (STC) \ d -+STCL SAVE -STCM d -STCL d SAVE-STCM SCIO -CIO I- READ 110 \ /READ I/O * Fig. 4. Variable match unit. tialized to the start of the name being 
sought. The first character in this name is read and placed in 
COMP as circuit goes to 2. As stated earlier, when the first character from the 1/0 buffer does not match 
the contents of ST, the state becomes 8. If the mismatch was caused by the EOL token in the ST the EOLFF is set to 1 and state 0 is reached. If the mismatch was due to 
a A at the present ST location, the STC is decremented by 5 which steps over the 2 four-digit numbers stored after a A and the circuit returns to 2 to try a match on the next ST 
entry. If the mismatch is caused by a digit then this is statement number 
information 
Chapter 31 1 System design of a FORTRAN machine 377 Table 8 CIO CP COMP SAR SAVE SClO SH R SR SSAR STC s, Sd SO (Y A x d EOL MATCH Counter for 
the input output buffer, 4 BCD numeric char- acter (4 bits each), counts up. Can be set to any given num ber. Program Counter. (During execution 
it points to 
the state- 
ment to 
be executed, during 
loading it points to 
the loca- tion where the program is to be loaded.) 4 BCD numeric characters, counts 
up. Can be set to any given value. Comparator register, 
8 bits. During loading holds 
a char- 
acter to 
be matched with some other character in the memory, during execution 
saves the 
input symbol that drives the execution circuits. (Acts as second rank of Memory Buffer Register.) Save Address 
register, 4 BCD numerics. Counts 
down. Dur- ing loading holds the address of the last DO in a nest. During execution it is an auxiliary counter. 2 BCD (8 bits total) auxiliary register, each bit can be set independently of 
the others. 
4 BCD numeric register, 
holds temporarily 
the value of CIO. Special Shift register, 4 BCD character, can be shifted to the left 1 BCD character (4 bits) at a time. 24-bit register, used with the accumulator in the arithmetic unit. Bits 1-8, 9-16, 17-24 can be gated independently. Special Save register, 4 BCD numeric (used 
as auxiliary register in loading and execution). Symbol Table counter, 4 BCD character, counts down. The 8 bits in the MBR are decoded as a single alphabetic 
character (A-2). The 8 bits in the MBR are decoded as a digit (0-9) and bits 1-4 represent in BCD the value of the digit. The bits in the MBR are decoded as one of the following operators. + - / ( ), 8-bit character that 
precedes a subscripted 
variable name 
in the Symbol Table. 8-bit character that precedes the statement number of a last statement of a DO nest in the symbol table. An 8-bit character that follows the DO token in the program area. The 8 bits in the MBR are decoded as 2 BCD digits of 4 bits each. An 8-bit character that 
is placed at the current end of the Symbol Table. Signal that is generated when the content of the MER is identical to the content of the COMP. which requires a decrement of STC by 4 to get to the 
next entry. If an unmatched alphabetic character in 
the ST was the reason for the mismatch, this variable is read to its end in state 12 as was done in 
state 5. The only other ST symbol which could have caused a mismatch is an a, the array symbol. This symbol sends 
the VMU to state 9. If a match is now 
to occur, 
it will be with a subscripted variable name. Thus a match causes a 
transition from 9 to 13 and states 13 and 14 correspond to state 3 and 4 for a 
simple variable 
as matching proceeds. Reading an arithmetic operator in 
the 1/0 buffer causes 
transi- tion to 16 where a corresponding digit 
in the ST causes the AOKFF to be set and the circuit returns 
to 0, during which time it decre- 
ments the STC. This is necessary in order 
for the STC to hold 
the address of the first constant given in 
the DIMENSION state- ment which caused this 
ST entry. The transition 16 to 15 corre- sponds to the 
6 to 5 transition, the ST name is longer than the 
1/0 buffer name, and in state 15 the rest of the name is stepped over. Now, however, the next two words in the ST hold the address of the next ST entry. Therefore, these 
are saved and put into the STC during transition 15-17-7, which otherwise corresponds 
to the 
transition 5-7 for a single 
variable. If, however, there was no match in 
state 9, the circuit steps 
over the rest of the name in the ST in state 10 and initializes the STC to the next ST entry in 
the transition 10-11-12. Note that when the VMU returns to 
its 0 state after setting either EOL or OK or AOK flip-flops, the STC holds precisely the address needed for further action. An EOL needs to be replaced, starting at this STC address, with the new variable name. In the 
case of OK or AOK this STC address is the one to be placed in 
the program since it holds the data 
address for 
simple variables or the address of the required indexing constant for subscripted variables. After the calling circuit has used the VMU it has received one of the 3 signals from 
the VMU. For certain statements 
these signals can be used to detect syntax errors. If there are none 
then the calling circuit takes whatever further action 
is necessary on the variable name being scanned. The arithmetic statement 
loading circuit 
(Fig. 5) An arithmetic statement 
consists of a string of alphameric symbols, S,S,, grouped to 
form variable names, of numeric symbols, S,, grouped to form constants, and of arithmetic or other operator 
sym- bols, So, which separate them. 
The Arithmetic Statement loading circuit calls on the VMU circuit to find the variable names as has been described. It then puts a new name 
into the ST (if required) 
378 Part 4 1 The instruction-set processor 
level: special-function processors 
Section 4 I Processors based on 
a programming language 
START VMU 3 ARITH STAT/- -- A v W + - PROG 9 CP-(SSAR) I START READ 
, 8 LSFF.1 RES LSFF /' / ,/ I I / A /SHIFT SHR -SHR Fig. 5. Arithmetic statement 
loading. I /ADJUST SHR or it puts the data 
address into the program. The 8-bit BCD forms of the operator symbols are simply put into the program. The constants are put into the program after conversion to machine form. The state diagram of this circuit 
is shown in Fig. 5. The scan circuit 
signal ARITH 
STAT sends the circuit from 0 through 1 to 2. The scan circuit has saved 
the address of the beginning of this statement in a 
register SCIO. This is used to initialize the CIO so that this statement can 
be read from the beginning. The first symbol of an arithmetic statement, 
which must 
be a variable and not a digit, takes the circuit to 
state 3 after this symbol has been put 
into the program (S, + PROG) and the 
VMU initialized and started. Any one of the VMU signals is possible and valid and simply forces the circuit to 
state 5. During the 3-5 transition the circuit loads the appropriate address into the pro- gram when the name has matched. If it has not matched 
any existing name the circuit first goes 
to state 4 and puts the name into the Symbol Table 
before going to state 5. State 5 is that from which all further loading is accomplished. Variable names are separated by operators, which are loaded into the 
program by the cycle in state 5 (So -+ PROG). Note the convention that So repre- sents any operator 
symbol not explicitly specified on 
another exit from 5. Any variable names cause a transition to state 3 with the same output action as from 
state 2. Floating point constants 
are loaded via states 5-9-5. A decimal point indicates 
a floating point 
constant and takes the circuit to state 
7. (Note that a minus 
sign preceding a constant is simply an operator 
and is processed in 
state 5.) The SHR is cleared in preparation for the storing of the follow- ing digits in state 7. When E is received the digits of the fraction in the SHR are left adjusted 
(ADJUST SHR), if there are less than four of them, and 
placed in the program area. The exponent sign is found in the transition 8 to 9. The exponent digit together with 
the exponent sign bit is stored in the program area during 
the 
Chapter 31 1 System design 
of a FORTRAN machine 379 9 to 5 transition. Fixed point constants 
are handled in 
state 6. The important difference is that the 
digits are not left adjusted in 
the SHR and a 04 is put into the program as 
the exponent since a decimal point 
is assumed to precede the 
first data word. See 
Fig. 1. The $ takes the circuit to its initial 
state. If this statement happens to be the 
last in a 
DO nest, the Statement Number Load 
circuit has set the LSFF to 1. It has also 
put the ST address of the word following 
the A symbol of the first DO of the nest into the SSAR register. Since the program counter (CP) now holds 
the correct exit address for 
this DO statement it is placed at the address given by 
the SSAR during the transition to 
state 0. During the transition the signal START READ is also sent to the 
paper tape reader in order 
for it to put the 
next Statement into 
the 1/0 buffer. Hardware implementation 
of the VMU state diagram Each function mentioned in 
the paper 
plus some other auxiliary ones are initially represented 
in a state diagram form, such as 
the state diagram for the loading of the Arithmetic Statement (Fig. 5) and the 
Variable Match Unit (VMU) (Fig. 4). We will describe the method used to realize a circuit 
which will perform 
the function defined by 
a given state diagram (SD). 
As an example 
we will use the VMU. All 
the information needed is present on the SD. The operations on the right-hand side of the ﬁ/,, in the SD are the output operations required to 
be per- formed. In order to implement these operations 
we must specify the actual 
register gating signals, memory read and write 
signals, arithmetic unit 
signals, etc., required by them. We will call these various signals 
the microsteps of an output operation. Therefore 
to realize the SD of a given function we must 
implement the microsteps corresponding to 
the output operations. We begin by 
listing from the state 
diagram some output opera- tions and 
their corresponding microsteps. For example, in state 2 of Fig. 4, if a MATCH signal 
is present we are supposed to increment the CIO 
counter and then read 
the 1/0 buffer. Consequently the microsteps required are: 
TCIO This signal causes 
the CIO 
to be incremented by one. CIO- MAR This signal causes 
the CIO 
to be gated to the READ This signal 
initiates a 
memory read cycle. CHANGE STATE This signal causes 
the VMU to go from state memory address 
register. 2 to state 3. Therefore the execution of the above microsteps, 
in that order, would implement the 2-3 transition of Fig. 4. Some microsteps for the VMU are listed at the end of this Appendix. The largest number of microsteps for 
a transition 
from one state to another is 8, which occurs 
in the transition from state 8 to state 2. Once this maximum number of microsteps is determined, a control 
cycle counter is constructed, which 
can count 
as high 
as this maximum. Since in this case the number is 8 we need 
3 flip-flops to realize 
it. In addition, a ﬁone 
hot lineﬂ 
decoder is needed such that at each count 
one and 
only one line 
of the decoder has a ﬁoneﬂ at its output. Also needed is a state diagram counter which realizes the ﬁskeletonﬂ of the state diagram. This skeletal counter 
tells us which state we are in and which 
to change to, 
given the present input signal or symbol. Thus the skeletal counter ﬁknowsﬂ that if the circuit is in state 2 and a 
MATCH signal 
is present, it should change to state 3 upon receipt of a change 
state signal. The real- ization of such a skeletal 
counter has been described 
[Bashkow, 19641. Now 
we use the outputs of the skeletal counter which will 
indicate to us the state 
we are in, the outputs of the decoder of the control cycle 
counter, and 
the input lines (Sv, So, MATCH, NO MATCH) and connect them 
as shown 
in Fig. 6. Each AND gate in this figure has 
3 inputs except those not requiring 
input line information. One input comes from 
the input set (So, S,, MATCH, etc.). The 
second input comes from the state 
diagram skeletal counter which indicates a unique 
state of the state 
dia- gram, and 
finally the third 
comes from 
the control cycle counter. The output of each AND gate is a line indicating a unique 
micro- step. The 
AND™s feed OR gates, which actually energize 
the given microstep. For 
example the output lead 
of the ﬁREADﬂ Or gate 
is connected to the ﬁREADﬂ terminal of the memory. If we assume that the 
control cycle counts in sequence 1, 2, etc., then the 
lead numbered 
1 will go to the first microstep of each sequence. 
The one numbered 
2 will go the second, etc. Therefore we see that the following microsteps should 
be executed 
in the order listed below for states 0, 
1, 2, 5 of Fig. 4. The circuit which causes the execution is shown in Fig. 6. State 0 State 1 State 2 and START VMU CHANGE STATE CIO + SCIO 0100 0000 
1001 0101 - STC STC - MAR READ CHANGE STATE and MATCH INCREASE CIO CIO + MAR READ 
380 Part 4 I The instruction-set processor level: special-function processors 
Section 4 I Processors based 
on a programming language 
Ld S" MATCH NO MA1 States ' -0 1 2 VARIABLE ~ __ 4, 17 ' - I Clock Reset to 0 I E t- t- Start VMU Change ST CIO-SCIO - 01000000100~0101-STC STC -MAR pi - Fig. 6. State diagram implementation. 

Chapter 31 1 System design of a FORTRAN machine 381 CHANGE STATE CHANGE STATE DECREASE STC 
STC + MAR READ CHANGE STATE 
State 5 and d DECREASE STC 
DECREASE STC 
DECREASE STC 
SCIO + CIO CIO + MAR READ CHANGE STATE State 2 and NO MATCH State 5 and S, In state 0 of Fig. 4 a START VMU signal takes 
it to state 1. This is accomplished by 
the top 
AND of Fig. 6. The only microstep needed is CHANGE STATE. In state 
1 of Fig. 4, the next clock 
pulse (after reaching 
state 1) causes a transition 
to state 2. In this case we need 
to save CIO contents in register SCIO, (CIO + SCIO) set the STC to 4095 (4095 
+ STC shown above 
in BCD form) 
and get the contents of the address now 
in the Symbol Table 
Counter (READ(STC)). This latter is implemented by the two 
microsteps STC -+ MAR followed by 
a READ command to the 
core memory. This transition from 1 to 2 of Fig. 4 is accomplished by the next 5 AND gates shown 
in Fig. 6. The next AND gates shown 
accom- plish the transition from state 2 to 3 if there is a MATCH. The next AND accomplishes the transition from 2 to 8 if there is NO MATCH (in this case nothing need 
be done). Finally the lowest two groups of AND gates implement 
the required microsteps as the circuit changes from state 5 to 7 if a 4-bit digit code is sensed or causes the circuit to remain in state 5 after decrementing the 
STC if an 8-bit variable code 
is read. 
Chapter 32 A microprogrammed implementation of EULER on IBM System/360 Model 301 Helmut Weber Summary An experimental processing system for 
the algorithmic language EULER has been implemented in microprogramming on an IBM System/360 Model 30 using a second Read-only Storage unit. The system consists of a microprogrammed compiler and 
a microprogrammed String Language In- terpreter, and of an 1/0 control program written in 360 machine language. 
The system is described and results are given in terms of microprogram and main storage space required and compiler and 
interpreter performance obtained. The role of microprogramming is stressed, which opens 
a new dimension in the processing of interpretive code. The structure and content of a higher level language can 
be matched 
by an appropriate interpretive 
language which 
can be executed 
efficiently by microprograms on existing computer hardware. 
Introduction Programs written in a 
procedure-oriented language are usually processed in 
two steps. They are first translated into an equivalent form which 
is more efficiently 
interpretable; then 
the translated text is interpreted (ﬁexecutedﬂ) by an interpretation 
mechanism. The translation process is a data-invariant and flow-invariant operation. It consists of two parts-an analytical part, which analyzes the higher level language text, 
and a generative part, which builds up a string of instructions that can be directly inter- 
preted by 
a machine. The analytical part of the translator depends on the higher level language; 
the generative part depends 
on a 
set of instructions interpretable by a machine. Historically 
there was only one 
set of instructions which could be interpreted effi- ciently by a machine, its ﬁmachine language.ﬂ Figure 1 outlines this scheme. 
Some of the processors of the IBM System/360 family 
are microprogrammed machines. On them the ﬁ360 machine lan- guageﬂ is interpreted not by wired-in logic 
but by an interpretive microprogram, stored in control storage, 
which in turn is inter- preted by wired-in logic. Therefore, in a 
certain sense the 360 language is not the ﬁmachine languageﬂ of these processors but the (efficiently interpretable) language in which the processors of ‚Cvmm. ACM, vol. 10, no. 9, pp. 549-558, September, 1867. the System/360 family 
are compatible. The true ﬁmachine lan- guageﬂ of these processors is their microprogram language. This 
language is on a lower level 
than the 
ﬁ360 languageﬂ; it contains the elementary operations of the machine as operators and the 
elements of the data 
flow and storage as operands. Now it is conceivable to compile a program written in 
a higher level language 
into a microprogram language string. This string 
would undoubtedly contain 
substrings which occur over and over in the same sequence. We could call these 
substrings procedures and move them out of the main string, replacing their occurrence 
by a procedure call symbol, followed 
by a parameter designator pointing to the particular procedure. Our object program then takes on the appearance of a sequence of call statements. From here it is only a final 
step to eliminate the call symbols and furnish an interpreting mechanism which 
interprets the remaining se- quence of ﬁprocedure designators.ﬂ The process just 
described will result in the definition of a string language and the 
development of a microprogrammed interpreta- tion system to interpret texts in this string language. 
The situation is similar to the System/360 case: the string language corresponds 
to the 
360 language. Programs written in a 
higher level language are compiled into string 
language text 
to be stored in main storage. The string language 
interpreter corresponds to the microprogram Fig. 1. Processing programs written in 
higher level 
languages via trans- lation to machine language. I82 
Chapter 32 I A microprogrammed implementation of EULER on IBM System/360 
Model 30 383 which interprets 360 language texts. 
It consists of a recognizing 
part to read the next consecutive string element and to branch 
to an appropriate action routine 
and of action routines 
to execute the particular procedure called 
for by the string element. 
The essential difference 
between our situation 
and the 360 case is that the 
string language reflects the features of the particular higher level language as well as 
the features of the particular 
hardware better than the 
general purpose 
360 language. What is gained by defining this string language and by provid- 
ing a microprogrammed interpreter for it? From 
the method of definition described, it can be seen that the 
elements of the string language correspond directly 
to the 
elements of the higher level language after all simplifying 
data-invariant and flow-invariant transformations have been performed. But the elements of the string language are also well-adapted to the microprogram struc- ture of the machine. Therefore, during 
the compiling process (see Fig. 2) only a minimum 
of generation is necessary to produce the string language 
text. The compiler is shorter and runs faster. But the more important aspect is that object code execution 
is also faster. The string language 
interpreter in case 2 will be coded to take care 
of all necessary operations in 
a concise form, 
whereas in 
case 1 it will be necessary to compile a whole 
sequence of machine language instructions 
for an elementary operation in 
the higher level 
language. Examples of this are the 
compilation of 360 code for an add operation in COBOL 
of two numbers with 
different scaling factors or the compilation of machine instructions 
for table lookup or search 
operations, etc. In these cases the string language interpreter of Fig. 2 will execute a function much 
faster than the machine language 
interpreter of Fig. 1 will execute the equivalent sequence 
of machine language instructions. Therefore, 
object code execution will 
be faster in scheme 2. If object code performance 
is not as much in demand as object 
storage space economy, the string language 
interpreter can also be written 
such that the string language 
is as tightly packed 
as Input Doto Ovtput Doto Intermediate --t Analyrir Higher-Level intermediate text I I"tcrprcter Fig. 2. Processing programs written 
in higher level languages via trans- lation to interpretive language. possible so that the 
translated program 
is as compact as possible and will take up less storage space than the eqnivalent machine 
language program under the scheme of Fig. 1. These ideas are applied in an experimental 
microprogram sys- tem for the higher level 
language EULER 
[Wirth and Weber, 1966a and 1966133 described below. Problem 
areas in 
this approach are indicated and some ideas 
for future development are offered. Special considerations for EULER The higher level language EULER 
[Wirth and Weber, 1966a and 1966bl is a dynamic language. This means 
that for programs written in it many things have 
to be 
done at object code 
execution time which can 
be done at compile time for other languages. EULER also contains basic functions which 
do not have compara- 
ble basic counterparts in 
the machine languages of most machines. To compile machine 
code for these dynamic properties 
and for those special functions 
would require rather lengthy sequences 
of machine language instructions, which 
would consume considerable 
object code space 
and require high object 
code execution time. Therefore, for a language like EULER, interpretation at the 
string language level by 
an interpreter into which 
the dynamic features 
and special functions are included by microcode 
will yield much higher object code economy 
and object code performance 
than compilation to machine language and interpretation of this ma- 
chine language. Three examples from EULER are given here. 1. Dynamic type handling. To a variable in EULER, constants 
of varying type can be assigned dynamically. 
For example in A t 3; . . . 
; A c 4.51,,-,5; . . . ; A c true; . . . 
; A t ' . . . '; the quantities assigned to the variable A have the types: integer, real, logical, procedure. Therefore, 
in EULER each 
quantity has to carry its 
type indicator along and each operator 
operating on a variable 
has to perform a dynamic type test. 
The adding operator 
+ for instance in A + B has to test dynamically whether both operands are of type number (integer 
or real). This type testing is done by the String Language 
Interpreter in minimum time, whereas it would require extra instructions if the program were to be 
compiled to 360 machine language. 2. Recursive procedures and dynamic storage allocation. In EULER, procedures 
can be 
called recursively, e.g., F c 'formal N; if N = 0 then 1 else N * F(N - 1)'; 
384 Part 4 1 The instruction.set processor 
level: special-function processors 
and storage is allocated dynamically, 
e.g., new N; . . .; N t 4; . . .; begin new 
A; A t list N; In order to cope with these problems the EULER execution system 
uses a run time stack. Each operation 
is accompanied by stack pointer manipulations 
which by 
the microprogram can be 
accom- plished in minimum time (in general, 
even without extra time 
because they 
are overlapped with the operation proper), whereas extra instructions 
would be required, if the program were com- piled. 3. List processing. EULER includes a 
list processing system, 
and lists are of a general 
tree structure, 
e.g., A c (3, 4, (5, 6, 7), true, '. . .'); List operators are provided 
like tail and 
cat and subscripting: 
B cA[3]; C +- B cat A; C t tail C; The string language 
interpreter handles list operations directly 
and efficiently by special 
microprograms. If the program would 
be compiled to 360 machine language, a sequence 
of instructions would be required for each list operation. EULER system on IBM System/360 Model 30 An experimental processing system 
for the EULER language has been written to demonstrate 
the validity of these ideas. 
It is a system running under 
the IBM Basic Operating System and con- 
sists of three parts: 1 A translator, written in Model 30 microcode.' This 
trans- lator is a one-pass syntax-driven 
compiler which translates EULER source 
language programs into a reverse polish 
string form. An interpreter, written 
in Model 30 microcode,l which interprets string language 
programs. An 110 Control Program written in 360 machine language.2 
This IOCP links the translator and 
interpreter to the oper- ating system and handles all 110 requests of the translator and interpreter. 2 3 Stored in the second Read-only Storage (Compatibility ROS) of Model 30. "he 360 microprograms are stored in the first Read-only Storage (360 ROS) of the Model 30. Section 4 I Processors based on 
a programming language 
The system is an experimental system. Not 
all the features of EULER are included,-only the general principles that are to be 
demonstrated. The restrictions are: 
1 Real numbers are not included; 
only integers are recog- nized. The interpreter 
microprograms for 
the operators Divide, 
Integer Divide, Remainder, 
and Exponentiation have not 
been coded. 
The type 'symbol' is not included. No garbage collector 
is provided. Therefore, 
the system comes to an error 
stop if a list processing program has used 
up all available storage space 
(32K bytes). 2 3 4 Also for reasons 
of simplicity, the system is written only for 
a 64K System/36O Model 30 and the storage areas for tables, compiled programs, stacks 
and free space 
are assigned fixed ad- dresses. The string language into 
which source programs 
are translated is defined as closely 
as possible to the interpretive language used in the definition of EULER [Wirth and Weber, 
1966a and 1966b]. The question whether this is the ideal directly 
interpretable lan- guage corresponding to the 
EULER Source language given the Model 30 hardware is left open. Also no attempt is made to 
define the string language so that it becomes relocatable for use in 
time sharing or conversational processing mode. 
The three storage areas used by 
the execution system 
are: 1 Program area 
2 Stack 3 Variable area Program area. A translated program in string 
language consists 
of a sequence 
of one-byte symbols for the operators (+, -, begin, end, c, go to, etc.). Some of the symbols have trailer bytes 
associ- ated with them; for instance, the symbol +number has three trailer bytes for a 24-bit absolute value of the integer constant. 
The symbol reference (@) has two trailer bytes, 
one containing the block number (bn), the second one the ordinal number 
(on). 
Chapter 32 I A microprogrammed implementation of EULER on ISM System/360 Model 30 385 The operators then, else, and, or and ' have two trailer bytes containing a 16-bit absolute 
program address, e.g., 1-1 Other operators with trailer 
bytes are label and the 
list-building operator. Stack. The execution time stack consists of a sequence of 32-bit words. It contains block and procedure marks to control the proc- essing of blocks and procedures and temporary values of the various types. The first 4-bit digit 
of a word in stack always is a type indicator. The format of these words is given in Fig. 3. Variable area. 
The variable area is an area (32K bytes long) 
of 32-bit words used for 
the storage of values assigned 
to variables and lists (and also for auxiliary words 
in procedure 
descriptors; see type procedure in 
Fig. 3). The format of the entries is exactly the same as the format of the stack entries (see Fig. 3), the only exception being that a mark can never occur in 
the variable area. 
Microprogramming the IBM System/360 
Model 30 [Fagg et al., 19641 Microprograms are sequences of microprogram words. A micro- program word 
is composed of 60 bits and contains various fields which control the basic functions 
in the IBM System/360 Model 30 CPU. These basic functions 
are storage control, control 
of the I Type procedure IoW/"/A Type undefined U I I I I I value: magnitude 
in hexadecimal 
(< 169 Type logical value. true 1 13 >upMfl/J false 0 Type lahel I mp: the block in ahich the label is defined. na: 1Wiit absolute 
Drogram address mark pninter, 
points to the stack 
location of the mark for [5: dp [ I~C J Type reference mp: mark poiiiter. poinis to the stack Irwation of the mark for the tilnck in a-hirh the variahle i$ defined. lot: location nf n-nrd in variahle area which rontains value assigned trr variable. mp: mark pr,inter. p<,int- in the stack lncarion of the mark for the hlnck for prrjcediire, in which the prr~edure is defined. link: pointer tr, a anrd in variahle area 
which contains additional infnrmatir,n. hn: hlock niimher r,f the t,lnrk for procedure) in which the procedure is defined pa: 16-hit program 
arldresq. where string code for procedure starts. length: numher 
of elenicrit; in list I< 163) Ioc: are stored in coiiFecutive storage locations). 
16-bir lucatinn f,f first liit element in 
variable area 
(lists Mark A mark coiisistl; of 3 rrords in stack; it is hililt each time 
a block or a procedure 
is entered. static link: 
static link to mark of embracing block. hn: hlrrck numher. dynamic link. dynamic link to mark of emhracing block (or procedure), return address: &hit 
program address to which to return upon normal exit of procedure ifor procedure marks only. 
this field is 0 for hlock marks). The last stack 
word iti a mark is a list descriptor (see type list) for the variable list (in 
a hlork mark) or the actual parameter list 
(in a procedure 
mark). I Fig. 3. Format of words in stack and variable area. 

386 Part 4 1 The instruction-set processor level: special-function processors 
Section 4 I Processors based on 
a programming language 
2 BUS I , ™ CARRY Fig. 4. Simplified data 
flow of the IBM System/360 Model 30. data flow registers and the 
Arithmetic-Logic-Unit (ALU ), micro- program sequencing and branching control, and status bit-setting control. Microprogram words are stored in a 
Card Capacitor Read-only Storage (CCROS). Fetching one 
niicroprogram word and executing it takes 750 nsec, the basic machine cycle. Figure 4 shows in simplified form the data 
flow of the IBM System/360 (IBM 2030 CPU). It consists of a core storage 
with up to 65,536 8-bit bytes 
and a local storage 
(accessible by the 
microprogrammer but not explicitly by the 360 language pro- 
grammer), a 16-bit storage address register 
(M, N), a set of 10 %bit data registers (I, J, . . . 
, R), an arithmetic-logic-unit (ALU), con- 
necting 8-bit wide buses (Z, A, B, M, N-bus), temporary registers (A, E), switches and gates. Figure 5 shows the more important fields of a microprogram word. Only 47 bits are shown. Other fields contain various parity bits and special control 
bits. The field interpretation given in 
Fig. 5 is as 
for microprogram words in the second Read-only Storage unit (Compatibility 
ROS) if the machine is equipped with the 1620 Compatibility Feature. 
The meaning of the microprogram word 
fields is explained in connection with Fig. 6 which shows the symbolic representation of a microprogram word 
together with an example as it appears on a 
microprogram documentation sheet. The fields of the microprogram word 
can be grouped in five categories: 1 2 3 ALU control fields: CA, 
CF, CB, CG, CV, CD, CC Storage control fields: CM, CU Microprogram sequencing and branching fields: CN, CH, CL Status bit setting field: CS 4 5 Constant field: CK 
Chapter 32 I A microprogrammed implementation of EULER on IBM System/360 
Model 30 387 0000 000 1 0010 001 1 0100 0101 0110 01 11 1000 1001 1010 101 1 ~ _-__ __ ____________ _______~ 0 t I 1 NOOCC~SI LS X L 1 * L L - +I LL-SS RO * Store 3f X D 2 X 
H H X And Ht-S4 * Through Thr m Or ~zcs4,~z-s~ SI * GI UV-MN S 4 ** r0,roveC O-tS4.O-SS * 5 X RzValid dR LT-MN * XL t1,50vec 1-s L ALU corry RL X * 6 S XH +c,sovec 0-so so t=c * R 7 R X XOR I-SO R2 G7 D 8 D o-sz 52 53 L ANSNZ-S2 S6 57 T X'B' T 146 0 Wrlte MS * R o 0 c + +o N&;;;? * IJ-MN Y X K 3 -. 54 55 G X3A" h 0-S6 Fig. 5. IBM System/360 Model 30 microprogram word. (Detailed explanation 
is provided in text.) 
The field inter- pretation is given for microprogram words 
in compatibility ROS if the machine is equipped with the 
1620 compati- bility feature. Fields 
marked 'I*" contain designators not explained here 
in order not 
to confuse the basic principles. 1100 ALU control fields. On the line designated "ALU" in Fig. 
6, an ALU statement can appear. It 
will specify 
an A-source and a B-source, possibly an A-source modifier 
and a B-source modifier, an operator, a destination, and possibly a carry-in control and a carry-out control. 
CA is the A-source field. 
It controls which one 
of the 10 8-bit data registers is connected to the transient A-register and therefore to the 
A-input of the ALU. CB is the B-source field. 
It controls whether the R, L, or 
D-register or the CK-field is connected to the transient B-register and therefore to 
the B-input of the ALU. If "K" (CB = 3) is speci- fied in this field, the 4-bit constant field CK is doubled up; i.e., the same four bits 
are used as the high digit 
and the 
low digit. Between the A-register and the ALU input is a straight/cross 
switch and a high/low gate. Its function 
is controlled by the CF-field. Depending on the value of this field, no input is gated into the ALU (0) or only 
the low (L) or high digit 
(H) is admitted. CF = 3 gates all eight bits 
straight through, 
whereas the codes CF = 5, 6, and 7 cross over the two digits 
of the byte before admitting the low (XL) or high digit 
(XH) or both digits (X). Between the B-register and the 
ALU input is a high/low gate and a true/complement control. The high/low gate is controlled by the CG-field in 
the same manner 
as the high/low gate in the A-input. The true/complement 
control is operated by the CV-field. It admits the true byte to the 
ALU (+) or the inverted byte (-) or controls a six-correct mechanism for decimal 
addition (@). The operator and carry controls are given by 
the CC-field. This 
field specifies binary addition without carry handling (+O), addi- X6X7 ROS ADDR 
CONSTANT ALU STORAGE STATUS SETTING BRANCHING SEQUENCE COORD- ~ COORD Format of symbolic representation 01 - 1101 RfKH-DC WRITE HZ -54, LZ-S5 G4.G5 c4 c4 ~ 
~ 
CD Example Fig. 6. Symbolic representation of a 
System/360 Model 30 micro- program word. 
388 Part 4 I The instruction-set processor level: special-function processors 
tion with injection 
of a 1 (+ 1) (for instance, to simulate subtraction 
in connection with 
the B-input inverter), addition with saving 
the carry in bit 3 of register S (+O,Save C, and +l,Save C), and 
addition using an old 
carry stored 
in bit 3 of register S and saving the new carry in 
this same bit (+C,Save C). Other codes specify 
logical operations (AND, OR, XOR). The CD-field specifies 
into which 
register the result of the ALU operation is gated. Any one of the 10 data registers can be speci- 
fied. Z means that the 
ALU output is gated nowhere 
and will be lost. Storage control 
fields. On the 
line designated ﬁstorageﬂ in Figure 
6, a storage 
statement can appear. It 
will specify 
whether this microcycle is a ready cycle, 
a write cycle, a store 
cycle or 
a no-storage access cycle, 
and from where the storage address is supplied (CM-field) and whether storage access is to main storage 
or local storage (CU-field). Note that a full storage cycle (1.5 psec) corresponds to two read-only storage cycles (750 nsec). The codes CM = 3, 4, or 5 specify read cycles. The addresses are supplied from the register pairs 
I], UV, and LT, respectively. A read cycle 
reads one byte of data from core storage 
into the 
storage data register R. 
A write cycle regenerates 
the data 
from the storage data regis- ter R at the 
address supplied in 
the last read cycle. A store cycle acts exactly as 
a write cycle except 
that it inhibits in the read cycle immediately preceding it 
the insertion of the data byte 
from storage into 
the R-register. The CU-field specifies 
whether storage access should 
be to main storage (MS) or to a 
local storage of 256 bytes not explicitly ad- dressable by 
the 360 language programmer. Microprogram sequencing and brunching. Each microprogram 
word is stored at a unique address in 
ROS. A 13-bit ROS address register (W3. . . W7, X0. . . X7) holds the address of the word being executed. For 
the symbolic representation of a microprogram (Fig. 6) the ROS address is given in hexadecimal in the upper 
right corner, and 
the last two bits 
of this address are repeated in binary on the upper margin. After execution 
of a microprogram step, the next sequential word will not 
be executed. Instead 
the address of the next word 
to be executed 
is derived as follows. The high five hits (W) remain the same, unless 
they are changed by a special command in 
the microword, not explained here (so-called module 
switching). The next six bits (XO. . .X5) are supplied 
from the CN-field (written in hexadecimal in the symbolic representation of Fig. 6). The low two bits 
are set according to 
conditions specified 
in the CH 
and CL fields. X6 is set according 
to the condition specified by CH. Section 4 I Processors based on 
a programming language 
For instance, 
if CH = 8, then the bit R2 is transferred to 
X6; if CH = 6, then X6 is set to 
one if in the last ALU operation a carry 
had occurred. It is set to zero if no carry had occurred. X7 is controlled by CL. 
If, for instance, CL = 0, then X7 is set to 
zero; if X7 = 5, then X7 is set to one if both digits in R are valid decimal digits (Le., RO. . .R3 5 9 and R4. . .R7 5 9), X7 is set to zero if either digit in R is not a 
valid decimal digit 
(Le., RO. . .R3 > 9 or R4. . . R7 > 9). This microprogram 
sequencing scheme 
allows a four-way branch after the execution of each microprogram word. 
Status bit setting. 
The CS-field allows the unconditional or condi- 
tional setting of certain status bits to be specified, combined in 
Register S. If, for instance, CS = 3, then S4 is set to one if the result of the ALU operation performed in 
this microprogram cycle 
shows a zero 
in the high digit (Le., ZO = Z1 = 22 = 23 = 0); S4 is set to zero otherwise. 
At the same time, S5 is set to one if the result of the ALU operation shows a zero 
in the low digit (Le., 24 = Z5 = Z6 = 27 = 0); S5 is set to zero otherwise. 
If CS = 9, then S2 is set to one if the result of the ALU operation is not zero (i.e., 
at least one of the bits ZO. . .Z7 is equal to 
1). If the result of the ALU operation is zero, then S2 is not changed. Constuntfield. The 4-bit CK-field is used for various purposes. 
One instance explained in the ALU statement is to supply a constant 
B-source for an 
ALU operation. Other examples not explained 
here any further are the addressing of a few specific 
scratchpad local storage locations, module switching 
(replacement of the high part W of the ROS address), and the control of certain special functions. Symbolic representation 
of microprograms. Microprograms 
are symbolically represented as a network 
of boxes (Fig. 6) each representing a microword, 
connected by nets indicating 
the pos- sible branching ways. Figure 7 gives an example of a microprogram (to be 
explained in the next section). There exist programming systems to aid 
in the development of microprograms. They contain 
symbolic translators to translate the contents of a box according to Fig. 6 into the 
contents of the actual fields of the microprogram word according 
to Fig. 5. A drawing program generates documen- 
tation (Fig. 7 is 
drawn with 
such a program). 
These systems usually 
also contain programs for 
simulation and generation 
of the actual ROS cards. String language interpreter for EULER The string language interpreter for EULER 
is entirely written in Model 30 microcode. It consists of a few microprogram steps 
to read the next sequential symbol from 
the program string and to 
Chapter 32 I A microprogrammed implementation 
of EULER on IBM System1360 Model 30 389 Fig. 7. Microprogram for the operators AND, OR, and THEN. do a function branch 
on the symbol and of a group of micropro- gram routines which 
perform the necessary operations for the program byte read. These 
routines also take care 
of dynamic type testing and stack pointer manipulations. The routines are equiva- 
lent to the routines described in 
the definition of the string lan- guage for 
EULER [Wirth and Weber, 1966a and 1966b]. Figure 7 shows, as 
an example, the microprogram to interpret 
the program string symbols and (internal representation X'52''), or X'50' and then X'53'. These operators test 
if the highest entry in the stack is a value 
of type logical. The logical operators in EULER work in the FORTRAN sense, not in 
the ALGOL sense: if after the evaluation of the first operand the result is determined (false for and, true 
for or), then the second operand is not evalu- ated but 
skipped over. If an and operator finds the value false, then a branch occurs to the 
program address 
given in the two 'X 'mi' represents the hexadecimal number composed of the digits n (n = 0,. , , 
,9, A,. . , , F). trailer bytes. If an and finds the value true, then it deletes 
this value from the stack and proceeds to the next symbol in 
the pro- gram string 
(to evaluate the second operand of and). Similarly if an or operator finds the value true, then a branch occurs to the 
program address given 
in the two trailer 
bytes. If an or finds the value false, then it deletes 
this value 
from the stack and proceeds to the next symbol 
in the program string. 
The then operator is a conditional branch 
code: it deletes the logical value 
from the stack. If this value 
was false, then a branch is taken to the program address given in 
the two trailer bytes. If this value 
was true, then the next symbol in the program string is executed. The pointer to the symbol in 
the program string 
(the instruction counter) is located in 
the functionally associated pair of registers I and J in the Model 30. 
The pointer to the left-most byte of the highest entry in the stack (the stack pointer) 
is located in the two registers U and V in the Model 30. In the following the individual steps in 
this microprogram 
are explained in more detail. 
390 Part 4 I The instruction-set processor 
level: special-function processors 
Section 4 1 Processors based on 
a programming language 
Location Location 
Address in Figure Description Address in Figure Description 1161: 11 17: 1171: 11 5D: 11C4: c1: The instruction counter 
IJ addresses main stor- 
age. The addressed 
byte in main storage is read out into the 
storage data register 
R. The instruction counter 
is updated by adding 1 to register J. A possible carry is saved to be added 
to 1. The operator has 
been read out from main storage into R. It is also transferred (through the ALU)to register 
G. A four-way branch 
occurs on the two 
highest bits 
RO and R1 of the oper- ator. For the operators 52, 53, and 
50 this branch goes to ROS word 1171, 
whereas other operators cause a branch to 1170, 1172, 
or 1173, indicated by the three 
lines not continued. To complete the updating of the instruction counter, the carry from 1161 
is added 
into I. The first byte of the highest entry of the stack is addressed by UV and read out into 
R. A fur- ther four-way branch on the 
operator is made 
(G2, G3). For 
our operators the branch 
goes to 115D. The high order byte 
of the highest stack 
entry has been read out of storage into R. It contains the type 
of entry in the 
high digit and 
if this 
type was logical then it contains the value true (1) or false 
(0) in the second digit. This 
byte is 
tested by adding X'DO' to it and observing the 
result, ignoring the carry. S4 
is set to 1 when the type 
was 3 (logical) otherwise 
to 0. S5 is set to 1 when the low digit of this byte 
was 0 (value false), S5 is set to 0 when the low digit of this byte 
was 1 (value true). Another 
four- way branch occurs on the bits G4 and 
G5 of 
the operator. If the 
operator is 
50(or), 51 (cannot occur), 52 (and), or 53(then), then a branch 
to 11C4 occurs. 
The next byte 
is read from the program string, it is the high byte 
of the two-byte program 
ad- dress trailing the operator. The instruction 
counter is updated 
again by adding a 1 to J, saving a 
possible carry. Another four-way branch 
occurs on the bit G6 of the operator and the value of 
the stack entry. 
If the operator was and or 
then (G6 = 1) and the value was false (S5 = l), then branching to llCB occurs; if the operator was or (G6 = 0) and the 
value was true (S5 = O), then branching 
to llC8 occurs. If the operator was or (G6 = 0) and the value was false (S5 = l), then branching c2: c3: c4: L4: 11CB: G5: llC3, J6, J7: 11 1E: llC3, J6. L7: 111F: llCE, N8, N9: 1144: llC8: J5: llC9: N5: 11CA: 45: to 11C9 occurs. 
If the 
operator was and or then (G6 = 1) and the value was true (S5 = 0), then branching to 11CA occurs. This word 
is executed 
for the operators and 
and then when the value was false. 
Here the type test is made. 
If the 
type was not logical (S4 = 0). then a branch to llCl occurs. If the type was correct, then the microprogram proceeds to fetching the trailing 
program address (two 
bytes) to store it as the new instruction counter in IJ. This is done for 
the and operator 
(G7 = 0) in this word and 
the following two 
words llC3 and 11 1E; for the then operator (G7 = 1) it is done in this word and the words 11C3 and 
11 1F. The two bytes trailing of the operators and or or are stored as the new instruction counter IJ. The operation is completed. The microprogram 
branches back to 1161 to 
read out the next operator. The two bytes trailing of the operator then are stored as the new instruction counter in IJ. The carry-saving bit S3 is forced to zero. The stackpointer is decremented by four (the operator '-' means complement add) which 
in effect deletes 
the highest entry from the stack. Observe that when these two words are 
entered from lllF (then operator with 
value false) the microprogram will not go through 
1145 be- cause we have forced S3 to zero in l l l 
F. The operation is 
completed, and the microprogram 
branches back 
to 1161 to read out the next 
operator. This word 
is executed for the operator or when the value was true. Similarly 
as in llCB, the typetest is taken. For types not logical a branch 
to llCl occurs. If the type was correct, then 
the microprogram 
proceeds to fetching the trailing program 
address (two bytes) to store it as the new instruction counter in IJ (words llC3, 111E). This word 
is executed for the 
operator or when the value was false. A 
typetest is made. 
If the type was correct, then the trailing 
program ad- dress is skipped 
and IJ 
is updated by 1 twice in 11C4, 11C9 (possible carries 
out of J handled 
in 11CF or 
1145). The stackpointer is decre- mented by four in llCE, 1144. This word 
is executed 
for the operators 
and and then when the value was true. A typetest 
is made. If the type 
was correct then the 
trailing 
Chapter 32 I A microprogrammed implementation 
of EULER on IBM System/360 Model 30 391 Location Address in Figure Description address is skipped, IJ is updated by 1 twice in llC4, llCA (possible carries out 
of J handled in llCF or 1145). The stackpointer is decre- mented by four in llCE, 1144. 11C1, G6,L6,N6 These words are executed when a typetest occurs. An error code 
01 is set up in L and a branch occurs to the error 
routine not drawn 
here. 11% 11CD: It can be seen from 
Fig. 7 that the execution times 
of the microprograms including the readout of the operator (I-Cycle) are the following: and or then 6 pet' (8 microprogram steps) 6 psec (8 microprogram steps) 6 psec for value true (8 microprogram steps) 7.5 pec for value false (10 microprogram steps) In order to compare 
this with a hypothetical EULER system for System/360 
language, let us assume that the 
compiler produces 
in-line code (which probably will give 
the highest performance although it will be very wasteful with respect to storage space). Then a reasonable sequence 
for and might be: CLI 0 (STACK), LOGFALSE BE ANDFALSE CLI 0 (STACK), LOGTRUE BNE TYPEERR SH STACK, = ™4™ Timing: true: YO psec; false: 32 psec. This comparison seems to indicate 
that the microprogram in- terpreter is about an order 
of magnitude faster than the 
equivalent program in 360 language. However, this comparison will only 
yield such a high factor 
for functions of EULER which do not have 
simple System/360 language counterparts (as for instance 
the list-operators, begin-, end-, 
and procedure-call-operator) or where the overhead for dynamic testing and stackpointer manipulation 
is heavy as in the above example of the logical operations. For functions which do have System/360 language counterparts and which are slower so that the 
overhead is relatively lighter 
as, for instance, arithmetic operations (especially 
for real 
numbers), the microprogrammed interprete- will still be faster than the System/ 360 language program, 
but not by a factor of 10. ‚The cases where carries occur in the IJ and UV updating are disregarded for timing purposes. 
The total ROS space requirement 
for the String Language In- terpreter is: Coded routines 1000 microwords Routines for real number 500 microwords (estimated) Divide, Exponentiation, 
etc. 400 microwords (estimated) Garbage collector ~ 600 microwords (estimated) handling 2500 microwords EULER compiler The translator to translate 
EULER source language 
into the Re- verse Polish 
String Language is a one-pass, syntax-driven 
compiler. The syntax of the language and the 
precedence functions F and G over the terminal and nonterminal symbols are stored in table 
form in Model 30 main storage. There is also main storage space reserved for translation tables 
for character delimiters and word delimiters and for a compile time stack, a name table, and, 
of course, for the compiled code. All these areas 
are at fixed storage locations because 
of the experimental nature of the system. The microprogram consists of the following parts: A routine reads the next input character 
from the input buffer to translate it to 
a 1-byte internal format, 
if it is a delimiter, or to collect it into a name buffer if it is part of an identifier, or to convert it to 
hexadecimal if it is part of a numeric constant 
and to collect the number into a buffer. This ﬁprescanﬂ requires 100 + microwords. As soon as 
an input unit 
is collected (delimiter, 
identifier, number) the main parsing loop 
is entered which makes use 
of the precedence tables and the 
syntax table in 
main stor- age. This syntactic analyzer loop requires 100- micro- words. When the parsing loop identifies a 
syntactic unit to 
be reduced, it 
calls the appropriate 
generation routine which performs essentially 
the functions described 
as the semantic interpretation rules in the EULER definition. The micro- program space required 
for these programs amounts to approximately 250 ROS words. If a syntactic error is detected, the system signals 
an error 
and does not try to continue with the compilation process. Though this procedure is totally inadequate for a 
practically useful system, it was deemed sufficient to prove the essential point. For this 
minimum error analysis and for linkage 
to the 360 microprograms (IOCP), approximately 60 micro- words are required. 
392 Part 4 I The instruction-set processor 
level: special-function processors 
The total compiler microprogram space is therefore approxi- 
mately 500 ROS words. The total main storage space required 
is approximately 1200 bytes. The speed of this compiler 
is limited by the 
speed of the card- reader of the system (1000 cards/minute). This excellent per- 
formance has 
three main reasons: (1) EULER as a simple prece- dence language is a language extremely 
easy to compile. (2) The functions of a compiler are mainly of a table lookup and bit and byte-testing type. Microprogramming is extremely well-suited for 
these kinds of operations. (3) Since the target language is String Code and 
not, for example, 360 Machine Language, 
the generative part of the compiler is relatively short. 
It is very difficult to assess the individual contributions 
of these three main reasons 
to the high compiler performance. Therefore, 
it is not possible at this stage to make a statement as to whether the nature of the language EULER or the fact that the compiler is microprogrammed is the dominant factor. Development of the microprogram Since there is no higher level language 
to express microprogram procedures and no compiler to compile microcode, the micropro- grams were written in the symbolic language explained 
in Fig. 6. Actually the process was a 
hand translation of the algorithms in the EULER definition to the symbolic microprogram language. The microprograms were translated 
into actual microcode and simulated before they were put on the System/360 Model 
30 by means of a general microprogram development system. Outlook and general discussion It is hoped that the 
development of this experimental system for 
EULER shows that with the help 
of microprogramming we can create systems for 
higher level languages 
or special applications, 
Section 4 I Processors based on 
a programming language 
which utilize existing computer hardware to 
a much higher degree than conventional programming systems. Among the thoughts which are raised by this scheme are 
the following: There should be an investigation to determine the ideal directly interpretable 
languages which correspond to higher 
level languages. Although 
several attempts have been made 
to define string languages for interpretive systems (for in- 
stance in 
Wirth and Weber [1966a and 1966bI and Mel- bourne and Pugmire [1965]), to the author™s knowledge no work has 
been published which attacks this question 
in a 
general and theoretically founded manner. A proliferation of interpretive languages and the develop- ment of microprogrammed interpreters can be 
justified when better tools are developed to reduce 
the cost of microprogramming. It is necessary that we be able to ex- press microprogramming concepts (and also machine design concepts) in a 
higher level language 
form and that we 
develop compilers which translate the microprograms from higher level language form to actual 
microcode. Also, good microprogram simulation and debugging tools are called for. The whole relationship between programming, micropro- gramming, and machine design should 
be viewed with a common denominator: how should the tradeoffs be made such that the 
ultimate goal can be reached more 
effec- tively, . . . 
how to solve a 
user™s problem? Green [1966] offers some thinking in this direction 
but the state of the art has to progress further before we will have a complete understanding of what these relationships 
and tradeoffs are. References WebeH67; FaggP64; 
GreeJ66; HainL65; MelbA65; WirtNBBa, 66b; FOR- TRAN Specifications and 
Operating Procedures, IBM1401, IBM Systems Ref. Lib. ‚224.1455-2. 
Part 5 The PMS level This part presents the PMS structure dimension 
of the computer space. The sections are arranged 
in order of increasing organizational 
structure complexity. The sections 
are as follows; 1 Pc; 1 Pc with multiple 
Pio; multiprocessing with 
n Pc; parallel processing with n Pc; computers which are networks; and networks of computers. In Chap. 37 Lehman defines the terms multiprogramming, 
multiprocessing, and parallel processing. 393 

Section 1 Computers with one central processor The computers with one 
Pc and no 
Pio™s control T and Ms in either of two ways. First, the Pc contains the 
K for T and Ms; second, a separate K controls a data 
transmission while Pc initializes the K. In the latter case, a K is like a P 
where each 
instruction is received from Pc instead of being fetched auto- matically by K itself. processing concurrency is 
difficult to achieve. The structure is first discussed in Part 2, Sec. 1, page 90. The SDS 910-9300 series The SDS 910-9300 series is 
presented in Chap. 42 and is dis- cussed in Part 6, Sec. 2, page 542. The input/output and the 
interrupt system are especially 
interesting. The Whirlwind I computer Whirlwind (Chap. 6) controls data transmissions between Ms or T 
and Mp 
by using Pc. Thus, arithmetic and 
input/output 395 
Section 2 Computers with one central 
processor and multiple input 
/output processors The computer structures discussed in this section are manu- factured mainly 
by IBM. The reason 
for this bias toward IBM 
is that only fairly elaborate or 
very specialized structures have Pio™s; computers of other manufacturers which have Pio™s 
tend to have also 
the more general multiprocessing capability1 
that would place them in Sec. 3. The DEC PDP-8 The PDP-8 is presented in Chap. 5, and its 338 P.display ap- pears in Chap. 25. Discussions are given 
in Part 2, Sec. 1 and Part 4, Sec. 1, respectively. For this section, the reader should look at the 
methods for transmitting data between Ms or T 
and Mp. Three 
methods are used: Pi0 
or P.display is used to control T.displays (Chap. 25); 
Pc directly transmits a word 
to the 
buffer of a K for low-data-rate 
devices, here 
a K may request data, using the program interrupt; and 
a K transmits data directly 
to Mp. The IBM 1800 Chapter 33 describes the 1Pc-9Pio IBM 1800 computer. 
There are five Pi0 types, depending on the 
components they control. Although we classify them as Pio™s, they are barely processors 
since the instruction counter has a very restricted behavior. Unless the data channel 
has ﬁdata chainingﬂ capability (in effect a 
jump instruction), 
it is not a processor. The IBM 7094 II The IBM 7094 
II computer is discussed in Part 6, 
Sec. 1, page 515; its description appears in Chap. 41. The earlier 
709 was about the first 
computer to use independent Pio™s. UNIVAC (Chap. 8) has 
a very extensive K for data transmission con- 
current with 
processing, whereas the 701 and 704 both 
required Pc to control each data word transmitted. The Pio™s 
of the 7094 II might be looked at as an overreaction or 
overdesign inspired by the 701-704. ‚For example, the CDC-3600 [Casale, 19623, and the 
SDS Sigma 7 [Mendelson and England, 19661. The structure of System/360, Part I-outline 
of the logical structure The structure of 
the 360 is presented 
in Part 6, Sec. 3. A 
dis- cussion of an alternative implementation of 
the 360 
by the authors of this book, using multiprocessors, is given (page 585). 
Chapter 43 gives an overview of the ISP, and Chap. 44 presents the implementations of various 360 models. The implementa- tions of physical processors 
to give multiple logical processors 
using microprogramming are interesting. IBM is rather conserv- ative in regard to providing structures convenient 
for multi- programming; and a multiprocessing 
design appears 
too com- plex for them to attempt 
outside a 
research environment. The engineering design of the Stretch computer Stretch (also known as Model 7030) and the 
UNIVAC LARC [Eckert, et 
al., 19591 are perhaps 
the first computers with the principal design goal of maximizing numerical 
computing power. Stretch, aptly named because of its influence on the technology (and on the IBM organization), 
was initiated by the Atomic Energy Commission at Los Alamos. It was designed to interpret large-scale scientific programs for 
nuclear engineer- ing. Like a 
number of other high-risk major 
developmental efforts in the computer field, Stretch 
was not outstandingly successful as a computer 
system. Only 
a few(5 - 10) were built at a cost substantially 
exceeding their contract 
price and with 
performance only 
modestly better 
than the art at the 
time of their production. 
However, again in common with 
other similar 
efforts, they had a 
substantial positive effect 
on the 
state of 
the art. In the Stretch case, in particular, the 2.18-microsecond Mp core technology 
developed for Stretch was transferred to the 7090. In fact, this was a major 
contribution to why Stretch was only modestly better 
than 7090. The design 
goal was per- formance 100 times an IBM 704. The computer is described at a high 
level in Chap. 34. Buchholz™s book 
on Project Stretch [Buchholz, 19621 is outstanding as a text on computer struc- tures and as a description 
of Stretch. It should be read 
by all computer designers. Computers built to maximize numerical computing power also include, besides the UNIVAC LARC for the Lawrence Radia- 396 
Section 2 1 Computers with one central processor and multiple 
input/output processors 397 tion Laboratory at Livermore, the Control Data 6600 (Chap. 39), and the 
IBM System/360, 
Models 91 and 85. Stretch derives its power through: 1 Compound and complex ISP instructions 2 A PMS structure with 
Mp(2.18 ps/w),Pc(0.25 
- 1 ps/w), Pio™s, and a satisfactory switch 
between P™s and Mp 
3 Many data-types 
4 Parallelism within the Pc, involving concurrent interpre- 
tation of the instruction stream 
using the 
ﬂInstruction look-ahead™™ mechanism 
The last of these, internal Pc parallelism, is 
the most novel. Stretch was possibly the earliest computer to make use of it; each of the 
other ﬁmaximumﬂ power C™s 
listed above also uses some version of instruction look-ahead, for each of these ﬁmaximumﬂ systems is faced with how to obtain computing power that goes beyond the basic logic 
and memory technology available at the time the 
system is 
designed. The 
conclusion, reached in all these cases, is to move toward internal paral- lelism. In Stretch the instruction look-ahead mechanism 
fetches the next several instructions and partially interprets 
each future instruction. The mechanism is elaborate compared 
with the 
straightforward instruction 
stack in the CDC 6600 (Chap. 39, page 489). The Stretch look-ahead complexity stems 
from par- tially interpreting instructions 
which may later have to be un- done. Stretch uses a basic Mp(core; 
16384 w; (64 + 8 parity) b/w; tc:2.18 ps). Sixteen Mp™s can be connected to the 
P™s via the S(™Memory Bus; 
time multiplexed). The 8 parity bits 
are used to give single-error correction 
and double-error detection, 
which is a very substantial amount 
of error protection compared 
with standard design practice. This is the memory that was incor- porated in the IBM 
7090 and 
became operational even before Stretch was delivered. Thus, 
as is often the 
case with large development efforts, the by-products are as important as the main product. There is a single well-designed physical 
Pio, called the Ex- change, consisting of 
several logical Pio™s. Its ability to have the state of all 
the logical Pio™s 
accessible in Mp is useful and important. This design 
seems better than the data channels 
in the IBM 
709-7094 series. It is almost a prototype for 
the IBM System/360 Pio™s. The Stretch word 
length is 64 bits. It has operations 
on the following data-types: binary integers, decimal integers, address 
integers, variable-length integers, 
boolean vectors, 
single and double floating point. 
The length of 
thevariable integer is 
speci- fied by parameters in the instruction. Noisy-mode floating-point data provide a 
method of introducing a roundoff error in the least significant bit under program control. 
Thus a problem can 
be run in conventional and noisy modes and the results com- pared. An instruction is either 32 or 64 bits. The ISP processor state has an instruction counter, a 
dou- ble-length accumulator, 15 index registers, 
about 6 
registers, and about 100 miscellaneous bits. Computing 
power is obtained 
by having an instruction set with complex instructions. Hence, there is 
an instruction for almost 
every possible operation, 
though inverse subtract and 
inverse divide instructions are lacking. However, there is a ﬁmultiply and addﬂ instruction. Stretch has the complete set of 16 operators for boolean vec- tors. Compound instructions, formed 
from a sequence of sim- pler instructions, also increase power. 
These instructions specify the array element 
to be accessed, 
an operation on the element, and a calculation 
to get the next element, 
in a single instruction. Notice that several of these instructions are oriented toward operations on arrays (i.e., matrices), which are 
the type of numerical-analysis tasks for which the system was built. Multiprogramming was done with Stretch [Codd et al., 19591 and undoubtedly had some influence within 
IBM. Stretch has a pair 
of bounds registers to relocate and protect a single 
program. The interrupt scheme for Stretch [Brooks, 
1957al was better than that 
of existing IBM 
computers, though 
it is not described in Chap. 34. The importance of 
Stretch lies in the 
by-products it inspired and its influence on IBM, encouraging a concern 
with hardware project management. 
The elaborate ISP and the complex im- plementation of Stretch 
may not have been worth the effort, 
especially when 
one compares 
this computer with 
the later, larger but elegant CDC 6600. It is, however, interesting to note that Stretch was used as 
a central component 
in an early spe- 
cialized multiprocessor system called 
the IBM Harvest [Herwitz and Pomerene, 19601, which provides extremely 
powerful data- processing capabilities. 
PILOT, the NBS multicomputer system The National Bureau of Standards™ PILOT computer (Chap. 35) was first described in 1959. At that time it was a multiple computer; by our criteria, we classify it as a multiple-processor 
computer, as shown by its PMS structure (Fig. 1). However, 
398 Part 5 1 The PMS level Section 2 1 Computers with 
one central processor and multiple 
input/output processors I Mp(l ps/w; 60 w; 16 b/w) -Pc('Secondary Computer)-T.console - Pc('Primary Computer)-T.console - Pio('Third Computer) Mshagnetic tape) - T(reader) + Fig. 1. National Bureau of Standards' PILOT computer PMS diagram. unlike present multiprocessors 
with several identical proces- sors, each PILOT 
processor is different. PILOT is a good example of an early attempt to use multi- processors; successors look 
little like it. It has one 
of the best analytical discussions of 
any computer [Leiner et al., 19571. With this machine there 
was an attempt to 
resolve the contro- 
versy between the short-word EDSAC (17 bits) and the long- word Institute for 
Advanced Studies 
computers (40 bits) by providing a processor 
and memory (i.e., computers) for each problem. Only the first 
computer had substantial Mp, and the 
other computers, or 
processors, could be concerned only 
with the first 
computer. The third computer was introduced to proc- ess devices 
such as Ms(magnetic tape) and used a plugboard program memory. 
The idea of an independent processor (IBM 7094) or computer (CDC 6600) for input/output processing is 
used now, 
though it is doubtful that PILOT inspired these de- signs. The capacitor-diode store is novel and daring for 
the tech- nology. Two- and three-address computers 
are used in the pri- 
mary and 
secondary computers. The secondary 
computer, with 
16-bit words, is not very useful; its memory is very limited, and 
it is essentially used only for address calculations. The book- keeping operation for a three-address computer could 
easily keep a small processor busy. 
Chapter 33 The IBM 1800 Introduction This third-generation computer is constructed with hybrid-circuit technology (semiconductors bonded 
to ceramic substrates) 
known as SLT (Solid Logic Technology). 
It has a core primary 
memory. The 1800 is designed for process 
control and real-time applica- 
tions. It is nearly identical to 
the IBM 1130, which 
is designed for small-scale, 
general-purpose, and scientific calculation appli- cations. The two 
C™s perform about the same for 
computation bound problems. The 1130 and 1800 are not program compatible with the ﬁuniversalﬂ IBM System/360 series, though introduced at about the 
same time. However, 
the 1800 uses terminals and secondary memories similar 
or identical to the 
System/360. These are organized about 
the standard 
IBM System/360 8-bit byte. 
Thus their common information media provide 
a link between the two. Hence an 1800 
is sometimes connected to the System/360 as a preprocessor. The relative performance 
of the IBM 1130, 1800, 
and the IBM System/360 can be seen on page 586. The 1800 has 
a better cost/performance ratio than 
a System/360, Model 40 
and has the performance of a Model 30. 
From now on we will refer 
only to the IBM 1800, although much applies to the IBM 1130. The 1800™s interface facilities include a large number 
of T™s which can connect to different physical processes; 
a multiple priority interrupt facility with 
fast response; multiple Pio™s which can transfer information 
at high data rates;™ and a complete 
instruction set 
for real-time, nonarithmetic processing. We include the 1800 because it 
is a typical, 16-bit, real-time, 
process control computer. The ISP is the most straightforward of the IBM computers in the book (and perhaps the nicest). The several different Pio™s and their 
implementations are unusual and should be carefully studied. Important aspects of the 1800 include the PMS structure as it links to real-time 
processes, e.g., analog processes; the straightforward Pc 
ISP (Appendix 1 of this chapter); the specialized Pio™s for real-time T™s; the Pc 
implementation; and 
the Pi0 implementation. The chapter 
is written to expose and explain these 
aspects.2 By comparing the 1800 with Whirlwind, an 
evolutionary pro- gression can be seen. Their ISP™s are similar but, because of better lAkhoigh we refer to the data channels as Pio™s, they have a very limited ISP for a Pio; in fact, they might better be called Ks. ‚Some of the material in the chapter ha5 been abstracted from the JBM 1800 Functional Characteristics 
Manual. technology, the 1800 shows 
an increase in capability. 
The 1800 Pc has a medium-sized state (ISP has 
six registers) including three index registers. The implementation is not elegant; a 
single register 
array and 
adder would provide the basis for 
a straightforward 
Pc implementation. The 1800 has 
features which facilitate higher 
information processing rates compared with Whirlwind. 
The major change between Whirlwind 
and the 1800 machines 
was brought about by the decreasing cost of registers and primary 
memory. In the 1800, all K‚s have independent memory (usually 
1 - 2 words or 
characters) so that concurrent operation 
of almost all the T and Ms via their K™s is possible. In contrast, Whirlwind 
has only a single, shared register in Pc, and 
only one device can 
operate at a time. 
Lower hardware costs allow 
multiple Pio™s in the 1800. The Pio™s represent an unusual approach 
to information processing 
in this period. The Pio™s which process 
standard disk, magnetic tape, and card 
reader are conventional, but the Pio™s for analog and 
process signals 
are novel and interesting. 
The latter 
Pio™s are the 
most unusual 
part of the 1800, and they allow independent pro- grams in each Pi0 to do some very 
trivial processing tasks such 
as alarm-condition monitoring independent of Pc. However, 
the Pio™s are limited; 
for example, 
it is difficult to transmit or receive 
a data block between Ms and Mp (using a Pio) without surrounding 
the data 
block with Pi0 control words (thereby transmitting 
the control words). The interrupt system is typical of second- and third-generation 
computers and 
is comparable to the SDS 900 series 
(Chap. 42). In later computers 
interrupt conditions are used to determine a fixed address to which the processor interrupts. There are 
generally many conditions (100 
to 1,000), 
but only a few discrete levels (8 to 20). The 1800 depends on program polling within a discrete 
interrupt level; each level 
has a unique, 
fixed address. A principal ISP design problem 
is the addressing of the 65,536- word Mp. 
Thus, a 
16-bit number has to be 
generated within Pc 
for an address. In 
this regard the 1800 behaves like 
the 12-bit machines which have 
to address a 212 (4,096) word memory, 
and the modes or methods 
the 1800 uses for addressing are reasonable. It should be noted that it is relatively difficult to write programs which do not modify themselves. 
For example, the instruction, Store Status, is changed by its execution. 399 
400 Part 5 1 The PMS level Section 2 I Computers with one central 
processor and multiple 
input/output processors A peculiar feature of the 1800 is its storage protection (see page 
408). This 
feature should provide program relocation capability 
in addition 
to protection, but it does not. PMS structure A simplified picture of the IBM 1800 structure is given in Fig. 1, without Pio('Data Channel)'s 
and K('Device Adapter)'s. Each T and 
Ms have a 
K which connects Pc's In and Out 
Bus, the S('Pc to K). Some K's attach to 
Pio's and some directly to Pc. Information can be transferred 
between Mp and K via Pi0 at rates up to 0.5 megawordis or 8 megabitsis. The IBM Configurator (Fig. 2) gives the restrictions on the possible structures, together with 
minute L details. It is presented as an alternative 
to the PMS structure (Fig. 1). The Configurator is intended to show the "permissible structures" but does not show the logical or physical structure. The PMS diagram (Fig. 
3) alternatively shows the physical-logical hardware structure 
and performance parameters. 
lt should be noted that a PMS diagram with the information of the computer component Configurator (Fig. 2) would require slightly more 
de- tails (and space). The central processor'-primary memory The IBM 1800 is a fixed-word-length, binary computer with 4, 8, 16, or 32-kword memories 
of 16 + 1 + 1 bits, and a 
memory cycle 
time of 2 or 4 microseconds. Of the 18 bits 1 bit is used as 
a parity 
check (P bit) and 1 bit is used for storage 
protection (S bit). The 
Pc instruction set operates 
on 16-bit and 32-bit words. 
Indirect addressing and three 
index registers 
are used in address modifica- 
tion. The Pc has a 24-level interrupt system, three interval 
timers, and a console. The Pc interrupt 
is a forced 
branch (jump) in 
the normal program sequence based upon external 
or internal Pc conditions. The devices and conditions that cause interrupts are 
hardwired in fixed priority levels. An interrupt request is not honored 
while the level of the request itself or any higher 
level is being serviced, or if the level requested is masked. Examples 
of interrupt condi- tions are: 1 An external process condition that requires attention is detected. 'IBM name: the Processor-Controller or PC. PROCESSOR- 1 PROCESS I/O CONTROLLER I pn.log Input Points DATA PROCEjSING I/O Fig. 1. IBM 1800 data acquisition 
and control system. (Courtesy 
of International Business Machines Corporation.) 
Chapter 33 1 The IBM 1800 401 t' MWS MPX, R I DIGITAL INPUTS I Fig. 2. IBM 1800 data-acquisition and control-system configurator. 
(Courtesy of International Business Machines Corporation.) 
402 Part 5 I The PMS level Section 2 I Computers with one central processor and multiple 
input/output processors I A I I I I I I I I I I I I I I I I I I I I I I + I I I I I I I I I I I t i 
Chapter 33 1 The IBM 1800 403 
T ~ ANALOG INPUTS 
404 Part 5 I The PMS level Section 2 I Computers with one central 
processor and multiple 
input/output processors T.console - I K (t i me)+ T(#l : typewriter)- T(#2:4: page: printer)+ T(#5; typewriter)- "ST: T(#6:8: paqe; printer)+ Mpl PC"- s? K ~ T(incrementa1 point plot)' K T(paper tape: reader 
1punch)- Pi04 K T(card; reader1 punch)- Pi0 K- s-Ms(#~:~; magnetic tape)- Pi0 (#I : 3) Pi0 K T ( '?ystem/360 interface)- Pio6-s Ms (removable ;d i skpak)- K(#l:3)-SS-KT #I:R; diqital: input; 1 w; c contacts llogic voltage 3 K(#4:6)-S-KT iqi tal : event pulse: t 1 1 input: counters; c (#1:16: 8b)I(#l:R: 16 b) digital: contact inputs; to: inter- 
upt: 16 b K-S - K(#l :4)-S-KT I :4: digital: output: contactllogic voltage/ ulse: 16 b pios-sLK- S-KT ' 10113 1:4: analoq: b output: P i 07- t?-S-K- :-L #1:-1024: analog: input; a I I I I I , voltage, current: (+lo1 I I +20 1+50/+100 /+200j+500) 'I I mv1+5 v1+10 VI (-20)ma) p io8- K L L Z - - - - - 
- - - 'Mp(core: 214 p/w: 4096 - 32768 w; 2Pc('1801 11802; 1 - 2 w/instruction; technology: hybrid: Mps(- 6 w): 1 address/ (16, parity, protect) b/w) instruct ion: -1965) 3~(1~n BUS, out BUS) 4Maximum of 9 Pi0 per C 'Pio('Digita1 Input Data Channel) 
"Pio('Digita1, Analog Output Data Channel) 
'Pio('Anal0g Input Data Channel) 
'Optional Pi0 to control analog channel;(structure is qreatly simplified) 'K('ADC; analog: input: 9, 12, 15 b/w; i .rate: 9 ... 24 kw/s) Fig. 3. IBM 1800 PMS diagram (simplified). 

Chapter 33 I The IBM 1800 405 2 3 An interval timer has counted a previously set time interval. A magnetic-tape drive 
has completed a 
data transfer previ- 
ously requested and is ready for another request. 
An operator has initiated an interrupt from the Pc console. A device such as 
a typewriter has just printed a character and is ready to receive the next one. 4 5 Primarymemory communication and data transmission with terminals and secondary mentory 
Two methods 
are used to transmit data between Mp and Ms, or Mp and T. First, low-speed devices 
are controlled directly by the program. Each character or word 
of data is transmitted to or from the Pc and onto T by 
means of an Execute I/O(XIO) instruc- 
tion. The Pc program and device synchronization 
are accomplished by using the interrupt mechanism. Devices 
operating under direct program control include 
typewriter, printer, plotter, paper 
tape reader and 
punch, analog-to-digital converters, contact sense, voltage-level sense, pulse 
counters, etc. The second method of transferring data is via the Pio(™Data Channe1)™s. The Pi0 program 
is started by the XI0 instruction of the Pc. The transfer of data words then proceeds under control 
of the specified Pio, 
completely asynchronous to and in parallel 
with Pc 
program operation. The Pi0 gains 
Mp access independent of Pc (Pc operation is suspended for one Mp cycle). During 
the Mp cycle, the data are 
taken from 
or placed into core storage 
by Pi0 (via internal Pc control and 
registers). As soon as the Pi0 has 
been satisfied, which normally takes 
one cycle, 
the Pc 
proceeds. The logical state of the Pc, or the Instruction-set Processor, is not changed by 
Pio™s access to Mp. This 
method of access is referred to as ﬁcycle 
stealing.ﬂ Devices (Ms 
and T) operating under Pi0 control include magnetic tapes, 
disks, line printer, card 
reader- punch, and 
the link to the 
IBM System/360. Some devices can operate 
under both Pc and Pi0 control, depending on their characteristics and the configuration, e.g., analog input, analog output, digital input, and 
digital output. Process Z/O, controls and transducers Analog inputs. Analog-input equipment includes analog-to-digital converters, multiplexors, amplifiers, 
and signal conditioning equip- 
ment to 
handle various analog-input signals. The data input 
rates are up to 20,000 16-bit samples per second, with 
program selecta- ble resolution and external synchronization. 
There can be 1,024 (via relay) and 256 (via high-speed solid 
state) multiplexed analog- 
input channels connected to a 
single K (analog-to-digital con- 
verter). The Confignrator (Fig. 2) shows the allowable inputs. 
Digital inputs. The Digital Input provides up to 
384 process 
in- terrupts; up to 1,024 bits of contact sense, digital input, or parallel register input; and 128 bits of event input 
counters as 1-, 8-, and 16-bit counting registers. Analog outputs. Up to 128 analog 
outputs can 
be provided. 
Digital outputs. Digital Outputs provide up to 2,048 bits of pulse output, contacts, and 
registers. ZO processors (data channels) Pio(™Data Channels) 
give a T 
or Ms the ability to communicate directly with 
Mp. For example, if an input unit requires a primary 
memory cycle 
to store data that 
it has collected, 
the Pi0 communi- cates directly with Mp and 
stores the data. The Pio™s run even if Pc is waiting. The Pio™s have two registers: a Word Count which is used to count 
the number of words being transferred in a 
block between a device and Mp 
memory; and a Channel Address which points 
to the 
next word 
transferred in a block. The Channel Address is also used 
to select 
the next instruc- tion in 
the program for 
the next block transfer 
task. Two basic types 
of Pio™s are used, nonchaining and 
chaining.™ The Pio™s provide the ability to transfer either a single block 
(nonchaining) or multiple blocks (chaining) directly 
to Mp inde- 
pendent of Pc. The central processor Registers in the physical 
processor Figure 4 
shows the relationship of the registers in Pc, together with those in the Instruction-set Processor. Those registers acces- 
sible by 
the program are shown with an *. All the registers are accessible from the console. A description of the functions of each register is given below. 
Storuge address register 
(SAR). All Pc references to Mp are selected or accessed 
by this 16-bit register. Pi0 references to Mp use the Channel Address Register (CAR) 
of the active Pio. Instruction register (I)*. This 16-bit 
counter register holds 
the address of the next instruction. Storuge buffer register (B). This 16-bit register 
is used for buffering 
all word transfers with Mp. 
‚A descriptive name undoubtedly concocted 
by one of IBM™s marketing departments. 
406 Part 5 I The PMS level Section 2 1 Computers with one central 
processor and multiple 
inputloutput processors Console I Core Storage ' A d d r e ? I I -I s sing c - Timers + 1 I U I Operotion 1 Monitor I t+ 4 I + If w 0 B 5PS 1 I A* D I #I 1 Connected to t In Bus Input Dev i ces Connected to Output Devices 
out Bus Control Registers *regis ers accesslb 
to Instruct on I sc (6) Overflow*, Carry* 
:t Processor **allows processor registers to be reed or written Fig. 4. IBM 1800 Pc data flow. (Courtesy of International Business Machines Corporation.) Arithmetic factor 
register (0). This 16-bit register is used to hold one operand 
for arithmetic and 
logical operations. The Accumu- lator provides the other 
factor. Accumulator (A)", This 16-bit register contains 
the results of any arithmetic operation. It can be loaded 
from or stored into core storage, shifted right 
or left, and otherwise manipulated by 
specific arithmetic and logical instructions. Accumulator extension (Q)". This register is a 16-bit low-order 
extension of the Accumulator. 
It is used during multiply, divide, 
shifting, and 
double-precision arithmetic. Shift control counter 
(SC). This 6-bit counter is used primarily to control shift operations. Accumulator temporury (U). The U register is used to store A temporarily during an instruction or an operation which requires the A's facilities. OP register (OP). This 5-bit register is used to hold the operation code portion of an instruction. Index registers'. The three l6-bit registers are used in effective- address calculations. 
Chapter 33 I The IBM 1800 407 Op Code OverJlow and carry indicators". The two indicator bits 
associated with the Accumulator are Overflow and Carry. 
The Overflow indicator can be turned on by Add, 
Subtract, or Divide instruction and indicates a result larger than 
can be represented in the Accu- mulator. The Overflow indicator can also be 
turned on by a Load- status instruction. 
Once Overflow is on, it 
will not be changed 
except by testing the indicator, or by 
a Load-status or Store-status 
instruction. The Carry indicator provides the information that a carry (or borrow) from the high-order position of the Accumula- tor has occurred. The Carry indicator is used with the Add, Subtract, Shift-left, Load-status, Store-status, 
and Compare instructions. F T Displacement In-bus. This 18-bit bus is a link(L) used to carry information from a K to Pc. Generally only 16 of the 18 bits are 
used, although transfers to magnetic tape can be made 
three 6-bit characters. 
Out-bus. This 18-bit 
bus is used to carry information from 
Pc to 
a K. Instruction-set processor The operation of the Pc 
from a program viewpoint follows. The ISP registers 
were declared 
(") in the previous section 
and in Fig. 
4. The ISP registers are the 
18bit I, A, Q, XR [l, 2, 31, and the 
1-bit Overflow and Carry. An SSP description of the 1800 appears in Appendix 1 of this chapter. It is incomplete in the following respects: The memory protect bit checking is not described; the illegal (undefined) 
in- struction action is not described; double word data must be aligned 
on even and odd address word 
boundaries or else 
a fault occurs; 
and the IO instruction and interrupt operation are not given. Instruction formats. Two basic instruction-word formats are used, one word (Fig. 5) and two 
word (Fig. 6). 
The bits within the instruction words are used in the following manner: OP Operation Code. 
These 5 bits define 
the instruc- tion. I I Fig. 5. IBM 1800 one-word-instruction format. (Courtesy of Inter- national Business Machines Corporation.) I5 8910 15 0 1 Fig. 6. IBM 1800 two-word-instruction format. (Courtesy of Inter- national Business Machines Corporation.) F Format bit. A 0 indicates a single-word instruc- tion, and 1 a two-word 
instruction. Tag. These 2 bits specify which 
of the three 
index registers is used in address modification or 
the shift count. Displacement. These 8 bits are usually added to the instruction register 
or the index register 
speci- fied by T 
for one-word 
instructions. The modified address is defined as 
the Effective Address 
(EA). If T is 00, the displacement is added to the in- struction register (then EA = I + DISP). The displacement is in two's complement form if nega- tive, with the sign in bit 8. The bit 
in position 8 is automatically extended 
to the 
higher-ordered bits (0 to 7) when the displacement is used in EA generation. Indirect addressing. This 
bit is used only 
in the two-word-instruction format. If 0, addressing will be direct. If a 1, addressing will 
be indirect. Only one level of indirect addressing is permitted. (The Load Index and Modify Index 
and Skip instruc- tions have exceptions, as shown in the ISP descrip- tion.) Branch Out. This bit is used to specify that the Branch or Skip 
on Condition (BSC) instruction is to be 
interpreted as a Branch Out (BOSC) when used in an interrupt 
routine. Conditions. These 6 
bits select 
the indicators that are to be 
interrogated on a BSC or BSI instruction. The bit, assignments for conditions 
are: Cond( 10) A = 0 Cond(l1) A < 0 Cond(l2) A > 0 Cond( 13) Cond(l4) (Carry = 0) Cond( 15) (Overflow = 0) These 16 bits usually specify 
a core 
storage address 
T DISP SA BO COND (A( 15) = 0) that is, A is eoen ADDRESS 
408 Part 5 I The PMS level F=O (direct addressing) 
t (direct addressing) 
(F = 1) A (1A = 0) Section 2 1 Computers with 
one central processor and 
multiple input/output processors (F = 1)~ (1A = 1) (indirect adressing) T = 00 T = 01 T = 10 T= 11 EA t I + DispS EA t XR[1] + Disp EA c XR[2] + Disp EA t XR[3] + Disp EA t Address EA t Address + XR[1] EA t Address + XR[2] EA c Address + XR[3] EA c C(Address)§ EA t C(Address + XR[l]) EA t C(Address + XR[2]) EA t C(Address + XR[3]) in a two-word instruction. The address can be modified by the contents of an index register or 
used as an indirect 
address if the IA bit is on. Effective-address generation. 
The Effective Address (EA) 
is devel- oped as shown in Table 
1. The instruction set is divided into five classes as 
shown in Table 
2. Storuge protection. The storage-protection 
facility protects the contents of specified individual locations 
of Mp from change due to the erroneous storing 
of information during the execution of a program. The status 
of each location is identified as ﬁread onlyﬂ or ﬁread/writeﬂ by the condition of the Storage Protect Bit, S. The Store-status instruction is used to write and clear Storage Protect Bits. The execution of this instruction 
is under control of the Write Storage Protect Bits switch on the console. Any attempt by the program to write 
into a read-only protected location results 
in a storage-protect 
violation which causes 
the Internal Interrupt 
(the highest priority 
interrupt). Instruction interpretation 
process The simplified Pc data-flow block diagram 
(Fig. 4) shows instruc- tions and data entering 
and leaving memory 
via the B register. Additional bits in Pc hold the P and 
S bits for Mp. Input devices send data and instructions to the B register via the 18-bit In-bus. 
Output devices receive 
data from the B register via the 18-bit Out-bus. Eighteen bits can be transferred between 
Pc and K(mag- netic tape). 
As each stored-program 
instruction is selected, its various parts (op code, format bit, etc.) are directed to 
the control registers via the B register and the Out-bus. The control registers 
decode and 
interpret each instruction 
before the instruction is executed. Except for Pi0 operations, 
all instructions and data in memory 
are addressed by the Storage Address Register 
(SAR). SAR obtains the memory address from 
the I register or 
the A register. The Table 2 Instruction set 
class Znstnrction Indirect addressing Mmmnic Load and store Arithmetic Shift Branch I/O Load accumulator 
Double load Store accumulator 
Double store Load index 
Store index 
Load status Store status 
Add Double add 
Subtract Double subtract Multiply Divide And Or Exclusive Or Shift Left 
instructions: Shift left logical (A)t Shift left logical (AQ)t Shift left and count (AQ)t 
Shift left and count (A)? Shift Right instructions: Shift right 
logical (A)t Shift right arithmetically (AQ)t Rotate right (AQ)™ Branch and 
store I Branch or skip on condition Modify index and 
skip Wait Compare Double compare Execute 1/0 Yes Yes Yes Yes $ Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No No No No No No No Yes Yes * No Yes Yes Yes LD LDD STO STD LDX STX LDS STS A AD S SD M D AND 
OR EOR SLA SLT s LC SLCA SRA SRT RTE BSI BSC (BOSC) M DX WAIT CMP DC M XI0 t Letters in parentheses indicate registers involved in shift operations. $See the section for the individual instruction 
(MDX and LDX). 
Chapter 33 1 The IBM 1800 409 contents of the I register are developed by one 
of the following means, depending on the Pc operation: 1 2 The I register is incremented for each instruction. The effective address of each instruction is developed in the accumulator (A register) and then transferred to 
SAR. The contents 
of the accumulator are saved in an auxiliary (U) register during effective-address computation. If the instruction was a 
branch, the contents of SAR is transferred to the I register. 
The following examples 
illustrate the data 
flow or instruction interpretation process for 
the Load Accumulator 
(LD) instruction. One-word load instruction Instruction Cycle A register transfers to U register. I register transfers to SAR (I register is then incremented). SAR addresses the memory location 
containing the instruc- tion. Memory location transfers 
to the B register and Out-bus. 
Control registers store 
various parts of the instruction (op code, format, 
and tag). Displacement is stored in the D register. a b Displacement (D register) is added to A register. If tag = 00, I register transfers 
to A register. If tag # 00, the specified XR transfers to A register. Execute Cycle 
9 10 11 SAR addresses data word. 12 13 A register transfers 
to SAR (effective address). 
U register transfers 
to A register. Data word transfers 
to B register. B register loads 
into A register (via 
D register). Two-word load instruction, direct addressing 
Instruction Cycle 1 1 2 A register transfers 
to U register. I register transfers 
to SAR (I register is then incremented). 
3 SAR addresses the memory location 
containing the instruc- tion (first word). Memory location 
transfers to B register and Out-bus. Control registers store various parts of the instruction (op code, format, and tag). If tag # 00, the specified XR transfers to A register. 4 5 6 Instruction Cycle 2 7 8 9 10 11 I register 
transfers to SAR (I register is then incremented). SAR addresses second word 
of instruction. Second word 
of instruction (address) is read into B register. Address (from B register) is stored in D register. a b If tag = 00, D register transfers 
to A register. If tag # 00, D register is added to A register (A register contains contents of XR) Execute Cycle 
12 13 14 15 16 A register transfers 
to SAR (effective address). U register transfers to A register. SAR addresses memory at effective address 
(data word). Data word transfers 
to B register. B register loads 
into A register (through D register). Central-processor communication 
with the controls' Direct program controZ of the controls Pc direct programmed control of 1/0 devices is on 
the basis of single-word or 
character-at-a-time transfers for 
each XI0 instruc- tion executed. One 
data word or 
character is transferred to 
or from Mp to K. The XI0 instruction specifies an 1/0 Control Command 
(IOCC) with a function 
of Control, Sense, Read, or 
Write to a controlled device. This 
command is either directly to a device 
or to a Pio. It is possible for the program sequence to execute an XI0 instruction to 
a device 
that is busy responding to a previous XI0 instruction. Each device has a Busy indicator, which signals 
whether or not the device can 
accept data 
or control information. (Incorrect program sequence timing may cause undetected errors.) 'IBM name: Adapter or Device Adapter. 
410 Part 5 I The PMS level Section 2 1 Computers with one central processor and multiple 
input/output processors It is possible for 
a device operating synchronously with the program to request a data word transfer before 
the program sequence is ready to service the request. Devices with this poten- tial have a 
ﬁprogram checkﬂ indicator 
to signal when data have been lost (that is, Pc has not 
kept up with the device). Execute Z/O instruction (XZO) This instruction 
is used for programmed 1/0 operations and to initialize Pio; it may be either 
one or two words in length, as specified by 
the F bit. In the two-word instruction 
the address is either a direct or indirect address, as 
specified by the IA bit. For proper operation 
the effective address must 
be an even ad- 
dress. The effective address 
is used to select a two-word 
1/0 Control Command (IOCC) from storage. The IOCC specifies the 1/0 operation, 1/0 device, and core storage address. The format of the two-word IOCC follows, with an explanation of the assigned fields: 
Area := IOCC[1](0:4). The area field specifies 
a unique segment of 1/0 which may be a single 
device (1442 Card Read-Punch, 1443 
Printer, etc.) 
or a group of several units (magnetic-tape 
drives, serial 1/0 units, contact sense units, etc.). (Area 00000 is used to address system devices such as 
the console and the 
Interrupt Mask Register.) Function : = IOCC[1](5:7). The primary 1/0 functions are speci- fied by the 3-bit function code 
of the IOCC: 000 001 010 011 100 Removes an 1/0 device from on-line status and places it in a ﬁfreeﬂ mode. 
Write Transfers a single 
word from storage to an 1/0 unit. The address of the storage location 
is provided by the Address field 
of the 1/0 Control Command. 
Read Transfers a single word from an 1/0 unit to 
storage. The address of the storage location 
is provided by the 
Address field 
of the 1/0 Control Command. 
Sense Interrupt Level Directs the selected 1/0 device to make its status 
available in 
the Accumulator as 
the Interrupt Level Status Word (ILSW). Control Causes the selected device 
to interpret the address and/or Modifier of the IOCC 
as a specific control action. Examples are feed card and 
load interrupt mask register. 101 110 111 Initialize Write 
Initiates a Write 
operation on 
a device or unit which will subsequently make data transfers from storage via a Pc. Initialize Read Initiates a Read 
operation from a device or unit which will subsequently make data transfers to storage via a Data Channel. Sense Device Reads the selected device status word into 
the Accu- mulator. A Device Status 
Word (DSW) and the 
Process Interrupt Status Word (PISW) are sensed with this instruction. If Area 00000 is specified, the Console status and Interval Timer status 
may be brought into 
the Accu- mulator as specified by a 
unit address code in the Modifier field. 
The current contents of the Accumulator are destroyed by the execution of Sense Interrupt Level, Sense Device, Initialize 
Read, Initialize Write, 
Read, or Write. Modijier : = IOCC[1](8:15). This %bit field provides additional detail for either Function or Area. For example, if the Area spe- cifies a disk and if the Function specifies Control (100) then a particular modifier code specifies the direction of the Seek opera- tion. In 
this case, 
the Modifier serves to extend the function. If, however, the Area specifies 
a group of 1/0 devices, and if the Function specifies Write (OOl), then the 
particular unit address is specified by the modifier. Address : = lOCC[O]( 0:15). The meaning prescribed 
for this 16-bit 
field is dependent upon the Function specified by this 
1/0 Control Command: If Function is Initialize Write 
(101) or Initialize Read (110), 
then Address specifies 
the starting address of a table in 
storage (an 1/0 block). The contents of this table are data words and control 
information. If Function is Control (100) and if, for 
example, Area speci- fies the 1443 Printer, the Address may specify a 
specific control action. 
If Function is Sense (011 or ill), the Address field 
is ignored. Instead, an increment of time equivalent to a memory 
cycle is taken, during which the selected 1/0 device or Inter- rupt Level places its 
status word in the accumulator. 
Chapter 33 1 The IBM 1800 411 4 If Function is Write (001) or Read (010), 
the Address speci- fies the storage location of the data word. XI0 execution interpretation process 1 The EA of the XI0 is developed in the accumulator (A) and routed to the 
Storage Address Register 
(SAR) to locate the IOCC (as for 
any EA). Bit position 15 of SAR is forced on to select the EA + 1 where the IOCC Area, Function, and 
Modifier are found. The Area, Function, and Modifier are routed through the B register to the Out-bus to the 
control of the device speci- fied by the Area. Bit position 15 of SAR is turned off to allow the address portion of the IOCC word to be 
transferred from the Mp location specified by 
the Effective Address 
(EA) to the B register. If the Function is an Initialize Read, Initialize Write, or Control, the address part of the IOCC is routed through 
the B register to the 
Out-bus. The address part of the Initialize Read/Write IOCC goes to the 
Channel Address Register (CAR) 
of Pio. If the Function is Read or 
Write, the address is routed from the B register through the A regis- ter to the SAR. SAR addresses the memory location 
to or from which the data are 
transmitted. 2 3 4 5 Interval timers Three timers are provided to supply real-time information to the program. They 
are in core-storage locations 
0004 (Timer A), 0005 (Timer B), and 0006 (Timer C). Each timer is incremented ac- 
cording to its 
associated or permanent time 
base and can 
be hardwired to be 0.125, 0.250, 0.5, 1, 2, 4, 8, 16, 32, 64, or 
128 milliseconds. The timers can be started or stopped under program control. When the 
count reaches zero, 
an interrupt 
is requested on the level assigned 
to the timers. Interrupt The interrupt feature 
provides an automatic branch 
from the normal program sequence, based upon an external condition. 
A maximum of 24 external interrupt levels (groups) are available, arranged in order of priority. Twelve external interrupt levels are standard. Each interrupt level has 
a unique core-storage address assigned to it. Several devices 
may be connected to a single inter- rupt level, and program polling 
can be used to differentiate the possible signals 
causing the interrupt. The Interrupt 
Level Status Word, ILSW, is used to identify the specific condition causing 
its interrupt level to request service. Internal interrupt. When any one 
of the following error conditions occur, there is an internal interrupt 
in Pc: 
an invalid op code; a Mp 
parity error (an 
even number of bits); a storage-protect violation; and Channel Address Register check error. The internal interrupt takes priority 
over all 
external interrupts and cannot 
be masked. A mask register exists for the masking and unmasking of inter- rupt levels. An interrupt level that is masked cannot initiate a request for service 
until it has been unmasked. Device status word (DSu/?. DSW indicators usually fall 
into three 
general categories: 1 2 3 Routine status conditions Error or exception interrupt conditions Normal data or service-required interrupts Process interrupt status 
word indicators (PISW). The PEW indi- cators are physically located in Pc and are turned 
on by events external to the 
computer, e.g., contact closures or voltage shifts. 
IO processors1 The Pc initializes each Pi0 with an XI0 instruction. The Pi0 has 
priority to the extent that, when the 1/0 device is ready to send or receive 
a data word, the Pc is stopped while the word transfers 
to or from core storage. Pc data and 
conditions are undisturbed except for the memory locations that receive data from an input 
device. 1/0 devices that are to be operated concurrently must be on separate Pio's. The XI0 instruction for a Pi0 specifies an 
1/0 Control Com- 
mand (IOCC) with a function 
of Initialize Read or Initialize Write. However, even though a device operates 
with a Pio, the XI0 instructions in 
Pc are used to sense device status and for control. Registers Channel address register. The Channel Address Register (CAR) 
is a 16-bit 
register used 
to store 
the Mp address of the next word 
that will be addressed by 
the Pio. Each Pi0 has a 
CAR. Pi0 and its associated CAR are selected when their assigned 1/0 device is selected by the Area Code and Modifier of an IOCC 
word. CAR is incremented by 1 after each transfer of its contents to CAB. 'IBM name: Data Channel (DC). 
412 Part 5 I The PMS level Section 2 1 Computers with 
one central processor 
and multiple input/output processors Channel address buffer. 
A common Channel Address Buffer (CAB) is used by 
all Channel Address Registers to address Mp. When a cycle steal request occurs, 
the CAR for the requesting Pi0 is transferred into the Channel Address Buffer. Channel-address-register check bit. Channel Address Register (CAR) checking 
is provided to ensure that the 
first word addressed 
by a selected 
CAR is the first word of the correct data table. Thus the check determines if a Pc program has 
set up the Pi0 program 
correct1y.l A CAR check is made for all devices after the address from the IOCC word is transferred to the selected CAR. A bit- by-bit comparison is made between the contents of the selected CAR and the contents of the B register. If any of the corresponding bits are not equal, a 
CAR check error has occurred. This CAR check error 
terminates the Pi0 task and initiates 
an internal inter- 
rupt. Word count register. 
A Word Count Register is provided in each Pio. The Word Count Register is loaded with 
the contents of the word-count portion 
of the data table, (2:15). This register 
is decremented each time a 
data word is transferred from (to) the data table. Scan control 
register. A Scan Control Register is provided in 
each Pi0 that has chaining ability. 
Scan Control register bits are stored in the first word of the first data table (bit 
positions 0 and 1) and in the 
second word 
(bit positions 0 and I) of the second data table and 
all subsequent data tables in a chain. 
The Scan Control Register controls 
the 1/0 device and the Pi0 operation at the end of the data table 
as follows: single scan 
of data table 
and stop with 
an interrupt; single scan 
of data table and stop 
(no interrupt); continuous scan 
of this data table 
or a different data table with 
an interrupt at the 
end of this table; and continuous scan 
of this data table 
or a different data table with 
no interrupt. The IO processor program operation 
The sequence of steps for 
a Pi0 program 
is given below. 
The memory map or format 
of the program is shown in Fig. 
7. 1 Pc issues an XI0 instruction which references 
the IOCC word and initializes Pio. The Area Code and Modifier of the IOCC select the 1/0 device. Function specifies the type of operation (Initialize 
Read or Initialize Write, 
etc.). 2 'Not a completely arbitrary program fault to 
check, since 
processors are in- volved. 3 a The address portion 
of the IOCC word is stored in 
CAR for the selected Data Channel and 
1/0 device. A CAR check is made between the selected CAR and the B register. b 4 5 A cycle steal is requested by Pio; CAR transfers to CAB. CAB addresses core storage for 
the first word of the data 
table while CAR is 
being incremented by 1. The first word of the data table contains 
a Scan Control bits (bit positions 0 and 1) b Word Count (bit 
position 2 to 15) These are transferred to their 
respective registers in 
the 1/0 device. This is the end of the first cycle steal. When another cycle-steal request 
from Pi0 occurs, 
CAR, which was incremented in step 5, now transfers the next higher address to CAB. CAB 
then addresses core storage 
while CAR is being incremented. The first data word is transferred to 
or from the 1/0 device via the B register and Data Channel. The Word Count Reg- ister in the 1/0 device is decremented by 1. This is the end of the second cycle-steal cycle. 
6 7 8 Steps 7 and 8 now continue on a cycle-steal basis; that is, they occur as the 1/0 device requests 
data transfers. The CAR is incremented with each 
data transfer and the WCR is decremented. This sequence continues 
until the last data word of the data 
table is transferred. The last word transfer 
is sensed by 
the WCR reach- ing zero 
or through some 
indicator in the device. If the device does not have chaining ability, no 
more demands 
for data transfer are made until the device is reinitialized with another 
XI0 instruc- tion. Chaining. These steps 
are for the second and all subsequent data tables. See above 
for steps 1 through 8. 9 The contents 
of the word following 
the last data word in 
the first data table are 
transferred to CAR. This word must 
contain the address of the next data table. a 10 When the 
next cycle is requested, CAR is transferred to CAB to address core storage. The contents 
of the first word of the next data table is transferred to the B register. This word must 
contain the address of itself. 10 b CAR check is performed and CAR is incremented When the 
next cycle steal is requested, CAR is transferred to CAB and CAB addresses Mp. The Scan-control bits and Word-count bits are transferred from the second word 
of by 1. 11 
Chapter 33 I The IBM 1800 413 0 15 0 15 X10 Instruction sc Word Count Word Count = 22 SC = Continuous with No Interrupt 1001 First Data Word Word Count = 54 ingle Scan 1002 1022 1-1 Last Data Word a 2002 1-1 First Data 
Word c I 1 2055 Last Data Word b. ond Stop with an Interrupt Fig. 7. IBM 1800 data-channel tables for chaining memory maps. (a) First data table; (b) second data table. (Courtesy of International Business Machines Corporation.) the data 
table to their respective registers. CAR is incre- mented by 1. Data are 
transferred to (from) the 1/0 device on a 
cycle- steal basis via 
the B register and the Data 
Channel. CAB addresses core storage to transfer a data word to the B register. Each time CAB addresses core storage, 
CAR is 
incremented by 1. When the next cycle-steal request 
occurs, CAR is transferred to CAB. The Word-count Reg- ister is decremented for each word transferred. 
When the 
last data character is transferred (word count 
is decremented to zero), operation will continue as speci- fied by the Scan Control Register. (See above section for Scan-Control Register.) 12 13 Special data channels The four Pi0 types 
for special functions are: 
1 Analog input (block data transfers, and comparisons of analog inputs for limits) 2 Digital input/output 3 Analog output 4 Digital output Analog-input datu channels. Memory maps (Fig. 8a and b) illus- trate the 
command formats interpreted in the Analog Data Chan- nel programs. A list 
of limit values is placed in a table (Fig. sa), and each 
analog input is compared with the limits. The operation sequence is: Read a specific 
addressed analog voltage, called 
the multiplex' point (mpx); compare the input 
voltage with the limits stored in 
the table following the analog address 
(the limit word 
contains a high and low value in bits (0:7) and (8:15), respec- lThe IBM multiplexor is an S which allows multiple inputs 
to be read into the 
T(Ana1og to Digital Converter) sequentially. 
414 Pari 5 I The PMS level IO Section 2 1 Computers with one central processor and multiple input/output processors ADDRESS A First Mpx Point Location J"" ~IiIIIIII L1I11 dI1IIIIlI LIMIT WORD 11 ADDRESS 6 LIMIT WORD JlIII I~IIIIIII. Llncl IIIIII/II_ 00 ADDRESS C 11 ADDRESS D LIMIT WORD IO ADDRESS E JP. ETC. L = I, Limit Word Follows K = I, Perform Comparison 
I This word contains 
its own address MPX Address 47 Limits Not Used Second Mpx Point Comparison is Performed Third Mpx Point Fourth Mpx Point Comparison is Performed Fifth Mpx Point 31 I9 Not Used 31 23 A I-lnt. WR a. 3012 Location 2999 1:: I Word Count = 12 1 Starting Toble Addrerr(BO15) 
3000 I Multiplex Address I I 1 3001 1 Value 1 3011 T Value 11 T I I Locaticm 301 5 Car Check Word = 3015 1 3016 I %$ I Word Count = 25 I 3017 1 Multiplex Address I 1 3018 1 3041 T Value 35 T Value 12 3043 A/l 
-1nitiolize Read b. Location 3201 - 3202 3203 3204 3321 Loccrim 3402 - 3403 3434 3521 3522 SicrtinS IOCC 35 24 This word contains its own address LVord Count ADC dolue (47) ADC Flue (82) ADC Value (14) ADC Volue (47) ADC value (82) ADC Value (141 1 I Starting Table Addr. (3201) Starling Table Addr . IOCC 1 A/I - In1 . Rd. II d. Fig. 8. IBM 1800 data-channel analog-input 
instruction format and memory maps. (a) Multiplexor address table with limit 
words for comparisons. (b) Data table, chained sequential control. 
(c) Multiplexor address table, random addressing. (d) Analogto-digital converter storage 
tables, random addressing 
(used with a second data channel). 

Chapter 33 I The IBM 1800 415 Word Count= m + 1 I Control I I Initial Digital Input Group 
Address 1 Scan Word Count = n + 1 Control I D or A Output Address I Data 2 I Data 1 I Data rn I a. I I z;rol I Word Count = 2m Digital Input Group 
Address1 I Data 2 I -- Data rn b. I Data n C. Scan Control Word Count = 2n I Initial D or A Output Address 1 I Data 1 D or A Output Address Data2 D or A Output Addresses Datag d. Fig. 9. IBM 1800 data-channel digital or analog-output 
instruction formats and memory maps. (a) Digital input, 
sequential; (b) digital input, random addressing; (c) digital or analog output, sequential; (d) digital or analog out- put, random addressing. (Courtesy of International Business Machines Corporation.) 
tively); and if the analog-input value lies outside 
the limit range, initiate an interrupt. Figure 8h describes a second 
use of this data channel. Pi0 
accepts a sequence of analog inputs and packs them into 
a table following the address initiation instruction. 
The analog inputs from the T™s are either 
fixed or selected in 
a cyclic 
fashion from a 
Multiplexor. Two Pio™s can be used concurrently: One Pi0 controls the input from a series 
of analog-input addresses (Fig. 8c); the second Pi0 
packs the corresponding analog values 
in a second 
table (Fig. 
84. Digital-input data channels. Digital parameters 
or events can 
be read into 
Mp under the control of a Digital-input Data Channel. The memory map (Fig. 9a) shows the control format for selecting and inputting 
a block or 
sequence of external data. The memory map (Fig. 9h) illustrates a more 
general ability 
to address inputs at random and read them into 
succeeding Mp locations. Digital- and analog-output data channels. 
Memory maps (Fig. 3c and d) show the program format 
used by 
the Digital- or 
Analog- output Data Channels. These channels 
output selected data points 
416 Part 5 I The PMS level Section 2 1 Computers with one central 
processor and multiple 
input/output processors to external analog or digital K™s. This Pi0 
is similar to the Digital- input Data Channel. Conclusions We have tried to show a typical, third-generation computer used for process 
control. Many 
of the facilities the 1800 possesses are general. The Pio™s are rather 
special, designed to monitor and control a process, independent of Pc. Although 
the Pio™s are powerful (by providing parallel data transmission), their use, like 
other multiprocessing systems, 
is nontrivial. The Pc ISP is fairly straightforward, and one 
should write a program using 
it to 
ap- preciate its 
simplicity. 
Chapter 33 I The IBM 1800 417 APPENDIX 1 THE IBM 1800 ISP DESCRIPTION Pc State A<O:15> Q<O:15> 1<0:15> XR[ 1 : 3 1<0 : l5> ov C R"" Mu State M[O: FFFFl 61<P, S ,0: l5> Pc Console State Check Stop Switch USPB Switch SPV Indicator Instruction Format instruction/i[0:1]<0;15> opd:4> := i[O]<n:4> shop<O:7> := opoi [0]<5.8,9> f := i[O]<5> t4:1> := i[O]ib:7> d&:15> := i[O]&:15> dsgna: 15>:= signgxtend(dc8mQ: 15;) a<0:15> := i[11<0:15> ia := i [Old> bo := i Cold> cond4:5> := i [O]<lO:l5; E.Ffectiv~ Address c'alculation Process z<O:15>:= ( (t = 0)A f -,(dsgn + I); (t # O)A 7 f i (dsgn + XR[t 1); (t = 0) A f A ia-> a; (t # 0) A f A (t = 0) A f A ia + M[n1; (t # 0) A f F\ ia + (M[a + XR[tll)) ia+(a + XR[t]); z'<0:15> := (? f -> (dsgn + I); f A 7 ia +a: f A ia -Mia]) Appendix I IBM 1800 ISP Description Accwnulator Accumulator Frtension ,for mu;tiplier, auotient and double Instruetior. Location Counter Index Registers 0iierfloi.i Indicator Carru Indi ea t or denotes running comutcr length Mp with Parity and Protect bits pc stops if storage protect wiolation occurs Write .Storage Protect Elits; enables the writing of bits in a arord Storage Protect Violation indicator: 
set to 1 if a memory reference is made to a orotected iuord operation code shift ooeration code count format; specifies a 1 or 2 word instruction tag: index register specification disnlacement or short address afldress irr'irect aciiress bit branch out bit coniYtions for test effective address 1 word, relative 7 word, relatioe, indexed 2 word, direct 2 ii)ord, direct, indexe,i 2 i,,ord, in?irect 2 word, indirect, indexed e.ffective address for index register ivstructions 
418 Part 5 1 The PMS level Section 2 I Computers with one central processor and multiple 
input/output processors APPENDIX 1 THE IBM 1800 ISP DESCRIPTION (Continued) zdin:15>:= (~z<15> +z + I; process for locating -econd operand for double length z<15> iz) xi<D:15> := (7 f idsgn; index increment f A 7 ia +a; f A ia ->M[al) s<0:5> := ( shift count caZcuZatCnn (t = 0) +d<10:15> (t # 0) ->XR[t]<lO:l5>) Tnstruction Interaretotion Process Run-(instruction[O:ll +M[l:I + I]; next fetch ~f +(I 61 + I); f +(I tl + 2); next 1 or 2 uord instruction Instructionuexecution) execute Instructicrr Yet anﬂ Tnstruction Fzerutior 
Pr0ces.s Instructiondxecution := ( Load and Arithmetic LO LDD (:= op = IlDOl) i (AOQ tM[z]OM[zd]); double load STO (:= op = 11010) 4 (M[z] +A); store accmlotor STD (:= op = IlOll) i (M[Z]oM[zd] tAoQ); double store A AD S (:= op = 10010) + (OV,CoA +A - M[z]); SD (:= op = IOOIl) -) (Ov,Col\Oa +AOQ - M[z]CCl[zd]): double subtract M D (:= op = 11000) + (A +M[zl): load accm lator (:= op = IOOOO) -> (Ov,Cd +A + M[z]): (:= op = IOOOI) + (Ov,CoAoQ (-Ana + M[zloM[zd]); add double add subtract (:= op = 10100) 3 (AOQ +A x M[z]): muztipzy (:= op = IOlOl) + (Ov,Q tAoQ / M[z]; A +A@ mod M[z]); divide iogica I instruct ion:; AND (:= op = ll100) +(A +A A M[z]); OR (:= op = IllOl) +(A +A v M[z]); logical or EOR (:= op = Illlo) ->(A +A @M[z]); loy:cal ezclusiiie or logical and Compare CMP (:= op = 10110) - ((A< M[z1) + (I I + I); compare (A = MCzl) - (1 1 + 2)); DCM (:= op = lOll1) + ((AW< M[zlP([zdl) + (I 1 + I); double comnare (AQ = M[zlM[zdl) - (I + I + 2)); Shifts SLA (:= shop = OOOlOcOcOO) +( shift left lopical A +A x 2™ [logical]; c -AG-I>); A@ +A@ x 2™ [logical 1; c ~A+,-I>): SLT (:= shop = 000106)o10) i( shift douhle lpft lopi?al SRA (:= shop = OOOllcO~O) +(A +A / 2™ shist right logical SRT (:= shop = OOOll~olO) +(Ana +Am / 2™): shift right A on? 0 RTE (:= shop = 0001 IOOOl 1) +(A@ <-AOQ / 2™ SLCA(:= shop 
= 00010oD;101) i( Shi.ft 7e.ft cv4 court P [logical]); (rotate)): rotate right A an? 0 
Chapter 33 I The IBM 1800 419 APPENDIX 1 THE IBM 1800 ISP DESCRIPTION (Continued) (t = 0) + (A +AX 2': c ~A+,-I>); (t f 0) + (A +normaIize(A); C~XR[t]<l0:19 t normal ize,exponent(A): XRC t1<8,9 + 0) 1 : SLC (:= shop = OOOlCn~ll) + (7 ((s = 0) V A<O>) + ( (t = 0) -? (PaQ t PaQ x 2'; C cA<s-l>): (t # 0) + (bQ tnorrnaIize(AnQ); COXR[t] t normal ize,exponent(A@)))); LOX (:= op = 01100) + ((t = 0) +(I tz'): (t # 0) + (XR[tl tz')); STX (:= op = 01101) + ((t = 0) + (M[z'] t I); (t # 0) + (M[z'I tXR[tl)); STS (:= op = OOlOl) + ( (f A bo) + M[z]<bc cond<l5>: ?bo + (M[ ~]<8: ID c OOOOcbcOOv; CoOv t 00)) ; LOS (:= i[o] = ooiomcoomoooooomm) + (C t i[Ol<llu: BSC (:= (op = 01001) A i<9>) - ( Ov t i[Ol<lP); ( (,skip,condition A f) + (I tz); skip,condition A 7 f) + (I t I + I): KID -) ov c 0); ekibcondition := ( (~OV A d<15>) V (iC A d<l4>) V (A<l5> A d<13>) V ((A > 0) A d<l2>) V (A<O> A d<l I>) V ((A=O) A d<l O>) ) BOSC (:=(op = OlOOl) A is>) - ( (skip,condition A 7 f) + (I + I + 1; Interrupt +I); (7 skip,condition A f) + (I +z; Interrupt +I); d<l$> -9 (OV to)); BSI (:= op = OIOOO) + ( if + (1 tz + I; M[z] t I); f + (d<15> - Ov + 0); -,skipjondition +(I tz + I; P[zl -1)); MDX (:= op = 01110) -f ( (t = 0) A f -(I +I + dsgn); (t = 0) A f +(M[al tM[al + dsgn; (Msum=O) v (M101<0> @Msum<O>) - (I - I + I)); Msurn,O:15> := (Mtal + dsgn) (t # 0) -t(XR[tl tXR[tl + xi; (Xsurn=o) V (~~[tla., @xsurnQO') - (1 + I + 1))); Xsurndl:15> := (XR[t] + dsgn) Wait (:= i = 3000~~) +(I t I - I); shift left and count load index or instruction counter store index or instruction countei store status load status branch or skip on condition overflow off carry off Accumulator even Accumulator greater than zero Accumulator negative Accumulator zero branch out of interrupts branch and store instruction regie modify index and skip local branch result zero or sign change result zero or 
sign change 
420 Part 5 1 The PMS level APPENDIX 1 THE IBM 1800 ISP DESCRIPTION (Continued) Section 2 I Computers with one central processor and multiple 
input/output processors IO Control Instruction: XI0 (:= op = OOOOl) + ( Execute I/O, not defined 
lOCC[O:ll eM[zlOM[zdl; next 
Execute,lO,inst ruct ion) ) end Instructiondzecution IO Instruction Format: IO Address<O:15> := iOCCC0l address if IO data IO Device or Area<9:4> := IOCCCII<o:4> io device name 10 Function<5:7> := lOCC[l3<5:7> IO Modifier<8:25> := lOCC[l]C8:15> device function details Device mode off line := (IO Function = 0) Device mode write := (IO Function = I) Device mode read := (IO Function = 2) Device mode sense Interrupt level := (IO Function = 3) Device mode control := (IO Function = 4) Device mode initialize write := (IO Function = 5) Device mode initialize read := (IO Function = 6) Device mode sense := (IO Function = 7) 
Chapter 34 The engineering design of the Stretch co m pu terl Erich Bloch 
Summary The Stretch computer is an advanced scientific computer with variable facilities for floating-point, fixed-point, and variable-field-length arithmetic and data-handling facilities. 
The performance goal of 100 x 704 speed is achieved by high-speed circuits, multiplexing, and simultaneous-operation 
technique of instruction and data-fetching, 
as well a overlap within 
the execution units. This massive overlap and multiplexing 
results in complicated recovery routines between the look-ahead and instruction units. 
These units are described in detail, as are the arithmetic 
units and 
significant algorithms used in the floating-point arithmetic. A flexible set of circuits using a current-switching technique 
with overriding-level facility is described, as well as the packaging of circuits on printed cards. 
The frame 
and gate concept 
is also shown. Performance 
figures and hardware count 
illustrate the size, complexity, and performance 
of the system. Introduction The Stretch computer [Dunwell, 19561 project was started in order to achieve two orders of magnitude of improvement in perform- ance over the then existing 704. Although this computer, like the 704, is aimed at scientific problems such 
as reactor design, hydro- dynamics problems, 
partial differential equation etc., 
its instruc- 
tion set 
and organization are such that it can handle 
with ease data-processing problems normally associated 
with commercial applications, such 
as processing 
of alphanumeric fields, sorting, and decimal arithmetic. In order 
to achieve the stated goal of performance, all factors 
that go into the computer design must contribute 
towards the performance goal; this includes the instruction set [Buchholz, 19581, the internal system organization, the data 
and instruction word length, 
and auxiliary features such 
as status-monitoring devices, the circuits, packaging, 
and component technology. No one of them by itself can give this hundred-fold increase 
in speed; only by 
the combining and interacting of these contributing factors can this performance 
be obtained. 'Proc. EJCC, pp. 48-59, 1959 This paper reviews the engineering design of the Stretch System with primary concentration on the central computer 
as the main contributor to performance. In it, these new techniques, devices, and instructions have been pushed to the limit set by 
the present technology and, therefore, its 
analysis will 
convey best 
the prob- lems encountered and the solutions employed. The Stretch system 
Early in the system design, it appeared evident that a six-fold improvement in 
memory performance 
and a ten-fold 
improvement in basic circuit speed 
over the 704 was the best one could achieve. 
To meet the proposed performance 
criteria, the system had to be organized in such a way that it took advantage of every possible overlap of systems function, multiplexing 
of the major portion of the system, processing 
of operations simultaneously, 
and anticipa- tion of occurrences, wherever 
possible. The system had to be capable of making assumptions 
based on 
the probability that certain events 
might occur, 
and means had to be provided to retrace the steps when the assumption proved 
to be wrong. This simultaneity 
and multiplexing of operations reflects itself in the 
Stretch System at all levels, from overall 
systems organiza- tion to the cycle of specific instructions. In the following descrip- tion, this 
will be discussed in more 
detail. If one considers the Stretch System (Fig. 1) from an overall point of view it becomes apparent that the 
major parts of the system can operate simultaneously: a The 2-psec, 16,384-word core memories are self-contained, with their 
own clocks, addressing circuits, 
data registers and checking circuits. 
The memories themselves 
are interleaved so that the first two memories have their 
addresses distrib- uted modulo 2 and the other four are interleaved modulo 4. The modulo-2-interleaved memories are used primarily for instruction storage; since, for high-performance instruc- 
tions, halfword formats 
are used, the average rate of ob- taining instructions 
is one per '/z psec. Similarly, a 
0.5-psec 42 1 
422 Part 5 I The PMS level Section 2 I Computers with 
one central processor and multiple 
input/output processors INSTRUCTION MEMORIES (MOD 2 INTERLEAVED) I OPERAND MEMORIES (MOD 4 INTERLEAVED) 2p SEC CORE 2p SEC CORE Zp SEC CORE, 2p SEC CORE 
,2p SEC CORE 2p SEC CORE GI GI/G Gi '-I GI I I1 11 I, I/ I MEMORY IN BUS 1 MEMORY OUT 
BUS I CENTRAL COMPUTER t 1 I/O EXCHANGE DISK SYNCH UNIT 1 DISK 1 1 CONSOLE 11 READER CONTROL ADAPTER ADAPTER f?$pt,; & 4 x lo6 WORDS - T 5 TAPE TAPE TAPE ADAPTER ADAPTER ADAPTER 729- IX TAPE Fig. 1. The Stretch system. Y - the transfer of information from and to the memories by 
a memory bus permits new addresses, information, or both to pass through the bus every 
200 mpsec. 
linked with the memories and the computer through the Exchange, which, after initial instruction 
by the computer, coordinates the starting of the 1/0 equipment, the checking and error-correction of the information, the arrangement of the information into memory words, and the fetching and storing of the information from 
and to memory. All these functions are executed without the 
use of the computer, so it can in the meantime continue 
its data processing and computation. The central computer 
processes and executes the stored program. Here, now, the simultaneity and multiplexing of functions has 
reached its ultimate. 
a h The simultaneously-operating Input/Output units are b C c data-word rate is achieved by the use of four modulo-4 organized memories. 
The addressing of the memories and Before discussing 
the computer organization, a few general features must be mentioned for completeness: 
Word length: fj4 bits plus eight bits for parity checks and error-correction codes. Memory capacity and 
addressing: A possible 256,000 words 
can be randomly addressed. These 
storage positions are all in external 
memory, except for the 32 first addresses. These positions consist 
of the internal registers (accumulators, time 
clocks, index registers). 
The instructions are single-address instructions with the exception of a number 
of special codes that imply the second address explicitly. 
The instruction set (Fig. 2) is generalized and contains a 
full set for single- 
and double-precision floating-point 
arith- metic, and a 
full set for variable-field-length 
integer arith- metic (binary and decimal). It also has 
a generalized 
set for index modification 
and a branching 
set, as well as a set of 
Chapter 34 1 The engineering 
design of the Stretch 
computer 423 1/0 instructions. All told, 765 different types of instructions are used in the system. The instruction format (Fig. 
3) makes use 
of both half and full words; 
half words accommodate indexing and floating- point instructions 
(for optimum performance these 
two sets of instructions use a rigid 
format), and full-word formats are used by 
the variable-field-length instructions. Notice 
that the latter 
specifies the operand field by the address of its left-most bit, the length of the field, and the byte1 size, 
as well as the starting point 
(offset) of the implied operand ‚Byte: a generic term 
to denote the number of bits to be operated 
on as d (accumulator). Both halves 
of the word are independently indexable. A general monitoring device 
used for 
important status triggers is called the Interrupt [Brooks, 19571 System. This system monitors the flip-flops which reflect internal mal- functions, result 
significance (exponent range, 
mantissa zero, overflow, underflow), program errors 
(illegal instruction, protected memory area), and input/output 
conditions (unit not ready, etc.). The status of these flip-flops can cause a break in the normal progression of the stored program for fix-up purposes. Their status 
is automatically interrogated e a unit by a variable-field-length instruction. at all times. 
COMPUTER VOCABULARY MODIFIER I EXAMPLES INSTRUCTION CATEGORY NUMBER OF INSTR CLASS VARIABLE FIELD LENGTH ARITHMETIC BINARY DECIMAL SIGNED ADD (TO 
MEMORY) UNSIGNED LOAD /STORE SAME SIGN NEGATIVE SIGN DIVIDE 280 32 3lN/DEC RADIX CONVERSION LOGIC CONNECTS I 48 I 16 LOGIC STATEMENT FLOATING POINT 
ARITHMETIC NORMALIZED UNNORMALIZ E D SAME SIGN OPPOSITE SIGN NEGATIVE SIGN NOISY MODE ADD (SINGLE 8 DOUBLE) LOADISTORE MPY/(SINGLE 8 DOUBLE) DIV (WITH REMAINDER) INTERCHANGE DIVIDE CUMULATIVE MPY SQUARE ROOT 
24 0 INDEXING ARITHMETIC DIRECT I M MEDl ATE 
PROGRESSIVE 43 BRANCHES UNCONDITIONAL INDEXING IN DICATOR BIT ﬁ{b SET 0 LEAVE BIT INVERT BIT STORE INST CTR 68 TRANSMIT/SWAP I10 INSTRUCTION 24 I TOTAL 735 Fig. 2. The instruction set. 
424 Pari 5 I The PMS level I I I I I I I I I I I I I I YTE 8 BYTE 7 BYTE 6 BYTE 5 BYTE 4 BYTE 3 BYTE 2 BYTE 1 Section 2 I Computers with one central processor and multiple 
input/output processors ECC PTY DATA FORMATS INDEX WORD ECC - I PARITY I+ COUNT REFILL VALUE LUAl INb I EXPONE NT~Y POINT DATA WORD ADR Isin/&! I I COUNT WORD MANTISSA ( FRACTION 1 ECC PARITY REFILL 4- " tLl 1-1 I PAR I I ADDRESS DI R ECT INDEX J OP I ,FLAG 0 I8 25 28 46 63 71 INSTRUCTION FORMATS 
I ADDRESS 1;' S OP\Ol I I POINT I1 I 0 18 28 31 'BINARY DECIMAL Fig. 3. Data word-and instruction word formats. 
organization of Stretch, where two instruction words and four operands can 
be fetched simultaneously. In 
addition, the execution of the instruction is done in parallel and simultaneously with the described fetching 
functions. All the units of the computer are loosely coupled together, each one controlled 
by its own 
clock system, which 
in turn is synchro- nized by a master 
oscillator. This 
multiplexing of the units of the computer results in a large number of registers and adders, since 
The Stretch computer If one considers the internal organization of the majority of corn- 
puters that have been produced during the 
last eight years (and the 704 is a case in point), the organization looks as shown in Fig. 4a. There is a sequential 
flow of 
instructions into the computer, 
and after due 
processing and execution, the next instruction is called from memory. 
Compare this with Fig. 4b, showing the 
Chapter 34 I The engineering design of the Stretch 
computer 425 time-sharing of the major computer organs is no longer possible. 
All in all, 
the computer has 3,000 register positions and about 450 adder positions. Despite the multiplexing and simultaneous operation of suc- cessive instructions, the result appears as if sequential step-by-step internal operation were 
utilized. This has 
made the design of the interlocks quite complex. Data flow The data flow through the computer 
is shown in Fig. 5 and is comparable to a pipeline 
which in a steady 
state (namely, once 
filled) has 
a large output rate 
no matter what 
its length. 
The same is true here; after start-up the execution of the instructions is fast and bears 
no relation at all to the stages it must progress 
through. DATA WORD 
1 INSTRUCTION INSTRUCTION FETCH INSTRUCTION DATA WORD 
INSTRUCTION EXECUTION 4 INSTRUCTIONS 4 DATA WORDS INSTRUCTION INSTRUCTION I UPDATING I I EXECUTION I 70 4 STRETCH Fig. 4. Comparison of Stretch and 
704 organization. 
426 Part 5 1 The PMS level r 2 WORD Section 2 I Computers with one central processor 
and multiple inputloutput processors I 2WORD FR EXCHANGE TO EXCHANGE I INSTR WORD BUFFFR~ A [OPERAND BUFFER I INSTR WORD BUFFER INDEXING UNIT 
OPERAND BUFFER OPERAND BUFFER e OPERAND BUFFER 
LOOK-AHEAD CHECKER IN BUS - v I1 ,;PiTRANSFER BUS 11 I I I OPERAND 171 I REGISTER ACCUMULATOR 1 A.B INTERRUPT 1 SYSTEM I AR ITH METlC 1 CHECK 11 ARITH CHECKER INBUS fi SERIAL ARITH UNIT 
Fig. 5. Stretch computer-units and data flow. The Memory Bus is the communication link between the mem- ories on one 
side and the 
exchanges and the 
computer on 
the other. It monitors the requests for storage to, 
or fetches from, memory, and sets up a priority scheme. 
Since 1/0 units cannot hold 
up their requests, the exchange will get highest priority, 
followed by the computer. In the computer the instruction-fetch mechanism 
has priority 
over the operand-fetch mechanism. 
All told, the memory bus gets requests 
from and assigns priority to eight differ- ent channels, Since memory 
can be accessed from 
multiple sources, and once accessed it is on its own 
to complete its cycle, 
a busy 
condition can exist. Here again, the memory bus tests for busy conditions 
and delays the requesting unit until 
memory is ready to be inter- rogated on data fetches. The return address is remembered and the requesting unit receives the information when it becomes available. To accomplish this, from the time information is re- quested the receiving data register is in a reserved status. 
Requests for stores and fetches can be processed at a 200 mpsec rate and 
the time, if no busy or priority conditions 
exist, to return the word to the requesting unit 
is 1.6 psec, a 
direct function of the memory read-out 
time. The Instruction Unit [Blaauw, 19591 is a computer of its own. 
It has its 
own instruction set, its own small memory for index word storage, and its own 
arithmetic unit. During its operation as many as six instructions can be at various stages 
of execution. The Instruction Unit fetches 
the instruction words 
from mem- 
Chapter 34 I The engineering design of the Stretch 
computer 427 ory, it steps the instruction counter, and performs the indexing of instructions and the initiation of data fetches. After a preliminary 
decoding of the class of instruction, it recognizes its own instruc- 
tions and executes indexing 
instructions. On branches, conditional 
or unconditional, the instruction unit executes these. In the case of conditional branches, 
it makes the assumption that the 
branch will not be successful. This assumption 
and the 
availability of two full-word buffer 
registers keep the flow of instruction to the computer 
continuous. Therefore, the rate of instructions entering the instruction unit is for all practical purposes independent of the memory cycle. Since, for high 
speed instructions, half-word formats are used, four of these at any one 
time can 
be in buffer storage. As soon as the instruction unit starts processing an instruction, it is re- moved from 
the buffer, 
thus making room for the next memory- 
word access 
(Fig. 6). Incidentally, half-word instructions and full-word instructions can be intermixed within 
the same word, 
and therefore the latter 
can cross a word boundary. This permits maximum packing of instructions in memory and also serves as 
a facility for automatic program assemblers 
and compilers. The adder path, 
index registers, 
and transfer bus to look-ahead complete the instruction unit 
system (Fig. 6). It should be noted 
that the index registers 
are part 
of the instruction-unit data path, 
therefore permitting fast access 
(no long transmission lines) 
to an index word. 
There are 
16 index words 
available to the programmer. The index registers, consisting 
of multi-aperture cores, are oper- MEMORY OUT BUS 
- - D LOOKAHEAD LOAD LINES * \ y CHECKER IN BUS Y V D MEMORY ADDRESS BUS J. D Fig. 6. Instruction unit. 
428 Part 5 I The PMS level Section 2 I Computers with one central prbcessor and multiple 
inputloutput processors ated in a non-destructive 
fashion, since 
in a representative 
pro- gram, the index word 
is used nine out 
of ten times without modi- fying it. This permits 
fast operation under these conditions, and additional time is only applied where modification is involved. After processing 
through the instruction unit, the updated (in- dexed) instruction enters a level of the Look-ahead (Fig. 5). Besides the instruction, all 
necessary information, its associated instruction counter value, and certain tag 
information are also stored in the same level. 
The operand, already requested by 
the instruction unit, will enter this level directly and will be checked 
and error- corrected while awaiting 
transfer to the arithmetic 
units for execu- 
tion. An interlocked counter mechanism in the look-ahead keeps 
its four levels 
in step, preventing out-of-sequence execution 
of in- structions, even if all information for a succeeding one 
is available, before the previous instruction has been started. 
The pre-accessing of operands by the look-ahead and of instruc- tions by 
the instruction unit 
leads sometimes 
to embarrassing positions, for which 
a fix-up routine must be provided. Consider 
the program (n) STORE Accumulator m 
(n+ 1) LOADR (n+2) ADDm and assume instruction (n) is in look-ahead, waiting for execution. 
If (n + 2) now enters the look-ahead, a reference to m cannot be made, since the data 
stored in that position is subject to change by the STORE instruction. 
The look-ahead must recognize this 
and ﬁforwardﬂ the result of instruction (n), when received, to 
the level where (n + 2) is stored. Another example 
is the case where the instruction unit 
assumed that a conditional 
branch would not be executed. 
This instruction is stored in look-ahead and, when it 
is recognized that the 
branch was successful, all modifications 
of addressable registers made by 
the instruction unit in the meantime must be restored. Look-ahead in this case acts as a recovery memory for this information. A similar condition exists when interrupts occur due to arithmetic results. The look-ahead here again has the data stored pertaining to registers which 
were modified erroneously 
in the meantime. The restoring and recovery routines described break 
into the instruc- tion unit processing, interrupting temporarily the flow of instruc- tion and their indexing. The arithmetic units described later are slaves to the look- ahead, receiving not only operands and 
instruction codes but also the start-execution signal. Conversely, 
the arithmetic units signal to the 
look-ahead the termination of an operation and, in the case of ﬁTo Memoryﬂ operations, 
place into the look-ahead the result word for transfer 
to the proper memory position. Arithmetic units The design of the arithmetic units was established along lines 
similar to the design of look-ahead and the 
instruction unit. 
Every attempt was made to speed up the 
execution of arithmetic opera- tions by 
multiplexing techniques 
and overlapping of the algo- rithm, where mathematically 
permissible. The arithmetic units, consisting 
of the Serial Unit 
and the 
Parallel Unit, use the same arithmetic registers, namely a double- 
length accumulator 
(A$) consisting of 128 bits and a double-length 
operand register (C,D) consisting of 128 bits. The reason for 
the use of the same arithmetic registers is the fact that at any time, a shift from floating-point 
to variable-field-length operation (or vice uersa) can be made by 
the program. Therefore, 
the result obtained by a floating-point operation can serve as the starting 
operand for a variable-field-length operation. The chief reason for 
the double- length registers is the definition of maximum field length to be 
64 bits. The field can start with any bit 
position, and therefore can cross the word boundary. The executions of floating-point mantissa 
operations and 
varia- ble-field-length binary multiply and divide operations 
are per- formed by 
the parallel unit, whereas the floating-point exponent 
operation and 
the variable-field-length binary and decimal add- 
type operations are executed by 
the serial unit. The square-root operation and 
the binary-to-decimal conversion algorithm are executed in unison by 
both units. Salient features 
of the two units will now be described. The serial arithmetic 
unit [Brooks et al., 19591 (Fig. 7). The serial arithmetic consists of a switch matrix 
which can extract 16 con- secutive bits from A,B and C,D. These 16 bits then can 
be aligned in such a way 
that the 
low-order bit of a field as specified by 
the in- struction is at the right end of the field. This 
wrap-around circuit then feeds into a carry-propagate 
adder or, in 
case of logical-con- nect instructions, into 
the logic unit. At the adder output, 
a true complement unit and a binary-to-decimal correction 
unit are used for subtract and 
decimal operations. The inverse process 
of ex- tracting is used to insert 
the processed byte back into the 
register without disturbing any neighboring 
positions. Notice 
that in one clock cycle, the information is extracted, the arithmetic 
is per- formed and the result inserted back into the registers. In addition, the arithmetic information is checked by 
parity checks on the switch matrices 
and by duplication and comparison of the arith- metic procedure in a duplicate unit. 

Chapter 34 1 The engineering design of the Stretch 
computer 429 I WRAP AROUND (8 OF 16) FR LOOK-AHEAD 1 1- WRAP AROUND (8 OF 16) KCUMULATORS -+I - 8 BIT TRUE 
/COM P - PASS AROUND (8 BITS) TI OPERAND REGISTERS *I TRUEKOMP 8 BIT (8 BITS) PASS AROUND TRUEICOMP 1 DECIMAL CORRECT WRITE IN I 16-16 1 WRITE IN MATRIX I I Fig. 7. Serial arithmetic unit. 
Parallel arithmetic unit. The parallel arithmetic unit 
(Fig. 8) is designed to execute floating-point operations with a 
maximum of efficiency. Since both single- and double-precision arithmetic is performed, the shifter and adder exist in a 
double-length format 
of 96 bits. This insures almost 
the same performance for single- 
and double-precision arithmetic. The adder 
is of a carry-propaga- tion type with 
look-ahead over 
4 bits at a time 
to reduce the delay that normally results 
in a ripple-carry adder. 
This carry look-ahead results in a delay time of 150 mpec for 96-bit binary-number additions. All additions and subtractions are made in one's com- 
plement form with automatic end-around carry. The shifter is capable of shifting up to 4 positions to the right and up to 
6 positions to the left. This shifter arrangement takes care of the majority of shifting operations encountered under 
normal operation. Where higher-order shifts 
are required, a suc- cessive operation is set up between the 
parallel unit register and the shifter. To expedite the execution of the multiply instruction, 12 bits 
430 Part 5 I The PMS level Section 2 I Computers with one central processor 
and multiple input/output processors I 3 BITS 3 BITS 3 BITS UNIT REGISTER I7 CARRY PROPAGATE ADDER 100 BITS SHIFTER III ADDER 1 3 BITS CSA 2 I 1 I I s2 I C1 c2 J. I 1. CSA 3 I 1 SUM REG CARRY REG Fig. 8. Floating-point arithmetic unit. 
of the multiplier are handled within 
one cycle. This is accom- plished by 
breaking the 12 bits into groups of three bits 
each. The action is from right to left and consists of decoding each group of three bits. By observing the lowest-order bit of the next higher group, a 
decision is made as 
to what multiple 
of the multiplicand one must add to the partial product. 
Since only even 
multiples of the multiplicand are available, subtraction and addition 
of the multiples can 
result. The following example will 
elaborate this point: (MCD means multiplicand) Groups n+4 n+3 11 + 2 n+l Multiplier, 12 bit group 
xxo 011 110 101 n 010 Octal value 3 6 5 2 If two addition5 
of multiples were permitted 
4 x MCI) 6 x MCD 6 x MCD 2 x MCD -1 x MCD -1 x MCD Instead of subtracting 1 x ,MCD in n + 1, subtract 8 X MCD in n. 2 x MCD -8 x MCD 6 x ,MCII 4 X MCD 6 x iMCD -8 x MCD Resulting decoding 4x MCD -2x MCD 6 x MCD -6 x MCD The four multiple multiplicand 
groups and the partial product of the previous cycle are now 
fed into carry-save adders of the form, 
Chapter 34 I The engineering 
design of the Stretch computer 431 Sum S =AWBWC Carry C‚ = AB + AC + BC There are 
four of these adders, two 
in parallel followed by two more in series (Fig. 8). The output of Carry-Save Adder 
4 then results in a 
double-rank partial product, 
the product 
sum and the 
product carry. For 
each cycle this 
is fed into Carry-Save Adder 
2, and, during 
the last cycle, into 
the carry-propagate adder, for accumulation of the carries. Since 
no propagation 
of carries is required in 
the four cycles, 
where multiple multiplicands are 
added, this operation 
is fast 
and is the main contributor 
to the fast multiply-time of Stretch. The divide scheme 
[Robertson, 19581 has a similarity to the multiply scheme. Multiples 
of the divisor are used, namely, 
3/2 x divisor, 3/4 x divisor and 1 x divisor. This, plus shifting 
over strings 
of ones and zeros, results 
in the generation of the required 48 quotient bits within 
thirteen machine cycles. Most machines using 
a nonrestoring divide method require 
48 cycles for 48 quotient bits. The following example explains this 
technique. This scheme depends 
on the use of normalized divisors: DIVIDEND (DD) = 101000000000000 DIVISOR (DR) = 1100011 2™s COMP DR (DR) = 0011101 3/4 DR = 100101001 (a) Using skip ouer 1/0 only: 101000000000000 DIVIDEND 1 101 101 
Step 1: 0011101 ADD DR Remainder negative, 
1st quotient hit 
= 0; shift one position. Leading 1 indicates that next quotient bit 
must be 1; Q,Q2 = 01 011010000 REMAINDER Step 2: 1100011 ADD DR 100101 - 11 Overflow: Remainder positive and Q:$ = 1, leading zero indicates Q4 = 0 1011100 REMAINDER Step 3: 0011101 ADD DR 1llJ001 Negative remainder; Qn = 0; leading 1™s indicate QBQ7Q8 = I11 Number of quotient bits per cycle: Cycle 1: 01 = 2 Cycle 2: 10 = 2 Cycle 3: 0111 = 4 (b) The same problem uith hotli skip ozjer 1/0 and 3/4 - %3/2 complement: 101000000000000 11011010000 Step 1: 0011101 Same as before, QIQ2 = 01 100101001 11 11 1 1001 Step 2: Add 3/4 DR This (by table 
look-up) indicates QRQ4QsQ6Q7Q8 = 100111 Quotient bits generated 
per cycle: Cycle 1: 01 = 2 Cycle 2: 100111 = 6 In general, this 
method results in the generation of 3.7 quotient bits per subtraction. While the 
mantissa operations of multiply and divide are performed by 
the parallel unit, the 
serial arithmetic unit executes the exponent arithmetic. Here again 
is a case where overlap and simultaneity of operation is used to special advantage. Checking. The operation of the computer 
is checked in its entirety and correction codes are employed where data transfers from memory and input-output 
units are involved. In particular, 
all information sent to memory has 
a correction code 
associated with it, which is checked for accuracy on its way from 
memory. If a single error is indicated, then correction is made and the 
error is recorded via a maintenance 
output device. Within the machine, 
all arithmetic operations are checked, 
either by parity, duplica- 
tion, or 
a ﬁcasting out 
threeﬂ process. These checks are overlapped with the execution of the next instruction. Hardware count. 
Figure 9 shows the percentage 
of transistors used 
in the various sections 
of the machine. It becomes obvious that the parallel unit and the instruction unit 
use the highest percent- age of transistors. In case of the parallel unit this is due to the extensive circuits for multiply and to the 
additional hardware 
to achieve speed 
up of the divide scheme. 
In the instruction unit, 
the controls consume 
the majority of the transistors, because of the high multiplexed operation encountered. Performance. The performance comparisons in Fig. 10 show the increase in speed achieved, 
especially in floating-point operations, 
432 Part 5 1 The PMS level 10,500 Section 2 1 Computers with one central 
processor and multiple 
input/output processors 6.0 2 UNIT 17,900 8,600 IO ,000 10,000 8,700 32,700 3,000 24,500 6,000 169,100 MEMORY CONTROLS 15.6 1 1-1/2 5.9 1 IO. 5 1 - 1/2 1 21 .o 2-1/2 1/2 14.5 1 3.5 1/2 100.0 18 INSTRUCTION UNIT DATA PATH CONTROLS LOOK-AHEAD DATA PATH CONTROLS ARlTH REGISTERS SERIAL ARITH UNIT DATA PATH CONTROLS FLOATING PT 
UNIT DATA PATH CONTROLS ~___ CH ECKl NG INTERRUPT SYSTEM TOTAL DOUBLE CARDS 4,025 SINGLE CARDS 18,747 POWER 21 KW NO. OF TRANSISTORS 1 % OF TOTAL 1 NO. OF FRAMES 17,700 19,500 2 3-1/2 Fig. 9. Component count. over the 704. It should be noted that for a large number of prob- lems this 
particular increase in all arithmetic speeds is almost proportional to the performance increase of the problem as a whole, since the instruction execution-times are overlapped to a great extent with 
the preparation and fetching of instructions. Simulation of Stretch programs on the 704 proved a performance 
of 100 x 704 speed in mesh-type calculations. Higher performance 
figures are achieved where 
double- or 
triple-precision calculations 
are required. 
Chapter 34 I The engineering design 
of the Stretch computer 
433 Circuits Having reviewed 
the systems organization of Stretch, it 
is now of interest to discuss briefly 
the components, circuits, 
and packag- ing techniques used to implement the design. The basic component used in Stretch 
is the high-speed drift 
transistor which 
exists in both an NPN and a PNP version. This transistor has 
a frequency cut-off of approximately 100 mc and for high-speed operation 
must be kept out 
of saturation at all times. 
This then explains why both the PNP and 
NPN version are used: mainly to avoid the problem of level translation, which would 
be required due to the potential difference of the base and the col- lector. This difference 
is 6 volts, an optimum point for this device. 
Figure 11 shows the basic circuit configuration. It consists of a current source, represented by the -30 volt supply and resistor R. The functional operation 
of the circuits consists of two possible 0 PER AT I ON 1. FLOATING POINT 
EXPONENT RANGE 
MANTISSA BITS 
FLOATING ADD FLOATING MPY 
FLOATING DIV LOADISTORE 2. BINARY VARIABLE 
FIELD LENGTH ARITH BIT RANGE 16 ADD/ LO AD/STOR E BIT [ MPY FIELD DIVIDE 3. DECIMAL AR ITH METI C DIGIT RANGE 5 DIGITS LOAD /STORE 4. MISCELLANEOUS ERROR COR R ECTlO N CHECK I NG WORD SIZE IBM 70 4 +_ 128 22 27 84 pSEC 204 pSEC 216 pSEC 24 pSEC NO NO 36 BITS IBM 705 1 MEM CAPACITY 119 pSEC 799 pSEC 4828 pSEC 204 pSEC NO YES STRETCH 2 2048 22 48 1 .O pSEC 1.8pSEC 7. OpSEC O.6pSEC 1 TO 64 2.0pSEC IO .O pSEC 15.0 pSEC 1 TO 21 3.5 pSEC 40.0 pSEC 6 5.0 pSEC 3.2 pSEC YES YES 64 BITS Fig. 10. Comparison of Stretch and 
7051704 operation times. 
434 Part 5 1 The PMS level Section 2 I Computers with one 
central processor and 
multiple input/output processors AN TRUTH TABLE CIRCUIT DIAGRAM AN I+ I - II- IL I A hi -- OUTPUT -5.2V - 5.6V - 6V INPUT +.5v MIN- MAX -+ .4v REF ov REF SIGNAL V 0 LTAG ES 6.4V /ILuLLL76.5V CIRCUIT RESPONSE DELAY X 20MpSEC OUTPUT Fig. 11. Current switching circuits (+AND). 
Chapter 34 I The engineering design 
of the Stretch computer 
435 AN OP SYMBOL TRUTH TABLES 6 m A + CIRCUIT (AN) CIRCUIT AN - + 30 - OUTPUTS INPUTS A,B8X Z22ZE~:~ MIN-MAX SIGNAL VOLTAGES REF GND ONLY X INPUT ONLY - 6.0V ym - 6.4 6.5 (ALL OUTPUTS) OUTPUT DELAY= 20MpSEC INPUT CIRCUIT RESPONSE Fig. 12. Third-level circuit. 
436 Part 5 I The PMS level Section 2 I Computers with one central processor and multiple 
input/output processors CIRCUIT TRUTH TABLES CIRCUIT DIAGRAM AN ON MIN- MAX SIGNAL VOLTAGES Eli --- AND a A 442il 63.4: A. +6V + 6V i1.21, - 6V - 6V 1. .3.5 d +6V REF GN D REF GND BEG OF CHAIN - .35 END OF CHAIN (4) A+ Fig. 13. Emitter-follower circuit. 
Chapter 34 I The engineering design of the Stretch 
computer 437 paths represented by transistor 
A or C. Which path 
is chosen by the current depends on the condition existing on base A. If point A is positive with respect 
to ground by 0.4 volts, that particular transistor is cut off, making the emitter of transistor C positive with respect 
to the base and, therefore, making C conducting. The current supplied by the current 
source (6 ma) will then flow through transistor C to the load $. Output 6, then, is positive by 
0.4 volts with respect 
to the -6 volt reference. This indicates 
at @ the equivalent function 
impressed on A. At the same time, 
s is negative with respect 
to the -6 volt power supply by 0.4 volt, representing, therefore, 
the inverse of the function impressed on A. Conversely if A is negative with respect 
to the ground reference, transistor A is the conducting one, keeping emitter C negative with respect to its base. The current flows through transistor A, making @ positive with respect to -6 and @ negative with respect to -6. Again, the output of @ reflects the function impressed on A, whereas If an additional transistor 
now is paralleled with A, it becomes obvious that only if both bases A and B are positive will output - represents the inverse of the function. @ be positive and $ negative. If any or none 
of the bases A and B are positive, then @ will be negative and will be positive. In other words, an AND 
function is obtained on output @. This principle, which 
is reflected in all 
the circuits, is essen- tially the principle of current switching or current steering. Logical functions for the PNP circuits are, therefore, 
a +AND or -OR. Two outputs from each circuit block are available: the AND function and the inverse of the AND function. A dual circuit exists for NPN 
transistors with input levels at -6 volts and output levels at ground. This circuit 
will give 
the +OR or -AND function. A thorough investigation of the systems design showed 
that the circuits described 
so far are versatile enough to be used throughout the system. However, there are 
enough special cases (resulting from the many data buses and registers throughout the machine) that could use a distributor function 
or an overriding function. 
This caused 
the design of a circuit which 
permitted great savings in space and transistors by 
adding a third voltage level. 
Figure 12 shows the PNP version of the third-level circuit. Fig. 14. The circuit package. 
438 Part 5 I The PMS level Section 2 I Computers with one central processor and multiple 
inputloutput processors If transistor X were eliminated, 
then transistors A and B in conjunction with 
the reference transistor C would work normally 
as a current switching circuit, in this case a +AND circuit. If transistor X is added with the stipulation that the down level of X is more negative 
than the lowest possible level 
of A or B, it becomes apparent that when X is negative, the current 
will flow through that branch of the circuit in preference to branch @ or +, regardless of inputs A and B. Therefore, the output 
of @ and @ will be negative, provided input X is negative. Output Ill is the inverse of input X. If, however, X is positive, then the status 
of A and B will determine the 
function @ and 5 implicitly. This demonstrates the overriding function 
of input X. Similarly, the NPN version (not shown) results 
in the OR function of C+ if input X is negative and 
in a positive output at @ and T, regardless of status A and B, if X is positive. Again minimum and maximum signal swings are shown in Fig. 12. The speed of the circuits described so far depends on the number of inputs and the 
number of circuits driven from 
each load. The response of the circuit 
is anywhere between 12 and 25 mpsec per logical step with 
18 to 20 mpsec average. The number of inputs allowable per circuit is eight. The 
number of driven circuits is three. Additional circuits are needed to drive more than three bases and where current 
switching circuits communicate over long lines, termination networks must 
be added to avoid reflections. To improve the performance of the computer in certain critical places, emitter-follower logic is used as shown in Fig. 13. These circuits, having a 
gain less than one, after a number 
of stages require the use of current switching circuits as level setters and gain devices. Both AND 
and OR circuits are available 
for both a ground-level and a -6-level 
input. Change from a -6-level circuit to 
a ground-level circuit is obtained by applying 
the ap- propriate power supply 
levels. Due to the variations in inputs and 
driven loads, the circuits must be designed so that the 
load can vary over 
a wide range. This 
resulted in instability 
which had to be offset by the feedback capacitor C shown in the circuit. All functions needed in the computer can be implemented by the use of the aforementioned circuits, including flip-flop opera- tion, which is obtained by tying a PNP 
current switch block and an NPN current switch block together with proper feedback. 
- - A circuit package using 
the smaller of the two printed 
circuit boards shown 
in Fig. 14, called a single card, contains AND or OR circuits. It should be mentioned 
that the 
printed wiring is one-sided and that besides the components and transistors, a rail is added which permits 
the shorting or addition of certain loads depending on the use of the circuits. This rail then has the effect of reducing the different types 
of circuit boards in the machine. Twenty-four different boards are used and of these, two types reflect approximately 70% of the total single card population. Due to the large number of registers, adders, and shifters used 
in the computer, it seems reasonable 
that functional packages could be employed economically, because of wide usage. This 
results in the high-density package also shown 
in Fig. 14, called Packaging The circuits described in the last paragraph are packaged in two ways: Fig. 15. The back panel. 
Chapter 34 1 The engineering 
design of the Stretch computer 439 a Double 
Card, which has 4 times the capacity of a single card and which has wiring on both 
sides of the board. Furthermore, 
components are double-stacked; and again, the rail is used to effect circuit variations 
due to different applications. 
Eighteen double card types are 
used in 
the system. Approximately 4,000 double cards are used, housing 
60% of the transistors. The rest of the transistors are on approximately 18,000 single cards. The cards, both single and double, are assembled in 
gates, and two gates are assembled into a frame. 
Figure 15 shows the gate 
back-panel wiring, using wire-wraps; 
and Figs. 16 and 17 the frame construction, both in 
a closed and open version. To achieve high performance, special emphasis 
must be placed on keeping noise to a 
low level. This 
required the use of a plane 
Fig. 17. The frame 
(extended). which overlies the whole back panel, against which 
the intercircuit wiring is laid. In addition, the power-supply distribution 
system must be of such a low 
impedance that extraneous noise cannot induce circuit 
malfunction. For this reason, 
a bus 
system, consist- 
ing of laminated copper 
sheets, is used to distribute the power to each row of card sockets. The wiring rules are such that single- conductor wire is used up to a maximum of 24ﬁ, twisted pair 
to a maximum of 36ﬂ, unterminated coax to a maximum of 60ﬂ, and terminated coax to a maxirniim of 100 feet. The whole back-panel 
construction and the 
application of single wire, twisted pair, or 
coax are calculated by a 
computer program to minimize the noise on each circuit node. 
The two gates of a frame are a sliding pair with 
the power supply mounted on the sliding portion. 
All connecting wires between frames are coax and arrayed in layers 
which are formed into a drape. References BlaaG59; BrooF57a, 59; RuchW58; DunwS.56; RobeJ58; RlosRliO; BnchW57, 62; RrooF6O; CockJ59; CoddE,5iY, 62. Fig. 16. The frame (closed). 
Chapter 35 PILOT, the NBS multicomputer system1 A. L. Leiner / W. A. Notz / J. L. Smith A. Weinberger Summary PILOT, the new NBS system, possesses both powerful external control capabilities and versatile 
internal processing capabilities. It contains three independently operating computers. 
The primary 
and secondary computers each 
utilize only 16 basic types of instructions, thus 
providing a simple code 
structure; but because so many variations of the formats are possible, a wide variety of computing, data-processing, and 
informa- tion-retrieval operations 
can be performed with these 
instructions. The secondary computer is specially adapted for performing so-called 
ﬁred- tapeﬂ operations, and both 
the secondary and the primary computers, acting 
co-operatively, can carry out special complex sorting or search operations. 
The third computer 
in the system, called the format controller, is specially adapted for performing editing, inspecting, and format 
modifying opera- tions. The 
system is equipped to 
transfer information concurrently along several input-output trunks, though only two are planned 
for the near future. Using two such 
trunks, it is possible to maintain two continuous 
streams of data simultaneously flowing between any two 
external units and 
the internal 
memory, without interrupting the 
data-processing program. 
The system can operate with a wide variety of input-output devices, both digital and 
analog, either proximate or remotely located. The external control capabilities 
of the system enable the machine to supervise 
this wide family of external devices and, on an unscheduled basis, 
to interrupt or redirect its overall program 
automatically, in order to assist or manage them. At the National Bureau 
of Standards (NBS) a new large-scale digital system has been designed for carrying out a wide range of experimental investigations 
that are 
of special importance to the Government. The system can be utilized for investigating new or stringent applications of these general types: (1) data-processing applications, in which 
the system can be used for 
performing accounting and information-retrieval operations 
for management purposes; (2) mathematical applications, in which the system can be used for 
performing mathematical calculations for scientific 
purposes, including 
scientific data-reduction; (3) control applica- tions, in which the system can be used for performing real-time 
control and simulation operations, 
in conjunction with analog computer facilities or 
in conjunction with other instrument 
instal- lations, remotely located if necessary; and (4) network applications, 
‚P~oc. EJCC, 71-75 (1958). in which 
the system can be used in conjunction with other digital computer facilities, forming an interconnected communication network in which all 
the machines can work together collabora- tively on large-scale problems 
that are 
beyond the reach of any single machine. Because the system was 
designed for such varied 
uses (ranging from automatic search and interpretation of Patent Office records to real-time scheduling 
and control of commercial aircraft 
traffic), the system is 
characterized by a variety of features not ordinarily associated with a single 
installation, namely: 
a high computation rate, highly flexible control facilities 
for communicating 
with the outside world, 
and a wide repertoire 
of internal processing formats. 
The system contains three independently programmed computers, each of which is specially adapted for performing certain classes of operations that frequently occur in 
large-scale data-processing applications. These computers intercommunicate in 
a way that permits all three of them to work together concurrently 
on a 
common problem. 
The system thus provides a working model of an integrated multicomputer 
network. System organization 
Exclusive of data-storage and peripheral equipment, 
the central processing and control units 
of the over-all system 
contain ap- 
proximately 7,000 vacuum tubes and 165,000 solid-state diodes. The basic component for these units 
is a modified 
version of the one megacycle package used 
in the NBS DYSEAC, which in turn was evolved from the hardware used in NBS Electronic Automatic 
Computer (SEAC). As a result of a more effective 
logical design 
and faster memory, however, 
the new NBS system will 
run more than 100 times faster 
than SEAC on programs involving only fixed-point operations; for programs involving 
floating-point ma- nipulations, the advantage exceeds 1,000. The arithmetic speed of the new system derives in a large part from connecting a novel type of parallel adder to a diode-capacitor memory 
capable of providing one random access per microsecond. The system contains seven 
major blocks, 
which are indicated in Fig. 1, namely: (1) the primary computer, in 
the lower center 440 
Chapter 35 1 PILOT, the NBS multicomputer system 441 INPUT-OUTPUT CONTROL c FORMAT INPUT-OUTPUT CONk1,R,RENT CONTROLLER TRUNKS TR4NSFERS I Table 1 Arithmetic operation times 
(including 4 random access times to last 
memory) SECONDARY STORAGE 
HIGH SPEED INTERNAL 
MEMORY HIGH SPEED INTERNAL 
MEMW 68-817 WORDS.. . CONCURRENT 16-BIT WORDS, De14 60 STORAGE LOCATIONS ---------- --.I TRANSFERS . 32,768 TOTAL ADDRESSIBLE STmAGE WORDS Total time (microseconds) A VI v) ADDRESS D4T4 FOR 0 2 62 3- D a0 Y) PROGRIM 8 DAT4 FOR SECOND4RY Operation v Minimum- Average maximum 
1 Fixed-point Addition, Subtraction, Comparison . . 
Fixed-point Multiplication . . . 
. . 
. , . . 
. . 
. . . 
.31 . . 
. . 
.22-40 Fixed-point Division . , . , . . 
. . . . . 
. . 
, . , , 
. , .73 . . 
.72-74 Floating-point Addition, Subtractiont , . . . 20 . . . . . 
. 19-21 Floating-point Multiplication, 
. , . . . 
, . . 
, . . . . .37 . . 
. .28-46 t For shift of 4 bits. 7.5. . . 
. . 
. 6-9 8 ~ MANUAL, DATA 4ND DISPLAYS ~~~~~~o~!~~~,ﬁ,s CONTROL SIGN4LS 4 of the figure, (2) the primary storage, upper center; 
(3) the second- ary computer and the secondary storage, right; 
(4) the input-output 
control, upper left; 
(5) the external storage units, 
upper far left; (6) the external input-output units such as readers, printers, 
and displays, lower 
far left; and 
(7) lower left, the external control 
containing the special features 
that facilitate communication with 
people and 
devices in the world outside 
the system which 
is remotely located 
if necessary. Interchanges of information between the system and the 
outside world can take place 
at any time, on 1 ARITHMETIC a PROGRAM PROGRAM ARITHMETIC 
8 CONTROL UNIT FSUCESSING UNIT TWO-ADDRESS BINARY, 
PROCESSING UNIT CONTROL UNIT BINARY a DECIMAL, THREE-ADDRESS ~~ .FIXED a FLOATING l$~~~Oﬂ * SYSTEM 16-811, DIRECT INSTRUCTIONS EXPLICIT NEXT 
POINTS, FULLa HALFWJrnS sE%%~k?$xT 4ND mNTRDL SIGN4S INSTRUCTION (I6 VARIETIES) 4 (16TYPES) (16 BASIC TYPES) a completely impromptu 
basis, at the instigation of either the system or the external world, or both acting 
jointly. The primary computer, a 
high-speed general-purpose com- puter, contains both an arithmetic unit 
and a 
program control unit of considerable versatility. This 
computer can carry out a 
variety of high precision 
arithmetic and logical processing 
operations, in 
either binary or decimal code and 
in a wide 
variety of word lengths and formats. Its partner computer, 
the secondary computer, spe- cializes in short-word operations, usually manipulations on address numbers or other ﬁred-tapeﬂ 
information, which it supplies auto- matically as needed to the primary program. 
The third computer 
of the system, called the format controller (see input-output con- trol in 
Fig. l), is specially designed for 
carrying out 
editing, inspecting, and format-modifying operations on data that are flowing in or out of the internal memory via the peripheral external units of the system. All three computers, and all the external units 
of the system, share access privileges 
to the 
common high-speed internal memory, which 
is linked to the 
input-output and 
external storage units 
via independent trunks for effecting data-transfers. 
Transfers of data can take place 
between the external units, 
the memory units, 
and the computers concurrently 
without interrupt- 
ing the progress of the computational program. Because 
of the flexibility of the format controller, incoming data can be accepted NBS PILOT ELECTRONIC DATA-PROCESSER 
Fig. 1. Over-all block diagram for PILOT. 
442 Part 5 I The PMS level Section 2 1 Computers with one central 
processor and multiple 
input/output processors from a wide variety of external devices 
and in a wide variety of formats. format, the respective lefthand and righthand halves of each double operand are processed simultaneously in a single 
instruc- tion time, and the two independent half-word results are written 
back in the corresponding halves of the full-length result location. 
Functions of the major units The specific functions of the major units can be 
described briefly as follows: 
Primary computer Arithmetic and processing unit. Using a 64-bit number word with algebraic sign, this unit carries out 7 different types of arithmetical operations, 5 types of choice (branch) operations, and 2 types of logical pattern-processing operations. 
See Table 
2. Arithmetical operations can be performed in any 
of 16 possible formats. For example, arithmetic can 
be performed using either a pure binary or a binary-coded decimal number code, and in both 
fixed-point and floating-point notation. Fixed-point operations 
can also be carried out in a special half-word format 
in which two independ- ently addressable half-words are stored in 
a single 
full-word storage 
location. These two half-words can be processed either separately, as independent words, or concurrently in 
duplex format. 
In duplex Table 2 Types of internal operations Program control unit. The program control 
unit interprets 
and regulates the sequencing of instructions in the program. It operates with a 68-bit 
binary-coded 3-address instruction word. 
See Table 
3. Each instruction word contains 
three 16-bit codes 
which specify the addresses of each of two operands, alpha and beta, and 
usually the address of the result of the operation, gamma, in the main memory. The memory location of the next instruction word 
is specified by a 16-bit 
address number contained in one 
of 16 possi- ble base registers; a 
4-bit code 
in the instruction word (d-digits) 
specifies which one of the base registers contains 
the desired word. Whenever a register is so used as a 
next-instruction address source, 
its contents are automatically increased by unity. 
Choice instruc- tions, used for 
program branching, from time to time may cause a new alternative address number to be inserted in any one of the base registers. 
This register 
is then used as 
the source of the address number of the next instruction. Primary computer Name Abbreoiation Secondary computer Name Abbreviation Arithmetic operations: Add Augment Subtract Multiply Divide Square-root Shift Nonnumerical processing operations: 
Transplant Segment 
with Shift Generate Boolean Functions Choice operations: Compare, Algebraic Compare, Modulus Compare, Equality Check Scale Compare Boolean Functions AD AG SB MP DV SH SQ Clear add Hold add Store positive Transfer Increase Decrease Logical Multiply 
Compare, Zero Compare, Righthand Bit Compare, Lefthand Bit Compare, Negative Check Primary and Proceed Check Primary 
and Wait Regulate Primary 
Computer Replace Primary 
Instruction Secondary Take Input from Primary 
TL GB CA CM CE cs CB ca ha SP tr in de Im CZ cr CI cn CP rP ri si cw Control operations: Transfer Between Storage Units TS Regulate Secondary 
Computer RS Leiner, Notz, Smith, Weinberger-PILOT 
Chapter 35 1 PILOT, the NBS multicomputer system 443 Table 3 Contents of primary instruction word Digits numbered 1 through 68 68-65 64-61 60-57 56-53 52-49 
48-45 44-41 40-37 36-33 32-29 
28-25 24-21 20-17 16-13 12-9 8-5 4-1 
Tags Address alpha Address beta Address 
gamma Next Code for Mon. Instn. Operation Break 
Point - OOO? a- b- C- d- Param. Basic e- Digits Digits Digits Digits eter 
Type Digits Addresses alpha, beta, and 
gamma written in the instruction word are subject 
to automatic modification if desired by writing a 1-digit in a 
specified bit position. Such addresses 
are called relative addresses. Each of the three 
addresses (a, /3, and y) in each instruction word contains a 
4-bit code group, 
called the a-, b-, and c-digits respectively, in 
which any 
base register identification 
number (0 through 15) may be written. When 
this is done, the address number to which the computer 
actually refers is equal to the sum (modulo 216) of the address number stored 
in the designated base register plus an address-modification 
constant, indicated in the remaining 12 bits of the 16-bit address 
segment of the instruction word. Primay storage units Fast access memory. Because of budget limitations, the initial installation of the system will 
contain only a relatively 
small section of internal memory of the diode-capacitor type. This diode-capacitor memory, originally 
developed at NBS in 1953, is very fast; i.e., capable of providing one 
random access 
per micro- second, but it 
has the disadvantage of relatively high cost 
per word of storage. This type of memory is available in 
modules of 256 words subdivided as follows: Numerical information 
Algebraic signs 
and tags Parity check digits Total word 
length 64 bits 4 bits 4 bits 72 bits The over-all system 
is designed to accommodate up to 32,768 internally-accessible full-words, which 
may be held in 
storage units 
with access times 
ranging from 1 microsecond (psec) 
to 32 psec. Thus the minimum fast access 
memory can be backed up with a much larger 
and slower magnetic-core memory. Inter-memory trunsfer trunk. Provision is made for transferring blocks of information between 
the various internal storage units 
in the system, concurrently with computation. 
The size of the block transferred may 
range from a single word 
to the 
entire contents of the memory, and the addresses between which the information is transferred are specified by 
a single programmed inter-memory transfer instruction. Automatic interlocks are pro- vided to insure that all future references which 
the program may make to any memory positions involved 
in the inter-memory transfer operation are automatically made 
after the data have 
been shifted to the new locations. Secondary computer 
Arithmetic and processing 
unit. The secondary computer is a high-speed independently programmable general-purpose 
com- puter that 
operates in conjunction with the primary computer and 
can perform 16 distinct types of operations using 16-bit words. 
These operations include 6 arithmetic-processing 
operations, 4 choice operations, 1 nonnumerical processing operation, and 5 operations that transfer digital information or control-signals 
be- tween the primary and 
the secondary computers. 
See Table 
2. Operation times for 
the secondary computer average about 2 psec. Both computers operate concurrently and can 
transfer infor- 
mation back 
and forth between each other. One 
of the principal functions of the secondary computer is to carry out so-called ﬁred-tapeﬂ operations, such as: 
(1) counting iterations, (2) syste- matically modifying the addresses of the operands and 
instructions referred to by the primary program, 
(3) monitoring the primary program, and 
(4) various special tasks. Through the use of special subroutines for the secondary computer, both 
computers acting co-operatively can be made to carry out a wide variety of complex operations without unduly complicating 
the writing of the primary computer programs. Examples 
of such operations are: (1) special types of sorting, (2) logarithmic search, (3) routines involving cross-referencing, or items selected according 
to an attached code, (4) error analyses, and (5) operations involving small 
numerical fields. 
444 Pari 5 1 The PMS level Section 2 I Computers with one central processor and multiple 
input/output processors Secondary storage unit. Associated with the secondary computer is the secondary storage unit which consists 
of 60 storage locations containing 16-bit words. Sixteen 
of these locations can be used as base registers 
by the primary computer and may be selected 
by the primary computer according to the a-, b-, c-, and d-digits in the primary instruction word. The contents of the registers selected by the primary computer in this way are automatically added to the 
address numbers specified in the primary computer instruction word. The secondary storage unit 
is also capable of being addressed directly by the primary computer. The fifteen 4-word blocks 
of the secondary storage are identified by 
15 special primary address numbers. Other addressable registers associated 
with the secondary storage hold the address numbers 
of current and next instruction words in the primary program. Program control unit. The secondary computer program operates with a 2-address instruction system, the addresses referring to words in the secondary storage unit, including the base registers. 
See Table 4. 
From time to time the primary instruction program may order the insertion of a new instruction 
into the secondary instruction register or 
may order the transfer of data in either direction between the primary storage units 
and the secondary storage unit. 
The secondary computer program may also cause data to be 
transferred into the secondary storage unit from the primary instruction register and can also cause information 
to be 
trans- ferred into the primary instruction register from 
a location in 
the main memory. 
Using these facilities, the secondary computer can inspect 
each instruction word in the primary program as 
it is selected from the primary store and, acting 
upon specifications written into 
the secondary program, can cause 
the primary instruction either to be executed as written or to be 
replaced by a new instruction 
word from a memory location 
determined by the secondary. Other types of discrimination can be effected by the secondary that depend upon the result of a primary 
operation, such as an overflow, 
jump, etc. These features facilitate 
the use of interpretive programming methods. Table 4 Contents of secondary instruction word 
Digits numbered 1 through 16 16 13 12 7 61 Operation code (0-15) Address ﬁgﬂ Address ﬁhﬂ Input-output control Concurrent input-output trunks. 
The concurrent input-output 
trunks have 
the function of controlling the transfer of information in either direction between the internal 
memory and the external storage units. 
All input-output transfers are initiated 
by a 
single internally programmed instruction, and are 
carried out by 
the trunk units with the aid of automatic interlocks similar to those used in the inter-memory transfer trunk for preventing interfer- ence with the 
progress of the computing 
program. The size of the block of data that is transferred may 
range from a single word 
to the entire contents 
of the memory and may be directed to any addresses. Using 
two such trunks, it is possible to maintain two continuous streams 
of data simultaneously flowing between the internal memory and any 
two external storage units 
without interrupting the progress of the computations. Format controller. Data that are passing in and out 
of the internal 
storage system via 
the input-output trunks are subject to further concurrent processing by the format controller. The format con- troller is an independent 
internally-programmed data-processing 
unit specially designed for 
carrying out general-purpose editing, inspecting, and format-modifying operations on incoming or 
out- going data. Programs for 
the format controller are stored on removable plugboards, 
and the primary computer program is able to direct the format controller to select whichever particular format program may be appropriate from among the small library of format programs 
contained on the boards currently attached to the 
machine. Among the typical kinds of programs that the format controller can carry out 
are: (1) searching of magnetic tapes for words 
bearing identifying addresses or 
other coded labels specified by 
the internal program, with selective input or output of data at these selected 
tape locations, (2) insertion of incoming data for the internal 
storage units 
of the system into address locations specified by 
the incoming data itself, (3) conversion and rearrangement of data that are stored on external units in 
formats not compatible with 
the formats used 
in the internal 
units; e.g., binary-decimal character conversion, adjustment of word-length modules, etc. External storage 
External storage 
in the initial installation 
of the system will consist 
mainly of magnetic tape units. Because 
of the flexibility of the format controller, it will be possible to supplement these tape units later with a 
wide variety of other types of external units 
without making any significant changes 
in the existing equipment. 
Chapter 35 I PILOT, the NBS multicomputer system 
445 Input-output units 
The system is designed to operate with a wide 
variety of input- output devices, both digital and analog. Input readers and printers. 
Flexowriter units and paper-tape read- ers and punches will be available in the initial installation. 
Punched card input 
readers and high-speed printers, along with their auxiliary controls, 
may be attached to the 
format controller in the manner indicated 
in the preceding paragraph. 
Displays. Two types of displays are provided for: (1) pilot-light display of data and control information in the various registers 
and flip-flops throughout the system, in order to aid the rapid diagnosis of equipment malfunctions of programming faults, and (2) picture-tube display of real-time data stored in the internal 
memory of the system. This 
kinematic diagram 
type of display is very important when performing dynamic simulation operations 
which require 
visual presentation of the simulated data in real- time to the 
human operators. External control Manual-monitor control. The term 
ﬁmanual-monitorﬂ was coined at NBS several years ago 
to describe certain types of control operations that are initiated either 
manually by the machine operator or by the machine itself under conditions which 
are specified by means of external switch settings. The former is referred to as a manual operation and 
the latter 
is called a monitor operation because 
the machine must monitor 
its internal program to determine precisely when the operation should be performed. The type 
of operation to be performed as well as 
the conditions under which it is to be performed 
are specified by means 
of external switch settings. This feature provides for 
convenient communication 
between the data-processor and the operator, 
and allows the operator to monitor the progress of the program automatically, to insert new data and instructions, and to withdraw intermediate results con- veniently, without need 
for advance preparation 
of special pro- 
grams. This 
is particularly useful in debugging programs and in checking equipment malfunctions. Monitor operations are performed by the machine whenever 
the conditions specified by 
the external switch settings occur in the course of the program; e.g., every 
time the program refers 
to a new 
instruction, any time 
the program refers 
to an instruction to which a special monitor breakpoint symbol (e-digits) 
is attached, any time an arithmetic overflow occurs, 
etc. By pairing a 
particular type of manual-monitor operation with 
a selected 
set of conditions, a variety of special composite operations can 
be performed. Remote controls. Manual-monitor operations 
can be specified and initiated by external devices as well 
as by human operators. Since all of the external switch settings control 
only d-c voltages, the external devices can even be remote 
from the machine itself, and from a distance, via ordinary electrical transmission lines, 
they can exercise supervisory 
control over the internal 
program of the machine. This makes 
it possible to harness together two or more remotely located data-processing machines, 
and have them 
work together co-operatively on a common task. Each member 
of such an interconnected network of separate data 
processors is free at any time 
to initiate and dispatch 
special control 
orders to any of its partners in the system. As a consequence, 
the supervisory control over the common task may be shared among the various members of the system, and may be passed back and forth from one machine to the 
other as the need arises. References LeinA57, 59 
Section 3 Computers for multiprocessing and parallel processing 
The computers in this section are probably the most 
general in the book. Although 
the general PMS model for a computer in Chap. 3, page 65, characterizes these 
computers, the struc- ture by Lehman (Chap. 37) most closely fits the model. The Burroughs computers that are presented have multiple Pc™s;~ however, K™s are used for control of 
device K™s, rather than Pio™s-perhaps a 
wise choice. 
D825-a multiplecomputer system for command and control The Burroughs 
D825 computer is discussed, together with other stack processors, in Part 3, Sec. 5, page 257. Chapter 36 emphasizes the PMS structure and operating system charac- 
teristics necessary in a multiprocessor system. 
‚As does the B 8500, a successor to the D825; however, its successor, the B 8501, IS designed with Pio™s. Design of the B 5OOO system This computer (Chap. 22) is discussed, together with other stack 
processors, in Part 3, Sec. 5, page 257. A survey of problems and 
preliminary results 
concerning parallel processing and 
parallel processors Chapter 37, by M. Lehman, provides a 
very good introduction to the concepts of multiprogramming, multiprocessing, and parallel processing. A specific 
multiprocessor computer struc- 
ture is postulated to provide parallel 
processing. The 
processing ability of the structure 
is analyzed at the 
instruction level. It is significant that the 
paper is by an IBM scientist. IBM has not been particularly advanced in the use of multiple arithmetic processor computers. 446 
Chapter 36 D825-a m ult i ple-corn puter system for command and 
controll James P. Anderson / Samuel A. Hoffman Joseph Shifman / Robert 1. Williams Introduction The D825 Modular 
Data Processing System 
is the result of a Burroughs study, initiated several years ago, 
of the data 
processing requirements for command and control systems. The D825 has been developed for operation in the military environment. The initial system, constructed for the Naval Research Laboratory 
with the designation AN/GYK-3(V), has been completed 
and tested. This paper reviews the design criteria analysis and design rationale that led to the system structure of the D825. The implementation and operation of the system are also described. Of particular interest is the role that developed for an operating system program in coordinating the system components. Functional requirements 
of command and 
control data processing By ﬁcommand and control systemﬂ is meant a system 
having the capacity to monitor and direct all aspects 
of the operation of a large man and machine complex. Until now, 
the term has been applied exclusively to certain 
military complexes, but could as well 
be applied to a fully 
integrated air traffic control system or even 
to the operation of a large industrial 
complex. Operation of com- mand and control systems is characterized by an enormous quan- tity of diverse but interrelated tasks-generally arising in real 
time-which are best performed by 
automatic data-processing equipment, and are most effectively 
controlled in a fully 
integrated central data processing facility. The data 
processing functions alluded to are those typical of data processing, plus 
special func- 
tions associated with servicing displays, 
responding to manual insertion (through 
consoles) of data, and 
dealing with communica- tions facilities. 
The design implications of these functions 
will be considered here. Aoailability criteria. The primary requirement of the data-proc- essing facility, above 
all else, 
is availability. This 
requirement, essentially a 
function of hardware reliability and maintainability, ‚AFIPS Proc. FJCC, vol. 22, pp. 86-96, 1962 is, to the user, simply 
the percentage of available, on-line, opera- tion time during 
a given 
time period. Every 
system designer must 
trade off the costs of designing for reliability against 
those incurred by unavailability, but in no other application 
are the 
costs of unavailability so high as those presented in 
command and control. Not only is the requirement for hardware 
reliability greater than 
that of commercial systems, but downtime for the complete system for preventive maintenance cannot 
be permitted. Depending upon the application, some greater or lesser portion of the complete system must 
always be available for primary system functions, and all of the system must be available most of the time. The data 
processing facility may also 
be called upon, except 
at the most critical times, 
to take part in exercising and evaluating the operation of some parts of the system, or, in fact, 
in actual simulation of system functions. During such exercises 
and simula- tions, the system must 
maintain some (although perhaps partially and temporarily degraded) 
real-life and real-time capability, 
and must be able to return quickly to full operation. An implication here, of profound significance in system design, 
is, again, the requirement that most of the system be always available; there must be no system elements (unsupported 
by alternates) perform- ing functions 
so critical that failure at these points could compro- 
mise the primary system functions. Adaptability criteria. Another requirement, equally difficult to achieve, is that the 
computer system must 
be able to analyze the demands being made upon 
it at any given time, and determine from this 
analysis the attention and emphasis that should be given to the individual tasks of the problem mix presented. The working configuration of the system must be completely adaptable 
so as to accommodate the diverse problem 
mixes, and, moreover, must respond quickly 
to important 
changes, such 
as might be indicated by external alarms or 
the results of internal computations 
(exceed- ing of certain thresholds, for example), or to changes in the hard- ware configuration resulting from the failure of a system 
compo- nent or from its intentional removal from 
the system. The system 447 
448 Part 5 1 The PMS level Section 3 1 Computers for multiprocessing and parallel processing 
must have the ability to be 
dynamically and automatically 
re- structured to a working configuration 
that is responsive to the problem-mix environment. Expansibility criteria. 
The requirement of expansibility is not unique to command and control, but is a desirable feature in any application of data processing equipment. However, the need for expansibility is more acute in command and 
control because of the dependence of much of the efficacy of the system upon an ability to meet the changing requirements brought 
on by the very rapidly changing 
technology of warfare. Further, it must be possi- ble to incorporate new functions in such a way that little or no 
transitional downtime results in any hardware area. 
Expansion should 
be possible without incurring 
the costs of providing more capability than is needed at the time. This ability of the system to grow to meet demands 
should apply not 
only to the conventionally expansible 
areas of memory and 1/0 but to 
computational devices, as well. 
Programming criteria. Expansion of the data-processing facility should require no reprogramming of old functions, and programs for new functions 
should be easily incorporated into the 
overall system. To achieve this capability, programs must 
be written in a manner 
which is independent of system configuration or 
problem mix, and should even be interchangeable between sites performing like tasks 
in different 
geographic locales. Finally, because of the large volume of routines that must be written for a command and 
control system, it should be possible for 
many different 
people, in different locations 
and of different areas of responsibility, to write portions of programs, and for the programs to be 
subse- quently linked together by a 
suitable operating system. Concomitant with the 
latter requirement and with 
that of configuration-independent programs is the desirability of orienting system design 
and operation toward 
the use of a high-level pro- 
cedure-oriented language. 
The language should have the features of the usual algorithmic languages for scientific 
computations, but should also 
include provisions for 
maintaining large files of data sets which may, 
in fact, 
be ill-structured. It is also desirable that the language reflect the special nature of the application; this is especially true when the language is used to direct the storage and retrieval of data. Design rationale for the data-processing 
facility The three 
requirements of availability, adaptability, and 
expansi- bility were the motivating considerations in developing the D825 design. In arriving at the final systems design, 
several existing and proposed schemes for 
the organization of data processing systems 
were evaluated 
in light 
of the requirements listed above. Many of the same conclusions 
regarding these and other schemes in the use of computers in command and control were reached 
inde- pendently in a more recent study 
conducted for the Department 
of Defense by the Institute for Defense 
Analysis [Kroger et al., 19611. The single-computer system. The most obvious system 
scheme, and the least acceptable for command and 
control, is the single-com- puter system. This 
scheme fails to meet the availability require- 
ment simply because the failure of any part-computer, memory, 
or 1/0 control-disables the entire 
system. Such 
a system was not given serious 
consideration. Replicated single-computer 
systems. A system organization that had been well known 
at the time these considerations were active involves the duplication (or triplication, etc.) of single-computer systems to obtain availability and greater processing rates. This 
approach appears initially attractive, inasmuch as programs for 
the application may be split among two or more independent single-computer systems, using as 
many such systems as 
needed to perform all of the required computation. 
Even the availability requirement seems satisfied, 
since a redundant system may be kept in idle 
reserve as backup for the main function. On closer examination, however, 
it was perceived that such a system had many disadvantages for command and control appli- 
cations. Besides requiring considerable 
human effort to coordinate the operation of the systems, and considerable waste of available machine time, the replicated single computers were found 
to be 
ineffective because of the highly interrelated way in 
which data and programs are frequently used in command and 
control appli- 
cations. Further, the steps necessary to have the redundant or backup system take over the main function, should the need arise, would prove 
too cumbersome, 
particularly in 
a time-critical ap- plication where constant monitoring 
of events is required. Partially shared memory schemes. It was seen that if the replicated computer scheme were 
to be modified by the use of partially shared memory, some 
important new capabilities would arise. 
A partially shared memory can take 
several forms, but provides principally for some 
shared storage and some storage privately 
allotted to individual computers. The shared storage may 
be of any kind-tapes, discs, 
or core-but frequently is core. Such a system, by providing a 
direct path 
of communication between computers, goes a long 
way toward satisfying the requirements listed above. 
Chapter 36 I D825-a multiple-computer system for 
command and control 
449 The one advantage to be found in having some memory private 
to each computer is that of data protection. This advantage 
van- ishes when it is necessary to exchange data between 
computers, for if a computer failure were to occur, 
the contents 
of the private memory of that computer would be lost to the 
system. Further- more, many tasks in the command and 
control application require 
access to the same data. If, for example, it would be desirable to permit some privately stored 
data to be 
made available to the 
fully shared memory or to some other private memory, considerable 
time would be lost in transferring 
the data. 
It is also clear that a certain amount of utilization efficiency is lost, since some private memory may 
be unused, while another computer 
may require more memory than is directly available, 
and may be forced to transfer other blocks of data back to bulk storage to make way for the necessary storage. It might be added in passing that if private 1/0 complements are considered, the same questions 
of decreased overall availability and decreased 
efficiency arise. 
Muster/sluve schemes. 
Another aspect of the partially shared memory system is that of control. A number of such systems 
employ a master/slave scheme to achieve control, a technique wherein one 
computer, designated the master computer, coordi- nates the work done by the others. The master computer might be of a different character than the 
others, as in the PILOT system, developed by 
the National Bureau 
of Standards [Leiner 
et al., 19571, or it may 
be of the same basic design, differing only 
in its 
prescribed role, as 
in the Thompson Ram0 Wooldridge TRW400 (AN/FSQ-27) [Porter, 19601. Such a scheme does recognize 
the importance, for multicomputer systems, of the problem of coordi- nating the 
processing effort; 
the master computer is an effective means of accomplishing the coordination. However, there are 
several difficulties 
in such a design. The loss of the master com- puter would down the whole system, 
and the command and control availability requirement could 
not, consequently, be 
met. If this weakness is countered by providing the ability for the master control function to be automatically switched 
to another processor, there still remains an inherent 
inefficiency. If, for example, 
the workload of the master computer becomes very 
large, the master becomes a system bottleneck resulting in 
inefficient use of all other system elements; and, on the other hand, 
if the workload fails 
to keep the 
master busy, 
a waste of computing power results. The conclusion is then reached that a master 
should be established only when needed; this is what has been done 
in the design of the D825. The totally modular scheme. 
As a result of these analyses, certain implications became 
clear. The availability requirement dictated 
a decentralization 
of the computing function-that is, a multi- plicity of computing units. However, the nature 
of the problem required that data be freely communicable among these several computers. It was decided, therefore, that the memory system would be completely shared by 
all processors. And, from 
the point of view of availability and efficiency, it was also seen 
to be 
unde- sirable to associate 1/0 with a 
particular computer; 
the 1/0 control was, therefore, also decoupled from the computers. Furthermore, a system with several 
computers, totally shared memory, and decoupled 
1/0 seemed a perfect structure for satis- 
fying the adaptability requirements of command and 
control. Such a structure resulted in 
a flexibility of control which was a fine match for the dynamic, highly variable, processing requirements to be 
encountered. The major problem 
remaining to realize 
the computational potential represented by 
such a system was, 
of course, that of coordinating the many system elements to behave, at any given time, like a system specifically designed 
to handle the set of tasks with which it was faced at that 
time. Because of the limitations of previously available equipment, an operating system program 
had always been identified with the equipment running the pro- gram. However, in the proposed design, 
the entire memory was to be directly accessible to all computer modules, and the 
operat- ing system 
could, therefore, 
be decoupled from any specific com- 
puter. The operation of the system could be coordinated 
by having any processor in the complement run 
the operating system only 
as the need arose. It became clear that the 
master computer had actually become a program stored in totally shared memory, a transformation which was also seen 
to offer enhanced program- ming flexibility. 
Up to this point, the need for identical computer modules had not been established. The equality of responsibility among com- 
puting units, which allowed each 
computer to perform as the master when running the operating system, led finally to the 
design specification of identical computer 
modules. These were 
freely interconnected to a set of identical memory 
modules and a 
set of identical 1/0 control modules, the latter, in turn, freely inter- 
connected to a highly variable 
and diverse 1/0 device comple- ment. It was clear that the complete 
modularity of system ele- ments was an effective solution 
to the problem of expansibility, inasmuch as expansion 
could be accomplished simply 
by adding 
modules identical to those in the existing complement. It was also clear that important advantages and 
economies resulting from the manufacture, maintenance, 
and spare parts provisioning for 
iden- tical modules also 
accrue to such a system. Perhaps the most important result of a totally modular organization is that redun- 
450 Part 5 I The PMS level Section 3 1 Computers for 
multiprocessing and 
parallel processing dancy of the required complement 
of any module type, for greater reliability, is easily achieved by incorporating 
as little as one additional module of that type in the system. Furthermore, the additional module of each type need not be idle; the system may be looked upon as operating with 
active spares. Thus, a design structure based upon complete modularity was set. Two items remained 
to weld the various functional modules into a coordinated 
system-a device to electronically interconnect the modules, and an operating 
system program 
with the effect of a master 
computer, to coordinate the activities of the modules into fully integrated system operation. In the 
D825, these two 
tasks are carried out by the switching interlock and the Automatic Operating and Scheduling Program (AOSP), respectively. Figure 1 shows how the various functional modules are interconnected 
via the interlock in a matrix-like fashion. System implementation Most important in the design implementation of the D825 were studies toward practical realization 
of the switching interlock and the AOSP. The computer, 
memory, and 1/0 control modules permitted more conventional solutions, but were each 
to incor- porate some unusual 
features, while many of the 1/0 devices were selected from existing equipment. With the exception of the latter, all of theses elements are discussed here briefly. (A summary of D825 characteristics and specifications is included at the end 
of the paper.) Switching interlock. 
Having determined 
that only a completely 
shared memory system would 
be adequate, it was necessary to find some way to 
permit access to any memory by any 
processor, and, in fact, 
to permit sharing 
of a memory module by two or more 
processors or 
1/0 control modules. A function distributed physically through all of the modules of a D825 system, but which has 
been designated 
in aggregate the switching interlock, effects electronically each of the many brief interconnections by which all information 
is transferred among computer, memory, and 
1/0 control modules. In addition to the electronic switching function, the switching interlock has the ability to 
detect and resolve conflicts such as 
occur when two or more 
computer modules attempt access to the same memory module. The switching interlock consists functionally of a crosspoint switch matrix which effects 
the actual switching of bus intercon- nections, and a bus 
allocator which resolves all 
time conflicts resulting from simultaneous 
requests for access 
to the same bus or system module. 
Conflicting requests are queued 
up according to the priority assigned to the requestors. Priorities are pre- emptive in that the 
appearance of a higher priority request will cause service 
of that request before service 
of a lower priority request already in the queue. Analyses of queueing probabilities have shown that queues longer than one are extremely unlikely. 
The priority scheduling function 
is performed by the bus allo- 
cator, essentially a set of logical matrices. The conflict matrix 
detects the presence of conflicts in requests for 
interconnection. The priority matrix resolves 
the priority of each request. The logical product of the states of the conflict and priority matrices determines the state 
of the queue matrix, which 
in turn governs the setting of the crosspoint switch, unless the requested module is busy. The AOSP: an operating system program. The AOSP is an operating system program 
stored in totally 
shared memory and therefore available to 
any computer. The 
program is run only as 
needed to exert control over the system. The AOSP includes its own executive routine, an operating system for 
an operating system, as it were, calling out additional routines, 
as required. The con- figuration of the AOSP thus permits 
variation from application to application, both in sequence and quantity 
of available routines 
and in disposition of AOSP storage. The AOSP operates effectively on two levels, one for system 
control, the other 
for task processing. 
The system control function embodies all 
that is necessary to call system programs 
and associated data from some 
location in 
the 1/0 complement, and 
to ready the programs for execution 
by finding and allocating space in memory, and initiating the proc- essing. Most of the system control function (as well 
as the task processing function) consists of elaborate bookkeeping for: pro- 
grams being run, programs that are active (that is, occupy memory space), 1/0 commands being executed, 
other 1/0 commands waiting, external 
data blocks to be received and decoded, 
and activation of the appropriate programs to handle such 
external data. It would be inappropriate here 
to discuss the myriad details of the AOSP; some idea of its scope, however, can be obtained 
from the following list 
of some of its major 
functions: 1 Configuration determination 2 Memory allocation 
3 Scheduling 4 5 Reporting and 
logging Program readying and 
end-of-job cleanup 
Chapter 36 I D825-a multiple-computer system for command and control 
451 Fig. 1. System organization, Burroughs D825 modular data processing system. 

452 Part 5 I The PMS level Section 3 I Computers for multiprocessing and parallel processing 
6 Diagnostics and confidence checking 7 External interrupt processing The task processing 
function of the AOSP is 
to execute all program 1/0 requests in order to centralize scheduling problems and to protect the 
system from 
the possibility of data destruction by ill-structured or conflicting programs. 
AOSP response to interrupts. 
The AOSP function depends 
heavily upon the comprehensive set of interrupts incorporated in the D825. All interrupt conditions are transmitted 
to all computer modules in the system, and each 
computer module can respond to all interrupt conditions. However, to make it possible to dis- tribute the 
responsibility for various interrupt conditions, both system and local, each computer module has an 
interrupt mask register that controls the setting 
of individual bits of the interrupt register. The occurrence of any interrupt causes one of the system computer modules to leave the program it has been running and branch to the 
suitable AOSP entry, entering 
a control mode 
as it branches. The control mode 
differs from 
the normal mode of operation in that it 
locks out the response to some low-priority 
interrupts (although recording 
them) and 
enables the execution of some additional instructions 
reserved for 
AOSP use (such as 
setting an interrupt mask register or 
memory protection registers, or transmitting an 1/0 instruction to an 1/0 control module). In responding to an interrupt, the 
AOSP transfers control to the appropriate 
routine handling 
the condition designated 
by the interrupt. When the interrupt 
condition has been satisfied, control is returned to the original object program. Interrupts caused by normal operating 
conditions include: 1 2 3 Real-time clock 
overflow 4 Array data absent 5 Computer-to-computer interrupts 6 16 different types of external requests Completion of an 1/0 operation Control mode entry (normal mode halt) Interrupts related 
to abnormalities of either program or 
equipment include: 1 2 Arithmetic 
overflow 3 Illegal instruction 
Attempt by program 
to write out of bounds 4 Inability to 
access memory, or an 
internal parity error; 
parity error 
on an 1/0 operation causes termination of that operation with suitable indication 
to the 
AOSP 5 Primary power failure 6 7 Automatic restart after primary power 
failure 1/0 termination other than 
normal completion While the reasons for 
including most of the interrupts 
listed above are evident, 
a word of comment on some of them is in order. 
The array-data-absent interrupt is initiated when a reference 
is made to data that is not present 
in the memory. Since all 
array references such as A[k] are made relative to the 
base (location of the first element) of the array, it 
is necessary to obtain this address and to index it by the value k. When the base of array A is fetched, hardware 
sensing of a presence 
bit either 
allows the operation to continue, or initiates the array-data-absent interrupt. In this way, keeping track of data in use by 
interacting programs can be simplified, as 
may the storage allocation 
problem. The primary power 
failure interrupt is highest priority, and always pre-emptive. This 
interrupt causes all 
computer and 
1/0 control modules to terminate operations, and to store 
all volatile information either in memory 
modules or in 
magnetic thin-film registers. (The latter are 
integral elements of computer modules.) This interrupt protects the system from 
transient power failure, and is initiated when the primary power 
source voltage 
drops below a predetermined 
limit. The automatic 
restart after primary power 
failure interrupt is provided so that the 
previous state of the system can be 
recon- structed. A description of how an external interrupt is handled might clarify the general interrupt procedure. Upon the presence of an external interrupt, the computer which has 
been assigned respon- 
sibility to 
handle such interrupts automatically stores the contents of those registers 
(such as the program counter) necessary to subsequently reconstitute its 
state, enters 
the control mode, and goes to a standard (hardware-determined) location where a branch 
to the external request routine 
is located. This routine has the responsibility of determining which external request line requires servicing, and, after 
consulting a 
table of external devices (teletype buffers, console keyboards, displays, 
etc.) associated with the interrupt lines, the computer 
constructs and transmits an input instruction to 
the requesting device 
for an initial message. The computer then makes an entry 
in the table 
of the 1/0 complete program (the program that handles 1/0 complete interrupts) 
to activate the appropriate responding routine 
when the message is 

Chapter 36 I D825-a multiple-computer system for 
command and control 
453 read in. A check is then made for the occurrence of additional external requests. Finally, the computer 
restores the saved register contents and returns in 
normal mode to the 
interrupted program. AOSP control of 1/0 activity. As mentioned above, control of all 1/0 activity is also within the province of the AOSP. Records are kept on the condition and availability of each 1/0 device. The locations of all files within the computer 
system, whether on magnetic tape, drum, disc file, card, or represented as external inputs, are also recorded. A request for input by file name is evaluated, and, if the device associated with this name is readily available, the action 
is initiated. If for any reason 
the request must 
be deferred, it is placed in a program queue to await conditions 
which permit its initiation. 
Typical conditions which would cause 
deferral of an 1/0 operation include: 1 2 3 No available 1/0 control module or channel. The device in which the file is 
located is presently in use. The file does not 
exist in the system. In the latter 
case, typically, a message would 
be typed out 
on the supervisory printer, asking for 
the missing file. The 1/0 complete interrupt 
signals the completion of each 1/0 operation. Along with this 
interrupt, an 1/0 result descriptor is deposited in an AOSP table. The status relayed in this descriptor 
indicates whether or not the operation was successful. If not successful, what went 
wrong (such as a parity error, 
or tape break, card jams, etc.) is indicated so that the 
AOSP may initiate the 
appropriate action. If the operation was successful, any waiting 
1/0 operations which can now proceed are initiated. 
AOSP control of program scheduling. Scheduling in the D825 relies 
upon a 
job table maintained by the AOSP. Each entry is identified with a name, priority, precedence requirements, 
and equipment 
requirements. Priority may 
be dynamic, depending upon 
time, external requests, other programs, or a function of many variable 
conditions. Each time the 
AOSP is called upon to select a 
program to be 
run, whether as a result of the completion of a program 
or of some other interrupt 
condition, the job table is evaluated. In a real-time system, situations occur wherein 
there is no system 
program to be run, and machine 
time is available for other uses. This time could be used for auxiliary 
functions, such as confidence routines. The AOSP provides the capability for program segmentation at the discretion of the programmer. Control macros embedded in the program code inform the AOSP that parallel processing 
with two or more 
computers is possible at a given point. In addition, the programmer must specify 
where the branches indicated 
in this manner will join following the parallel processing. Computer module. The computer 
modules of the D825 system 
are identical, general-purpose, arithmetic and 
control units. 
In deter- 
mining the internal structure 
of the computer modules, two con- 
siderations were uppermost. 
First, all programs and data had to 
be arbitrarily relocatable to 
simplify the storage allocation 
func- tion of the AOSP; secondly, programs would 
not be 
modified during execution. The latter 
consideration was necessary to mini- mize the amount of work required to pre-empt a program, 
since all that would have to be saved to reinstate the interrupted 
pro- gram at a later time 
would be the data 
for that program and the register contents of the computer module running the program at the time 
it was dumped. The D825 computer modules employ 
a variable-length in- struction format made up of quarter-word syllables. Zero-, one-, 
two-, or three-address syllables, as 
required, can be 
associated with each basic command syllable. An implicitly addressed accumulator stack is used in conjunction with 
the arithmetic 
unit. Indexing of all addresses 
in a command 
is provided, as well 
as arbitrarily deep indirect addressing for 
data. Each computer module includes 
a 128-position thin-film 
mem- ory used for 
the stack, and also for 
many of the registers of the machine, such as 
the program base 
register, data base register, the index registers, limit registers, 
and the like. The instruction complement of the D825 includes the usual fixed-point, floating-point, logical, 
and partial-field commands 
found in any reasonably large scientific 
data processor. Memory module. The memory modules consist 
of independent units storing 
4096 words, each of 48 bits. Each unit has an individ- ual power supply 
and all of the necessary electronics to control the reading, writing, 
and transmission of data. The 
size of the memory modules was established as a compromise between a module size small enough 
to minimize conflicts wherein two 
or more computer or 1/0 modules attempt access to the same mem- ory module, 
and a 
size large enough 
to keep the cost of duplicated power supplies and addressing logic 
within bounds. It might be noted that for a larger modular processor system, these trade-offs 
might indicate that memory modules of 8192 words would 
be more suitable. Modules larger than this-of 16,384 or 32,768 words, for example-would make 
construction of relatively small equipment complements meeting 
the requirements set forth 
above quite 
454 Pari 5 I The PMS level Section 3 I Computers for 
multiprocessing and parallel processing difficult. The cost of smaller units of memory is offset by the lessening of catastrophe in the event of failure of a module. I/O control module. The 1/0 control module executes 1/0 opera- tions defined 
and initiated 
by computer module action. In keeping with the system objectives, 
1/0 control modules are not assigned to any particular computer 
module, but rather are treated in much 
the same way as memory modules, with automatic resolution of conflicting attempted accesses via 
the switching interlock function. 
Once an 1/0 operation is initiated, it proceeds independently until 
completion. 1/0 action is initiated by the execution of a transmit 1/0 instruction in 
one of the computer modules, which delivers 
an 1/0 descriptor word from 
the addressed memory location 
to an inactive 1/0 control module. The 1/0 descriptor is an instruction to the 
1/0 control module that selects the device, determines the direc- tion of data flow, the address of the first word, and the number of words to be transferred. Interposed between the 1/0 control modules and the physical external devices is another crossbar switch designated 
the 1/0 exchange. This 
automatic exchange, similar 
in function to the 
switching interlock, permits two-way data flow between any 1/0 control module and any 1/0 device in the system. It further enhances the flexibility of the system by providing 
as many possible external data transfer paths as there are 
1/0 control modules. Equipment complements. 
A D825 system can be assembled (or expanded) by 
selection of appropriate modules in any combination of: one to four computer modules, one to 16 memory modules, Table 1 Specifications, D825 modular data processing system 
Computer module: Computer module, type: Word length: Index registers: (in each computer module) 
Magnetic thin-film registers: (in each computer module) Real-time clock: (in each computer module) Binary add: Binary multiply: Floating-point add: 
Floating-point multiply: Logical AND: Memory type: Memory capacity: 1/0 exchanges per system: 
1/0 control modules: 1/0 devices: Access to 1/0 devices: Transfer rate per 1/0 exchange: 1/0 device complement: 
4, maximum complement 
Digital, binary, parallel, solid-state 48 bits including sign (8 characters, 
6 bits each) plus parity 
15 128 words, 16 bits per word, 0.33.psec 
read/write cycle time 10 msec resolution 
1.67 psec (average) 36.0 eec (average) 7.0 psec (average) 
34.0 psec (average) 0.33 psec Homogeneous, modular, random-access, linear-select, ferrite-core 65,536 words (16 modules maximum, 4096 
words each) 
1 or 2 10 per exchange, maximum 64 per exchange, maximum All 1/0 devices available to every 1/0 control module in exchange 2,000,000 characters per second 
All standard 1/0 types, including 67 kc mag- 
netic tapes, magnetic drums and discs, card and paper 
tape punches and 
readers, char. acter and line printers, 
communications and display equipment one to ten 1/0 control modules, one or two 1/0 exchanges, and one to 64 1/0 devices per 1/0 exchange in any combination 
selected from: operating (or system status) consoles, magnetic tape transports, magnetic drums, magnetic 
disc files, card punches and readers, paper tape perforators and readers, supervisory printers, high-speed line printers, 
selected data converters, special real-time 
clocks, and intersystem 
data links. Figure 2 is a photograph 
of some of the hardware of a com- pleted D825 system. The equipment 
complement of this system 
includes two computer 
modules, four 
memory modules (two per cabinet), two 
1/0 control modules (two per cabinet), 
one status display console, 
two magnetic tape units, two magnetic drums, 
Fig. 2. Typical D825 equipment array. 
Chapter 36 I D825-a multiple-computer system for command and control 
455 a card reader, 
a card punch, 
a supervisory 
printer, and an electro- static line printer. D825 characteristics are summarized in Table 1. Summary and 
conclusion It is the belief of the authors that modular systems (in the sense discussed above) are a natural solution to the problem of obtaining greater computational 
capacity-more natural than simply to build larger and faster machines. More 
specifically, the organiza- tional structure of the D825 has been shown to be a suitable basis for the data 
processing facility for command and control. Although the investigation leading toward this 
structure proceeded 
as an attack upon a number of diverse problems, 
it has become evident that the 
requirements peculiar to this area of application are, in effect, aspects of a single 
characteristic, which might 
be called structural freedom. Furthermore, it 
is now clear 
that the most unique characteristic 
of the structure realized-integrated opera- 
tion of freely intercommunicating, totally modular 
elements- provides the means for achieving structural freedom. For example, one requirement 
is that some specified 
minimum of data processing capability be always available, or 
that, under 
any conditions of system degradation due to failure or mainte- 
nance, the equipment remaining on line be sufficient to perform primary system functions. In the D825, module failure results 
in a reduction of the on-line equipment configuration but permits normal operation to 
continue, perhaps at a reduced rate. 
The individual modules 
are designed to be highly reliable 
and main- tainable, but system availability is not derived 
solely from this 
source, as is necessarily the case with more conventional 
systems. The modular configuration permits operation, 
in effect, with active 
spares, eliminating 
the need for total redundancy. A second requirement is that the 
working configuration of the system at a given 
moment be instantly reconstructable 
to new forms more suited to a dynamically and unpredictably changing work load. In 
the D825, all communication routes 
are public, all 
modules are functionally decoupled, all 
assignments are scheduled dynamically, and assignment patterns are 
totally fluid. The system of interrupts and priorities controlled 
by the 
AOSP and the 
switching interlock 
permits instant adaptation to 
any work load, 
without destruction 
of interrupted programs. The requirement for expansibility calls simply for 
adaptation on a greater time 
scale. Since 
all D825 modules 
are functionally decoupled, modules of any types may 
be added to 
the system simply by plugging into the switching interlock 
or the 1/0 ex- change. Expansion in all functional areas may 
be pursued far beyond that possible with conventional systems. It is clear, however, 
that the 
D825 system would have fallen 
far short 
of the goals set for it if only the hardware had been considered. The AOSP is as much a part of the D825 system structure as is the actual hardware. 
The concept of a ﬁfloatingﬂ 
AOSP as the force that molds the constituent modules of an equipment complement into a system 
is an important 
notion having an effect beyond the implementation of the D825. One interesting by-product of the design effort for 
the D825 has, in fact, been 
a change of perspective; it has become abundantly clear 
that computers do 
not rim programs, 
but that 
programs control 
computers. References AndeJ62; KrogM61; LeinA,57; PortREiO; ThomRH3 
Chapter 37 A survey of problems and preliminary results concerning 
parallel processing 
and parallel 
processors1 M. Lehman Summay After an introduction which discusses the significance of a trend to the des@ of 
parallel processing systems, the paper describes some of the results obtained to 
date in a project which aims 
to develop and evaluate a unified hardware-software parallel processing 
computing system and the techniques for its use. normal circumstances, with 
all units operational, 
each could be assigned a specific activity within an overall control program. As a result of the multiplicity of units in such Multiprocessing Systems, failure of any one 
would degrade, but not immobilize, the system, since a supervisor program 
could re-assign activities and configure the failed unit out 
of the system. Subsequently, it was recognized 1. M ultiprogramm ing, multiprocessing, 
and parallel processing A brief review 
of the literature, 
of which a partial listing is given in the bibliography, reveals an active and growing interest in 
multiprogramming, multiprocessing, and parallel processing. These three terms distinguish three modes of usage and also serve 
to indicate a certain historical development. We cannot here attempt to 
trace this history 
in detail and 
so must rely 
on the bibliography to credit the contributions from industrial, university, and other research and development 
organizations.  the^ emergence of autonomous input-output devices first sug- gested [Gill, 
19581 the time-sharing of the processing and periph- eral units 
of a computing 
system among several 
jobs. Thus surplus capability that could not 
be applied to the processing of the leading job in a batch processing load, at any stage of the compu- tation, could be usefully applied to successor jobs in 
the work load. In particular, while any 
computation was held up for some 
1/0 activity, the single main processor 
could be used for 
other compu- tation. The necessary decision-taking, scheduling, 
and allocation procedures were 
vested in a supervisor program, within 
which the user-jobs were embedded, 
and the resultant mode 
of operation was termed Multiprogrumming. The use of computers in on-line control situations 
and for other applications giving rise 
to ever-more stringent reliability 
and availability specifications, resulted in 
the construction of systems including two 
or more 
central processing units [Leiner et al., 1959; 
Bright, 1964; Desmonde, 
1964; McCullough et al., 19651. Under 'Proc. IEEE, vol. 54, no. 12, pp. 1889-1901, December, 1966 that such systems 
had advantages over a single processor system 
in a more general environment, 
with each 
processor in the system having a multiprogramming 
capability as well. 
Finally, following from ideas 
first exploited in the Gamma 60 Computer [Dreyfus, 19581, there has come 
the realization that multi-instruction counter systems can speed 
up computation, par- ticularly of large problems, 
when these may be partitioned into sections which 
are substantially independent of one another, and which may therefore 
be executed concurrently-that is, in parallel. 
When the 
several units of a multiprocessing system 
are utilized 
to process, in parallel, 
independent sections of a job, we exploit the macro-parallelism [Lehman, 19651 of the job, which 
is to be 
distinguished from micro-parallelism 
[Lehman, 19651, the relative independence of individual machine instructions, exploited in look-ahead machines. This 
mode of operation is termed Purullel Processing and, as in PL/I [IBM OS/360, PL/I Language Specifica- tion, Form C28-6571, p. 
741, the execution of any program 
string is termed a Tusk. We note that parallel processing may, 
and normally will, 
include multiprocessing activity. 2. The approach to parallel processing system design 
In the previous section we indicated 
that the 
prime impetus 
for the development of parallel processing systems arose from 
their potential for high 
performance and 
reliability. These systems 
may operate as pools of resources organized in symmetrical classes and it is this property that promises High Auuilubility. They also possess a great reserve of power which, 
when applied to a single problem with the appropriate 
degree of parallelism, can yield high 
456 
Chapter 37 I A survey of problems and preliminary results concerning parallel processing and parallel 
processors 457 performance and fast turn around time. 
Surplus resources can be applied to other 
jobs, so that the system is potentially efficient, displaying a peak-load 
averaging effect and hence high utilization 
of hardware [Corbato 
and Vyssotsky, 19651. The concept of sharing in parallel processing systems 
and its related cost reduction is not, however, limited 
to hardware. Perhaps 
even more 
significant is the common use 
of data-sets maintained in a system 
library or 
file, and even concurrent access during execution from a high- speed store. This may 
represent considerable economy 
in storage space and in processing 
time for 1/0 and internal memory. hierarchy transfers. But 
above all [Corbato and Vyssotsky, 19651 it facilitates the sharing of ideas, experience, 
and results and a cross fertilization among 
users, a 
prospect which 
from a 
long term point of view represents perhaps the most significant potential of large, library-oriented, multiprocessing 
systems. Finally, in this brief summary of the basic advantages 
of parallel processing systems, we refer to their intrinsic modularity, which may 
yield an expandable system in 
which the only effect 
of expansion on 
the user is improved performance. 
Adequate performance of parallel processing systems 
is, how- ever, predicated on an appropriately 
low level 
of overhead. Allo- cation, scheduling, and supervisory' strategies, in particular, 
must be simplified and the related procedures minimized 
to comprise a small 
proportion of the total activity in the system. The system design must be based on performance objectives that permit a user 
to specify a 
time period and a tolerance within which 
he requires and expects to receive results, and the cost for which these 
will be obtained. In general 
the entire system must yield 
minimum throughput time 
for the large job, adequate response time to the terminal requests in conversational mode, 
guaranteed throughput 
time for real-time 
tasks, and minimum cost processing for 
the batch-processed small job. 
These needs 
require the development of an executive and supervisory system 
integrated with 
the hard- ware into a single, unified 
computing system. Finally, 
the tech- niques and algorithms of classical computation, of problem analy- 
sis, and of programming, must be modified and new, intrinsically 
parallel procedures developed 
if full advantage is to be gained from exploitation of these parallel 
systems. Our studies to date represent but a small 
fraction of the ground that will have to 
be covered if effective parallel processing systems are to come into their own. It is, however, abundantly clear that such systems will 
yield their potential 
only if the design is ap- proached on a broad but unified front ranging 
from problem ' We differentiate intuitively between executive and supervisory activities. The former are those whose costs 
should be chargeable to the individual user directly, whereas the latter are absorbed in the system running costs. analysis and usage techniques, through executive strategies 
and operating systems, to logic design 
and technology. We therefore 
present concepts and results from 
each of these areas, as 
obtained during our preliminary investigation 
into the 
design and use of parallel processing systems. 
3. Language 3.1 The analysis of high level language requirements for parallel processing has received considerable 
attention in 
the literature. We may 
refer in 
particular to 
the paper by Conway 
[1963] which discussed the concepts of Fork, Join, and Quit, and the 
recent review by Dennis 
and Van Horn [1966]. Recognizing that programming languages should 
possess capa- bilities that express the structure of the computational algorithm, Schlaeppi [ 19??] has proposed augmentations 
to PL/I-like lan- 
guages that portray the macro-parallelism in numerical algorithms. 
These in turn have been 
reflected in proposals for machine- 
language implementation. As examples we discuss Split, Terminate, Assemble, Test and Set or Wait (interlock), Resume, Store-Test and Branch, and External Execute instructions. We describe 
here only the basic functional elements, from 
which machine instructions 
for actual realization will be composed as suggested by practical programming experience. 
Parallelism in high level languages 
3.2 Split provides 
the basic task-generating capability. 
It indicates that in addition to continuing the execution of the present instruction 
string in normal fashion a 
new task, or set 
of tasks, may be initi- ated, execution starting at a specified 
address or set of addresses. Such potential tasks will 
be queued to 
await pick-up by 
an appro- 
priate processing unit. Terminate causes cessation 
of activity on a task. 
The terminat- ing unit will, of its own volition, access 
an appropriate queue 
to obtain its next task. 
Alternatively, it may execute an 
executive allocation-task to determine which of a number of task-queues is to be accessed next 
according to the current urgency status 
of work in the system. Assemble permits the merging of several tasks. The first (n - 1) tasks in 
an n-way parallel set belonging 
to a single 
job, reaching the assemble instruction terminate. The nth task, however, will proceed to execute 
the program string which constitutes 
the continuation of all n tasks. Machine level 
instructions for tasking 
458 Part 5 1 The PMS level Section 3 I Computers for 
multiprocessing and parallel processing Test and Set or Wait 
provides an interlock facility. 
Thus a number of tasks all operating on a 
common data set may be required to filter through certain sections of program or data, one 
at a time. This may be achieved by 
an instruction related to 
the S/360 test and set instruction [Falkoff et al., 19641, but causing the task finding the specified location to be already set to go into a wait state. System efficiency 
requires that processors do not idle, so that the waiting task will 
generally be returned to queue and the processor released for 
other work. He.wme directs a processor 
or processors waiting as a result 
of a test on a specified 
location, to proceed, or 
more generally, that specified waiting tasks that have been returned 
to queue be re-activated to await the spontaneous availability 
of an appropri- 
ate processor. Test and Branch Storage Location permits communication be- 
tween parallel tasks based on tests analogous to the register tests 
of uniprocessors, but associated with the contents 
of storage loca- tions. This is desirable since 
processor registers 
are private to 
the processor and inaccessible from 
outside. External Execute is a special case 
of the general interaction facility discussed in Section 
4 that permits related 
tasks to influ- ence one another. This can be achieved through 
the application of instructions already 
discussed. It is, however, more efficient to provide a new facility akin 
to the Interrupt concept. 
By applying this Interaction function, 
a task 
may cause other specified tasks to execute an 
instruction at a specified location, each on comple- tion of its present instruction. Thus, 
for example, a 
number of processors searching for a particular item in 
a partitioned list can be caused to abandon the search when the item has been located by one, while processors searching for other items, or otherwise 
busy, will 
not be redirected. 4. Interaction 4.1 The interaction concept An extension of the task interaction concept introduced in 
the preceding section is fundamental to efficient parallel processing. In the particular example cited, the interaction, in 
the form of an external execute instruction, 
forms part of the computational procedure. In fact, many other situations arise 
in which processing for inter-task communication may 
be detached from problem processing and be 
carried through concurrently 
in autonomous units, thereby increasing system utilization. We therefore 
propose to associate with each active unit in the 
system an autonomous Interaction 
Controller. Groups of controllers are linked by a special bus. This 
provides facilities whereby 
any one unit 
may, at a given 
time, act 
as a command or signal source with all other units potential recipients. By thus systemizing 
inter-unit communication and making it a concurrent activity, we 
both increase system utilization and remove a maze of intercon- necting cables. Succeeding 
subsections describe some 
of the func- tions that the 
controllers fulfill and, briefly, one hardware proposal for their realization. 4.2 Interaction activities 
In present-day 
systems there already exist activities of the type to be classified as 
interaction. Thus, for example, in System/S6O we find a CPU to Channel Halt I/O facility, channel interruptions 
of processors, and timer interruptions. 
In extending 
the concept we differentiate among 
three classes of interaction. PROBLEM INTERACTION. These relate to 
logical dependencies between tasks, and will generally require waits, forced branches, 
or terminations. Search termination, previously discussed, 
is an example of this type interaction, as are data and instruction- sequence interlocks. 
EXECUTIVE INTERACTION. This activity 
is concerned primarily with the allocation of system resources. 
Consider, for example, 
the problem of processing interrupts in 
a parallel processing system. 
These will usually 
not need to interrupt a computing activity, but may await the spontaneous availability 
of a unit at a Terminate, a natural lx-eakp0int.l If an interrupt does become critical it should not be applied to 
a specific physical 
unit. Instead 
the interruption should be steered to that unit which, by 
virtue of the work it is processing, may be classed as Most Interruptable. Selection 
of the latter may be obtained ahead 
of time and is maintained by the interaction system, on 
the basis of the relative urgency 
of tasks. Another example 
of executive interaction concerns the constant provision of queue status information 
to all active units. Besides simplifying scheduling activity this 
may prevent units from 
access- ing empty queues, reducing both 
storage and executive interfer- 
ence. Similarly, units can be caused to access a previously 
empty queue when an entry 
is made, obviating continuous 
testing of queue status. 
'This is possible in a parallel processing system since tasks are smaller than jobs and since 
there are 
many processors. Furthermore, units operate anonymously. That is, on picking up a task, a unit records 
the task identity in an 
internal register and its own identity in a table associated with 
the work queue. Other processors do not, therefore, know how tasks and processors are matched 
at any time, since this is a matter of chance, and determination would require an extensive and wasteful table search. 

Chapter 37 I A survey of problems and preliminary results concerning parallel processing and parallel 
processors 459 The interaction system also supports other 
activities associated with accounting, recording, 
and general system supervision. 
SYSTEM INTERACTION. System interaction provides controls and interlocks for operation and maintenance of the physical system. 
It includes, for example, interchange of information between active units about the validity of storage map entries, storage 
protection control, queue interlocks, checks 
and counts of unit availability, the initiation of routine and emergency diagnostic 
and maintenance activity, and the 
isolation of malfunctioning units. 
SUMMARY. The preceding paragraphs have 
indicated some of the many applications 
of an interaction controller. The common property which, for practicality, has been used to identify poten- 
tial interaction activities is that they should be autonomous rela- 
tive to the main computational stream 
and that 
their execution should not require 
access to storage. 4.3 The interaction controller 4.3.1. The basic system hardware architecture. 
It is not intended 
to give a 
full description 
of an interaction 
controller in the present paper. We shall, however, outline its basic 
structure, indicate its 
mode of operation, and list some of the proposed interaction instructions, termed Directiues. As a first step we introduce, in 
Fig. 1, a diagrammatic descrip- tion of an overall representative hardware 
system. This consists of central processors (Pi) with local storage 
(LSi), 1/0 processors (SCi), storage modules 
(Si), a requestor-storage queue (Qi), and a communication system functionally equivalent 
to a crossbar 
switch. iln 1/0 area, including a bulk-store, files, channels (Ch), devices, device control units 
(Cu), and interconnection networks, is indicated in 
less detailed fashion. 4.3.2. lnteraction controllers. Interaction controllers (IC) are associated with all central and 1/0 processors, and communicate with each other 
over a 
special bus. Similarly 
localized interaction systems may provide 
a facility for certain classes of 1/0 units or devices to interact 
amongst themselves. 
To be economically feasible, the Interaction Controller must be simple. Figure 2 illustrates a structure which includes 
about two hundred and fifty bits of storage, of which about half are organized in registers. The remainder are used as status bits or appear in the 
controller-processor interface. Control 
is obtained from a read-only store, 
whose capacity depends 
on the size of the directive repertoire (an interaction 
directive being 
analogous to a processor 
instruction) and the number of interaction functions it is required to implement. 
Controller connection to the ten-bit wide interaction bus 
is by means of OR gates. When an interaction 
is occurring, one and only one controller will be in command of the bus. Figure 3 illustrates the sequence of events required to implement an inter- action. The controller required by its associated 
processor to initiate 
an activity 
will await availability of the bus, indicated by an ALL ZERO state, and will then attempt 
to seize control by 
transmitting a unique identifying four-out-of-eight code. Should more than one controller attempt to seize the bus at the same time, 
a conflict 
resolution procedure is initiated. This 
is based on the simultaneous transmission by all requesting controllers 
of a second, 
two byte, 
identifying code. Each byte consists of one or more ones followed by all zeros. A simple comparison by 
each controller of its trans- - Fig. 1. A representative system 
hardware configuration. 
460 Part 5 I The PMS level + t+- Section 3 I Computers for multiprocessing and 
parallel processing ~~ ~__~~_~~ .. Task ident Seizure code 
Registers ~~~~~~-___ Status bits Processor or channel interface -I -- -I Job ident 7-v Interlock id. reg. ~~ ~ 
- .-I F, Interaction bus Fig. 2. The interaction controller. 
mitted signals with the state 
of the bus, identifies 
to itself that controller having the most ones 
in each byte, 
since it will have found a match 
on both comparisons. This 
enables it to 
seize the bus and to switch to the 
command state. All remaining controllers remain in the listening state. The controller in 
command of the bus then transmits signals which select recipients 
for the directives which are to follow. Other controllers ignore all further communications until the next selection signal appears. 4.4 Interaction directives 
A signal designating the interaction function required 
by a proc- essor is transmitted across the processor/controller interface, as the result of the execution of some processor 
instruction. The processor will 
then generally continue 
its execution sequence unless or until it is required to pass on 
a second interaction func- tion before 
a previously issued 
function has been completed. 
Upon receipt of the interaction 
command, and 
after successful seizure of the bus as described, the command controller may 
initiate Interaction required a7 Bus free ? Seize bus i Conflict? &L Ernit order or question Fig. 3. The interaction sequence. execution of the interaction by transmitting a sequence 
of one or 
more directives to the selected 
units. A basic set of directives is listed in 
Table 1. The Compare directives are most frequently used to seize the bus and to select 
a subset 
of the controllers for the receipt 
of subsequent directives. The remaining units ignore further direc- tives until alerted 
by an 
Attention signal or 
until Free Bus provides the release that permits waiting 
controllers to attempt 
to seize the bus. Receive provides for 
transmittal of data between 
control- lers; for example, transmission 
of a machine 
instruction to 
a se- lected set of controllers, followed by the directive Interact. Thus this sequence could 
realize the basic interaction function. External Execute is, however, considered 
so fundamental to efficient exploi- 
tation of a parallel processing system 
that we include 
it as an Table 1 Send and 
Compare Compare Received Set Status 
Bits Interact External Execute 
Attention Free Bus 

Chapter 37 1 A survey of problems and preliminary results concerning 
parallel processing and 
parallel processors 461 explicit directive. Status 
bits that may be set or reset by appropri- ate directives, provide data on the status 
of various systems 
queues, on the interruptability 
of given processors, on Wait 
status, and so on. 5. Storage communication The fact 
that interest in 
large parallel processing systems 
is in- creasing rapidly 
as technology enters into the integrated 
or mono- 
lithic era is no coincidence. Such systems will 
not, in 
fact, be practical for general purpose application until miniaturization reaches the stage where the large amount of hardware required 
can be assembled in compact fashion. This 
need is most apparent when one considers 
communication between the high-speed store and the various classes of processors, which 
may collectively be termed Requestors. Already in presently available systems, the transmission delay between 
storage and requestors is of the same order of magnitude as the storage cycle time; and cycle times 
are still decreasing. Formulation of a hardware model as 
in Fig. 
1 led to the 
imme- diate conclusion that feasibility of the interconnection of large numbers of units had first to be established. Many 
possible systems 
were considered, 
and preliminary studies concluded that the 
crossbar switch was the most appropriate system for 
early study in view of its regular structure, simplicity, and basic modularity. More particularly, monolithic crossbar modules 
are visualized which it will be possible to interconnect to provide networks of any required dimensions. Alternatively, or additionally, other interconnections of these modules can provide 
highly available, multi-level trunking systems. In addition to the 
switch proper, 
the crossbar network requires a selection 
and control mechanism. It is moreover appropriate to locate the queues, which store all but one of a group of conflicting requests, within the switching area. A switch complex, as 
in Fig. 4, has been designed for 
a system configuration 
including twenty- four requestors, 
thirty-two memory modules, thirty-two data 
plus four parity bit words, and sixteen plus 
two parity bit addresses. The result of this design 
study shows that the 
size and com- plexity of such a switch 
is not excessive for 
a large scale system. 
In its simplest form 
and using standard high-performance logical devices, with a 
fan-in of four, a fan-out of ten and a 
four-way OR capability, its 
use leads to a worst case 
delay of some seven logical 
levels in the control and queue decision circuits and two levels in each direction of the switch. The switch 
uses between two and 
three times as many circuits as a central processor such 
as the model 75 of System/36O. While this, in itself, 
represents a 
consid- erable amount of hardware, it is still an order of magnitude less than the 
hardware found 
in the units that the switch is intercon- necting. Moreover, its regular structure and simple, repetitive logic suggest 
ultimate economical realization using monolithic circuit techniques. 6. Usage 6.1 The executive 
system The basic properties outlined 
in Sec. 2 give parallel processing systems the potential 
to overcome many of the ills and shortcom- 
ings that presently beset 
computer systems. For maximum effec- tiveness, the system must 
be library- or file-oriented. It can, 
how- ever, be exploited efficiently only 
if the overhead resulting from executive control and supervisory activity does not strangle the system. More 
particularly, the gains from 
the sharing of resources and any 
peak averaging effect must exceed 
any additional over- head due 
to resource allocation procedures, conflict resolutions, 
and other processing activity arising 
from the concurrent operation of many units. 
Thus a 
unified and integrated 
design approach is required in which software and hardware, 
operating system and processing units, lose their separate 
identities and merge into one End of storage 1 1 
Storage cycle, select, To other reqL SConneri ' switching sec From other decoders I:- Decoder, To other scanners Request signal,, I 181 tors and other )n inputs .ccept signal, 
I Decision section i Crossbor switch signal, Fig. 4. The centralized crossbar switch. 
462 Part 5 I The PMS level Section 3 I Computers for multiprocessing and parallel processing 
overall complex, 
for which allocation and scheduling procedures, 
for example, are as basic and as critical as arithmetic operations. Equally significant to the 
successful exploitation of parallel processing potential are the 
problems of data management, man- machine interactions; 
and, most generally, problem 
preparation and usage of the system. We restrict 
the present discussion to brief comments on programming techniques 
for task 
generation and on the development of algorithms possessing macro-parallelism. In particular we indicate that multi-instruction-counter systems can be profitably applied to the solution of the large problems 
whose computing requirements 
tax the speed capability and storage of the largest computer and the patience of their users. In the fol- lowing section 
we evaluate these proposals by quoting some per- formance measurements 
obtained from an executing simulator. 
6.2 Programmed task generation 
Study of the usage of parallel processing systems for 
the rapid solution of large real-time problems 
involves two aspects. On 
the one hand we must consider the development of algorithms dis- playing an appropriate 
form of macro-parallelism. On the other hand programming techniques must be developed for efficient 
exploitation in terms 
of both problem- and machine-oriented instructions, such 
as those discussed 
in Sec. 4. It is appropriate to discuss programmed task generation first. For simplicity we consider 
a job 
segment that requires n executions of a procedure I. The procedure will itself include modification of index registers or 
other changes that distinguish the individual tasks. We assume that on completion of all n tasks, a new proce- dure J should be initiated. Moreover, should processing power be available at a time when n executions of I have been initiated 
but not all n completed, we assume that an independent procedure 
K, belonging to the 
same job, may be initiated. In the simplest case K will be a terminate instruction 
which releases the processor, and makes it available to process other work as 
determined from the work-queue complex. AZO B=O c=o ST IF N - B 5 1 THEN GO TO IN Suppress split if nth task being initiated A=A+l IF A 2 P THEN GO TO IN Split if less than p proces- sors allocated SPLIT TO ST B=B+1 IF B > N THEN GO TO FIN 
IN If all n I-tasks started, proceed with 
K CALL I PROCEDURE C=C+l IF C < N THEN GO TO IN If all n I-tasks completed, proceed with 
J CALL J PROCEDURE FIN CALL K PROCEDURE Execution of split and terminate instructions involves executive overheads, so that these instructions should 
not be used indiscrim- inately. Within a system 
in which a maximum 
of p processors are available to a job, 
it is pointless 
to partition a job, 
at any one time, into more than p tasks. It is, however, undesirable 
to guarantee 
a user that p processors, or even more 
than one processor, will 
execute his program. A simple task generation scheme that makes as many entries in 
the task queue as there are 
potentially concur- rent parts 
of the algorithm (for example, from a 
loop containing 
a split instruction) 
is inefficient when that number is much larger than the number of processors that happen 
to be available. The technique also leads to very large queues. 
An alternative, termed Onion Peeling by us, puts the instruction sequence 
containing the split at the head 
of procedure I and ends each execution of the procedure with 
a terminate. This restricts 
the queue length 
for this job segment to one but it otherwise is as inefficient 
as the previous method. A Modilfied Onion Peeling scheme (MOP) restricts the split and terminate overhead to at most one morel 
than the 
number of processors actually applied 
to the segment. It also ensures that processing is completed as quickly and as efficiently as possible 
with the number of processors that become available to the job segment. Thus if during execution no further 
processors are freed, the n tasks are executed sequentially with only one split and no terminate. If, on the other hand, 
some other number of processors is used for execution, the procedure is speeded up accordingly. The maximum number p of processors that may be applied to the job may be limited by the number of processors in the system and available, or by executive 
edict. The basic scheme was illustrated by 
the above program, 
in which the first expressions following the ZEROing of counters ensures that no unnecessary splits 
are queued. 
'This is not quite accurate. The simple MOP algorithm presented here does not explicitly interlock the split seqnence. There is therefore a possi- bility that unnecessary task-calls 
may be queued during the 
execution of the split which is to generate the nth task. The 
probability of this is, however, small, while the degradation arising from an interlock could be significant, and the algorithm in the form given appears more economical. 
Chapter 37 1 A survey of problems and 
preliminary results concerning 
parallel processing and 
parallel processors 463 6.3 Macro-parallelism Commonly used 
numerical algorithms, data processing procedures, and computer programs are generally sequential 
in nature. The reason for 
this is largely historical, 
a consequence of the fact that the Mechanisms, human, mechanical, and electronic, used 
in developing and executing these procedures have 
been incapable 
of significant parallel activity, 
other perhaps 
than the simultane- ous, coordinated use of many humans. The advent of parallel processing systems 
thus calls for the modification of accepted techniques to 
expose any inherent 
parallelism. The resultant pro- cedures must 
then be further adapted to make parallel 
tasks of such a magnitude that the 
overhead involved 
in their 
generation becomes insignificant. But 
the ultimate benefit from 
parallel execu- 
tion will be obtained only by 
going back to the problems them- selves. These must be analyzed anew. 
Algorithms must 
be devel- oped that make it possible to exploit the parallel executing 
capa- bility, by 
introducing into the mathematical and program model 
parallelism that ultimately reflects the parallelism of the physical system or phenomena 
being studied. In this 
need to return to 
fundamentals, the situation is somewhat analogous to the early days of electronic computing, when attempts at commercial ap- plication were largely frustrated until 
it was realized that wide- spread application required 
the development of new techniques, rather than the 
adaptation and mechanization of existing proce- dures. At the present time, however, our 
direct activity in 
problem analysis has concentrated mainly on 
the adaptation of existing numerical techniques for parallel 
processing, for 
problems in which the basic macro-parallelism was self-evident. 
These include, 
for example, linear algebra 
and the solution of elliptic partial 
differential equations. 
In these areas 
the extent and nature of the parallelism had previously led to proposals for vector processing systems such as Solomon 
[Slotnick et al., 1962; Gregory and McReynolds, 19631 and Vamp [Senzig and Smith, 19651. Other areas in which the parallelism is self-evident hut where vector 
processors prove less effective are those in 
which the algorithms model distinct physical activities 
such as in file processing and Monte Carlo techniques. 
For all significant problems investigated 
[Schlaeppi, 19??] it was possible to establish the existence of parallel tasks of such a length that tasking overheads could 
be expected to 
be negligible. Other classes of problems have 
been studied, both in terms of the extension of existing algorithms and the development of new ones. In particular we refer to the 
extraction of polynomial roots [Shedler and Lehman, 19661, solution of equations [Shedler, 19661, and the 
solution of linear differential equations 
[Niever- gelt, 19641, [Miranker and Liniger, 19671. These various studies, not all directly related to 
the present project, 
were more mathe- matical in nature, 
and to the 
best of our knowledge, no attempt has yet been 
made to develop efficient parallel computer programs. Thus, while numerical methods are beginning to emerge which enable the exploitation of macro-parallelism in the solution of time-limited problems, 
and from which it appears that significant reductions may 
be obtained in throughput times, much 
work remains to be done on re-programming the problems themselves. 7. Simulation 7.1 It has been our experience 
with simulation that its principal function as a 
design tool 
is to focus attention on features 
that require investigation and explanation. Many results, 
qualitative and quantitative, that are 
obtained during 
simulation experiments 
may also be obtained analytically. It is, however, the insight and understanding gained 
from the design of simulation experiments 
and the 
analysis of their results that draws attention to specific details and difficulties. The undeniable value of simulation in development and design is therefore quite different from that in system evaluation, where meaningful performance figures may be obtained when the work load is well defined. 
Simulation as a design tool 
7.2 The executing simulator In the present study simulation was seen as 
fulfilling a number of additional functions. In 
particular it 
made available a usable working model of a parallel processing system. 
This would 
give potential users the incentive to undertake actual 
programming and to gain limited operational experience. 
An executing simulator 
was also required for the investigation of what is commonly regarded as the most immediate question in parallel processing, the extent of performance degradation 
due to storage-access interference and executive (queue-access) interference. Such 
an executing simulator 
is now operational 
and its use is discussed in 
the next section. We 
note parenthetically 
that a limitation of this type simulator is its speed. For 
the evaluation of total system performance over any length of time, particularly 
when using a 
computer itself much slower than the 
simulated system, only 
gross, nonexecuting, sim- ulation is reasonable [Katz, 
19661. The system presently modeled 
in the executing simulator in- 
cludes the processors, switch, and Storage Modules of Fig. 1. The storage modules 
are accessed through 
a fully 
interleaved address 
464 Part 5 I The PMS level Section 3 I Computers for multiprocessing and parallel processing 
structure, though it is clear that in any realization interleaving 
will be partial, both 
to sustain high 
availability and to decrease storage interference 
between independent 
jobs. The individual processors have a 
System/36O-like structure [Blaauw and Brooks, 19641 and execute an augmented subset of S/36O machine lan- guage. The nonstandard instructions added to the repertoire in- clude the functions discussed in Section 4. 
The local store 
LSi, to be 
used also as an 
instruction buffer, is however not included 
in the model for which 
the interference results are quoted 
in the next section. The simulator configuration is parameterized so that, for example, 
the numbers of storage modules and processors, instruction execution times 
(in storage 
cycles), and the nature of statistics gathered and printed 
may be selected for each run. The program itself is modular, and both 
system features and measure- ment facilities may be expanded or modified as required. 7.3 Simulator experiments 
7.3.1 Kernels. Simulation experiments fist concentrated on an investigation of storage interference 
arising in the execution of typical kernels from 
numerical analysis. The results indicated that under the limited condition of the experiments and for a storage module-to-processor ratio of two, interference would degrade performance by less than twenty percent, 
dropping to some five percent for storage module-to-processor ratio of eight. Addition of a local processor 
store and its use as an instruction buffer effectively eliminated interference, as expected, indicating that it had been substantially due to instruction-fetch interference. These results 
were considered 
to have been generated 
under conditions too 
restrictive to permit generalization. In particular 
each set referred 
only to concurrent executions of a single loop. 
Thus more recent experiments have included 
many runs of a matrix-multiply subroutine and 
the solution of an electrical net- 
work problem using an 
appropriately modified version 
of the Jacobi variant of the Gauss-Seidel solution 
of a set of linear alge- braic equations. 
7.3.2 The matrix multiplication. 
The Matrix Multiply program was written in two versions. A classical sequential program ex- cluding all the special instructions 
provided the standard on which 
measurement of the parallelism overhead and interference could be based. The second, parallel, program used 
the onion peeling rather than 
the MOP algorithm described 
in Sec. 7.2. The product matrix was partitioned by rows, 
with the computation of each comprising one task. The experiments were 
performed for 
square matrices of dimensions thirty-nine and forty with from one to sixteen processors 
and sixteen to sixty-four storage modules. Two sizes of matrices were 
used to isolate the effect of commensurate periodicities of array mapping 
with the address structure of the store, which demonstratively had significant influence on 
the results. Instruction execution times for the most frequently executed 
instructions used in the experiment are given in Table 2. These times exclude 
the instruction fetch time (one 
instruction for each fetch), 
since these are overlapped unless storage conflict occurs, when a request 
must be queued. The arithmetic 
operations may also include a 
data fetch 
(RX instructions) in 
which case a further store access time is required. In the absence of an internal instruction buffer, processors 
executing the same program 
string interfere 
with each other 
continuously during instruction fetches. 
To minimize this effect for loops that are short relative 
to the 
width of the interleaving, it is profitable to unwind such loops by 
repetition so that the 
resultant string 
stretches as far as possible across 
the interleaved store. The program was 
unwound in this way. 
We note, however, that it is in fact better [Rosenfeld, 19651 to repeat the loop, appropriately modified, several times across 
the interleaved store, directing successive processors 
to successive, hut unconnected, loops. This 
can decrease 
interference by as much as twenty percent 
over the previous case. 
Some results 
of the simulation are given in Table 3 and plotted in Figs. 5 and 6. We note that running time (col. 4) 
is defined as 
the interval between the start 
of the first processor on 
its first task and the completion, by 
the last processor 
to finish, of its final task. Since 
an onion 
peel technique 
has been used for 
the splitting, there is an interval (of order 70 storage cycles) 
between the start of suc- cessive tasks. 
There is also an initial interval 
(87 memory cycles) in which the first processor initializes the program. Finally, the finish of processors is staggered and, in particular, 
for the sixteen- processor case, 
eight processors are assigned two tasks (rows) 
in succession, and eight, three tasks. The former processors will, 
of Table 2 Instruction Execution time in storage cycles Fixed Point Addition 0.4 Floating Point Addition 0.5 Floating Point 
Multiplication 1 .o Floating Point Division 2.0 Split 25.0 Terminate 25.0 New Task Fetch (Part of Terminate) 25.0 
Chapter 37 I A survey of problems and preliminary results concerning parallel processing and parallel 
processors 465 "3 m 400K- I - 5 300K- g 200K- m m rn - 100K- Table 3 Results of the matrix 
multiply simulation 
Parallel processor progrom program 8 Uniprocessor (40x40) "(401 40) N,, = 64 16 tosks x \: 40tasks 1 2 3 4 5 6 7 8 9 10 11 No. of Total Storage interference Exec. 
NO. of Storage No. of storage Matrix 
Run proc. intel$ storage 
utilization proc. mods. dim. time time Time 
% % accesses % Notes 1 1 2 4 8 16 16 16 16 64 64 64 64 64 64 32 16 64 40 40 
40 
40 40 40 40 
40 39 427 429 216 109 56 35 38 47 33 427 429 432 436 445 46 1 507 639 428 1.02 0.21 1.77 5.79 14.4 30.3 75.9 207 .O 26.1 .2 NA 0.05 NA 0.4 0.33 1.3 0.39 3.3 0.68 7 .O 0.76 17.7 0.88 48.2 0.64 6.5 NV 459K 460K 460K 460K 460K 460K 460K 460K 427K 1.69 Sequential program 1.68 Interference between 3.3 instruction & data fetches 6.6 13.0 25.0 45.4 72.1 26.9 Note: All times in thousands of storage cycles. NA-Not Applicable NV-Not Available # Acc. x # Proc. % Storage Utilization = Roc. time x # Mods. Col. 9 x Col. I .. - - Col. 5 x Col. 2 course, terminate considerably earlier than the latter. Thus, as indicated by the corresponding entry in column four, 
the particu- lar mode of partitioning is not optimum if the shortest execution 
time is to be obtained. From a system 
efficiency point of view, however, and in actual 
operation with other jobs and tasks in the 
system, it is of no consequence since 
processor idling does not actually occur. New tasks, perhaps arising 
from quite different jobs, are initiated, according to some scheduling strategy, 
whenever a processor becomes spontaneously available. 
Fig. 5. Execution time for matrix multiply. Time (4Ox40)x(40x 40) +. N,, =64 420KI E\ 30K Total delay due to storage interference '""LNP 10K 5 Number of processors Fig. 6. Total processor time and interference 
in matrix multiply 
modules. In addition to run time, 
we define a 
total processor time (col. 5). This represents 
the sum total of time that individual processors were active in 
the program and is therefore a reflection 
of total processor running cost. Storage 
interference (cols. 6, 7) measures the total time 
that processors were inactive 
due to 
attempts to initiate simultaneous accesses to the same storage module. 
It occurs also when only a single processor is 
applied, when it repre- sents a conflict 
between a data fetch and an attempt by the overlap circuit to initiate an instruction fetch from the same module. 

466 Part 5 I The PMS level Section 3 1 Computers for 
multiprocessing and parallel processing STORAGE KILOCYCLES II"" $400 350 2 300 // 16 STORAGE MODULES INNER LOOP SIZE I I --2 EOUnTlONS ---A--- 3 EOUATIONS 4 EQUATIONS Id: l,i 5 EOUATIONS -.-)(-.- II' 40 30 20 Fig. 7. Total processor and throughput 
times in electrical network analysis-16 storage modules. 
Executive interference (col. 8) represents processor hold-ups 
due to the 
simultaneous attempts by two or more processors to access the system work-queues. 
These interferences are 
of course repre- sentative of a whole class of effects that can lead to performance degradation in parallel 
processing systems. 
In Table 
3 interference has been related 
to the number of interleaved storage modules and to the 
number of processors. In 
an actual system it is of course a complex function of the number of storage modules, of the degree of address interleaving, of the relationship between active jobs and the degree of program and data sharing, and of the total system utilization of storage. In optimizing a 
design, the numbers of processors and storage mod- ules and the addressing scheme must be fixed subject to constraints 
related to cost, total storage capacity, the capacity 
of available storage modules, the degree of availability desired, 
and the 
ex- pected nature of the work load. Processor utilization of storage alone is not very significant, 
since a critical factor 
is the 1/0 storage activity present, 
the degree of storage utilization 
required to get program and data 
into the high-speed store and to output results. We include utilization figures for these executions 
in Table 3, to aid in 
analysis of the system behavior 
but not for 
evaluation purposes. 7.3.3 The electrical network analysis problem. This problem 
represents the solution of a set of simultaneous linear equations, 
described by a sparse coefficient matrix. 
The technique 
used for 
its solution on 
the executing simulator 
essentially comprises 
a relaxation procedure. Extensive runs 
have been made 
using a specific thirty-six node 
network, yielding twenty-six 
equations with 
up to four terms in each equation. 
From the wealth of results obtained we present representative 
sets that indicate some general trends 
related to the characteristics and performance 
of the parallel processing system. Available 
space will not permit, 
however, detailed analysis in the present paper, nor does 
it permit a 
discussion of the equally interesting results obtained concerning speed 
of convergence, in particular, and other STORAGE KILOCYCLES 32 STORAGE MODULES 600 550 INNER LOOP SIZE - 2 EQUATIONS ---e--- 3 EQUATIONS - ----- 4 EOUATIONS _.-x-.- 5 EWATIONS 8 350 300 W I- z 3 E P NUMBER OF PROCESSORS Fig. 8. Total processor and throughput 
times in electrical 
network analysis-32 storage 
modules. 
Chapter 37 I A survey of problems and preliminary results concerning parallel processing 
and parallel processors 467 STORAGE KILOCYCLES 64 f 500 550L n. 250 8 200 100 64 STORAGE MODULES INNER LOOP SIZE f 2 EWATIONS ----A---- 3 EQUATIONS -..+..- 4 EQUATIONS 5 EOUATIONS I 2 3 4 
5 6 7 8 9 1011 1213141516 NUMBER OF PROCESSORS Fig. 9. Total processor and throughput times 
in electrical network analysis-@ storage modules. effects which must be understood within 
the framework of a numerical analysis of the relaxation solutions. 
Figures 7,8, and 9 present the basic performance data, through- put time, and total 
processor time, for a total of one hundred and 
forty-four cases. The variables are the number of processors in the system (12 cases), the size of the inner loop as 
represented by 
the number of currents (from 2 to 5) evaluated in the loop, and the number of interleaved storage modules 
(16, 32, 64). These curves clearly indicate the 
reduction in throughput time 
to be obtained 
from the use of parallel processing, the consequent increase in processor cost 
due to interferences of various sorts, 
the resultant effect of diminishing returns, and the actual increase in throughput time, when 
too many processors chase too few 
equa- tions and generally 
get seriously ﬁinto each other™s way.ﬂ 
For the smaller inner 
loops and when interference 
between processors is low, total processor times vary 
somewhat erratically. The causes for this are related 
to the relaxation pattern and the rate of convergence in each case. In 
fact there appears 
strong circumstantial evidence 
that an ad hoc procedure, 
which does 
not guarantee sequential evaluation 
of the equations, improves per- formance. This point, however, requires further study. Figure 10 reproduces some of the results of the previous three figures for 
the case of a five-equation inner loop. Table 
4 lists these same results as 
a percentage of the time using one processor and compares them 
with the reciprocal of the number of processors. Figure 11 indicates storage interference 
and parallel processing overheads as 
a function 
of the number of processors, with storage modularity again a parameter and an 
inner loop again comprising 
STORAGE KILOCYCLES 5 EQUATIONS 260 IN A LOOP - w + 240- a 220- - - 9 200- 0 - 180- -I 160- e 140- n - $ - - 120 100 - - - -0- 16 STORAGE MODULES -+A-- 32 STORAGE MODULES 64 STORAGE MODULES $ I- 601 40 501 20 NUMBER OF PROCESSORS Fig. 10. Total processor and throughput times 
in electrical network analysis with number of storage modules as a parameter. 

468 Part 5 1 The PMS level Section 3 I Computers for multiprocessing and parallel processing 
Table 4 using one processor, 
with a five equation inner loop Run time for resistor network system relative to the run time 
Relatiae time 
100 Number of 16 Storage 32 Storage 64 Storage processors modules modules modules No. of processors 1 2 4 6 7 8 9 10 11 12 14 16 100% 52.8 29.5 22.4 20.9 19.2 17.8 17.6 16.8 17.5 17.3 17.7 100% 51.2 27.9 20.3 17.9 16.8 15.2 14.5 13.9 13.9 13.2 13.7 100% 51.2 27.1 19.5 17.1 
15.8 14.2 13.7 
12.9 13.0 
11.7 11.7 100% 50.0 25.0 16.7 14.3 12.5 
11.1 10.0 9.1 8.3 7.2 6.3 the evaluation of five currents. Storage interference has previously 
been defined. The parallel processing overhead represents 
as a percentage the excess of total number of storage cycles 
required for execution, 
excluding storage interference 
cycles, when more than one processor is used, relative to the number of cycles re- quired by a one-processor execution. % " T 2345678910111213141516 NUMBER OF PROCESSORS Fig. 11. Storage and executive 
interference. 20 3 IO 0 F I2345678910111213141516 NUMBER OF PROCESSORS Fig. 12. Storage utilization and cost /performance factors. Actual counts during 
execution show 
that in general some sixty-seven percent of store access are instruction fetches in 
this program and some thirty-three percent 
are data fetches. Thus incorporation of a substantial instruction 
buffer in each processor clearly reduces all interference by an order of magnitude, since of the four ways 
in which a storage interference can occur, only one-a data fetch 
conflicting with a 
data fetch-remains in the inner loop. Moreover, these 
measurements refer to a processor in which arithmetic speeds, as in Table 2, are of the order of magni- tude of a memory cycle time, which implies 
a somewhat powerful 
processor. Thus in every sense 
the interference figures are worst case results 
which, with the performance curves to which they relate, support the view that storage interference 
is not a serious obstacle to parallel processing. The four contours drawn 
on these curves 
represent lines of constant storage module-to-processor ratio. They slope slightly 
upward due 
to the 
statistical Marbles and Boxes [Rosenfeld, 19651 effect previously 
referred to. Figure 12 presents two sets of data, based on the five-equation line loop. The upper 
family of curves relates to storage utilization. 
The reservations made 
at the end 
of Sec. 7.3.2, with reference 
to the significance of utilization figures, also apply. The second family of curves represents a 
first attempt at 
estimating the relative quality of processing, that is, some function of a cost/performance 

Chapter 37 1 A survey of problems and preliminary results 
concerning parallel processing and 
parallel processors 469 factor. Such a 
factor is intuitive and environment-sensitive, de- pending on the relative concern for speed and for costs 
of various sorts. For the present data we 
have chosen to display a function: ™ = throughput time 
x total processor time where K is a constant, throughput time 
a measure of the speed of computation, and total processor time a measure of the cost. K 8. Conclusion I11 this paper we have 
presented some thoughts on parallel process- ing. In particular we have chosen 
to survey the topic by including 
an extensive bibliography and some of the results of our work in this area. The discussion has had to be brief, but our intention has been to convey the picture of the potential that parallel processing systems 
offer for the future development 
of computing. The key to successful exploitation lies in a 
new, unified, and scientific approach to the entire problem of the design and usage of computing systems. The development of large, integrated sys- tems raises many problems, 
but there can 
be no doubt 
that eco- nomic solutions to these will be found. Their development 
should comprise a significant 
part of the computer system architectural design effort 
of the next few years. 
Any ultimate evaluation of a parallel processing system 
within a working environment depends 
on actual operating 
experience. This in turn requires the existence of a system 
and the interest of users. Only 
when usable systems become available will the concept of parallel processing in 
integrated systems be accurately evaluated. References BlaaGM; BrigH64; ConwM63; 
CorbF65; DennJ66; DesmW64; DreyP58; 
FalkA64; GillS58; GregJ63; KatzJ66; 
LehmM65; LeinA5Q; McCuJ65; MiraW67; NievJ64; RoseJ65; SchlII??; ShedGGBa, b; SlotD62; SmitR64; 
PL/I Language Specification, FormC28-6571 Bibliography ,411eM63; AmdaC62; AndeJ63, 6.5; .%rdeB66; BaldF62; 
BIaa664; Brig•Ifi4: BuchW62; BussB63; CoddE62; ComfW65; ConwM63; CorbF62, 65; CritA6:); DaleRB5; DennJ65, 66; DesmW64: DijkE65; DreyPJX; ErnsH63; EstrGW, 63; EwinR64; FalkA64; ForgJ65; FranJ57; GillS58; 61asE65; 
GregJ63; HellH61, 66; KatzJ66; KinsH64; KnutD66; LehmM6Xa, 6311, 65; LeinA59; LourN59; MarcM63; McCaJ62; 
McCnJ65; MeadR63; MillW63; MiraW67; NievJ64; OssaJ65; PennJ62; 
RoseJ65; SchlH??; SeehRB3; SenzDB5; ShedC66a, 6611; SlotD62; SmitR64; SquiJ63; 
StraC59; VyssV65; WirtN66; IBM OS/.360 PL/Z Language Specijication, Form C 28-6571; Proc. IFIP1062. ﬁSymposium on Multi-Programmingﬂ 1963. 
Section 4 Network computers and computer 
networks The RW-400 and 
the CDC 6600 are actually computer networks 
by our definition of a computer (Chap. 2, page 17). Yet because of the restrictions on the quantity 
and location of the compo- nents in these structures, we still consider them to 
be com- puters. On the other hand, 
two or more computers which are 
separated physically, yet connected, constitute a computer network. Computer 
networks will appear in the future; it is important to 
understand the basis for them. The RW-400-a new polymorphic 
data system Chapter 38 presents the RW-400 (also called the AN/FSQ-27), a later version of the Ramo-Wooldridge RW-40 originally de- 
signed in 1959. The diagram (page 478) gives an indication of the relationship and names of the components. The PMS structure in Fig. 1 has more configuration 
details. At least six RW-400™s were built for military 
command and 
control applica- tions (although the number of 
computers of a type in existence has little to do with a machine™s worth 
or ability). The RW-40 ISP as given in Appendix 1 of Chap. 38 is a good example of a processor 
with a two-address instruction set. The ISP does not have index registers; 
it has a small state consisting of the accumulator (A), a limited extended accumu- lator (B), the program counter 
(P), and about 6 state bits. The Pc is limited by its ability to address directly only 
a 1,024-word Mp. The ISP is undoubtedly sufficient for 
solving the kinds 
of problems encountered 
by the computer and compares favorably 
with Whirlwind and the IBM 1800. The RW-40 introduced 
multiple parts for 
reliability [Roth- 
man, 19591. Multiple C™s (or Mp-Pc and Mp-Pio) are provided for redundancy and capacity. However, 
the S(™Centra1 Ex- change) which 
provides communication among 
the C™s may not have redundant parts. The multiple-computer concept can 
be viewed as the forerunner to our present computer networks, 
in which the central switching element 
is the Telephone Ex- 
change. Over a longer 
time span, the RW-400 may 
be most significant as a pioneer. However, 
the whole system, 
with the exception of 
the small Mp™s, 
is nicely designed. The 
problem of low speed 
T(typewriter, display)™s is handled well by trans- ferring data from Mp-Pc to Ms(drum) for concurrent and 
independent T and P activity. Similar solutions 
are common for managing T activity by using an M, local to particular T™s, and local C™s. The structure should be compared with the CDC 6600 (Chap. 39) and the network 
examples in Chap. 40. The CDC 6400, 6500, 6600, 6416, 
and 7600 The CDC 6600 development began in 1960, using high-speed transistors and 
discrete components of 
the second generation. The first 6600 was delivered in September, 1964. Subsequent compatible successors included the 6400, in April, 1966, which was implemented as a conventional Pc(a single shared 
arith- metic function unit instead of the 10 D™s); the 6500 
in October, 1967, which uses two 6400 Pc™s; and the 6416 in 1966, which has only peripheral and control 
processors. The 
first 7600, which is nearly compatible, was delivered in 1969. The dual 
processor 6700, consisting of two 
6600 Pc™s was 
introduced in October, 1969. Subsequent modifications to the series in 1969 included the extension to 20 peripheral and control processors with 24 channels. CDC also marketed a 6400 with 
a smaller number of peripheral and 
control processors (e.g., 
6415-7 with 7). 
Reducing the maximum PCP number to 7 also reduced 
the overall purchase 
cost by approximately $56,000 
per processor. The computer organization, technology, and construction are described 
in Chap. 39. ISP descriptions for both the Pc and Pc (‚Peripheral and Control 
Processors/PCP) are 
given in Ap- pendices 1 and 2 of Chap. 39. To obtain the very high logic speeds, the components are placed close together. The logic cards 
use a cordwood-type construction. The logic is direct-coupled transistor logic, with 5 nanoseconds propagation 
time and a clock of 
25 nano- seconds. The 
fundamental minor cycle is 100 nanoseconds and the major cycle is 1,000 nanoseconds, also 
the memory cycle time. Since the component 
density is high (about 
500,000 transistors in the 6600), the logic is cooled by conduction to a plate with 
Freon circulating through 
it. This series is interesting from many aspects. It has remained the fastest operational computer 
for many years. Its large 470 
Section 4 1 Network computers and computer networks 
471 Mp-Pc' Mp- Pi0 K-Ms drum; 0 - 17 rns; 1 ! [I6 ~s/w. 8192 w 1 K-T(I .. ines, cards, paper 
tape)- K-T ( ' Mas ter Con so I e) - .. .. .-- 7 M 'Peripheral Buffer: drum: w 1 S- M 'Display Ruffer: drum; [8l9? w ] Pc(2 address/instruction; Wps (- 34)! technoloqy:transistor: descendants:RW-400, AN/FSQ 27) Wp(core1 10 us/w: 1024 w; (26,2 parity) b/w) K('Periphera1 Buffer) K('Disp1ay Ruffer) Fig. 1. RW-40 (Polymorphic) PMS diagram. component count almost implies it cannot exist as an opera- PMS structure tional entity. 
Thus it is a 
tribute to an organization, and the project leader-designer Seymour 
Cray, that a large 
number exist. There are 
sufficiently high data bandwidths within the system so that it remains balanced for most 
job mixes (an uncommon feature in large C's). It has high performance Ms.disks and T.displays to avoid bottlenecks. The Pc's 
ISP is a nice variation of the 
general-registers processor and allows for very efficient encoding of programs. The 
Pc is nicely 
multi- programmed and can be 
switched from job to job more quickly than any other 
computer. Ten smaller C's control the main Pc and allow it to spend time on useful (billable) work rather than its own administration. The independent multiple data operators in the 6600 increase the speed by at least 2y2 times over a 6400 which has a shared D. Finally, it realizes the 10 C's in a unique, interesting, 
and efficient manner. Not many com- puter systems can claim half as many innovations. A simplified PMS structure of the C('6400, '6600) is given in Fig. 2. Here we see the C(io; # 1:lO) each of which can access the central 
computer (Cc) primary memory 
(Mp). Figure 2 shows CC Fig. 2. CDC 6600 PMS diagram (simplified). 
472 Part 5 I The PMS level Section 4 I Network computers and computer networks 
why we consider the 6600 to be fundamentally a network. Each Cio (actually a general-purpose, 12-bit C) can easily serve 
the specialized Pi0 
function for 
Cc. The Mp of 
Cc is an Ms 
for a Cio, of course. By having a powerful Cio, more complex input-output tasks can 
be handled without Cc intervention. These tasks can include data-type conversion, error 
recovery, etc. The 
K™s which are connected to a Cio can also be 
less complex. Figure 2 
has about the same information as Thorton™s Fig. 1 block diagram (Chap. 39). A detailed 
PMS diagram for 
the C(™6400, ‚6416, ‚6500, 
and ‚6600) is given in Fig. 3. 
The interesting structural 
aspects can be seen from this diagram. The four configurations, 6400 
- 6600, are included just 
by considering the pertinent parts of 
the structure. That is, a 6416 has no large Pc; a 6400 has a 
sin- gle straightforward Pc; a 6500 has two Pc™s; and the 
6600 has a single powerful 
Pc. The 6600 Pc has 10 D™s, so that several parts of 
a single instruction stream 
can be interpreted in paral- lel. A 6600 Pc also has considerable M.buffer 
to hold instruc- tions so that Pc need not wait for Mp fetches. The implementation of 
the 10 Cio™s can be seen from the PMS diagram (Fig. 3). Here, only one physical processor 
is used on a time-shared basis. Each 0.1 ps a new logical P is processed by the physical P. The 10 Mp™s are phased so that a new access 
occurs each 0.1 ps. The 10 Mp™s are always busy. Thus the i.rate is 10 x 12 b/ps or 120 megabits/s. This process of 
shifting a new Pc state into position each 0.1 ps has been likened to a barrel by CDC. A diagram of the 
process is shown in Fig. 4. The T™s, K™s, and M™s 
are not given, although it should be mentioned that the 
following units are rather unique: a K for the management of 
64 telegraph lines 
to be connected to a Cio; an Ms(disk) with four 
simultaneous access ports, each at 1.68 megacharls data transfer rate, and a capacity 
of 168 megachar; an Ms(magnetic 
tape) with a K( # 1:4) and S to allow simultaneous transfers to 4 Ms; 
the T (display) 
for monitoring the system™s operation; 
K™s to other C™s and Ms™s; and con- ventional T(card reader, 
punch, line printer, 
etc.). ISP The ISP description of the Pc is given in Appendix 1, Chap. 39. The Pc has a 
very clean, straightforward scientific-calculation- oriented ISP. We can consider 
it a variation on the 
general- register structure because the Pc state has three sets of 
general registers. Their 
use is explained both in Chap. 39 and its 
Ap- pendix 1. This structure assumes that a program consists of several read accesses to a large array(s), a 
large number of 
operations on these accessed elements, followed by occasional write accesses to store results. 
We would agree that this is a valid assumption for 
scientific programs (e.g., look at a FOR- TRAN arithmetic statement), and it is probably valid 
for most 
other programs 
as well. Cc has provisions for multiprogramming in the form of a 
protection and 
relocation address. The 
mapping is given in the ISP description for both Mp and Ms(™Extended Core Storage- / ECS). Appendix 2, Chap. 39, has an ISP description of the PCP. Appendix 2 includes a figure which shows the instruction de- coding and 
execution as well. The 
6600 PCP is about the same as the early CDC 160. The PCP has an 18-bit A register 
because it has to process addresses for the 
large Cc. One interesting aspect of 
the 6600 which we question is the lack of communication 
among all components at the 
ISP (pro- gramming) level. When 
Pc stops, it has no way of explicitly informing any other components. 
There are 
no interprocessor interrupts. An io device cannot interrupt 
a Pio, nor can Pio™s communicate with 
one another except by 
polling. The state switching for Pc is, however, elegant, since a Pi0 
can request Pc to stop a job, 
store Mps, and resume a new task in one instruction. (The t.save 
+ t.restore - 2 ps.) The operating system The Cio™s 
functions are data transmission between a 
peripheral device and the large Cc via the Cio™s Mp with 
some data trans- formation or conversions: complete task management, 
includ- ing initiation, termination, and 
error handling; 
and manage- ment of 
Pc. The Cio™s perform in about the same manner as the C(™Attached Support Processor) in the N(™360 ASP) (Chap. 40, page 506). The operating-system software 
is managed by a single fixed 
Cio. The remaining nine Cio™s are free, 
and as io tasks arise in the system, the Cio™s assign themselves to particular tasks, carry 
out the 
tasks, and then 
free themselves to take on other tasks. 
The operating-system software 
resides in Mp(Pc) (that is, Cc) accessible to all Cio™s and includes: 1 The variables 
which determine the state of 
a particular job, e.g., data pointers 
to Ms(disk, ‚ECS), running time, a list of jobs 
to do, etc. 
2 Programs for the 
Cio™s a Parts of the operating system used by 
the Cio re- sponsible for the 
system management b IO management programs (or programs to get the task management program 
from Ms) which the Cio™s use 
Section 4 1 Network computers and computer networks 
473 M('Barre1; working; IO w; 51 b/w; 0.1 ps/w) Mp(#O:Y)'- S"-Pc3 (bO:9)-Stm-S #1.12 T('Dead Start Console)- I K-L(l vs/w; I2 b/w)- 
i v 41 [fixedjrK-STT(bl :2; CRT; 
display)- Mp4 (#0:31)-S6- ll LT (key board) - 'Read Pyramid; 
buffer: 12 b/w: M(workinq: (1+2+3+4+5): 12 b/w): !\ .2 p/w) 77 S(4 K: I6 Ms)-Ms" (#0:15) 1 CB L(#2,3,4: to:'Extended Core Coupler) 
J c9 'Mp(core; 1.0 ps/w; 4096 w; 12 b/w) ZS(time multiplex: 
.I ps/w; 12 b/w) 3Pc('Peripheral and Control Processor; #0:9; time multiplex:.l p5/w: 1 address/instruction: 12 b/w; MpsC'Program Counter, Accumulator) 1 ,2 wlinstruction) 4Mp(core: 1 .O ps/w; 4096 w: (5 x 12) b/w) 'S(time multiplex: 0.1 ps/w: 60 b/w) 'Ms('Extended Core Storage/ECS; 
3.2 ps/w; (125952 / 8) w: (8 x (60, I parity)) b/w) 7See Chapter 39 for operation. *Only present in CDC 6500 'No C('Centra1)in CDC 6416: CDC h500 and CDC 
6400 do not have K('Scoreboard), separate D's, and M('lnstruction Stack). Pc('6600; 15, 30 b/instruction: techno1ogy:transistor: 
- 1964: data: si,bv,w,sf,d D('Shift) D('Boo1ean) D(#I: 2: ~lncrement) D ( 'Branch) D('Add; 0.3 ps) O('Long Add) D(#1:2: Multiply; 1 ps) D('Divide: 2.9 ~s) Ips (f 1 ip flop: -16 w)-S('Swi tchboard) i I I M .worki ng .- .- Fig. 3. CDC 6400, 6416, 6500, and 6600 PMS diagram. 
474 Part 5 1 The PMS level I2 3 4 5 6 7 IO II 12 Section 4 I Network computers and computer networks 
0 I2 3 4 5 6 CENTRAL MEMORY (60) 71011121314 10 MEMORIES, 4096 WORDS EACH, 12-BIT 
ttl 1121 1 L 1121 * 1 REAL TIME # ,121 EXTERNAL EWIPMENT 
Fig. 4. CDC 6600 peripheral and control processors. (Courtesy 
of Control Data Corporation.) 
Section 4 I Network computers and computer networks 
475 yps flip flop: 27.5 ns/w: 
-S D('Long Add) D(' Increment) D ( Pop" I at i on Count) D('Boolean) - 11; 16 w; 60 b/w -S K M.workinq: instruction D ( 'Shift) -c 12 w: 60 b/w 1- - - 
- - - - - - - - -1 interpreter D('Normalize) M 'Instruction Stack: D('Floatinq Add) flip floD: 27.5 ns/w; D('Floatinq Multiply) I D ( ' Float i nq Divide) In a typical system, one 
might expect to find the following assignment of PCP's to be: 1 Operating-system execution, including scheduling and 
management of Cc and all Cio's Display of job status data 
on T(display) 2 3 Ms(disk) transfer management 
4 5 L( # 1:3; to:C.satellite) 6 Ms(magnetic tape) 7 T(64 Teletypes) 8 9 Free 10 Free T(printers, card 
reader, card punch) Free to be used 
with Ms(disk) and Ms(magnetic tape) CDC 7600 The CDC 7600 system is an upward compatible member of the CDC 6000 series. Although the main 
Pc in the 7600 is compati- ble with the main 
Pc of the 6600, instructions have been 
added for controlling the io section and for communicating between Large Core Memories/LCM and Small Core Memory/SCM. 
It is expected to compute at an 
average rate of four 
to six times a C('6600). 
The PMS structure (Fig. 5) is substantially different from that of the 6600. The C('7600 Peripheral Processing 
UnitIPPU), unlike the C(l6600 Peripheral and Control 
Processor)'s, has a loose coupling with 
the main 
C. The PPU's 
are under control of the main C when transferring words into SCM via K('Input- Output Section). The 15 C('PPU)'s have 
8 input/output chan- nels. These channels, which can 
run concurrently, provide 
the link between C('PPU) and peripheral Ms's and 
T's. Some of the PPU's are located 
in the same physical space as the Pc. Ms(#0:7)' Mp(#0:31)"-S3TFJc5 S -t(M.buffer: core to core transfers1 TI Basic N('CDC 7600) Fig. 5. CDC 7600 computer PMS diagram. 
476 Part 5 1 The PMS level The 7600 Pc can be interrupted by a clock, 
the PPU™s, and trap condition within 
the Pc. A breakpoint address, BPA, can be set up within Pc such that, on the 
program reaching 
BPA, a trap is initiated. This interruption scheme is in contrast to that of the 6600, which could not be interrupted or trapped. 
The 7600 interrupt may be a 
reaction to the lack of intercom- munication in the 6600. Conclusions Although the 6600 was somewhat behind its announced delivery schedule and represented a significant drain on the 
financial resources of CDC, it is now clear 
that it is a successful 
product. Section 4 1 Network computers and computer networks 
There have 
been instances of very large computers 
not being carried to completion either for financial 
or technical 
reasons. The 6600 seems to be the first large computer to achieve these marks of success. Here we are interested in the 6600 because it has held the ﬁworld™s largest computerﬂ title for so long. Computer-network examples 
In Chap. 40, we present examples of seven computer networks. 
There is a dearth of both computer networks and of papers on computer networks. 
This chapter takes 
examples from papers and from knowl- edge of several existing or 
proposed networks. 
Chapter 38 The RW-400-a 
new polymorphic data system1 
R. E. Porter Summary The RW-400 Data System, based 
upon modularly 
constructed, independently operating and 
flexibly connected components, 
is the logically evolved snccessor to conventional computer 
designs. It provides the means by which information processing 
requirements can be met with 
equipment capable of producing timely 
results at a cost commensurate with problem 
economic value. System obsolescence is minimized by the expandahility in numbers and types 
of processing modules. 
Real time reliability is assured by component duplication 
at minimum cost 
and by the advanced design techniques employed in the system™s manufacture. Man-machine commu- nication facilities are 
program controlled for maximum 
flexibility. Parallel processing and parallel 
information handling modules increase the system™s speed and adaptability when handling complex computing workloads. This 
polymorphic design truly represents 
an extension of man™s intellect through electronics. The RW-400 Data System is a new design concept. It was devel- oped to meet the increasing demand for information processing equipment with adaptability, 
real-time reliability 
and power to cope with 
continuously-changing information 
handling require- 
ments. It is a polymorphic system including a variety of function- ally-independent modules. These are interconnectable through 
a program-controlled electronic switching center. Many pairs 
of modules may 
be independently connected, 
disconnected, and re- connected, in 
microseconds if need be, to meet continuously- varying processing requirements. The system can assume whatever configuration is needed to handle problems of the moment. Hence 
it is best characterized by the term ﬁpolymorphicﬂ-having many shapes. Rapid, program-controlled switching 
of many pairs 
of func- tionally-independent modules 
permits nondisruptive system ex- pandability, operating reliability, simultaneous multi-problem 
processing capability, and man-machine intercommunication feasibility. These are only partially found 
in computers 
of conven- tional design. Computer users have been 
forced heretofore 
to match problems to computer 
limitations. Problem changes posed 
serious reorien- tation and reprogramming difficulties. Changes from 
one computer ‚Datumnution, vol. 6, no. 1, pp. 8-14, January/Fehruary, 1960. to another model, due to 
growth in 
applications, often resulted 
in large 
expenditures of time and money. During maintenance 
or malfunction of a conventional computer its entire processing capacity is shut down. Real 
time processing reliability cannot be maintained on an around-the-clock basis. The conventional ma- 
chine must process its problems serially. This serious limitation is only partially alleviated 
by time-sharing or computing-ele- ment-doubling designs. The high cost-per-hour 
of conventional computer operation rules 
out direct 
man-machine intercommuni- 
cation during other 
than emergency situations. The radically-new polymorphic design concept of the RW-400 Data System was evolved 
by Ramo-Wooldridge engineers to pro- vide a practical solution to those information 
processing problems now inadequately handled 
by conventional computer designs. The RW-400 is a powerful new tool in the field of intellectronics-the extension of man™s intellect by electronics. System description The RW-400 Data System contains an optional 
number and variety of functionally-independent modules. These communicate 
via a 
central electronic 
switching exchange. 
Each module is designed, within practical 
economic and functional limits, 
to maximize system adaptability over a wide range 
of problem types 
and sizes. This new design embodies the latest proven 
electronic design techniques, assuring high processing speeds and high equipment reliability. The RW-400™s modularity assures reliable, round-the- 
clock processing 
of information with controllable computing ca- pacity degradation during module maintenance or malfunction. 
Practical man-machine intercommunication is achieved in 
the RW-400 system 
by use of program-controlled information display 
and interrogation consoles. Figure 1 shows the over-all system design. Modules 
of various types communicate through 
a central exchange switching 
center. Computing and buffering modules provide control 
for the system. These modules are self-controlled and make possible completely independent processing of two or more 
problems. One of the computer modules may be designated the master computer and 477 
478 Part 5 I The PMS 
level Section 4 I Network computers and computer networks 
DISPLAY CONTROLLING I COMPUTING BUFFERING I 4 SWITCHING CENTER I. I I- I I I AUXILIARY STORAGE INPUT OUTPUT Fig. 1. The RW-400 data system. in this role 
initiates and monitors actions of the entire system. An alert-interrupt network is provided to allow coordinated system action. Therefore, the system as applied to given information processing problems may change on a short range (microsecond) basis, thus providing, through programming, a self-organizing aspect to the system. In addition, the system may change through 
the years as the applications change. The most efficient 
and eco- 
nomical complement 
of equipment is applied to the problem at all times. An RW-400 system is built around an expandable Central Exchange (CX) to which a number 
of primary modules may be attached. These are: Computer Modules (CM); self-instructed Buffer Modules (BM); 
Magnetic Tape 
Modules (TM); Magnetic Drum Modules (DM); Peripheral 
Buffer Modules (PB); and console communication Display Buffer Modules (DB). 
How many 
modules are put together in a system is entirely a function 
of system application. In addition to primary system modules, 
punched card, punched tape, 
high speed printing and control console devices 
are available. These handle 
nominal system 
in- put/output requirements. Additional man-machine communica- 
tion devices such 
as interrogation, display and control consoles, may be included in the system as problem requirements dictate. A Tape Adapter (TA) module 
is available to provide compatibility 
with magnetic tape of other computers. Information 
generated at Flexowriter inquiry and recording 
stations may 
be directly re- 
ceived by 
the system via 
the Peripheral Buffer Module. This 
latter module also buffers 
the receipt of TWX and punched tape infor- mation. The way in 
which a particular RW-400 Data System functions depends on the number and 
type of each module included. It may initially be composed of the minimum number and variety of modules needed to do a small problem 
or the initial part of some large but yet-to-be-defined problem. Such a system would work 
much like a conventional computer. 
It would probably include 
a buffer module and thus have a parallel data handling capability not found 
in the conventional design at a comparable price. The initial system installation may then be augmented by the timely addition of modules. 
Chapter 38 1 The RW-400-a new polymorphic data system 479 A buffer module 
(BM) has the capability to control its acquisi- 
tion and dissemination of information independently. The buffer provides a 
computer module with parallel data handling capability 
without complicating 
the problem processing program with the conventional intermixture 
of arithmetic and housekeeping in- 
structions. Information 
previously generated by the processing program may 
be appropriately disposed of within the system while processing continues. Data needed at a subsequent time in 
the processing may be retrieved from system storage in advance 
of need while processing progresses. 
The simultaneity of these oper- 
ations not only materially 
increases over-all processing 
speed but also increases 
the practical utility 
of the less costly types of in- ternal system storage such 
as a magnetic tape. The computer (CM) or buffer (BM) modules, 
when acting in 
a controlling capacity, may initiate connection to an 
information storage or handling module during that part 
of the processing program when the two can 
work profitably 
in unison. The pair of modules thus interconnected neither 
affect nor are affected by 
other modules. Logical interlocks 
prevent unwanted cross talk among modules. An intermodule communication system lets con- 
trolling modules 
signal status or 
alert other 
such modules 
of their need to 
communicate. The decision by a module receiving 
an alert signal to permit interruption 
or to proceed is optional with 
that module. The optional interrupt feature 
is that needed to 
make the often-discussed but seldom-used program interrupt capability both 
useful and practical. Programs may thus 
permit interruptions only at convenient points 
in the processing sequence. Modules may 
be assigned, under program control, 
to work together on a 
problem in proportion to its needs. 
As soon as a module™s function is complete for a given 
problem, that module may be released for reassignment to some other task. The system is thus self-controlled to match processing capacity to each 
prob- lem for the time necessary to do the 
job. Full system capacity may be brought to bear upon a very 
large problem 
when needed. This capacity may be apportioned among a number of smaller problems 
for simultaneous processing, program compilation, program 
checkout, module 
maintenance etc., 
when it is not needed 
for maximum system effort. 
From the preceding system description, it is apparent that 
such equipment can be expanded from a 
modest initial installation 
into a very powerful 
and comprehensive information 
processing cen- ter as requirements warrant. 
More specific descriptions of prin- cipal system modules follow 
to give the reader a better feel for how this system 
might perform 
his information processing work. The functional modules The key to appreciative understanding 
of the power of the RW-400 lies in knowledge of intermodule connection. It is appropriate to describe the Central Exchange (CX) unit first, then follow with descriptions of the various modules. 
The central exchange The Central Exchange performs the vital function 
of intercon- necting a pair of modules whenever requested 
to do so by either a computer or a buffer module. Since internal programmed control 
is only possible within a computer or a buffer 
module, one of the interconnected pair 
of modules must be either a computer or a buffer. The time in which any connection may 
be made or broken is about 65 microseconds. An exchange has 
basic capacity to connect any 
of 16 computer 
or buffer modules 
to any of 64 auxili- ary function modules. There is nothing sacred 
about the number 16 since it is possible to extend the CX module™s interconnection matrix through 
design modification 
when need arises. The CX is an expandable, program-controlled, 
electronic switching center capable of connecting or disconnecting 
any available pair of modules in roughly the time of one computer 
instruction execu- tion. Figure 2 illustrates the permissible module interconnections 
within the Central Exchange. Every intersection 
on the illustration represents 
a possible 
connection between modules. The ﬁx-edﬂ intersections indicate typical connections in force at any point in time. The control logic of the CX module™s connection table prevents more 
than one interconnection on any horizontal (controlling) 
or vertical (con- 
trolled) data path representation 
on the diagram. When connec- tion is requested of the Central Exchange while 
one of the re- quired modules is already carrying out a previous assignment, 
the requesting module 
can be programmed to sense this condition 
and wait until 
connection can be 
made without interference. Should waiting be undesirable, the requesting module 
can go on 
about its business and check back 
later to see when the desired connec- 
tion can be made. There is an implication here, of course, that knowing the kind of a system 
he is dealing with, 
a programmer requests connections 
in advance 
of need whenever possible. Provision for master-slave control is included via an Assignment Matrix established within 
the CX module by 
a computer module previously assigned 
to master status. 
Such a provision 
is necessary to preclude inadvertent 
connection requests 
from unchecked programs or malfunctioning control modules from affecting sets 
of modules simultaneously 
processing another problem. Connection 
requests are therefore essentially filtered 
through both 
an assign- ment and 
an interconnection 
validity matrix prior 
to being acted 

480 Part 5 I The PMS level Section 4 1 Network computers 
and computer networks 
TM Fig. 2. The Central Exchange connection 
matrix. upon by the Central Exchange. The computer module manually 
assigned to master status is the only one permitted to cause the interconnection of a pair of modules which does not include itself. The computer module (See 
Fig. 3) The Computer Module (CM) is a self-sufficient, general purpose, 
two-address, parallel word, fixed point, random access computer. Its internal magnetic core 
memory has a capacity of 1024 words. 
A computer word consists 
of 26 information 
bits and 2 
parity bits. Each parity bit is associated with the 13-bit half word transferred in parallel 
via the Central Exchange to other system modules. 
The instruction repertoire of the CM consists of 38 primary instructions whose various modes effectively result 
in over 300 different oper- ations. Of the 39 available CM-400 instructions, 
24 may be classi- fied as ﬁarithmeticﬂ and 
10 as ﬁprogram controlﬂ 
or ﬁsequence determiningﬂ instructions. Five additional instructions may 
be classified as ﬁexternalﬂ or ﬁinput/outputﬂ instructions. All but three of the 24 arithmetic instructions fit into a symmetric scheme 
of classification wherein there are 
seven basic operations, each having three distinct modes. The seven basic 
operations are-add, subtract, absolute subtract, multiply, divide, 
square root and insert. The three modes are-Replace, Hold 
and Store. If we let 
the capital letter 
ﬁGﬂ identify the first operand, ﬁHﬂ identify the second operand, an 
ﬁ™ﬂ signify an 
arbitrary operation, 
the sym- bol ﬁ+ﬂ indicate replace, and ﬁAﬂ the word in the accumulator, then the three modes may be characterized as: Replace: H ™ G + H, A Hold: H 
G+ A Store: A G+ H, A The three 
remaining arithmetic operations are Add Accumulate wherein the contents of H and 
G are added to the 
Accumulator; 
Chapter 38 1 The RW-400-a new polymorphic data system 
481 Multiply Accumulate wherein 
the contents of H are multiplied by G and added to A; and Transmit where 
the contents 
of G are stored in H. The ten program control instructions are Store, Store 
Double Length Accumulator, Load Accumulator, Insert Mask in the S Register, Stop, Link 
Jump, Compare Jump, Tally Jump, Test Jump and a Multi-purpose 
Shift. The five external instructions 
are those which 
cause data to be transmitted to or received from a device 
external to the com- puter. Each command 
is multi-purpose in nature and hence equiv- alent to several conventional external instructions. 
The commands are-Command Output, Data Input, Conditional Data Input, Data Output and Character 
Transfer. A comprehensive discussion 
of the variation of each of these commands is not pertinent 
to this article. 
* Suffice it to say that commands are available for carrying out a wide variety of intermodule data communication. The interrupt capability of a Computer Module is a logical generalization of the ﬁtrappingﬂ feature 
found on several conven- 
tional computers. It permits the automatic interruption 
of a pro- gram, at the option of the program, when the computer 
module receives an ﬁalertﬂ 
that a condition requiring 
attention has arisen. 
It can be used to warn the program when an error of some type has occurred, minimize unproductive computer waiting time while another module completes its task, eliminate many programmed status test instructions 
and provide a convenient 
means of sub- jecting one computer module to the 
control of another. Program control of interruptions within a 
CM-400 is accomplished through 
the sense register S. This register may 
be filled with an interrupt 
J I CONTROL LOGIC 1 L OP ADDRESS ADDRESS 
INSTRUCTION REGISTER INPUT LINES b MAGNETIC CORE STORAGE CENTRAL EXCHANGE I - rJtl OUTPUT LINES TXCHANGE RFGISTFR c. L- CONTROL PANEL 
INTERRUPT SENSING ACCUMULATOR 
REGISTER ACCUMULATOR EXTENSION I I ALERT CONDITIONS Fig. 3. The CM-400 Computer Module. 
482 Part 5 I The PMS level Section 4 1 Network computers and computer networks 
RW-400 analysis console. 
mask by means 
of the Insert S instruction. A bit by bit correspond- ence exists between the S register and the interrupt register and the interrupt 
register I to which the alert lines are connected. A Test Jump instruction can be used 
to examine the coincidence between these registers 
of an alert signal in a bit position corre- sponding to a one in the S register mask. If an alert is received by the computer during the execution of an instruction, control 
will be transferred to memory location 
ﬁ0ﬂ at the 
end of the instruction if, and only if, 
(a) the sense bit corresponding to the 
alert is a ﬁone,ﬂ (b) the 
master sense bit is a ﬁone,ﬂ 
and (c) 
the in- struction was not an 
ﬁInsert s.™™ The master sense 
bit in 
the S reg- ister may 
be programmed 
to permit the interrupt to take place 
according to the interrupt 
mask or to inhibit 
interrupt until the program can conveniently cope with it. 
All instructions being executed at the time 
an interrupt condition occurs are completed before the interruption is allowed to take place. Figure 3 schematically illustrates the Computer 
Module™s pri- mary registers 
and the interconnecting information paths. Typical two-address 
addition and subtraction times are ap- proximately 35 microseconds including memory access time. Mul- tiplication takes about 80 microseconds, and division and square 
root about 130 and 170 microseconds respectively. 
Before attempting to draw a 
comparison between a CM and a deluxe conventional computer the 
reader should bear in mind the trade offs in features 
versus cost; parallel 
processing versus 
sequential processing; independent information handling versus program complicating ﬁhousekeepingﬂ; and 
real time system reli- ability versus periodic inoperability. The only valid comparison 
is that between the RW-400 Data System and a conventional 
computer applied 
to the same task. 
The contribution to the RW-400 system made by 
the Buffer Modules can be 
better assessed by the reader after the following description has been considered. 
The buffer module A Buffer Module consists 
of two independent 
logical buffer 
units, each having 
1024 words of random access 
magnetic core 
storage and a number 
of internal registers used 
in performing its functions when in the self-controlling mode. A Buffer Module may be con- 
nected to a Computer Module so that the 
Buffer™s core storage is accessible to the computer 
as an extension of the computer™s own storage. A Buffer may also serve 
as an intermediary device 
between a computer and another 
module, such 
as a tape or drum, to minimize time conventionally lost 
in data transfers. The Buffer is capable of recognizing and executing 
certain instructions stored 
in its own memory. It can 
therefore be left to perform data han- dling functions 
on its own 
while computer modules are otherwise occupied. A Buffer Module may be connected to a Computer Module and the 
buffer 1024 word 
storage used as an indirectly addressed extension of the computer™s own working storage. When the ad- dress 1023 (all ones) appears in the operand field of a computer instruction to be 
executed, the computer 
is signalled that the 
operand refers to 
some cell 
in buffer storage. 
The computer 
then uses the number in the buffer read register 
R (or in the case of a few instructions, the buffer write register W) as the effective address designated 
by the operand field of the instruction. Ex- tended addressing may be used in either the 
first or second operand 
field of the instruction or in both operand fields. If extended addressing is used in only one 
operand field, the effective address 
designated by 
that field is the number in register R. A ﬁ1ﬂ is automatically added 
to the 
contents of the R register after the instruction is executed. If extended addressing is used in both operand fields of an instruction, the effective address 
of the first operand is the number in register R and the effective address 
of the second operand is one more than the 
number in register R. A ﬁ2ﬂ is automatically added 
to the contents of register R after the execution of this type of instruction. The R (or W) register may be preset to any desired initial condition by 
means of the computer™s Command 
Output instruction. All the commands being executed by the computer must be stored within the computer 

Chapter 38 1 The RW-400-a new polymorphic data system 483 module™s storage and 
may not be in buffer cells addressed 
by the computer at execution time. The extended addressing and buffer register indexing 
may be used to materially simplify repetitive data 
acquisition operations. 
The primary function 
of a Buffer Module is not, however, that of an auxiliary 
computer storage unit. The 
drum and tape modules more aptly serve this 
function in the RW-400 system. A Buffer Module is capable of operating autonomously and of controlling other modules such as 
Tape Modules, Drum Modules, Peripheral Buffers, Display Buffers, Printers or Plotters. This capability en- 
ables the Buffer Modules in a system to perform routine tape searching and 
data transferral tasks thereby freeing the Computer Modules to do more computing. In its 
ﬁself-instructionﬂ mode, the buffer executes 
its own internally 
stored program in much 
the same fashion as a computer. The memory of a Buffer Module will therefore be occupied 
by its own control 
programs as well 
as blocks of data which it is holding for transmission 
to other units. The buffer is used to acquire information 
from the relatively slower auxiliary storage and communication modules while the computer 
proceeds at high speed. 
Blocks of information retrieved in 
advance of computer need by the buffer may then be rapidly 
transferred to the 
computer™s own storage or 
operated upon as they stand in the buffer via 
the indirect addressing capability of the computer. Another feature of the buffer is its switching capability. Each Buffer Module is composed of two buffer units tied together. A unit function switching 
feature permits the employment of the two units together in an alternating mode of operation. Continuous information transfer from tape to 
computer, for example, 
may be accomplished without stopping the tape 
unit. A switching in- struction executed simultaneously by 
both units of a Buffer Module causes whatever devices were connected to the first unit to be connected to the second and vice versa. 
Now that the functional controlling 
modules and the module interconnection concept have been discussed, the more conven- tional auxiliary storage modules 
available with the system may be described to round out the 
processing capability of the system. The tape modules A Tape Module 
consists of an altered Ampex FR-300 tape transport plus the necessary power supplies and control circuitry to 
effect information reading, writing and control. One inch mylar 
tape is used. Information is written on 16 channels-two of which are clock channels. 
The remaining 14 channels consist of 13 informa- tion bits plus parity. The information reading or recording rate is 15,000 computer words per second. Data may be recorded on tape in variable 
blocks up to 
a maximum of 1024 words per block (the size of the storage available to 
hold the data in a sending or receiving module). Each 
block is preceded by a block identi- fication which 
permits selective tape information searching by a 
Buffer Module. Single blocks 
imbedded in a 
tape file of other blocks can be overwritten. 
A two-stack head permits automatic verification of each block as it is written. Readback parity errors 
are automatically detected during the writing process. Thus drop- 
out areas may 
be determined while the data is still available in a computer or buffer for recording elsewhere. 
A description of the RW-400™s tape handling capability would not be complete without mentioning 
the Tape Adapter 
(TA) module. This is a self-contained unit capable 
of performing the reading and writing of magnetic tapes in a format acceptable to the IBM 704 and 709 systems. The TA consists of an Ampex FR-300 half-inch digital tape transport, including dual gap head and 
servo control system; reading, writing and control circuits; 
and a module housing with its own 
blower and power supply. RW-400 Buffer Module. 
484 Part 5 I The PMS level Section 4 I Network computers and computer networks 
The drum module The Drum Module (DM) contains a magnetic drum 
with storage capacity of 8192 words. It may be connected to either a Computer or a Buffer Module through the Central 
Exchange. Average access 
time to the first word position 
on the drum is 8y2 milliseconds. Successive words 
are transmitted 
at the rate of 60,000 computer words per second. The Drum Module is conventionally used 
as an intermediate item 
storage device to minimize tape handling time. Special system communication modules The external data and man-machine communication 
of the RW-400 Data System are handled via drum buffer modules. 
A wide variety of asynchronously operated equipment 
is speed matched and program controlled through 
the features designed into these special system communication modules. The Peripheral Buffer (PB) provides 
input/output buffers for 
communication between Computer 
or Buffer Modules and rela- tively slow 
speed external devices such 
as Flexowriters, Plotters, Punched Tape Handlers, Teletype 
Lines and Keyboard Operated Equipment. The Peripheral Buffer stores its information in four pairs of bands which operate alternately 
as circulating registers. Each band contains eight input and eight output buffers for 
a total of 32 input buffers and 32 output buffers in each Peripheral 
Buffer Module. Each buffer is a drum 
band sector 64 computer words long. Conventionally one 
input and 
one output buffer sector are connected to each external device (such as a Flexowriter) 
to permit two-way communication between the external device and 
the RW-400 system. The display buffer A Display Buffer (DB) acts as a recirculating storage 
for the cathode ray tube display units in 
a Display Console. 
Information to be displayed is sent to the DB band associated with a 
particular display tube via the Central 
Exchange. The Display Buffer sends only status information back to other system modules 
upon request. The information displayed on any tube is controlled by 
the bit pattern sent to the 
Display Buffer. The display pattern is regener- ated 30 times per second 
to minimize image fading and 
flicker. The preceding explanation 
of the Display Buffer has little meaning to a reader 
unfamiliar with the 
features of the Display Console 
itself. This console 
is therefore described in more detail in the following paragraphs. Display consoles Display Consoles 
can give a problem ﬁanalystﬂ or ﬁmonitorﬂ a 
visual picture of the status or results 
of any information being handled by 
the RW-400 system. In addition to the actual Cathode 
Ray Tube, numerical indicator, signal lamp and typewriter infor- mation outputs, several types 
of keyboard activated system control and parameter entry 
facilities are provided on the console. The total man-machine communication 
facility represented by each console is designed to be 
primarily a function of the computer control programs initiated by the analyst via 
his console. A set of Display Control Keys generate messages which 
are recorded on a Peripheral 
Buffer sector for later interpretation and 
display generation by 
a computer program. A set of Process Step Keys are provided the analyst so that he can initiate 
prepro- grammed system processing variations. Associated 
with the Process Step Keys is an overlay or ﬁprogram card™ which permits 
the assignment of a variety 
of meanings to the 
set of Process Step Keys. Insertion of the overlay by 
the analyst gives him a unique 
label for each Process Step Key and automatically 
cues the controlling computer to assign the corresponding set of programs to each key message. A Data Entry Keyboard is provided on the console so that the analyst can enter control parameters when asked to do so via the display devices. 
A Joystick Lever affords the console operator a 
means of con- trolling the position of cross hair markers on 
the cathode ray display tubes. Associated with the joystick are control keys which may be used to send a 
message to the controlling computer speci- fying the coordinates of the cross hairs. Control programs may be written, for example, 
to act upon this information 
to reorient the display with respect 
to the 
area selected 
by the cross hair position. A Light Gun 
is also provided as a means of selecting any point 
on the cathode ray tube displays. The gun emits a small beam of light. With the beam centered on a given point on the cathode ray display tube, pressing the trigger results in the automatic generation of a message to the Peripheral Buffer specifying the address in the Display Buffer containing the coordinates of the selected point. A set of Status and Error lights are contained on the Display Console to provide the console operator with 
over-all knowledge 
of the system and thus minimize 
conflicting control requests and intermodule interference. For example, a Peripheral 
Buffer may not be ready 
to accept a console key message 
until after certain 
previously requested control actions 
have been completed. 
The Status Lights indicate this condition to the console operator so that he may act accordingly. The printer module The Printer Module (PR) is basically a 160 column, 900 
line per minute Anelex type printer. 
It receives information from 
either a Computer or a Buffer module via 
the Central Exchange. Indi- 
Chapter 38 1 The RW-400-a new polymorphic data system 
485 vidual characters to be printed are 
represented by 
a 6-bit 
code and are transmitted 
four to a computer word. Zero suppression, 
line completion and 
information block 
end codes are included for format control. A plugboard is provided for flexibility in columnar data arrangement. Paper feed is controlled by 
means of a loop of 7-channel punched paper tape. 
Control of the printing operation has been arranged 
so that the 
connected control module may send line headings from one 
set of memory locations, stop sending information while going 
to a different part of the memory, and then proceed to send data from this 
new set of memory locations 
to complete a 
line of print. The punched card modules 
The RW-400 Data System may be equipped with a 
high speed punched card 
reading module (CR) and an 
IBM card punch. The CR communicates with 
Computer or Buffer modules via 
the Central Exchange. It is capable of reading 80 column 
punched cards at the rate 
of 2,500 cards 
per minute. The card 
punch is connected to the system through the Peripheral Buffer Module (PB) since 
it is a relatively low speed device. Emphasis has 
not been placed 
on directly connected punched card equipment 
since the sources of large volumes 
of punched cards usually convert this data into magnetic tape form which 
may be more rapidly handled 
using the Tape Adapter 
Module (TA). 
References RothS59; Westc6O 
486 Part 5 I The PMS level Section 4 1 Network computers and computer networks 
APPENDIX 1 RW 40 ISP DESCRIPTION Appendix I RW-40 I5P Description The descriotion does not include Innut-Output instructions, interrupts nnri communication iiith the other comuters or processors. The description was taken from the Preliminaru Yanual of 1n.formation on the RCI-40 and is vo doubt -hanged in final machines. Pc State k26 : I > B<26: I> AB[O:l]<26:1>:= AB P<IO:l> ov SR<20:1> Parity error 
Program error Run Mp State M[1:1022]<26:1> ?c Console State CJk8: I> Control,oanel ,tes t Fxternal State for IO and Other Computers Tape-read External Jddress/EA<lO: I> M EO: IO23 146: 1 > I &ond<l9: I> IO ,Se 1 ect 0 : 1 > lOJata<l3: I > Instruction Format instruction/iQ6:l> f/opd:l> := iQ6:21> g<IO:l> := iQO:ll> j6:l> := g6:1> h<10:1> := i<IO:l> Operand Calculation Process GQ6:1$ := (GI; next (g = 17778) iExternalJddress t ExternalJddress + I) G'Q6:1> := ((4 = 0) - 0; (0 < g < 1777) +M[qlQ6:l>; (g = 1777) - M[External,AddressIQ6:1>) HQ6:1> := (HI; next (h = 1777) -tExternal,Address +External-Address + I) H'(26:b := ((h = 0) - 0; (o<k 1777) - id[h]<26:1> (g = 1777) + H[External,Address]<26: I>) Arithmetic register extension to A Arithmetic repister (double) Proaram Counter Overflow ,for arithmetic shifts, +, -, and / Sense Register for M~ and transfer to other comuters undefined command or incorrect seouence of IO commands Mp repistern n and 708.7 are tnnccessihle conditional .im switches communication indicator tune search flag register associated ?e to address another 
module extra memow being accessed by Fxternal Address register interrunt conditions to Pc 1 of 8 IO devices can be selected IO device Drrta function or OD code bits first address test selection parameter second ac'dress 
first onerani' second operand 

Chapter 38 1 The RW-400-a new polymorphic data system 487 IrstmcLion Interpr,eta&icq ~'YCJC~~S Run-(instruction <-M[P]: P <- P + 1; next !etch 1nstruction"execution) eTecu :c Trst,ructior Set nn? Tnsiructior .?'kecu?iPr nroePss Instruct ion-execu t ion := ( Transmit A~;titmetic ills complemencl Replace Add Hold Add Store Add Replace Subtract Hold Subtract Store Subtract 
Replace Absolute Subtract Hold Absolute Subtract Store Absolute Subtract Replace Multiply Hold Multiply Store Multiply ReDlace Divide Hold Divide Store Divide Replace Square Root Hold Square Root Store Square Root Accumulate Add Accumulate Multioly (:= op = 27) -> (H +G): (:= op = 0) + (Ov,A +H + G: next H' ,-A); (:= op = I) + (Ov,A tH + G): (:= op = 2) + (Dv,A +A + G; next H' +A): (:= op = 3) + (Ov,A i-H - G: next H' .-A): (:= op = 4) -,(Ov,A +H - G): (:= op = 5) + (flv,A -A - G: next H' +A); (:= op = 6) ->(A +abs(H) - abs(G): next H' +A): (:= op = 7) ->(A ,abs(H) - abs(G)): (:= op = IO) ->(A -abs(A) - abs(G): next H' +A): (:= op = 11) + (AR <-H x G: next H' <-A): (:= op = 12) -> (A6 1. H x C): (:= op = 13) -) (AB <-A x G: next H' <-A): (:=op= 14) ->((HrG) ->Ov+I; (H < G) + ( A,E <-H/G: next H' +A)): (:= op = 15) -) ((H Z G) '0" - I; (H < G) --i (A,B tH/G)): (:= = 16) +((A c) -jnv + 1; (A<G) +( A,E (-A/G: next H' ,-A)): (:= op = 17) -)(A ,-sqrt(H+G): next H' t A): (:= op = 20) -,(A -sqrt(H+G)): (:= op = 21) .i (A .-sqrt(A+G): next 
H' <-A): (:= op = 25) --> (A .-OvoA + H + C); (:= op = 26) + (A i-flvOA + H x G): 
488 Part 5 I The PMS level Section 4 I Network computers and computer networks 
(g<10:7> # 0) - ( A - ((g<IO> + Isond; 7 g<IO> - 17??777?7) h (96™ + SR; 7 g8> + 177777777) A (g& + 10WelectolOdata; 7 s<b - I????????) A (gq™ + CJS; igc/> - 177777777)); next (gQ> @Test) - (P - h)); The Test condition is a selected bit of A, or other Pc or IO bits. Test := ((j = 0) -0; (i I j i 32) -A<j\.; (j = 33) - (OV; Ov -0); (j = 34) - (Parity error; Parity error -0); (j = 35) - (Control-panel test; Control,panel-test -0); (j = 36) -, (Tape-read; Tape-read + 0) ; (j = 37) - (Programgrror; Program-error + 0)) Link Jump (:= op = 32) - ((g # 0) + (P + h; G4O:l: + P); (g = 0) - (P +h)); Tally Jump (:= op = 33) - ((G = 70) - (P ch); (G = 0) - ; (G >O) -(GI -G - 1; P -h); (G < 0) - (G - G + 1)); Compare Jump (:= op = 37) - (A < G) + P + h; Load A (:= op = 34) - (A - Oogoh); Insert S (:= op = 35) - (S - (A A (OOgOh)) v (S A 7 (bgoh))); Store AB (:= op = 36) (G * 8; H -A; (g = 0) A (h = 0) + (A + 8; B +A)) ) end Instruction,executior 
Chapter 39 Parallel operation in the Control Data 
66001 James E. Thornton History In the summer of 1960, Control Data began a project 
which culminated October, 
1964 in the delivery of the first 6600 Com- puter. In 1960 it was apparent that brute 
force circuit perform- ance and parallel operation 
were the 
two main approaches to more critical 
system control operations in 
the separate 
processors. The central processor operates from the central 
memory with relocating register and file protection for each program in central memory. periDheral control processors any advanced computer. 
This paper presents some of the considerations having to do with the parallel operations in 
the 6600. A most important and fortunate event coincided with 
the beginning of the 6600 project. This was the appearance 
of the high-speed silicon transistor, which survived early difficulties to become the basis for 
a nice jump in circuit performance. 
System organization 
The computing system envisioned 
in that project, and now called the 6600, paid special 
attention to two kinds of use, the very large the large problem, a high-speed floating point 
central processor with access to a large central memory was obvious. Not 
so obvious, but important to the 
6600 system 
idea, was the isolation of this central arithmetic 
from any peripheral activity. 
It was from 
this general line 
of reasoning that the 
idea of a multiplicity of peripheral processors was 
formed (Fig. 1). Ten such peripheral processors have access to the central memory on one side and the peripheral channels 
on the other. The executive control of the system is always in one of these peripheral proces- sors, with the others operating on assigned peripheral or control tasks. All ten processors have access to twelve input-output chan- 
nels and may ﬁchange hands,ﬂ 
monitor channel activity, 
and perform other related 
jobs. These processors have access to central memory, and may pursue independent transfers to and from this 
memory. Each of the ten peripheral processors contains its own memory 
for program and buffer areas, thereby isolating 
and protecting the 
‚AFIPS Proc. FJCC, pt. 2 vol. 26, pp. 3340, 1964. scientific problem and the 
time sharing of smaller problems. 
For The peripheral and control processors are housed in one 
chassis of the main frame. Each 
processor contains 4096 memory 
words of 12 bits 
length. There are 12- and 24-bit instruction formats to provide for direct, indirect, and relative addressing. Instructions provide logical, addition, subtraction, 
shift, and conditional branching. Instructions 
also provide single word or block transfers 
to and from any 
of twelve peripheral channels, 
and single word 
or block transfers 
to and from central memory. Central memory words of 60 bits length 
are assembled from five consecutive pe- 
ripheral words. Each processor has 
instructions to interrupt the central processor and to monitor the central 
program address. 
To get this much processing 
power with 
reasonable economy 
and space, a time-sharing 
design was 
adopted (Fig. 2). This design 
contains a register 
ﬁbarrelﬂ around which 
is moving 
the dynamic information for all ten processors. Such things as 
program address, accumulator contents, 
and other 
pieces of information totalling 52 bits are 
shifted around the 
barrel. Each 
complete trip 
around requires one 
major cycle or one thousand 
nanoseconds. A ﬁslotﬂ in the barrel contains adders, 
assembly networks, distribution 
network, and interconnections to perform one step of any periph- 
eral instruction. 
The time to perform this 
step or, in other words, the time through the slot, is one minor cycle or 
one hundred nanoseconds. Each of the ten processors, therefore, is allowed one 
minor cycle of every ten to perform one 
of its steps. A peripheral instruction may require one 
or more 
of these steps, depending on the kind of instruction. In effect, the single arithmetic and the 
single distribution and assembly network are made to appear as ten. Only the memories are kept truly 
independent. Incidentally, the memory read-write 
cycle time 
is equal to one complete trip 
around the barrel, or one thousand nanoseconds. 489 
490 Part 5 1 The PMS level 4096 WORD CORE MEMORY - PERIPHERAL PROCESSOR a CONTROL Section 4 I Network computers 
and computer networks 
4096 WORD 4096 WORD 405'6 WORD COREMEMORY - - CORE MEMORY - CORE MEMORY PERIPH ERAL 
PER1 PH ERAL PERIPHERAL 
PROCESSOR PROCESSOR PROCESSOR 
a CONTROL a CONTROL a CONTROL c 4096 WORD CORE MEMORY * PERIPHERAL PROCESSOR a CONTROL CORE MEMORY UKl CENTRAL MEMORY * 4096 WORD COREMEMORY PERIPHERAL *n 6600 CENTRAL MEMORY 4 PROCESSOR 6600 CENTRAL PROCESSOR a CONTROL PHERIPHERAL a CONTROL I PROCESSOR CORE MEMORY CORE MEMORY a CONTROL PROCESSOR a CONTROL PROCESSOR 4096 WORD CORE MEMORY PERIPHERAL '?- PROCESSOR a CONTROL 4096 WORD PERIPHERAL PROCESSOR a CONTROL Fig. 1. Control Data 
6600. PROCESSOR PROCESSOR 
REGISTERS MEMORI ES 1 cc PROCESSOR TIME-SHARED 
PROCESSOR - REGISTERS INSTRUCTION MEMORI ES CONTROL t L CENTRAL MEMORY (60) WRITE PYRAMID CENTRAL MEMORY I- (60) o I 2 3 4 5 6 7 1011 12 1314 EXTERNAL EQUIPMENT I Fig. 2. 6600 peripheral and control processors. 
Chapter 39 I Parallel operation 
in the Control Data 
6600 491 Input-output channels are bi-directional, 12-bit paths. One 12-bit word may 
move in 
one direction every 
major cycle, or 
1000 nanoseconds, on each 
channel. Therefore, 
a maximum 
burst rate of 120 million bits per second 
is possible using all ten peripheral processors. A sustained rate of about 50 million bits per second can be maintained in a 
practical operating 
system. Each channel may service several 
peripheral devices and may interface 
to other systems, such as satellite computers. 
Peripheral and control processors access 
central memory through an 
assembly network and a dis-assembly 
network. Since five peripheral memory references 
are required to make up one central memory word, 
a natural assembly network of five levels is used. This allows five references to be ﬁnestedﬂ in each network 
during any 
major cycle. The central memory is organized in independent banks with the ability to transfer central words every 
minor cycle. 
The peripheral processors, therefore, introduce at 
most about 2% interference at 
the central memory address control. 
PERIPHERAL AND CONTROL PROCESSORS 12 INPUT OUTPUT CHANNELS UPPER BOUNDARY LOWER BOUNDARY A single real time clock, continuously running, 
is available to all peripheral processors. Central processor The 6600 central processor may be considered the high-speed arithmetic unit 
of the system (Fig. 3). Its program, operands, 
and results are held in the central memory. It has no connection to the peripheral processors except through memory and except for two single controls. These are the exchange jump, which 
starts or interrupts the central processor from a peripheral processor, and the 
central program address which 
can be monitored by a peripheral processor. A key description of the 6600 central processor, as you 
will see in 
later discussion, is ﬁparallel by function.ﬂ This means 
that a number of arithmetic functions may 
be performed concurrently. To this end, there are ten 
functional units within the central CENTRAL PROCESSOR 24 OPERATING Fig. 3. Block diagram of 6600. 
492 Part 5 I The PMS level Section 4 1 Network computers and computer networks 
processor. These are the two increment 
units, floating add unit, fixed add unit, shift unit, two 
multiply units, divide 
unit, boolean unit, and branch unit. 
In a general way, each of these units 
is a three address unit. As an example, the floating add unit obtains two 60-bit operands from the central registers and produces a 60-bit result which 
is returned to 
a register. Information 
to and from these units 
is held in 
the central registers, of which there are twenty-four. Eight 
of these are considered index registers, 
are of 18 bits length, 
and one of which always contains zero. Eight are considered address 
registers, are of 18 bits length, 
and serve to address the five read central 
memory trunks and the two 
store central memory trunks. 
Eight are considered floating point regis- ters, are of 60 bits 
length, and are 
the only central registers to access central memory during a central program. In a sense, just 
as the whole central processor is hidden behind 
central memory from the peripheral processors, so, too, the ten 
functional units 
are hidden behind 
the central registers from 
central memory. As a consequence, a considerable instruction 
efficiency is obtained and an interesting form of concurrency is feasible and practical. The fact that a small 
number of bits can give meaningful definition to any 
function makes it possible to develop forms of operand and unit reservations needed for a 
general scheme of concurrent arithmetic. 
Instructions are organized in two formats, a 15-bit 
format and a 30-bit format, 
and may be mixed in an instruction word (Fig. 
4). As an example, a 15-bit instruction may call 
for an ADD, f rn I h OPERATION CODE 60 BITS 0 RESULT REG (I of 8) 4 151 OPERAND REG. (I of 8) 2nd C _J RAND REG (I of 8) Fig. 4. Fifteen-bit instruction 
format. designated by 
the f and m octal digits, from registers designated 
by the i and k octal digits, 
the result going to the register desig- nated by the 
i octal digit. In this example, 
the addresses of the three-address, floating add unit are only three bits 
in length, each address referring 
to one of the eight floating point registers. The 30-bit format follows this same 
form but substitutes for the k octal digit an %bit constant K which serves as one of the input 
oper- ands. These two 
formats provide 
a highly 
efficient control of concurrent operations. As a background, consider 
the essential difference between a general purpose device 
and a special device 
in which high speeds 
are required. The desiper of the special device 
can generally improve on the traditional general purpose device by 
introducing some form of concurrency. For example, 
some activities of a housekeeping nature may be performed separate from the main sequence of operations in separate hardware. 
The total time 
to complete a job 
is then optimized to 
the main sequence and excludes the housekeeping. The two 
categories operate concurrently. It would be, 
of course, most attractive to provide in a general purpose device 
some generalized scheme to 
do the 
same kind 
of thing. The organization of the 6600 central processor provides just 
this kind of scheme. With a multiplicity of functional units, 
and of operand registers and with a simple and highly efficient address- ing system, a 
generalized queue and reservation scheme 
is practi- cal. This 
is called the scoreboard. The scoreboard maintains 
a running file of each central 
register, of each functional unit, and of each of the three operand 
trunks to and from each unit. Typically, the scoreboard file is made up of two-, three-, and four-bit quantities identifying the nature of register and unit usage. As each new instruction is brought up, 
the conditions at the instant of issuance are set into the scoreboard. A snapshot is taken, so to speak, of the pertinent conditions. If no waiting 
is required, the execution of the instruction is begun immediately under 
control of the unit itself. If waiting is required (for example, an input operand may not yet 
be available in the central registers), the scoreboard controls 
the delay, and when released, allows the unit to begin its execution. 
Most important, this activity is accomplished in the scoreboard and the functional unit, and does not necessarily limit later instructions from being 
brought up and issued. In this 
manner, it 
is possible to issue a series 
of instructions, some related, some not, until no 
functional units are 
left free or 
until a specific register ib to be assigned more than one result. With just- those two restrictions on issuing (unit free and no double result), several 
independent chains of instructions may 
proceed concurrently. Instructions may 
issue every minor cycle 
in the 
Chapter 39 I Parallel operation 
in the Control Data 
6600 493 absence of the two restraints. The instruction executions, in com- parison, range 
from three minor cycles for 
fixed add, 10 minor cycles for floating multiply, to 
29 minor cycles for floating divide. To provide a relatively continuous source 
of instructions, one buffer register of 60 bits 
is located at the bottom of an instruction stack capable 
of holding 32 instructions (Fig. 
5). Instruction words from memory enter the 
bottom register of the stack pushing 
up the old instruction words. In straight line 
programs, only the bottom two 
registers are in use, 
the bottom being 
refilled as quickly as memory conflicts allow. 
In programs which 
branch back to an instruction in the upper stack registers, no 
refills are allowed after 
the branch, thereby 
holding the program loop completely 
in the stack. As a result, memory access 
or memory 
conflicts are no longer involved, and a considerable speed increase can be had. Five memory trunks 
are provided from memory 
into the central processor to five of the floating point registers (Fig. 6). One address register is assigned to each trunk (and therefore to the 
floating point register). Any instruction calling 
for address register result 
implicitly initiates 
a memory 
reference on that trunk. These in- 
structions are handled through 
the scoreboard and therefore tend to overlap memory access 
with arithmetic. For 
example, a new memory word 
to be loaded in a floating 
point register 
can be brought in from memory but may not 
enter the 
register until all 
previous uses of that register are completed. The central registers, therefore, provide 
all of the data 
to the ten functional units, and receive all of the unit results. No storage is maintained in 
any unit. Central memory is organized in 32 banks of 4096 words. 
Con- secutive addresses call for a different bank; therefore, adjacent addresses in one 
bank are in reality separated by 32. Addresses may be issued every 100 nanoseconds. 
A typical central 
memory information transfer 
rate is about 250 million bits per second. As mentioned before, the functional units 
are hidden behind 
the registers. Although 
the units might 
appear to increase hard- ware duplication, a pleasant fact emerges 
from this design. Each unit may be trimmed to 
perform its 
function without regard 
to others. Speed increases 
are had from this simplified design. 
As an example of special functional 
unit design, the floating multiply accomplishes 
the coefficient multiplication in nine minor 
cycles plus 
one minor cycle 
to put away the result for a total of 10 minor cycles, or 1000 nanoseconds. 
The multiply uses layers of carry save adders grouped 
in two 
halves. Each half concurrently forms a 
partial product, 
and the two partial products 
finally merge while the long carries 
propagate. Although this is a fairly 
large complex of circuits, the resulting device 
was sufficiently smaller than originally planned to allow two multiply units 
to be included in the final design. INSTRUCTION STACK 8 60417 WORDS I BUFFER REGISTER I FROM CENTRAL 
MEMORY ™ I 4 Fig. 5. 6600 instruction stack operation. 
494 Part 5 I The PMS level Section 4 1 Network computers and computer networks 
OPERANDS (60-BlT) (UP TO 8 WORDS Fig. 6. Central processor operating registers. To sum up the characteristics of the central 
processor, remem- ber that the 
broadbrush description 
is ﬁconcurrent operation.ﬂ 
In other words, any program 
operating within the central processor utilizes some 
of the available concurrency. The program need 
not be written in 
a particular way, although centainly 
some optimiza- tion can be done. The specific method of accomplishing this 
concurrency involves issuing as many instructions 
as possible 
while handling most of the conflicts during execution. Some of the essen- tial requirements for such a 
scheme include: 
1 Many functional units 2 Units with three address properties 3 Many transient registers with many trunks to and from the units 4 A simple and efficient instruction set Construction Circuits in the 6600 computing system use all-transistor 
logic (Fig. 7). The silicon transistor 
operates in saturation when switched 
ﬁonﬂ and averages about five nanoseconds of stage delay. Logic 
circuits are constructed in 
a cordwood plug-in module 
of about 2y2 inches by 
21/, inches by 0.8 inch. An average of about 50 transistors are contained in these 
modules. Memory circuits are constructed in 
a plug-in module 
of about six inches by six inches by 2% inches (Fig. 
8). Each memory module 
contains a coincident current memory of 4096 12-bit words. All read-write drive circuits and 
bit drive circuits 
plus address trans- 
lation are contained in the module. One such module 
is used for each peripheral processor, and five modules make 
up one bank of central memory. Logic modules 
and memory modules are held in upright hinged 
chassis in an X shaped cabinet (Fig. 9). Interconnections between modules on the chassis are made with 
twisted pair 
transmission 
Chapter 39 I Parallel operation in the Control Data 
6600 495 Fig. 7. 6600 printed circuit 
module. lines. Interconnections hetween chassis are made with coaxial cables. Both maintenance and operation are 
accomplished at a pro- 
grammed display console 
(Fig. 10). More than one of these consoles may be included in a system 
if desired. Dead start facilities bring .' Fig. 8. 6600 memory module. Fig. 9. 6600 main frame section. Fig. 10. 6600 display console. 
496 Part 5 I The PMS level Section 4 1 Network computers and computer networks 
the ten peripheral processors to a condition 
which allows infor- 
mation to enter from any chosen peripheral device. Such loads 
normally bring in an operating system which provides a highly sophisticated capability 
for multiple 
users, maintenance, and 
so on. The 6600 Computer has taken advantage 
of certain technology advances, but more particularly, logic organization advances 
which now appear to be 
quite successful. Control Data is exploring advances in technology upward within the same compatible structure, and identical technology downward, also within the same compatible 
structure. References AllaRM; ClayB64 
Chapter 39 I Parallel operation in the Control Data 
6600 497 APPENDIX 1 CENTRAL PROCESSOR ISP DESCRIPTION 
CDC 6400, 6500, 6600 Appendix I COC 6400, 6500, 
6600 Central Processor ISP Description Pc State P<17:0> x[0:7]<59:0> A[O:7]il7 :0> B[Ol<l7:0> := 0 E[ I : 7]<17 : O> Run EM4 7 : O> Address gut,of,range,rnode := EM<I 2> Operandgutaf ,rangeurnode := EM<13> lndef ini teaperandurnode := EM<14> The above description is incomplete in tha an alarm condition occurs "and" the mode is a one, the above Mp State MP [O :7777778 169:O> Ms [0 :2015232 169 :0> RA<I 7 : O> FL<I 7:0> RAECSB9:36> FLECK59 : 36> Addressaut df -range Memoru Mannina Process Program counter Main arithmetic registers. XL1:5], are implicitly loaded from X[6:71 are implicitly stored in Mp when A[l:5] are loaded. 
Mp when A[6:7] are Zoaded. as index registers. B registers are general arithvetic registers, 
and can be used 1 if interpreting instructions, 
not under program control. Exit mode bits mode's alarm allow conditions to trap Pc at Mp[RA]. Trapping occurs if main core memory of 218 w, (256 kwJ ECS/Extended Core 
Storage Program can only transfer data between reference for relocation) address register to map a logical Mp' field length - the bounds register which limits a program's reference or relocation register for Ms (Extended Core Storage) field length for ECS a bit denoting a state when memory mapping is invalid Mp and Ms. into physical Mp access to a range of Mp' Program cannot he executed in Ms. . //" This process maps or relocates a logical program, at location Mp', and Ms',into physical 
Mp and MS. Mp'[X] := ((X < FL) iMp[X + RAl); logical Mp' (X 5 FL) +(Run +O; AddressYoutdfurange -1)) Ms'[X] := ((X < FLECS) +Ms[Xl+ RAECSI); logical Ms ' (X 2 FLECS) + (Run +O; Address-out-of-range - 1)) Ezchange juq storage allocation map at locotion, n within Wp: The following Mp" array is reserved when Pc state is stored, and switched to another job. a Peripheral and Control Processor 
enacts the operation: The exchange instruction in iMp"+ Mp; Mp t Mp"). Mp"[ n]<53 :0> Mp"[n+1]<53:0> := RAoA[lloB[Il Mp"[ n+2]<53 :0> := FLoA[2]oB[2] Mp"[ n+3]<53 : 0> := EMoA[ 3]oB[3 ] Mp"[n+4] := RAECSoA[4]oB[4] Mp"[n+5] := FLECSoA[5]oB[5] Mp"[ n+6]<35 .0> : = A[ 6]oB[ 61 Mp"[n+71<35 :0> := A[7lOB[ 71 Hp"[n+lO 'n+I781:= X[O:7] := PoA[ 0300000008 8' 
498 Part 5 1 The PMS level Section 4 I Network computers and computer networks 
Instruction Format instructionQ9:0> frnd:O> := instructionQ9:24> frni <8 :O> := fmoi iQ:O> := instructionQ3:21> jQ:O> := instructionQ0:lb kQ : O> := instruction<l7:15> jkd:O> := jok K<17:0> := instruct ion<l7:0> long-instruction := ((fm < log) v (50 I fm < 53) v (60 s fm < 63) v (70 i fm < 73)) shortJnstruction := long instruction although 30 bits, most instructions are 15 bits; see operation code or function extended op code specifies a register or an extension to op code specifies a register specifies a register a shift constant 16 bits) an 18 bit address size constant 30 bit instruction Instruction Interpretation 
Process 15 bit instruction Instruction Interpretation 
Process A 15 bit (short) or 30 bit (long) instruction 
is fetched from Mp'-[P]q x 15 f 15 - 1:p x 19 where p = 3, 2, 1, or 0. bit instruction cannot be stored across word boundaries (or in 2, Mp' locations). A 30 a pointer to 15 bit quarter word which has instruction P<l>4 Run +(instructionQ9:15> +Mp'[P]<(p x 15 + 14):(p x 15)r; next Fetch p tp - I; next (p = 0) A IongJnstruction +Run to; (p # 0) A long-instruction -' ( instruction<l4:0> tMp'[PI<(p X 15 + 14):(p X 15)>: p tp - I); next Instruction,execution; next 
execute (p = 0) -(p -3; P tP + I)) Instruction Set and Instruction Execution Process Fd .tppj[AliR oeeurs. If (i Z 61 2 store is made to Mo'[A[ill. The description does not describe Address-but,of,range ease, il,hz.c% is treated like a null operation. etches or stores betueen Mp' and X[i] occur by loading or storing registers Alii. If 10 < i C 61 a fetch from ~nstruction~execution := ( Set A [i ]/SA   SA^ ~j + K" (fm = 50) - (A[i] cA[j] + K; next Fetchdtore);   SA^ ~j + K" (fm = 51) -(A[i 1 tB[j] + K; next Fetchdtore);  SA^ xj + ~11 (fm = 52) +(A[i I tX[jl<17:0> + K; next Fetchstore);  SA^ xj + Bk" (fm = 53) +(A[i 1 +x[j]d7:n>+ B[k]: next 
Fetch,Store);  SA^ ~j + Bk" (fm = 54) +(A[i] tA[j] + B[k]; next Fetch-Store);  SA^ - Bkl' (fm 
= 55) +(A[i ] tA[j] - B[k]; next Fetchdtore); "SAi Bj + Bk" (frn = 56) +(A[ i] tB[j] + B[k]; next Fetch,Store); "SAi Bj - Bk" (fm = 57) + (A[i] tB[jj - E[k]: next FetchJtore); Fetch-Store := ( (0 < i <6) + (X[il tMp'[A[ill); (i 2 6) + (Mp'[h[i] c X[i])) Operations on B and X Set B [i VSBi "SBi Aj + K" (frn = 60) 3 (E[ i] + A[ jl + K); process to get operand in X or store operand from X uhen A is written 
Chapter 39 I Parallel operation in the Control Data 
6600 499 "SBi Bj + K" (fm = 61) 4 (B[il +B[jl + K); "SBi Xj + K" (fm = 62) --f (B[il tX[j1<17:b + K); "SBi Xj + Bk" (fm = 63) + (B[i] +X[j]<l7:Lb + B[kl); "SBi Aj + Bk" (fm = 64) + (B[il +A[jl + BCkl); "SBi Aj - Ek" (fm = 65) + (BCil -A[jl - BCkl); "SBi Bj + Bk" (fm = 66) + (B[il + B[jl + BLkl); "sBi Bj - Bk" (fm = 67) + (Bcil + B[jl - B[kl); Set X[il/SXi "SXi Aj + K" (fm = 70) 4 (x[ 11 + sign,extend(A[j "SXi Bj + K" (fm = 71) --f (x[ 11 + sign,extend(B[j "SXi Xj + K" (fm = 72) -f (X[ il sign,extend(X[j] "SXi Xj + Bk" (fm = 73) --f (X[ il t sign,extend(X[j "SXi Aj + Bk" (fm = 74) + (X[ il + sign,extend(A[j "SXi Aj - Bk" (fm = 75) + (X[ il + signgxtend(A[j "SXi Bj + Bk" (fm = 76) + (X[ i] c sign&xtend(B[j "SXi Bj - Bk" (fm = 77) + (X[ i] csign&xtend(B[j Miscellaneous program controZ "PSI (:= fm = 0) + (Run t 0); "NO" (:= fm = 46) + ; d~unp unconditiowl program stop no operation; pass "JP Bi + K" (:= frn = 02) + (P + Sy i] + K; p + 3): jump Jwnp on X[j] conditions "ZR Xj K" (:= fmi = 030) + ((X[j] = 0) + (P tK; p ~3)); "NZ Xj K" (:= fmi = 031) + ((X[jl # 0) + (P cK; p ~3)); "PL Xj K" (:= fmi = 032) --f ((Xcj] z 0) --f (P t K; p t3)); "PIG Xj K" (:= frni = 033) + ((X[j] < 0) + (P +K; p t3)); "IR Xj K" (:= fmi = 034) + ( zero non zero PIUS 011 position negUtiUe out of range constant tests ((Y[j-M5:4e+ 3777)~ (X[jl%9:48>. 40nO)) +P +K; P -3); "OR Xj K" (:= fmi = 035) + ( (X[jl69:48%3777) V (XCj169:48>=4000)+ (P +K; p +3)I; indefinite form constant tests "DF Xj K" (:= fmi = 036) + ( (XLj169:48hl777) V (XCjI89:48>-6000) +(P +K; p +3)); "ID Xj K" (:= fmi = 037) + ( (XCjlB9:48hl777) V (X[j189:48X6000) - (P +K; p t3)); Jwnp on B [i 1. B lj ] comparison "EQ Bi Bj K" (:- frn = 04) +((B[i] = B[j]) + (P tK; p c3)); equal "NE Bi Bj K" (:= fm = 05) +((BE:] # B[j1) +rP -K; P -3)); not equaZ "GE Bi Bj K" (:= frn = 06) +((B[il 2 B[jl) + (P + K; p + 3)); greater than or 
@qua2 "LT Bi Bj K" (:= fm = 07) +((BCil <BCj1) +(P +K; P ~3)); less than "RJ K" (:= frni = 010) +( return jump Subroutine call M[KIB~:~P +04~rno~o(~ + ~)mooooo~; next (P tK + 1; p ~3)); "REC Bj + K" (:= fmi = 011) +( Peading (RECl and writing (WECi Mp with Extended Core Storage, subjec;ed to bounds checks, and Ma', Mp' mapping read extended core 

500 Part 5 I The PMS level Section 4 I Network computers and computer networks 
Mp'[A[nl:A[o] + B[jl + K-i] tMs'[X[O]:X[OI + B[j] + K-11); "WC Bj + K" (:= fmi = 012) + ( write extended core 
Ms'[X[OI:X[O] + B[jl + K-I1 -Mp'CA[Ol:A[Ol + B[jl + K-11); Fixed Point Arithmetic and Logical operations using 
X "IXi Xj + Xk" I:= fm = 35) +(X[il tX[jl + X[kl); "IXi Xj - Xk" (:= fm = 37) -(X[il tXCj1 - V[kl); "rxi Xk" (:= fm = 47)i (X[il csurn~nodulo~2(X[kl); "RXi Xj" (:= fm = 10 ) -(X[il +x[jl); "BXi Xj 2: Xk" (:= fm = 11 ) i(X[il tX[il +X[jl hX[kI); 8 "BXi Xj + Xk" (:= fm = 12) -(XCil tX[J] V X[kl); "RXi Xj - Xk" (:= fm = 13) i(X[il tX[JI @X[kl); "BXi - Xk" f:= fm = 14) i (XCI] -7 X[kl); 8 "RXi - Xk i. Xj" (:= fm = l5)i (X[i I tX[jl A 7 X[kl); "BXi - Yk + Xj" (:= fm = 16)i (X[i ] tX[j] v I X[k]); "BXi = Xk - Xj" (:= fm = 17)- (X[i] tXcj] @-X[k]); "LXi jk" (:= fm = 20) i(X[il tXCil 
x ZJk {rotate)); "AXi jk" (:= fm = 21) + (X[i] cX[i] / 2jk); "I Xi Bj Xk" (:= fm = 22) i ( ,R[j]<17> iX[i] tX[kl x EBCj1<55:0> (rotate); R[j]<17> +X[il cXCk1 / Z7 B[jl<lo:"> ); "AXi Bj Xk" (:= fm = 23) + ( +[j]<17> iX[i]cX[k] / 2B[j1<lo:o>: B[j]<17> iX[i] tX[k] x Z1 B[j1<5:"> {rotate]); "MXi jk" (:= fm = 43) i ( X[i]<59:59-jk+l> +2jk - 1; (jk = n) iX[il '-0); integer sum integer difference count the number 
of bits in X[k] transmit logical product 
logical sm logical difference 
transmit complement 
logical product and 
complement logical sun and complement logical difference and 
complement arithmetic right shift 
left shift nominally 
arithmetic right shift nominally 
form mask Floating Point Arithmetic using 
X Onlu the least significant (7.0) part of arithmetic is stored in Floating DP operations. "FXi Xj + Xk" (:= fm = 30) + (X[il tX[jl + X[kl {sf)); "FXi Xj - Xk" (:= fm = 31) + (X[il tX[jl - X[kl {sf)); "nxi Xj + Xk" i:= fm = 32) + (Xiil tX[jl + XCkl (ls.df1); 
floating dP Sum "nxi Xj - Yk" (:= fm = 33) i (X[il tX[jl - X[kl {ls.df)); floating dP difference "RXi Xj + Xk" i:= fm = 34) i ( floating sum floating difference 
XCil c round(XCj1) + round(X[kl) (sf)); round floating difference 
"RXi Xj - Xk" (:= fm = 35) i ( X[i] c round(X[j]) - round(X[kl) {sf)); "FXi Xj :': Xk" (:= fm = 40) i (X[ i] t X[j] x X[k] [sf)); 
"RXi Xj :': Xk" (:= fm = 41) + ( round floating product 
floating product 
X[i] +X[j] x X[k] [sf); next X[il tround(XCi1) {sf)); "DXi Xj * Xk" (:= fm = 42) i (X[i] tX[jl x X[k] [ls.dfl); floating dp product "FXi Xj / Xk" (:= fm = 44) i (X[i] +X[j] / X[kl {sf)); "RX~ xj / Xk" f:= fm = 45) + (X[i] t round(X[j] / X[k]) [sf)); round floating divide 
f bating divide normalize "NXi Rj Xk" (:= fm = 24) + ( X[i] tnormalize(X[kl) (sf); R[j] c normalize,exponent(X[kl) (sf]); 
Chapter 39 1 Parallel operation in the Control Data 
6600 501 "ZXi Bj Xk" (:= fm = 25) i L round and normaZize x[i] c round(X[k]) [sf]; next X[ i 1 c normal ize(x[ il) (sf) ; B[ j] t normal ize,exponent (X[ i 1) (sf)) ; "UXi BJ Xk' (:= fm = 26) 3 (B[j] cX[k]<58:48> (si]; unpack X[i] tX[k]<59,47:0> {si)); "PXi Bj Xk" (:= fm = 27) + (X[k1<58:4b t BCjl (si); pack X[k]<59,47:rD cX[il {si)) end Instmctionusxecution ) 
502 Part 5 I The PMS level Section 4 I Network computers and computer networks 
APPENDIX 2 PERIPHERAL AND CONTROL PROCESSORS, PCP, ISP 
DESCRIPTION CDC 6400, 6500, 6600, AND 6416 Appendix 2 CDC 6400, 6500. 6600, and 6416 Peripheral and Control Processors/PCP, 
ISP Description Pc State A<17: O> accmlator P<l I : o> Progrm Address Counter 
E.5, State M[0:40951<11:0> 4 M index[0:631<11:C™:= M[O:63]<11:D soecial arrau in PE reserved for index register C(™Centra1) State CPuP<17: E. CPM[O:77777781<59:O> the main Pc instruction address counter 
the Mp o.f main C IO Registers for Ci ‚PCPI C,OATA[O:63]<Il:O> C,ACTC 0 : 633 LFLG[O:631 denotes a ,full (or emptgl buffer at the K C,FCN[0:631 <I I :o> data buffers at peripheral K‚s a bit to denote ip 1 of the 64 K™s is active function or instruction register 
at a specific K Instruction Format Ins[ 0: 1 ]<I 1 : Cb instruction 2 w instruction: defined in terms of op codes, see Table, 
page 50; long-i nstruct i on short,instruction := 7 long,instruction 1 ZL instruction K5:D := lns[0]<11:6> function or op code dc5:D := Ins[0]<5:O> m~11:0> := Ins[l] address Dart drKl7:0> := dm i<ll LO> := Inall ]<I 1 :0> indirect bit d,sign<l I :O> := ( -7d<5> - OOd : d<5> +l d) md<lI:O> := ( (d = 0) -tn: (d # 0) + m + M[dl) Effective Address Calculation Process z := ((F<5:9 = 3) -id. (F<5:9 = 4) + i; (F<5:p = 5) --f rnd) Instruction Inteqjretation Process Run +(lns[Ol cMLP1; P tP + 1: next fetch Iongoinstruction + (InsCIl +MEPI; P tP + I): next lnst ruct ion,execution) execute 
Chapter 39 I Parallel operation in the Control Data 
6600 503 SCN i( AtAM); Implementation The IO x 52 bits in the barrel 
for the IO PC? IS? include: LDN +( Atd); A[0:9]<17:0, P[O:9]<Il :o, Temporary ffardware registers (not in the ISPl Q[O:3l<ll :0> K[O : 91<5 : O> T [O : 9 1<2 : O> LMC 4 ( A+A@dm) ; accumulators instruction address counters 
PSN +; null low order 6 bits of an instructim or address data 
six bits hold the operation code. The 3 bits specif? the trip count or state of an instruction's interpretation. Instruction execution := ( F = XsY8 8 00 PSN --1; nu 7 7 SHN i A+Ax2L igr X' 8 00 IO 20 30 40 50 60 70 06 PJN +( 7 A47> + ( 07 MJN -( A<17> +( LJM i ( Pi- md); RJM +( MCmd] tP; Pc md+l); LMN +( A(-A@d) ; LPN -( AcAAd) ; ~ SEN -,( AcA-d) ; LCN +( A +-d) ; ADNi ( A<- A+d) ; LPC + ( AtAAdm) : PSN +; EXN +( CPYPA) ; ADC i ( A<-A+dm) ; RPN + ( A-CP-P); SED -> ( 1 A00 +( -7 I SBI +( 7 RAI +( 401 +( SBM i( bcA-M[z]) d CWD -> ( CPM [A 1.- M[d: d+5 I) ; ADM i ( CRM + ( M[m:m+ SxM[dl-l It CPM[A:A+ Mldl-11); AcM[z ]-I ; next M [z ]<-A ) ; EJM +( 7 C,FLG [d ]+ c IJM i( - Pcm) ) ; 7 C,ACT [d ]+ ( FJM +( CvFLG [d ]+ ( CWM i( CPM [A :A+ M [d 3- I I+ M[m:m+ 5xM[d]-Il); OAM 4 ( (7 CJLG [d 1- CYDATA [d 1 +M[m:rn+Al)); AJM i( CdXTCdl-, ( t IAN -> ( A+ C,DATA[dl) : IAM + ( C,FLG[d li ( M [m : m+A I<- C,DATA[d 1) ) DAN --t ( CuDATA[dl c A) ; FAN +( <.A) ; C,FCN[d] ACN + f DCN i ( C,ACT[d] to) : I 1 end Instructionqxecution * 1 uord or short,instruction 
Chapter 40 Computer-network examples We are just entering the 
era in which general-purpose networks of computers make technical and economic sense. The requisite hardware and 
software development of operating systems and multiprogramming capability 
is still maturing. Thus, unlike 
the other PMS structures discussed in this book, there is no supply 
of operational systems with published descriptions upon which 
we can draw. Consequently, 
we have assembled several 
brief examples of networks to provide at least some illustrations 
of what is sure to be 
an important aspect of computer systems in the near future. 
The more interesting of these examples are still in the planning stages; those 
that exist currently are still highly 
specialized. Spatially distributed intercommunicating 
networks of digital devices have existed for a long 
time. But many of the ones that come most easily 
to mind are not computer networks. For example, the various airline reservation systems like 
American Airline™s SABRE [Plugge and Perry, 19611 have spatially distributed termi- 
nals (T™s) with a single 
Pc, possibly mediated by Pio™s or Cio™s. When there are 
several Pc™s, they are functionally integrated so as to provide the total capacity 
and reliability needed. 
Some military networks, such as 
the SAGE Air 
Defense System [Everett et al., 19571 have multiple computers 
(SAGE actually has a very 
large number). 
But they transmit 
to each other highly specialized data streams (for example, 
aircraft positional information 
for con- trol). The National Physics Laboratory of England has made a very 
comprehensive proposal for a general-purpose network 
[Davies et al., 19671, although we do not include 
it as a 
chapter. Again, it is just in the proposal stage. The Lawrence Radiation Laboratory (at Livermore) is no doubt the 
earliest and most impressive net- work. In terms of our PMS descriptions, a 
computer network (N) requires at least two C™s not connected through primary 
memory. Thus each 
C has a 
Pc and an 
Mp of its own and has to communi- cate with other C™s through messages. Duplex computers 
are thus defined as networks, 
provided they do not share Mp. For networks, 
links (L™s) are usually shown explicitly. 
In spatially distributed 
systems, both the time delays and the flow rates of the links are significant. The latter is so partly because 
the networks must make 
use of the telephone communication 
system, which 
exists inde- pendently of the networks, thus having parameters 
that do 
not correspond with any 
of the internal 
parameters of the individual computers. There may also be limitations of reliability, cost, accessing characteristics, and the size of the information unit that derive wholly from the links. For instance, many 
computer net- works would like to buy their 
transmissions from the telephone system for very 
short intervals (milliseconds), 
at very high data rates, and with short switching time 
(milliseconds), Le., bursts. Switching time and pricing policies within the telephone system conspire to make this a difficult thing to do. Thus, 
with networks, links become important independent 
components. One classification of networks (N™s) is by fixed or variable 
interconnection structure. Fixed structure may mean that the 
links are fixed permanently over the life of the network. However, 
fixed structure may mean only that connections once made 
must be held for long periods 
of time relative 
to the message flows. An example is the telephone switching 
system mentioned above, which looks like a 
variable switching 
structure at the 
level of human conversations, but like a fixed switching structure at the level of computer conversations. Figures 
la and IC show variable- structure systems; Fig. lb shows a fixed-structure system. 
In the former, any 
C can talk directly to any other C. 
In the 
latter, each C talks directly to only a 
few C™s; thus, to communicate with 
the other C™s, it must transmit through 
them as links; 
that is, it must use another C as an L. A second classification of N™s is by the nature of the delays suffered by the messages as 
they travel 
from an initiating C to a target C. Communication can 
be direct, in which case 
the only delays are those through the switches (S) and links (L) between the two C™s (Figs. la and lb). Alternatively, communication can 
involve storing messages at intermediate nodes (called store-and- forward communication), 
thus introducing additional 
memory delays into the 
communication but decreasing the demands 
for coordination between the two 
C™s. Although store-and-forward 
systems can be built with 
the intermediate nodes being Ks with buffer memories, 
in the present context 
the natural 
form for 
such a system 
uses the other C™s in the system as 
the intermediate 
nodes, as in Fig. IC. Several kinds 
of reasons can justify the existence of a particular network. The following list 
is adapted from Roberts 
[1967]: Load sharing. 
A problem (program 
and data) 
initiated at one C that is temporarily overloaded is sent to another for processing. 
The cost of transshipment must clearly be less than the 
costs of 504 
Chapter 40 I Computer-network examples 
505 ;/ / C Fig. la. Variable-structure direct 
switching network 
PMS diagram. delay in getting the problem processed. Load sharing implies 
highly similar 
facilities at the nodes of the network. Data sharing. 
A program is run at a node that has access to a large, specialized data base, such as a 
specialized automated library. It is less costly to bring the program to the data than 
to bring the data to the program. Program sharing. Data are sent to a C that has a specialized program. This might 
happen because of the size of the program (hence, fundamentally the same reason as 
data sharing), but it might also happen because the knowledge (i.e., initialization and error rituals) to run the program is available at one C but not at another. Specialized facilities. Within the network there need 
exist only one of various rarely used 
facilities, such as large random-access memories, or special display 
devices, or special-purpose array 
processors. c--L/[ I kL I / \ Fig. lb. Fixed-network PMS diagram. I Fig. IC. Store-and-forward network 
PMS diagram (using C switching) Message switching. There may be a communication task of such magnitude that sophisticated switching 
and control are worth- while. Reliability. If some components fail, others can 
be used in their 
place, thus 
permitting the total system to degrade 
gracefully. (At 
the present state of the art, 
peripheral computers are needed 
to isolate the periphery from the unreliability of the network, and vice versa.) Peak computing power. Large parts 
of the total system can be devoted for short periods 
to a single task, if there are important real-time constraints 
to be met. This depends on being able 
to fractionate the task into independent subtasks. Communication multiplexing. 
Efficient use of communication fa- cilities is obtained by multiplexing a number of low data-rate users, for example, 
T(typewriter; 150 b/s)™s. This may 
not be a reason for a 
network per se but may justify a 
larger network, 
provided that there is some reason for 
having one in 
the first place. Better communication. A community of users (e.g., a scientific 
or engineering community) 
that could mutually use the same pro- grams and data 
bases and converse about these directly (i.e., not by writing about 
them but in 
the context of mutual use) might become a much more productive community, with less duplication of work, faster communication 
of results, etc. Better load distribution through preprocessing. Some tasks require very high-data-rate communication 
with a computer. By doing preprocessing in a 
smaller computer, a reduced information rate can be sent to the more general system. 
With this general view 
of networks, let us consider several examples. 
506 Part 5 1 The PMS level ? Ms(disk) ... ls(magnetic tape) ... I rﬁ‚ Mp((.l - 5)megabyte) Pie. .. Pc(™IBM System/360 Model 
40. 50) T(card) . . . 
T(l i ne; printer). . . 
T (typewri ter) - - Section 4 I Network computers and computer networks 
ZBM ASP (Attached Support Processor) This first example (Fig. 2) is the simplest of all computer networks, consisting of two computers 
tied together, with 
each functionally 
specialized (and in addition required to be physically close). 
The function of Csupport is job setup and 
breakdown, that is, pre- processing and postprocessing. All T™s 
for the network are handled 
by it (except for Txonsole on C.main). The function of C.main is to process data. Thus this 
is an escalated version of the Pc-n Pi0 organization, where the Pio™s have been 
made into a Csupport and thus can take on additional functions. 
It should be compared with the CDC 6600 organization, which 
is C.main-10 Cio, 
but where the Cio™s are rather 
small Cio(4096 w; 
12 b/w) compared with the C.support. The ASP organization is the 360 analog of a system consisting 
of an IBM 7090-IBM 7040 
which emerged spontaneously in the early sixties at several IBM installations in order to deal with 7090 1/0 bottlenecks. Thus this kind 
of simple computer network has been with 
us for some time. In more detail, 
the advantages that are claimed for ASP are in reducing resource 
interference:l ‚Adapted from IBM System/360 Attached Support 
Processor (ASP) System 
Description, H20-0223-0. C ( ‚Ma i n) := Fig. 2. IBM System/360 Attached Support Processor system/ASP PMS diagram. The addition of smaller modules of Mp in the form of a second processor. The processing of the application is di- vided between the main processor and the support proces- sor, with each performing those functions for which it is best suited. 
The core requirements for the support processor are small in 
comparison with those for the main processor. With this division of responsibilities, the system can expand its capabilities 
with a minimum addition 
of storage. The elimination of concurrent use of Pc time 
on the main processor for processing 
support functions (such 
as printing). Because the clerical functions 
are assigned to the 
support processor, the main processor 
no longer shares 
Pc time 
between the support functions and the application pro- grams. Therefore, the application has the opportunity to use all the resources of the main processor 
to fiill capacity. The addition of selector channels. 
The channel capacity 
of the system has 
been increased by one or more additional 
selector channels 
attached to the support processor. An algorithm for efficient management of the direct-access storage devices for system 
input/output data 
sets. The algorithm was designed specifically to accommodate the data demands, the data 
set characteristics, 
and the 
available private devices. The input/output routines always know 
the position of the access mechanism, 
thereby ensuring mini- mum seek time when data are 
transferred to the devices. IBM cites the above reasons for using the ASP system. These views differ 
from ours on 
its usefulness. Ideally, a multipro- grammed single-processor or multiprocessor structure would easily provide all 
the above advantages without 
the overhead of having large Mp™s on two computers (both 
of which hold nearly 
the same operating system). Also, as we note in the introduction to 
the System/S60 (page 584), the support-computer functions can be handled in 
the main computer with very little loss of large Pc power (3 to 10 percent). A multiprocessor structure should also cause less overhead, by not 
passing data sets between two 
C™s. (Alternatively, in ASP this could be done by an S to common Ms from both C™s.) University of Texas network The structure shown in Fig. 3 is similar to ASP in that a C.main is used, with some job setup and breakdown being done in several other C™s. However, there are several of these C™s, and they provide independent power for small 
tasks where the setup 
time for the large system is greater than the 
computation time. They are also physically remote from C.main and thus serve to make the power of the central facility available 
at local sites. The Teletypes are 
Chapter 40 I Computer-network examples 
507 e I etype) Telephone Exchange) CDC 6600; Computation Center) ielephone Exchange ) L -C('CDC 1700; Linquistic Research Laboratory)- L - C('CDC 3100: College of Business Administration)- L - C('8231 Computer Terminal)+ I +(card)- T(1ine: printer)+ E. L(to: other C's off campus)- Fig. 3. The Computation Center, 
University of Texas, (Austin) Network PMS diagram. used to enter 
jobs directly to the 
C.main, where they 
are run in 
a batch mode. The network of Fig. 3 is that at 
the University of Texas, as derived from its internal planning memoranda. 
Similar systems 
are in existence or 
under construction 
at other universities. M.Z. ?: proposed network Figure 4 shows a network that is proposed for the M.I.T. campus [Bhushan, Stotz, 
and Ward, 19671. It moves to a more complex 
switching system, partly because 
there are two 
C.main's. Here an S(direct) is used in a non-store-and-forward 
mode as each C communicates directly with another. 
The communication rate between C's is 40 - 230 kb/s. 
(Note that at higher data rates a fairly large 
computer is necessary just to handle the 
store-and- forward message switching information rates.) 
The purpose of the network is to allow users of the small or terminal C's to get access to C('1BM 360/67) and C('GE-645). These two C's can, of course, communicate with 
one another. A large number of users are connected to 
T(typewriters) via the S('Te1ephone Ex- change). The Lawrence Radiation Laboratory 
(at Livermore) network The LRL network, 
started in 1964, appears to be the earliest general-purpose-computer network. 
It serves a user 
population of approximately 1,000, with several hundred simultaneous 
on line users. The network consists of five large computers 
(three CDC 6600s and two 
CDC 7600s), a 
switching computer (a 
DEC PDP-6 with two Pc's ant1 a 262 kword Mp 
and a 10"bit fixed- head disk for 
fast-access files), three terminal control 
com- puters (DEC PDP-8's), and a large 
central file (a 1012-bit IBM Photostore controlled 
by an IBM 1800 computer). Hardwired 4 megabit per second links 
connect the large computers 
to the switching computer. 
The terminal computers 
and the 
large file are also connected to 
the switching computer. 
The main purpose of the network is to gain access 
to the central filing, printing, and terminal facilities. Load sharing is not an important consideration because each 
of the large computers operates nearly autonomously. 
Thus little change was required in each system to be 
integrated to the network. Jobs enter the 
net- work in any 
of three ways-by the batch input 
terminals of a large computer; by the typewriter 
inputs of a large 
computer; or by the typewriter 
inputs of the terminal control 
computer which in turn connects to the central 
switch. Unlike most 
uni- versity computation centers, which 
provide service 
for many users with small jobs, the LRL network 
is oriented to users with (multiple) large jobs. T storage CRT: display: ... I [keyboard T 'Dataphone; (1.2 - ... iL.8) kb/s 3 3 .., - I5 char/s ('Dataphone). . . 1 ('Dataphone). , . T('Dataphone) . . . 
T ( 'Dataphone). . . 
C ( 'Satel 1 i te) . . . :(CRT; display). . . 3 'S('Te1ephone exchange: 
'S('Wideband Communications 
Center; (40.8 - 230.4) kb/s) (IO- 15) char/s,(l.2- 4.8) kb/s) Fig. 4. M.1.T.-network PMS diagram (proposed). 
508 Part 5 I The PMS level Section 4 I Network computers and computer networks 
I duDlexed file C's sharing a common secondary memory for long term filing I I .___._._._ ._._._. - .C(sf; M5) - - I -C (sf: Ms)Tl r-3- - - - - - - 
- - - 
I 1 l;Z;(cRT; console)- 1 LN, : .C(sf: Ms)- - I .\ u concentrators, I I I I I high speed message special systems, store and forward/sf LS" I -I F-s-x -I-- - I main processors with secondary memory (Ms) 'S(50 - 180 b/~ec)~ 'S(600 - 4800 b/~ec)~ 3S(40 - 50 kb/~ec)~ 'S(200- 2000 kb/sec; fixed) ixed, ('Telephone Exchange; direct), (C(sw: 3 L (200 - 2000 kb/s) -.- .- L(40 - 50 kb/s) ~(600 - 4800 b/5) -__- - - - - - - - L(50 - 180 b/5) I-C(sf: MsF----l ' console)- ~~ - ,I ,-;tT(card, lines, analoq, plot)) message concen- I I trators, speciall systems, store I and forward/sf L;(card, line, plot)- I ; I I I Teletype, - 3 u network periphery Fig. 5. Typical computer network PMS diagram. Typical local network We summarize 
in Fig. 5 the direction in which the last three networks are moving by presenting a hypothetical, local network, 
as it may mature on 
many large university campuses 
(and large industrial establishments). 
The network is conceived as a single 
computing facility, to serve a clientele with many heterogeneous 
but partially overlapping 
computing needs. An essential feature of the environment of the network is that the collection of com- puting resources it connects are not planned 
all at once but keep growing and changing in imperfectly controlled 
ways. This arises from the quasi-independent nature of the subparts of large uni- 
versities and engineering establishments. In 
any event, the network is a mixture of functionally independent and functionally special- 
ized C's. One probable feature is the duplexed C.files which handle all the Ms functions for all C's, except the C(1ibrary). A library's computer, though strongly coupled to 
the network, would have 
its own 
files and specialized terminals, including 
hard copy devices 
oriented to library needs. 
The C.file increases the requirements for the S.centra1 but provides much more economic Ms, as well as easing the ability to connect 
new C's into the system, since they 
immediately have 
access to an organized Ms. The reader should note that the 
four switches 
(S's) can be either fixed links, variable switches (e.g., Telephone Exchange), 
or a computer used as a 
direct switch or 
as a store-and-forward switch. The most interesting aspect of this network 
is that it has a general hierarchical structure 
and is like other hierarchical organi- 
zations. Here, the levels of the organization are based on data rates. For example, there is a very low-level computer which deals 
with the basic communication 
to typewriters at -150 b/s. This 
Chapter 40 I Computer-network examples 509 C switch 
concentrates several typewriters into a time-multiplexed 
2,400-b/s link. Several 
of the 2,400-b/s links 
can in turn be 
con- centrated prior to transmitting via a 50-kb/s link. 
Thus the general organizing principle, like that of most large organizations, 
is to handle problems at the lowest (cheapest) possible level. 
Another organization principle of the hierarchy is that only relevant infor- mation be passed between the 
levels. For example, 
encoding would be used so that only some 
fraction of the bits flowing 
at the 
periphery would enter the highest-level computers. At each of the levels we assume that specialized, time-shared computers are employed to handle the 
very simpler tasks 
of editing, simple calculations, etc. At the network periphery 
there are a 
number of terminal computers, Le., C(termina1; CRT, card, lines, analog, 
plot, key- board). Although they are computers, they behave 
as terminals. The DEC 338 (Chap. 25) is typical of this terminal class. Part of the periphery connects 
to other networks and part connects to specialized processes, e.g., 
a process control, or experimental apparatus on a dedicated basis. The peripheral computers 
are able to do local tasks 
independently of the larger, more unreliable computers. Combat Logistics Network/ComLogNet ComLogNet was developed for the U.S. Air Force in the early 1960s for the purpose of sending messages (or information) among T's [Segal and Guerber, 19611. It is built to 
transmit both at low N('ComLogNet) := lT('5ubscriber Station/SS) := (T(Te1etype I 'Compoun81 'Magnetic Tape Terminal4)) "See Fiqure 6~. 3T(1Compound) := rLr5,I 50, 3- r-S F'(card; reader)j 300,600 b/s M.buffer T(card; 
punch)+ T ( 'Tel etype)- 
L 1200,2400, -K-Ms(magnetic tape) - 4T Maqnetic Tape] := [[ 1 ] , Termi na 1 4800 b/s Ms, buffer c N('ComLogNet) 1 Distribution/ ADU; #I :II Fig. 6b. Combat Logistics Network/ComLogNet component relationships. N ( ComLogNet) := i('SC I / I I I I I I I 1 \ \ \ \ ', ('SC)' (SS)2 . . S(' ComLogNet) IN('Switchin9 Center/SC) See Figure 6d zTT('Subscriber Station/SS) See Figure 6a Fig. 6a. Combat 
Logistics NetworkKomLogNet PMS diagram. Fig. 6c. 
S('ComLogNet) PMS diagram. 
510 Part 5 I The PMS level Section 4 I Network computers and computer networks 
(10 char/s) and medium (1,200 - 4,800 b/s) speed, 
as shown 
in Fig. 6a. In this regard 
the network is simply a 
message switch for the three terminal types. It employs C's for the switching elements and is fundamentally a store-and-forward system. 
Had it not been 
for security, reliability, 
response time, and other considerations, it would have 
been possible to construct an 
equivalent system using standard lease wire switches 
(or telephone exchanges). In Fig. C (Communications. Data Processor/CDP) 
:= ~~~~~~~~~~p~; reader) 3 7T.console - C(CDP) := T(paper tape; 
reader)+ Ms(#l :3; drum) Ms(#1:48; magnetic 
tape)- L(to:C( 'External))- T(line; printer)+ T ( ' sys tem consol e) 
3 C('Tape Search Unit)' 'Mp(core; 1.5 p/w; 8192 w: 56 b/w) "C('Tape Search Unit/TSU := K-S-T(printer)+ Mp 'Data store; Pc I:.;;; 1 Mp 'Procedure; /5;:'96 b/w function: code trans Za- 3C ( 'Accumulation and D i s t r i but ion/ADU) := ;-K #i:Z5; ('low -K [BOO b/s) ] speed; 0 -601 -L4- b/s) I ('high speed); 601 - Pon J 4Link; communications lines Fig. 6d. ComLogNet N("3witching Center/SC) PMS diagram. 6b a tree is used to present the relationship of constituent members of ComLogNet. From 
it we see that at the first level ComLogNet 
has just a 
switch, links, and terminals (as 
shown in Fig. 64. The networks switch employs five specialized N('Automatic Electronic 
Switching Centers/SC)'s which 
communicate among each other 
(Fig. 6c). Terminals connect to the 
individual N('SC)'s and mes- sages are routed between two 
T's, either by a store-and-forward 
process within N('SC) or among two N('SC)'s. The individual N('SC)'s are located at five specific locations and consist of fixed computer configurations of five to seven C's. The structure of N('SC) (Fig. 64 is formed basically by a duplex C structure which handles 
most processing. 
Attached to the 
two C('Communications Data Processor/CDP) are two to four C('Ac- cumulation and Distribution Unit/ADU) which handle communi- cation-link processing. A C('Tape Search Unit) is used off line to process data from Ms(magnetic tape). The structures of C('CDP), C('Tape Search Unit), 
arid C('ADU) are defined within Fig. 
6d. ARPA network1 An experimental computer network (Fig. 
7a) is operational and connects 19 computer facilities associated 
with the contractors of the Information Technology Branch of the Advanced Research 
Projects Agency (ARPA). These contractors, all 
of whom are engaged in advanced research 
in computer science and technology, form a community in which 
to attempt a general-purpose network. 
Since several 
of the nodes in this network (e.g., M.I.T.; see Fig. 
4) will themselves 
be constructing networks at their own sites, the system has faced a good 
many of the design problems associated 
with such a network. Unlike many of the other networks discussed in this chapter, the ARPA network consists of sites that are 
physi- cally remote, 
that are 
each developing as total systems under independent management, 
and that have no 
agreed-upon func- 
tional specialization 
vis-i-vis each other. Furthermore, 
the uses that each node 
will make of other nodes will 
be the fairly general ones cited at the beginning of this chapter, 
as generated by a general scientific community. Since many of the institutions that will be tied in 
are major academic institutions, diversity will 
be guaranteed. The motivation behind the experiment is to reveal and begin to solve the technical problems of such general 
net- works, while also discovering which of the several advantages 
of using networks listed 
earlier (or others 
unmentioned) emerge as important. 'The Specific links, sites, 
etc., change with 
time; thus 
the actual structures we present are, by the nature 
of the experiment, almost guaranteed to be in error. 
Chapter 40 I Computer-network examples 
511 C('Dartmouth Colleqe) 
N('U Illinois) Santa Barbara 
I Fig. 7a. Advanced Research Projects 
Agency (ARPA) 
network PMS diagram (tentative). c ('Local) := C ('Host)- C (' Interface Messaqe Processor/lMP) 1 L(40.8 kb/s; to:N('ARPA)) I... [I ... Fig. 76. Advanced Research Projects Agency 
(ARPA) local-computer 
PMS diagram. Technically, the goals of the network are (1) to make a user (T) at any site behave 
as though it were 
a T at another site 
and (2) to let a 
C at any site 
use a C at another site for load, program, 
and data sharing. To each site 
has been added 
a special C('1nterface 
Message Processor/IMP). The C('IMP) has been designed by the creators of the network, and it provides the communality that will permit the network to function. One constraint in 
the network design is to make 
only small 
perturbations to the 
larger host computers. The C('1MP) is responsible for network messages among other nodes (i.e., 
to their 
C('1MP)'s) and for the interface between the network and the C (or N) at the local site. 
The local computer C('Host)-C('1MP) interface is shown in Figs. 
7h and 7c I N('Loca1) := I"" Fig. 7c. Advanced 
Research Projects Agency 
(ARPA) local-computer- 
network PMS diagram (tentative). ('Lodi , Cal i forn ia)' (I Li tt leton, Massachusetts)' S('Mojave, California)' X(#l !312 I S(manual;50 kb/s; 'Telephone Switching Centers) 2X(C('local)lN('locaI)) These N or C may communicate directly with one another or by using more L's can 
communicate via the S's. Fig. 7d. Advanced Research Projects Agency (ARPA) 
fixed switching centers 
PMS diagrams (tentative). 
512 Part 5 I The PMS level for a local computer and 
local network cases, respectively. The C('1MP) is a C('Honeywell516; 16 b/w; 12 - 16 kw; 1 p/w) with capability to 
connect to four to six links at a 5O-kb/s data rate. The ARPA network leases a set of fixed links, L(50 kb/s). These emanate from four Sfixed, 
as shown in Fig. 7d. Thus the fixed links between the various sites, 
as shown in Fig. 7a, are composed of the links in Fig. 7d. For example, the L(Carnegie- Mellon University; 
Bolt Beranak and Newman) goes from Carnegie- Mellon University 
in Pittsburgh, 
Pa., to Williamstown, Ky., to Littleton, Mass. (on one of the two 
links) to Bolt Beranak and Newman in 
Boston, Mass. The other L(Litt1eton; Williamstown) is part of L(University of Michigan; Lincoln 
Laboratory). With such a fixed-link system the network must operate in a store-and-forward fashion, with C('1MP)'s at each site carrying out this function. Thus the C('1MP) is required at each site, since 
there is no uniformity Section 4 I Network computers and computer networks 
in the other C's that are at a site and no control over their operation. Conclusions We feel the network is the most important computer structure 
in the book. Through understanding it, we 
will be able to organize more computing power than with any other structure 
and to achieve more reliability. The issues of switches and 
links are so vital that through understanding 
of them all computer structures 
will improve. 
References BhusA67; DaviD67; EverR57; PlugW61; RobeL67; SegaR61; 
IBM Systern/360 Attached Support Processor (ASP) System Description, H20-0223-0 
